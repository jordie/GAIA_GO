#!/bin/bash
# Compare speed of different local LLM models

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘        Local AI Models Speed Comparison                              â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Models to test
MODELS=(
    "llama3.2-vision:11B vision model"
    "moondream:1.6B tiny vision"
    "llama3.2:Text-only (fastest)"
)

# Test prompt
TEST_PROMPT="What's on this page? Answer in 5 words."

echo "ğŸ§ª Testing model response times..."
echo "Test prompt: \"$TEST_PROMPT\""
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

RESULTS_FILE="/tmp/model_comparison_$$.txt"
> "$RESULTS_FILE"

for model_info in "${MODELS[@]}"; do
    MODEL=$(echo "$model_info" | cut -d':' -f1)
    DESC=$(echo "$model_info" | cut -d':' -f2)

    # Check if model is installed
    if ! ollama list | grep -q "^$MODEL"; then
        echo "â­ï¸  $MODEL ($DESC) - Not installed"
        echo "$MODEL|N/A|Not installed" >> "$RESULTS_FILE"
        continue
    fi

    echo "Testing $MODEL ($DESC)..."

    # Warm up (first run is always slower)
    ollama run "$MODEL" "test" --verbose false > /dev/null 2>&1

    # Actual test
    START=$(date +%s%3N)  # milliseconds
    ollama run "$MODEL" "$TEST_PROMPT" --verbose false > /dev/null 2>&1
    END=$(date +%s%3N)

    DURATION=$((END - START))
    SECONDS=$(echo "scale=2; $DURATION/1000" | bc)

    echo "  â±ï¸  ${SECONDS}s"
    echo ""

    echo "$MODEL|$SECONDS|$DESC" >> "$RESULTS_FILE"
done

# Show results table
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ“Š Results Summary"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
printf "%-20s | %-10s | %s\n" "Model" "Time" "Description"
echo "---------------------+------------+--------------------------------"

# Sort results by time
sort -t'|' -k2 -n "$RESULTS_FILE" | while IFS='|' read -r model time desc; do
    if [ "$time" = "N/A" ]; then
        printf "%-20s | %-10s | %s\n" "$model" "$time" "$desc"
    else
        printf "%-20s | %8.2fs | %s\n" "$model" "$time" "$desc"
    fi
done

echo ""

# Find fastest
FASTEST=$(sort -t'|' -k2 -n "$RESULTS_FILE" | grep -v "N/A" | head -1)
if [ -n "$FASTEST" ]; then
    FASTEST_MODEL=$(echo "$FASTEST" | cut -d'|' -f1)
    FASTEST_TIME=$(echo "$FASTEST" | cut -d'|' -f2)

    echo "ğŸ† FASTEST: $FASTEST_MODEL (${FASTEST_TIME}s)"
    echo ""
    echo "ğŸ’¡ Recommendation:"

    if [ "$FASTEST_MODEL" = "llama3.2" ]; then
        echo "   Use: python3 fast_local_ai_browser.py llama3.2 \"task\" url"
        echo "   Note: Text-only (no vision), but fastest"
    elif [ "$FASTEST_MODEL" = "moondream" ]; then
        echo "   Use: python3 fast_local_ai_browser.py moondream \"task\" url"
        echo "   Note: Tiny model with vision support"
    else
        echo "   Use: python3 fast_local_ai_browser.py $FASTEST_MODEL \"task\" url"
    fi
fi

echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ“ Notes"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "  â€¢ First run is always slower (model loading)"
echo "  â€¢ Vision models analyze screenshots (slower but flexible)"
echo "  â€¢ Text models use HTML only (faster but no visual understanding)"
echo "  â€¢ Smaller models = faster inference"
echo ""
echo "To install missing models: ./setup_fast_local_models.sh"
echo ""

rm "$RESULTS_FILE"
