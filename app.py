#!/usr/bin/env python3
"""
Distributed Project Management Dashboard

A unified, password-protected web dashboard on port 8080 that manages:
- tmux sessions across multiple nodes
- Projects, milestones, features, and bug fixes
- Error aggregation from all nodes
- Offline workers and task queues
- Resource allocation across computers/accounts
- System status dashboard (real-time monitoring)
- Session monitoring and task routing
- Quality scoring and result comparison
- Claude, Comet, and Perplexity integration

Features:
- Consolidated web dashboard (previously on separate port 8081)
- Status monitoring for all system components
- Auto-confirm activity tracking
- Smart task routing
- Quality scoring
- Result comparison across multiple sources
- Foundation session monitoring

Usage:
    python3 app.py                    # Start server on port 8080
    python3 app.py --port 9000        # Custom port
    python3 app.py --debug            # Debug mode
"""

import hashlib
import io
import json
import logging
import os
import sqlite3
import subprocess
import sys
import threading
import time
import uuid
from datetime import datetime, timedelta
from functools import wraps
from pathlib import Path
from typing import Dict, List

from flask import (
    Flask,
    jsonify,
    make_response,
    redirect,
    render_template,
    request,
    send_from_directory,
    session,
    url_for,
)
from flask_compress import Compress
from flask_socketio import SocketIO, emit, join_room, leave_room

# Import activity timeline module
import activity_timeline

# Import batch task creation module
import batch_tasks

# Import dashboard layout module
import dashboard_layout

# Import centralized database module
import db as database

# Import kudos (team recognition) module
import kudos

# Import notification rules module
import notification_rules

# Import portfolio module
import portfolio

# Import project cost tracking module
import project_costs

# Import project dependencies module
import project_dependencies

# Import project permissions utilities
import project_permissions

# Import custom reports module
import reports as custom_reports

# Import resource allocation module
import resource_allocation

# Import retrospectives module
import retrospectives

# Import role management utilities
import roles as user_roles

# Import rollback manager module
import rollback_manager

# Import app-wide settings module
import settings as app_settings

# Import Slack integration utilities
import slack_integration

# Import sprint board management module
import sprint_board

# Import task conversion module
import task_convert

# Import task worklog module
import task_worklog

# Import webhook utilities for task events
import webhooks as task_webhooks
from db import get_pool_stats

# Import rate limiting and resource monitoring services
from services.rate_limiting import RateLimitService
from services.resource_monitor import ResourceMonitor
from services.background_tasks import get_background_task_manager
from services.rate_limiting_routes import rate_limiting_bp

# Import web dashboard modules
try:
    from auto_confirm_monitor import AutoConfirmMonitor

    AUTO_CONFIRM_MONITOR_AVAILABLE = True
except ImportError:
    AUTO_CONFIRM_MONITOR_AVAILABLE = False

try:
    from claude_auto_integration import ClaudeIntegration

    CLAUDE_INTEGRATION_AVAILABLE = True
except ImportError:
    CLAUDE_INTEGRATION_AVAILABLE = False

try:
    from comet_auto_integration import CometIntegration

    COMET_INTEGRATION_AVAILABLE = True
except ImportError:
    COMET_INTEGRATION_AVAILABLE = False

try:
    from perplexity_scraper import PerplexityScraper

    SCRAPER_AVAILABLE = True
except ImportError:
    SCRAPER_AVAILABLE = False

try:
    from quality_scorer import QualityScorer

    QUALITY_SCORER_AVAILABLE = True
except ImportError:
    QUALITY_SCORER_AVAILABLE = False

try:
    from result_comparator import ResultComparator

    COMPARATOR_AVAILABLE = True
except ImportError:
    COMPARATOR_AVAILABLE = False

try:
    from smart_task_router import SmartTaskRouter

    ROUTER_AVAILABLE = True
except ImportError:
    ROUTER_AVAILABLE = False

try:
    from status_dashboard import StatusDashboard

    STATUS_DASHBOARD_AVAILABLE = True
except ImportError:
    STATUS_DASHBOARD_AVAILABLE = False

try:
    sys.path.insert(0, str(Path(__file__).parent / "scripts"))
    from foundation_session_monitor import FoundationSessionMonitor

    SESSION_MONITOR_AVAILABLE = True
except (ImportError, Exception):
    SESSION_MONITOR_AVAILABLE = False

from correlation_id import (
    CORRELATION_ID_HEADER,
    CorrelationIdFormatter,
    CorrelationLogFilter,
    after_request_correlation,
    before_request_correlation,
    get_correlation_id,
    get_request_context,
)
from csrf_protection import (
    csrf_protect_all,
    generate_csrf_token,
    get_csrf_context,
    validate_csrf_token,
)

# Import dashboard cache management
from dashboard_cache import get_cache_manager, get_component_info
from milestone_summaries_api import register_milestone_summaries_routes

# Import OpenAPI/Swagger documentation module
from openapi import init_openapi

# Import quick access sidebar utilities
from quick_access import (
    get_pinned_ids,
    get_quick_access_data,
    get_recent_projects_for_user,
    pin_project_for_user,
    reorder_pinned_for_user,
    unpin_project_for_user,
)

# Import recurring tasks utilities
from recurring_tasks import calculate_next_run, check_due_recurring_tasks, spawn_recurring_task

# Import security headers middleware
from security_headers import SecurityHeaders

# Import distributed tracing with OpenTelemetry
from tracing import TracingConfig, add_span_event, get_trace_context, init_tracing, trace_span

# Import typing indicators for real-time collaboration
from typing_indicators import get_typing_manager

# Import utility functions for input sanitization
from utils import sanitize_string

# Configure logging with correlation ID support
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(correlation_id)s] %(name)s - %(levelname)s - %(message)s",
)

# Add correlation ID filter to all handlers
_correlation_filter = CorrelationLogFilter()
for _handler in logging.root.handlers:
    _handler.addFilter(_correlation_filter)
    _handler.setFormatter(
        CorrelationIdFormatter(
            "%(asctime)s [%(correlation_id)s] %(name)s - %(levelname)s - %(message)s"
        )
    )

logger = logging.getLogger(__name__)

# Application paths
BASE_DIR = Path(__file__).parent


def detect_environment():
    """Detect environment from env var or git branch."""
    env = (
        os.environ.get("APP_ENV", "").lower()
        or os.environ.get("ARCHITECT_ENV", "").lower()
    )
    if env:
        return env
    try:
        result = subprocess.check_output(
            ["git", "rev-parse", "--abbrev-re", "HEAD"],
            text=True,
            stderr=subprocess.DEVNULL,
            cwd=str(BASE_DIR),
        ).strip()
        if result == "main":
            return "prod"
        elif result.startswith("qa"):
            return "qa"
        return "dev"
    except Exception:
        return "dev"


APP_ENV = detect_environment()
DATA_DIR = Path(os.environ.get("DATA_DIR", str(BASE_DIR / "data" / APP_ENV)))
DB_PATH = Path(os.environ.get("DB_PATH", str(DATA_DIR / "architect.db")))
TEMPLATES_DIR = BASE_DIR / "templates"
STATIC_DIR = BASE_DIR / "static"

# Database connection settings (now in db.py, kept for backward compat)
DB_TIMEOUT = 30  # seconds
DB_RETRY_COUNT = 3
DB_RETRY_DELAY = 0.5  # seconds

# Task priority queue with aging configuration
# Aging increases effective priority over time to prevent starvation
# Formula: effective_priority = priority + (age_in_minutes * AGING_FACTOR)
TASK_PRIORITY_AGING_FACTOR = 0.1  # Points gained per minute waiting
TASK_PRIORITY_MAX_AGE_BONUS = (
    10  # Maximum bonus from aging (caps at 100 minutes)
)

# Dependency-based priority configuration
# Tasks that block other tasks get priority boosts
DEPENDENCY_BOOST_PER_BLOCKED = (
    2  # Priority boost per task that depends on this one
)
DEPENDENCY_BOOST_MAX = 20  # Maximum boost from blocking other tasks
CRITICAL_PATH_BOOST = 5  # Additional boost for tasks on critical path

# Task timeout configuration per task type (in seconds)
# Each task type can have its own timeout before being considered stuck
TASK_TYPE_TIMEOUTS = {
    "shell": 300,  # 5 minutes - shell commands
    "python": 600,  # 10 minutes - Python scripts
    "git": 180,  # 3 minutes - git operations
    "deploy": 1800,  # 30 minutes - deployments
    "test": 900,  # 15 minutes - test runs
    "build": 1200,  # 20 minutes - build tasks
    "tmux": 60,  # 1 minute - tmux operations
    "claude_task": 3600,  # 60 minutes - Claude AI tasks
    "web_crawl": 300,  # 5 minutes - web crawling
    "error_fix": 1800,  # 30 minutes - auto error fixes
    "maintenance": 600,  # 10 minutes - maintenance tasks
    "default": 600,  # 10 minutes - fallback for unknown types
}


def get_task_timeout(task_type: str) -> int:
    """Get the timeout for a specific task type."""
    return TASK_TYPE_TIMEOUTS.get(task_type, TASK_TYPE_TIMEOUTS["default"])


# Task auto-archive configuration
# Completed and failed tasks older than this will be moved to archive
TASK_ARCHIVE_CONFIG = {
    "enabled": True,  # Enable auto-archiving
    "archive_after_days": 7,  # Archive tasks older than N days
    "archive_statuses": ["completed", "failed"],  # Statuses to archive
    "batch_size": 100,  # Tasks to archive per batch
    "keep_recent_count": 1000,  # Always keep at least N recent tasks
    "auto_run_interval_hours": 24,  # Run auto-archive every N hours
}

# Track last auto-archive run
_last_auto_archive = None

# Task SLA (Service Level Agreement) configuration
# Defines expected completion times and alert thresholds per task type
TASK_SLA_CONFIG = {
    "shell": {
        "target_minutes": 5,
        "warning_percent": 80,
        "critical_percent": 100,
    },
    "python": {
        "target_minutes": 10,
        "warning_percent": 80,
        "critical_percent": 100,
    },
    "git": {
        "target_minutes": 3,
        "warning_percent": 80,
        "critical_percent": 100,
    },
    "deploy": {
        "target_minutes": 30,
        "warning_percent": 70,
        "critical_percent": 90,
    },
    "test": {
        "target_minutes": 15,
        "warning_percent": 80,
        "critical_percent": 100,
    },
    "build": {
        "target_minutes": 20,
        "warning_percent": 75,
        "critical_percent": 95,
    },
    "tmux": {
        "target_minutes": 1,
        "warning_percent": 80,
        "critical_percent": 100,
    },
    "claude_task": {
        "target_minutes": 60,
        "warning_percent": 80,
        "critical_percent": 100,
    },
    "web_crawl": {
        "target_minutes": 5,
        "warning_percent": 80,
        "critical_percent": 100,
    },
    "error_fix": {
        "target_minutes": 30,
        "warning_percent": 70,
        "critical_percent": 90,
    },
    "maintenance": {
        "target_minutes": 10,
        "warning_percent": 80,
        "critical_percent": 100,
    },
    "default": {
        "target_minutes": 10,
        "warning_percent": 80,
        "critical_percent": 100,
    },
}


def get_task_sla(task_type: str) -> dict:
    """Get SLA configuration for a task type."""
    return TASK_SLA_CONFIG.get(task_type, TASK_SLA_CONFIG["default"])


def calculate_sla_status(
    task_type: str, created_at: datetime, completed_at: datetime = None
) -> dict:
    """Calculate SLA status for a task.

    Returns:
        status: 'ok', 'warning', 'breached'
        elapsed_minutes: Time elapsed since creation
        target_minutes: SLA target
        percent_used: Percentage of SLA time used
        remaining_minutes: Time remaining (negative if breached)
    """
    sla = get_task_sla(task_type)
    target = sla["target_minutes"]
    warning_threshold = sla["warning_percent"]
    critical_threshold = sla["critical_percent"]

    now = completed_at or datetime.now()
    if isinstance(created_at, str):
        created_at = datetime.fromisoformat(created_at.replace("Z", "+00:00"))
    elapsed = (now - created_at).total_seconds() / 60
    percent_used = (elapsed / target) * 100 if target > 0 else 100

    if percent_used >= critical_threshold:
        status = "breached"
    elif percent_used >= warning_threshold:
        status = "warning"
    else:
        status = "ok"

    return {
        "status": status,
        "elapsed_minutes": round(elapsed, 1),
        "target_minutes": target,
        "percent_used": round(percent_used, 1),
        "remaining_minutes": round(target - elapsed, 1),
        "warning_threshold": warning_threshold,
        "critical_threshold": critical_threshold,
    }


def get_db_connection(timeout=DB_TIMEOUT):
    """Get a database connection with proper timeout settings.

    This is a wrapper around the centralized database module for backward compatibility.
    New code should use: from db import get_connection
    """
    return database.get_db_connection(timeout)


def execute_with_retry(func, max_retries=DB_RETRY_COUNT, delay=DB_RETRY_DELAY):
    """Execute a database function with retry logic for locked database.

    Args:
        func: Function that takes a connection and performs database operations
        max_retries: Maximum number of retry attempts
        delay: Delay between retries in seconds

    Returns:
        Result of the function call
    """
    last_error = None
    for attempt in range(max_retries):
        try:
            with get_db_connection() as conn:
                result = func(conn)
                conn.commit()
                return result
        except sqlite3.OperationalError as e:
            if "database is locked" in str(e) and attempt < max_retries - 1:
                last_error = e
                time.sleep(delay * (attempt + 1))  # Exponential backoff
                continue
            raise
    raise last_error


# Ensure directories exist
DATA_DIR.mkdir(exist_ok=True)

# Flask app
app = Flask(
    __name__, template_folder=str(TEMPLATES_DIR), static_folder=str(STATIC_DIR)
)
app.secret_key = os.environ.get(
    "SECRET_KEY", "architect-dashboard-secret-key-change-in-production"
)

# Initialize ORM for browser automation API
try:
    from db_orm import init_db
    init_db()
    logger.info("SQLAlchemy ORM initialized for browser automation API")
except Exception as e:
    logger.warning(f"Could not initialize ORM: {e}")

# CORS configuration removed - using manual after_request handler instead

# Session configuration for security
app.config["SESSION_COOKIE_SECURE"] = (
    os.environ.get("SESSION_COOKIE_SECURE", "false").lower() == "true"
)  # Set to True in production with HTTPS
app.config["SESSION_COOKIE_HTTPONLY"] = (
    True  # Prevent JavaScript access to session cookie
)
app.config["SESSION_COOKIE_SAMESITE"] = "Lax"  # CSRF protection
# Permanent sessions - never expire until explicit logout
app.config["PERMANENT_SESSION_LIFETIME"] = timedelta(days=3650)  # 10 years
app.config["SESSION_REFRESH_EACH_REQUEST"] = (
    True  # Keep session alive on each request
)

# Security Headers Middleware Configuration
# Adds HTTP security headers to all responses (CSP, HSTS, X-Frame-Options,
# etc.)
security_level = os.environ.get(
    "SECURITY_LEVEL", "moderate"
)  # strict, moderate, relaxed
security_headers = SecurityHeaders(app)

# Customize CSP for Socket.IO and external resources
security_headers.add_csp_source(
    "connect-src", "wss://*"
)  # Allow WebSocket connections
security_headers.add_csp_source(
    "connect-src", "ws://*"
)  # Allow WebSocket in dev

# Distributed Tracing Configuration (OpenTelemetry)
# Initialize distributed tracing for request tracking and performance
# monitoring
tracing_enabled = init_tracing(app)
if tracing_enabled:
    logger.info("Distributed tracing initialized with OpenTelemetry")

# Initialize rate limiting and resource monitoring services
app.rate_limiter = None
app.resource_monitor = None
app.bg_task_manager = None


def init_rate_limiting_services():
    """Initialize rate limiting and resource monitoring services."""
    try:
        # Initialize services
        app.rate_limiter = RateLimitService(database.get_db_connection)
        app.resource_monitor = ResourceMonitor(database.get_db_connection)
        app.bg_task_manager = get_background_task_manager()

        # Register background tasks
        app.bg_task_manager.register_task(
            task_name="cleanup_rate_limits",
            task_func=lambda: app.rate_limiter.cleanup_old_data(days=7),
            interval_seconds=3600,  # Every hour
            start_immediately=False
        )

        app.bg_task_manager.register_task(
            task_name="record_resource_metrics",
            task_func=lambda: app.resource_monitor.record_snapshot(),
            interval_seconds=60,  # Every minute
            start_immediately=True
        )

        app.bg_task_manager.register_task(
            task_name="cleanup_resources",
            task_func=lambda: app.resource_monitor.cleanup_old_data(days=30),
            interval_seconds=3600,  # Every hour
            start_immediately=False
        )

        # Start background tasks
        app.bg_task_manager.start()
        logger.info("Rate limiting and resource monitoring services initialized")
    except Exception as e:
        logger.error(f"Failed to initialize rate limiting services: {e}")


def setup_default_rate_limits():
    """Set up default rate limit configurations in database."""
    if app.rate_limiter is None:
        return

    try:
        # Check if configs already exist
        existing = app.rate_limiter.get_all_configs()
        if len(existing) > 0:
            logger.info(f"Rate limit configs already exist ({len(existing)} rules), skipping initialization")
            return

        # Create default configurations
        defaults = [
            {
                "rule_name": "default_global",
                "scope": "ip",
                "limit_type": "requests_per_minute",
                "limit_value": 1000,
                "resource_type": None
            },
            {
                "rule_name": "login_limit",
                "scope": "ip",
                "limit_type": "requests_per_minute",
                "limit_value": 100,
                "resource_type": "login"
            },
            {
                "rule_name": "create_limit",
                "scope": "ip",
                "limit_type": "requests_per_minute",
                "limit_value": 500,
                "resource_type": "create"
            },
            {
                "rule_name": "upload_limit",
                "scope": "ip",
                "limit_type": "requests_per_minute",
                "limit_value": 200,
                "resource_type": "upload"
            },
        ]

        for config in defaults:
            success = app.rate_limiter.create_config(**config)
            if success:
                logger.info(f"Created rate limit config: {config['rule_name']}")
            else:
                logger.warning(f"Failed to create config: {config['rule_name']}")

        logger.info(f"Default rate limit configurations created ({len(defaults)} rules)")
    except Exception as e:
        logger.error(f"Error setting up default rate limits: {e}")


# Initialize web dashboard modules
_dashboard_obj = None
router = None
ac_monitor = None
scorer = None
scraper = None
claude = None
comet = None
comparator = None
session_monitor = None

if STATUS_DASHBOARD_AVAILABLE:
    try:
        status_dashboard = StatusDashboard()
        logger.info("StatusDashboard initialized")
    except Exception as e:
        logger.warning(f"Could not initialize StatusDashboard: {e}")

if ROUTER_AVAILABLE:
    try:
        router = SmartTaskRouter()
        logger.info("SmartTaskRouter initialized")
    except Exception as e:
        logger.warning(f"Could not initialize SmartTaskRouter: {e}")

if AUTO_CONFIRM_MONITOR_AVAILABLE:
    try:
        ac_monitor = AutoConfirmMonitor()
        logger.info("AutoConfirmMonitor initialized")
    except Exception as e:
        logger.warning(f"Could not initialize AutoConfirmMonitor: {e}")

if QUALITY_SCORER_AVAILABLE:
    try:
        scorer = QualityScorer()
        logger.info("QualityScorer initialized")
    except Exception as e:
        logger.warning(f"Could not initialize QualityScorer: {e}")

if SCRAPER_AVAILABLE:
    try:
        scraper = PerplexityScraper()
        logger.info("PerplexityScraper initialized")
    except Exception as e:
        logger.warning(f"Could not initialize PerplexityScraper: {e}")

if CLAUDE_INTEGRATION_AVAILABLE:
    try:
        claude = ClaudeIntegration()
        logger.info("ClaudeIntegration initialized")
    except Exception as e:
        logger.warning(f"Could not initialize ClaudeIntegration: {e}")

if COMET_INTEGRATION_AVAILABLE:
    try:
        comet = CometIntegration()
        logger.info("CometIntegration initialized")
    except Exception as e:
        logger.warning(f"Could not initialize CometIntegration: {e}")

if COMPARATOR_AVAILABLE:
    try:
        comparator = ResultComparator()
        logger.info("ResultComparator initialized")
    except Exception as e:
        logger.warning(f"Could not initialize ResultComparator: {e}")

if SESSION_MONITOR_AVAILABLE:
    try:
        session_monitor = FoundationSessionMonitor()
        logger.info("FoundationSessionMonitor initialized")
    except Exception as e:
        logger.warning(f"Could not initialize FoundationSessionMonitor: {e}")
        session_monitor = None

# OpenAPI/Swagger Documentation
# Initialize API documentation with Swagger UI and ReDoc
init_openapi(app)
logger.info("OpenAPI documentation initialized at /api/docs")

# Response compression configuration for large payloads
# Compresses responses using gzip/brotli for better performance
app.config["COMPRESS_MIMETYPES"] = [
    "text/html",
    "text/css",
    "text/xml",
    "text/plain",
    "application/json",
    "application/javascript",
    "application/xml",
]
app.config["COMPRESS_LEVEL"] = 6  # Compression level (1-9, 6 is good balance)
app.config["COMPRESS_MIN_SIZE"] = 500  # Only compress responses > 500 bytes
app.config["COMPRESS_ALGORITHM"] = "gzip"  # Use gzip (widely supported)
compress = Compress(app)

# Register blueprints
try:
    from routes.system_overview import system_bp

    app.register_blueprint(system_bp)
except ImportError as e:
    print(f"Warning: Could not load system_overview blueprint: {e}")

try:
    from services.scheduler_routes import scheduler_bp

    app.register_blueprint(scheduler_bp)
    app.config["DB_PATH"] = DB_PATH  # Make DB_PATH available to blueprint
except ImportError as e:
    print(f"Warning: Could not load scheduler blueprint: {e}")

try:
    from services.task_alerts_routes import alerts_bp

    app.register_blueprint(alerts_bp)
except ImportError as e:
    print(f"Warning: Could not load task alerts blueprint: {e}")

try:
    from services.api_keys_routes import api_keys_bp

    app.register_blueprint(api_keys_bp)
except ImportError as e:
    print(f"Warning: Could not load API keys blueprint: {e}")

try:
    from services.integrations_routes import integrations_bp

    app.register_blueprint(integrations_bp)
except ImportError as e:
    print(f"Warning: Could not load integrations blueprint: {e}")

try:
    from services.health_routes import health_bp

    app.register_blueprint(health_bp)
except ImportError as e:
    print(f"Warning: Could not load health blueprint: {e}")

try:
    from services.gantt_routes import gantt_bp

    app.register_blueprint(gantt_bp)
except ImportError as e:
    print(f"Warning: Could not load gantt blueprint: {e}")

try:
    from services.roadmap_routes import roadmap_bp

    app.register_blueprint(roadmap_bp)
except ImportError as e:
    print(f"Warning: Could not load roadmap blueprint: {e}")

try:
    from services.archive_routes import archive_bp

    app.register_blueprint(archive_bp)
except ImportError as e:
    print(f"Warning: Could not load archive blueprint: {e}")

try:
    from services.watchers_routes import watchers_bp

    app.register_blueprint(watchers_bp)
except ImportError as e:
    print(f"Warning: Could not load watchers blueprint: {e}")

try:
    from services.session_groups_routes import session_groups_bp

    app.register_blueprint(session_groups_bp)
except ImportError as e:
    print(f"Warning: Could not load session_groups blueprint: {e}")

try:
    from services.attachments_routes import attachments_bp

    app.register_blueprint(attachments_bp)
    app.config["UPLOAD_DIR"] = DATA_DIR / "attachments"
except ImportError as e:
    print(f"Warning: Could not load attachments blueprint: {e}")

try:
    from services.auto_assign_routes import auto_assign_bp

    app.register_blueprint(auto_assign_bp)
except ImportError as e:
    print(f"Warning: Could not load auto_assign blueprint: {e}")

try:
    from services.status_history_routes import status_history_bp

    app.register_blueprint(status_history_bp)
except ImportError as e:
    print(f"Warning: Could not load status_history blueprint: {e}")

try:
    from services.task_suggestions_routes import suggestions_bp

    app.register_blueprint(suggestions_bp)
except ImportError as e:
    print(f"Warning: Could not load suggestions blueprint: {e}")

try:
    from services.quick_create_routes import quick_create_bp

    app.register_blueprint(quick_create_bp)
except ImportError as e:
    print(f"Warning: Could not load quick_create blueprint: {e}")

# Rate limiting and resource monitoring routes
try:
    app.register_blueprint(rate_limiting_bp)
    logger.info("Rate limiting API blueprint registered")
except Exception as e:
    logger.error(f"Failed to load rate_limiting blueprint: {e}")

try:
    from services.llm_metrics_routes import llm_metrics_bp

    app.register_blueprint(llm_metrics_bp)
except ImportError as e:
    print(f"Warning: Could not load LLM metrics blueprint: {e}")

try:
    from services.go_wrapper_monitor_routes import go_monitor_bp

    app.register_blueprint(go_monitor_bp)
except ImportError as e:
    print(f"Warning: Could not load Go Wrapper monitor blueprint: {e}")

try:
    from services.session_pool_routes import session_pool_bp

    app.register_blueprint(session_pool_bp)
except ImportError as e:
    print(f"Warning: Could not load Session Pool blueprint: {e}")

try:
    from services.crawl_results_routes import crawl_results_bp

    app.register_blueprint(crawl_results_bp)
except ImportError as e:
    print(f"Warning: Could not load Crawl Results blueprint: {e}")

try:
    from routes.todos import todos_bp

    app.register_blueprint(todos_bp)
except ImportError as e:
    print(f"Warning: Could not load todos blueprint: {e}")

try:
    from api.browser_automation import browser_api

    app.register_blueprint(browser_api)
except ImportError as e:
    print(f"Warning: Could not load browser_automation API: {e}")

try:
    from services.claude_templates import register_claude_template_routes

    # Will be registered after helper functions are defined
    _claude_templates_loaded = True
except ImportError as e:
    print(f"Warning: Could not load claude_templates: {e}")
    _claude_templates_loaded = False

# Configuration
CONFIG = {
    "DEFAULT_PORT": 8080,
    "ADMIN_USER": os.environ.get("ARCHITECT_USER", "architect"),
    "ADMIN_PASSWORD": os.environ.get("ARCHITECT_PASSWORD", "peace5"),
    "SESSION_TIMEOUT": 3600,  # 1 hour
    "HEARTBEAT_INTERVAL": 30,  # seconds
    "ERROR_RETENTION_DAYS": 30,
}

# Correlation ID Configuration
# Enable request correlation IDs for distributed tracing
app.config["CORRELATION_ID_ENABLED"] = (
    os.environ.get("CORRELATION_ID_ENABLED", "true").lower() == "true"
)


# Register correlation ID before_request handler (runs first)
@app.before_request
def correlation_id_before_request():
    """Set up correlation ID for request tracking."""
    if app.config.get("CORRELATION_ID_ENABLED", True):
        before_request_correlation()


# Register CORS handler for monitor API
@app.after_request
def add_cors_headers(response):
    """Add CORS headers for cross-origin monitor access."""
    if request.path == "/api/tasks/monitor":
        response.headers["Access-Control-Allow-Origin"] = "*"
        response.headers["Access-Control-Allow-Methods"] = "GET, OPTIONS"
        response.headers["Access-Control-Allow-Headers"] = "Content-Type"
    return response


# Register correlation ID after_request handler
@app.after_request
def correlation_id_after_request(response):
    """Add correlation ID to response headers and log request completion."""
    if app.config.get("CORRELATION_ID_ENABLED", True):
        return after_request_correlation(response)
    return response


# Add correlation ID to template context
@app.context_processor
def inject_correlation_id():
    """Inject correlation ID into all templates."""
    return {"correlation_id": get_correlation_id()}


# CSRF Protection Configuration
# Enable CSRF protection globally for all state-changing requests
app.config["CSRF_ENABLED"] = (
    os.environ.get("CSRF_ENABLED", "true").lower() == "true"
)


# Register CSRF before_request handler
@app.before_request
def csrf_before_request():
    """Validate CSRF token for state-changing requests."""
    if not app.config.get("CSRF_ENABLED", True):
        return None
    return csrf_protect_all()


# Add CSRF context to all templates
@app.context_processor
def inject_csrf_token():
    """Inject CSRF token into all templates."""
    return get_csrf_context()


# File upload configuration
UPLOAD_FOLDER = DATA_DIR / "uploads"
UPLOAD_FOLDER.mkdir(exist_ok=True)
ALLOWED_EXTENSIONS = {
    "txt",
    "pdf",
    "png",
    "jpg",
    "jpeg",
    "gif",
    "doc",
    "docx",
    "xls",
    "xlsx",
    "csv",
    "json",
    "xml",
    "zip",
    "tar",
    "gz",
    "log",
    "md",
    "py",
    "js",
    "html",
    "css",
}
MAX_CONTENT_LENGTH = 16 * 1024 * 1024  # 16 MB max file size
app.config["MAX_CONTENT_LENGTH"] = MAX_CONTENT_LENGTH


def allowed_file(filename):
    """Check if file extension is allowed."""
    return (
        "." in filename
        and filename.rsplit(".", 1)[1].lower() in ALLOWED_EXTENSIONS
    )


def secure_filename_custom(filename):
    """Sanitize filename while preserving extension."""
    from werkzeug.utils import secure_filename

    # Get secure version of filename
    safe_name = secure_filename(filename)
    if not safe_name:
        # Generate a random name if filename is empty after sanitization
        import uuid

        ext = filename.rsplit(".", 1)[1].lower() if "." in filename else "bin"
        safe_name = f"{uuid.uuid4().hex[:8]}.{ext}"
    return safe_name


# ============================================================================
# API ERROR HANDLING
# ============================================================================


class APIError(Exception):
    """Custom exception for API errors with status codes."""

    def __init__(
        self, message: str, status_code: int = 400, details: dict = None
    ):
        super().__init__(message)
        self.message = message
        self.status_code = status_code
        self.details = details or {}


def api_error_handler(f):
    """Decorator for consistent API error handling."""

    @wraps(f)
    def decorated(*args, **kwargs):
        try:
            return f(*args, **kwargs)
        except APIError as e:
            logger.warning(f"API Error in {f.__name__}: {e.message}")
            return (
                jsonify({"error": e.message, "status": "error", **e.details}),
                e.status_code,
            )
        except sqlite3.OperationalError as e:
            if "database is locked" in str(e):
                logger.error(f"Database locked in {f.__name__}: {e}")
                return (
                    jsonify(
                        {
                            "error": "Database busy, please try again",
                            "status": "error",
                            "retry": True,
                        }
                    ),
                    503,
                )
            logger.error(f"Database error in {f.__name__}: {e}")
            return jsonify({"error": "Database error", "status": "error"}), 500
        except sqlite3.IntegrityError as e:
            logger.warning(f"Integrity error in {f.__name__}: {e}")
            return (
                jsonify(
                    {
                        "error": "Data integrity error - duplicate or invalid reference",
                        "status": "error",
                    }
                ),
                409,
            )
        except json.JSONDecodeError as e:
            logger.warning(f"JSON decode error in {f.__name__}: {e}")
            return (
                jsonify(
                    {
                        "error": "Invalid JSON in request body",
                        "status": "error",
                    }
                ),
                400,
            )
        except ValueError as e:
            logger.warning(f"Value error in {f.__name__}: {e}")
            return jsonify({"error": str(e), "status": "error"}), 400
        except Exception as e:
            logger.exception(f"Unexpected error in {f.__name__}: {e}")
            return (
                jsonify(
                    {
                        "error": "Internal server error",
                        "status": "error",
                        "logged": True,
                    }
                ),
                500,
            )

    return decorated


def validate_required_fields(data: dict, required: list) -> None:
    """Validate that required fields are present in request data.

    Args:
        data: The request data dictionary
        required: List of required field names

    Raises:
        APIError: If any required fields are missing
    """
    if not data:
        raise APIError("Request body is required", 400)

    missing = [f for f in required if f not in data or data[f] is None]
    if missing:
        raise APIError(
            f"Missing required fields: {', '.join(missing)}",
            400,
            {"missing_fields": missing},
        )


def validate_id(id_value: any, name: str = "id") -> int:
    """Validate and convert an ID to integer.

    Args:
        id_value: The ID value to validate
        name: Name of the field for error messages

    Returns:
        The validated integer ID

    Raises:
        APIError: If ID is invalid
    """
    try:
        return int(id_value)
    except (ValueError, TypeError):
        raise APIError(f"Invalid {name}: must be an integer", 400)


# Initialize SocketIO for real-time WebSocket updates
# Using eventlet async mode for proper WebSocket support
# Eventlet provides true async WebSocket handling without Werkzeug limitations
try:
    import eventlet  # noqa: F401

    socketio = SocketIO(
        app,
        async_mode="eventlet",  # Use eventlet for proper WebSocket support
        cors_allowed_origins="*",
        logger=False,
        engineio_logger=False,
        manage_session=False,
        ping_timeout=60,  # Increase timeout for slower connections
        ping_interval=25,  # Send ping every 25 seconds
        max_http_buffer_size=10**6,  # 1MB max message size
        always_connect=True,  # Always allow connections
        transports=[
            "websocket",
            "polling",
        ],  # Support both WebSocket and polling fallback
    )
    print(
        "SocketIO initialized with eventlet async mode (WebSocket fully supported)"
    )
except ImportError:
    # Fallback to threading mode if eventlet not available
    socketio = SocketIO(
        app,
        async_mode="threading",  # Fallback mode (WebSocket limited)
        cors_allowed_origins="*",
        logger=False,
        engineio_logger=False,
        manage_session=False,
        ping_timeout=60,
        ping_interval=25,
        max_http_buffer_size=10**6,
        always_connect=True,
        transports=["websocket", "polling"],
    )
    print(
        "WARNING: SocketIO using threading mode - WebSocket support limited. Install eventlet for full support."
    )

# Initialize request logging middleware
from middleware.request_logger import init_request_logging  # noqa: E402

init_request_logging(app)

# Initialize task alert service with SocketIO
try:
    from services.task_alerts import get_alert_service  # noqa: E402

    _alert_service = get_alert_service(str(DB_PATH))
    _alert_service.set_socketio(socketio)
except Exception as e:
    print(f"Warning: Could not initialize task alert service: {e}")

# Initialize rate limiting and resource monitoring services
init_rate_limiting_services()

# Setup default rate limit configurations
setup_default_rate_limits()

# Code source directories to scan
CODE_SOURCES = [
    Path.home() / "gitrepos",
    Path.home() / "gitrepos/all_repos",
    Path.home() / "gitrepos/dashboard_parent",
    Path.home() / "gitrepos/general_util_parent",
    Path.home() / "gitrepos/pioneer_parent",
    Path.home() / "gitrepos/pyWork",
    Path.home() / "gitrepos/golang",
    Path.home() / "Desktop/gitrepo/pyWork",
    Path.home() / "Desktop/gitrepo/glgWork",
    Path.home() / "Desktop/gitrepo/nodeWork",
]

# ============================================================================
# DATABASE INITIALIZATION
# ============================================================================


def init_database():
    """Initialize the SQLite database with all required tables."""
    with get_db_connection() as conn:
        try:
            conn.executescript(
                """
            -- Users table for authentication
            CREATE TABLE IF NOT EXISTS users (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                username TEXT UNIQUE NOT NULL,
                password_hash TEXT NOT NULL,
                role TEXT DEFAULT 'user',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                last_login TIMESTAMP
            );

            -- Projects table
            CREATE TABLE IF NOT EXISTS projects (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL UNIQUE,
                description TEXT,
                source_path TEXT,
                status TEXT DEFAULT 'active',
                priority INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                deleted_at TIMESTAMP,
                deleted_by TEXT
            );
            CREATE INDEX IF NOT EXISTS idx_projects_deleted ON projects(deleted_at);

            -- Project dependencies table
            CREATE TABLE IF NOT EXISTS project_dependencies (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_project_id INTEGER NOT NULL,
                target_project_id INTEGER NOT NULL,
                dependency_type TEXT NOT NULL,
                description TEXT,
                metadata TEXT,
                created_by TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (source_project_id) REFERENCES projects(id) ON DELETE CASCADE,
                FOREIGN KEY (target_project_id) REFERENCES projects(id) ON DELETE CASCADE,
                UNIQUE(source_project_id, target_project_id, dependency_type)
            );
            CREATE INDEX IF NOT EXISTS idx_project_deps_source ON project_dependencies(source_project_id);
            CREATE INDEX IF NOT EXISTS idx_project_deps_target ON project_dependencies(target_project_id);
            CREATE INDEX IF NOT EXISTS idx_project_deps_type ON project_dependencies(dependency_type);

            -- Milestones table (groups of features/bugs)
            CREATE TABLE IF NOT EXISTS milestones (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                project_id INTEGER NOT NULL,
                name TEXT NOT NULL,
                description TEXT,
                target_date DATE,
                status TEXT DEFAULT 'open',
                progress INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                deleted_at TIMESTAMP,
                deleted_by TEXT,
                FOREIGN KEY (project_id) REFERENCES projects(id)
            );
            CREATE INDEX IF NOT EXISTS idx_milestones_deleted ON milestones(deleted_at);

            -- Features table
            CREATE TABLE IF NOT EXISTS features (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                project_id INTEGER NOT NULL,
                milestone_id INTEGER,
                name TEXT NOT NULL,
                description TEXT,
                spec TEXT,
                status TEXT DEFAULT 'draft',
                priority INTEGER DEFAULT 0,
                assigned_to TEXT,
                assigned_node TEXT,
                tmux_session TEXT,
                estimated_hours REAL,
                actual_hours REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                completed_at TIMESTAMP,
                deleted_at TIMESTAMP,
                deleted_by TEXT,
                FOREIGN KEY (project_id) REFERENCES projects(id),
                FOREIGN KEY (milestone_id) REFERENCES milestones(id)
            );
            CREATE INDEX IF NOT EXISTS idx_features_deleted ON features(deleted_at);

            -- Feature status history for workflow tracking
            CREATE TABLE IF NOT EXISTS feature_status_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                feature_id INTEGER NOT NULL,
                from_status TEXT,
                to_status TEXT NOT NULL,
                changed_by TEXT,
                comment TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (feature_id) REFERENCES features(id)
            );

            -- Feature links to TODOs and commits
            CREATE TABLE IF NOT EXISTS feature_links (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                feature_id INTEGER NOT NULL,
                link_type TEXT NOT NULL,  -- 'todo', 'commit', 'bug', 'error'
                linked_id TEXT,           -- ID of linked item (or commit hash)
                commit_hash TEXT,         -- For commits
                commit_message TEXT,      -- For commits
                commit_url TEXT,          -- For commits
                description TEXT,
                created_by TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (feature_id) REFERENCES features(id)
            );

            -- Bugs/Issues table
            CREATE TABLE IF NOT EXISTS bugs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                project_id INTEGER NOT NULL,
                milestone_id INTEGER,
                title TEXT NOT NULL,
                description TEXT,
                severity TEXT DEFAULT 'medium',
                status TEXT DEFAULT 'open',
                source_node TEXT,
                source_error_id TEXT,
                assigned_to TEXT,
                assigned_node TEXT,
                tmux_session TEXT,
                stack_trace TEXT,
                screenshot TEXT,
                context TEXT,
                category TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                resolved_at TIMESTAMP,
                deleted_at TIMESTAMP,
                deleted_by TEXT,
                FOREIGN KEY (project_id) REFERENCES projects(id),
                FOREIGN KEY (milestone_id) REFERENCES milestones(id)
            );
            CREATE INDEX IF NOT EXISTS idx_bugs_deleted ON bugs(deleted_at);

            -- DevOps tasks table
            CREATE TABLE IF NOT EXISTS devops_tasks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                project_id INTEGER,
                name TEXT NOT NULL,
                description TEXT,
                task_type TEXT DEFAULT 'maintenance',
                status TEXT DEFAULT 'pending',
                priority INTEGER DEFAULT 0,
                assigned_node TEXT,
                tmux_session TEXT,
                schedule TEXT,
                last_run TIMESTAMP,
                next_run TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                deleted_at TIMESTAMP,
                deleted_by TEXT,
                FOREIGN KEY (project_id) REFERENCES projects(id)
            );
            CREATE INDEX IF NOT EXISTS idx_devops_tasks_deleted ON devops_tasks(deleted_at);

            -- Cluster nodes table
            CREATE TABLE IF NOT EXISTS nodes (
                id TEXT PRIMARY KEY,
                hostname TEXT NOT NULL,
                ip_address TEXT NOT NULL,
                ssh_port INTEGER DEFAULT 22,
                ssh_user TEXT,
                role TEXT DEFAULT 'worker',
                status TEXT DEFAULT 'offline',
                services TEXT DEFAULT '[]',
                cpu_usage REAL DEFAULT 0,
                memory_usage REAL DEFAULT 0,
                disk_usage REAL DEFAULT 0,
                load_average REAL DEFAULT 0,
                process_count INTEGER DEFAULT 0,
                uptime_seconds INTEGER DEFAULT 0,
                last_heartbeat TIMESTAMP,
                last_health_check TIMESTAMP,
                health_status TEXT DEFAULT 'unknown',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                deleted_at TIMESTAMP,
                deleted_by TEXT
            );
            CREATE INDEX IF NOT EXISTS idx_nodes_deleted ON nodes(deleted_at);

            -- Node health alerts
            CREATE TABLE IF NOT EXISTS node_alerts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                node_id TEXT NOT NULL,
                alert_type TEXT NOT NULL,
                severity TEXT DEFAULT 'warning',
                message TEXT,
                metric_name TEXT,
                metric_value REAL,
                threshold_value REAL,
                status TEXT DEFAULT 'active',
                acknowledged_by TEXT,
                acknowledged_at TIMESTAMP,
                resolved_at TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (node_id) REFERENCES nodes(id)
            );

            -- tmux sessions table
            CREATE TABLE IF NOT EXISTS tmux_sessions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                node_id TEXT NOT NULL,
                session_name TEXT NOT NULL,
                window_count INTEGER DEFAULT 1,
                attached INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                last_activity TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                purpose TEXT,
                assigned_task_type TEXT,
                assigned_task_id INTEGER,
                FOREIGN KEY (node_id) REFERENCES nodes(id),
                UNIQUE(node_id, session_name)
            );

            -- Error aggregation table
            CREATE TABLE IF NOT EXISTS errors (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                project_id INTEGER,
                node_id TEXT,
                error_type TEXT NOT NULL,
                message TEXT,
                source TEXT,
                line INTEGER,
                column_num INTEGER,
                stack_trace TEXT,
                url TEXT,
                user_agent TEXT,
                http_status INTEGER,
                context TEXT,
                status TEXT DEFAULT 'open',
                assigned_bug_id INTEGER,
                occurrence_count INTEGER DEFAULT 1,
                first_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                last_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                resolved_at TIMESTAMP,
                deleted_at TIMESTAMP,
                deleted_by TEXT,
                FOREIGN KEY (project_id) REFERENCES projects(id),
                FOREIGN KEY (node_id) REFERENCES nodes(id),
                FOREIGN KEY (assigned_bug_id) REFERENCES bugs(id)
            );
            CREATE INDEX IF NOT EXISTS idx_errors_deleted ON errors(deleted_at);

            -- Task queue for offline workers
            CREATE TABLE IF NOT EXISTS task_queue (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_type TEXT NOT NULL,
                task_data TEXT NOT NULL,
                priority INTEGER DEFAULT 0,
                status TEXT DEFAULT 'pending',
                assigned_node TEXT,
                assigned_worker TEXT,
                retries INTEGER DEFAULT 0,
                max_retries INTEGER DEFAULT 3,
                timeout_seconds INTEGER,
                error_message TEXT,
                story_points INTEGER,
                estimated_hours REAL,
                actual_hours REAL,
                -- Hierarchy columns for parent/child relationships
                parent_id INTEGER,
                hierarchy_level INTEGER DEFAULT 0,
                hierarchy_path TEXT DEFAULT '/',
                child_count INTEGER DEFAULT 0,
                -- Soft delete columns
                deleted_at TIMESTAMP,
                deleted_by TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                started_at TIMESTAMP,
                completed_at TIMESTAMP,
                FOREIGN KEY (parent_id) REFERENCES task_queue(id) ON DELETE SET NULL
            );
            CREATE INDEX IF NOT EXISTS idx_task_queue_parent ON task_queue(parent_id);
            CREATE INDEX IF NOT EXISTS idx_task_queue_hierarchy_level ON task_queue(hierarchy_level);
            CREATE INDEX IF NOT EXISTS idx_task_queue_deleted ON task_queue(deleted_at);

            -- Task archive for old completed/failed tasks
            CREATE TABLE IF NOT EXISTS task_archive (
                id INTEGER PRIMARY KEY,
                original_id INTEGER NOT NULL,
                task_type TEXT NOT NULL,
                task_data TEXT NOT NULL,
                priority INTEGER DEFAULT 0,
                status TEXT NOT NULL,
                assigned_node TEXT,
                assigned_worker TEXT,
                retries INTEGER DEFAULT 0,
                max_retries INTEGER DEFAULT 3,
                timeout_seconds INTEGER,
                error_message TEXT,
                -- Hierarchy columns preserved from task_queue
                parent_id INTEGER,
                hierarchy_level INTEGER,
                hierarchy_path TEXT,
                child_count INTEGER,
                created_at TIMESTAMP,
                started_at TIMESTAMP,
                completed_at TIMESTAMP,
                archived_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
            CREATE INDEX IF NOT EXISTS idx_task_archive_type ON task_archive(task_type);
            CREATE INDEX IF NOT EXISTS idx_task_archive_status ON task_archive(status);
            CREATE INDEX IF NOT EXISTS idx_task_archive_completed ON task_archive(completed_at);
            CREATE INDEX IF NOT EXISTS idx_task_archive_archived ON task_archive(archived_at);

            -- Recurring tasks for scheduled/repeating tasks
            CREATE TABLE IF NOT EXISTS recurring_tasks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                description TEXT,
                task_type TEXT NOT NULL,
                task_data TEXT DEFAULT '{}',
                priority INTEGER DEFAULT 0,
                max_retries INTEGER DEFAULT 3,
                timeout_seconds INTEGER,
                recurrence_type TEXT NOT NULL,
                recurrence_interval INTEGER DEFAULT 1,
                recurrence_days TEXT,
                recurrence_time TEXT DEFAULT '00:00',
                cron_expression TEXT,
                timezone TEXT DEFAULT 'UTC',
                enabled BOOLEAN DEFAULT 1,
                last_run_at TIMESTAMP,
                next_run_at TIMESTAMP,
                last_task_id INTEGER,
                run_count INTEGER DEFAULT 0,
                failure_count INTEGER DEFAULT 0,
                max_instances INTEGER DEFAULT 1,
                catch_up_missed BOOLEAN DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                created_by TEXT
            );
            CREATE INDEX IF NOT EXISTS idx_recurring_tasks_enabled ON recurring_tasks(enabled);
            CREATE INDEX IF NOT EXISTS idx_recurring_tasks_next_run ON recurring_tasks(next_run_at);

            -- Project templates for quick setup
            CREATE TABLE IF NOT EXISTS project_templates (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL UNIQUE,
                description TEXT,
                category TEXT DEFAULT 'general',
                template_data TEXT NOT NULL DEFAULT '{}',
                is_builtin BOOLEAN DEFAULT 0,
                usage_count INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                created_by TEXT
            );
            CREATE INDEX IF NOT EXISTS idx_project_templates_category ON project_templates(category);

            -- Project-specific CLAUDE.md templates
            CREATE TABLE IF NOT EXISTS project_claude_templates (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                project_id INTEGER NOT NULL,
                template_name TEXT DEFAULT 'CLAUDE.md',
                content TEXT NOT NULL,
                description TEXT,
                is_active BOOLEAN DEFAULT 1,
                version INTEGER DEFAULT 1,
                parent_template_id INTEGER,
                variables TEXT DEFAULT '{}',
                created_by TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE CASCADE,
                FOREIGN KEY (parent_template_id) REFERENCES project_claude_templates(id)
            );
            CREATE INDEX IF NOT EXISTS idx_claude_templates_project ON project_claude_templates(project_id);
            CREATE INDEX IF NOT EXISTS idx_claude_templates_active ON project_claude_templates(project_id, is_active);

            -- Task dependencies (blocks/blocked_by relationships)
            CREATE TABLE IF NOT EXISTS task_dependencies (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_id INTEGER NOT NULL,
                depends_on_id INTEGER NOT NULL,
                dependency_type TEXT DEFAULT 'blocks',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (task_id) REFERENCES task_queue(id) ON DELETE CASCADE,
                FOREIGN KEY (depends_on_id) REFERENCES task_queue(id) ON DELETE CASCADE,
                UNIQUE(task_id, depends_on_id)
            );
            CREATE INDEX IF NOT EXISTS idx_task_deps_task ON task_dependencies(task_id);
            CREATE INDEX IF NOT EXISTS idx_task_deps_depends ON task_dependencies(depends_on_id);

            -- Task time tracking entries
            CREATE TABLE IF NOT EXISTS task_time_entries (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_id INTEGER NOT NULL,
                user_id TEXT NOT NULL,
                start_time TIMESTAMP NOT NULL,
                end_time TIMESTAMP,
                duration_seconds INTEGER,
                description TEXT,
                is_running BOOLEAN DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (task_id) REFERENCES task_queue(id) ON DELETE CASCADE
            );
            CREATE INDEX IF NOT EXISTS idx_time_entries_task ON task_time_entries(task_id);
            CREATE INDEX IF NOT EXISTS idx_time_entries_user ON task_time_entries(user_id);
            CREATE INDEX IF NOT EXISTS idx_time_entries_running ON task_time_entries(is_running) WHERE is_running = 1;

            -- Task attachments for file uploads
            CREATE TABLE IF NOT EXISTS task_attachments (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_id INTEGER,
                filename TEXT NOT NULL,
                original_filename TEXT NOT NULL,
                file_path TEXT NOT NULL,
                file_size INTEGER,
                mime_type TEXT,
                uploaded_by INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (task_id) REFERENCES task_queue(id) ON DELETE CASCADE,
                FOREIGN KEY (uploaded_by) REFERENCES users(id)
            );

            -- Task comments/notes
            CREATE TABLE IF NOT EXISTS task_comments (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_id INTEGER NOT NULL,
                content TEXT NOT NULL,
                author TEXT,
                author_id INTEGER,
                comment_type TEXT DEFAULT 'note',
                is_internal BOOLEAN DEFAULT 0,
                edited_at TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (task_id) REFERENCES task_queue(id) ON DELETE CASCADE,
                FOREIGN KEY (author_id) REFERENCES users(id)
            );
            CREATE INDEX IF NOT EXISTS idx_task_comments_task ON task_comments(task_id);

            -- Task links (related, duplicates, parent/child)
            CREATE TABLE IF NOT EXISTS task_links (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_task_id INTEGER NOT NULL,
                target_task_id INTEGER NOT NULL,
                link_type TEXT NOT NULL DEFAULT 'related',
                description TEXT,
                created_by TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (source_task_id) REFERENCES task_queue(id) ON DELETE CASCADE,
                FOREIGN KEY (target_task_id) REFERENCES task_queue(id) ON DELETE CASCADE,
                UNIQUE(source_task_id, target_task_id, link_type)
            );
            CREATE INDEX IF NOT EXISTS idx_task_links_source ON task_links(source_task_id);
            CREATE INDEX IF NOT EXISTS idx_task_links_target ON task_links(target_task_id);
            CREATE INDEX IF NOT EXISTS idx_task_links_type ON task_links(link_type);

            -- Sync state for offline sync support
            CREATE TABLE IF NOT EXISTS sync_state (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                client_id TEXT NOT NULL UNIQUE,
                client_name TEXT,
                last_sync_at TIMESTAMP,
                last_pull_at TIMESTAMP,
                last_push_at TIMESTAMP,
                sync_token TEXT,
                device_info TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );

            -- Sync change log for tracking modifications
            CREATE TABLE IF NOT EXISTS sync_changelog (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                entity_type TEXT NOT NULL,
                entity_id INTEGER NOT NULL,
                action TEXT NOT NULL,
                changed_fields TEXT,
                old_values TEXT,
                new_values TEXT,
                changed_by TEXT,
                client_id TEXT,
                sync_version INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
            CREATE INDEX IF NOT EXISTS idx_sync_changelog_entity ON sync_changelog(entity_type, entity_id);
            CREATE INDEX IF NOT EXISTS idx_sync_changelog_created ON sync_changelog(created_at);

            -- Sync conflicts for conflict resolution
            CREATE TABLE IF NOT EXISTS sync_conflicts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                entity_type TEXT NOT NULL,
                entity_id INTEGER NOT NULL,
                client_id TEXT NOT NULL,
                client_data TEXT,
                server_data TEXT,
                resolution TEXT,
                resolved_at TIMESTAMP,
                resolved_by TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
            CREATE INDEX IF NOT EXISTS idx_sync_conflicts_entity ON sync_conflicts(entity_type, entity_id);

            -- Custom field definitions for tasks
            CREATE TABLE IF NOT EXISTS task_custom_fields (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL UNIQUE,
                display_name TEXT NOT NULL,
                field_type TEXT NOT NULL DEFAULT 'text',
                description TEXT,
                options TEXT,
                default_value TEXT,
                is_required BOOLEAN DEFAULT 0,
                is_active BOOLEAN DEFAULT 1,
                validation_regex TEXT,
                min_value REAL,
                max_value REAL,
                display_order INTEGER DEFAULT 0,
                show_in_list BOOLEAN DEFAULT 0,
                show_in_filters BOOLEAN DEFAULT 0,
                created_by INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (created_by) REFERENCES users(id)
            );
            CREATE INDEX IF NOT EXISTS idx_custom_fields_name ON task_custom_fields(name);
            CREATE INDEX IF NOT EXISTS idx_custom_fields_active ON task_custom_fields(is_active);

            -- Custom field values for tasks
            CREATE TABLE IF NOT EXISTS task_custom_field_values (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_id INTEGER NOT NULL,
                field_id INTEGER NOT NULL,
                value_text TEXT,
                value_number REAL,
                value_date TIMESTAMP,
                value_json TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (task_id) REFERENCES task_queue(id) ON DELETE CASCADE,
                FOREIGN KEY (field_id) REFERENCES task_custom_fields(id) ON DELETE CASCADE,
                UNIQUE(task_id, field_id)
            );
            CREATE INDEX IF NOT EXISTS idx_custom_values_task ON task_custom_field_values(task_id);
            CREATE INDEX IF NOT EXISTS idx_custom_values_field ON task_custom_field_values(field_id);

            -- Task templates for common task types
            CREATE TABLE IF NOT EXISTS task_templates (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL UNIQUE,
                description TEXT,
                task_type TEXT NOT NULL,
                task_data_template TEXT NOT NULL DEFAULT '{}',
                default_priority INTEGER DEFAULT 0,
                default_max_retries INTEGER DEFAULT 3,
                default_timeout_seconds INTEGER,
                category TEXT DEFAULT 'general',
                icon TEXT,
                is_active BOOLEAN DEFAULT 1,
                usage_count INTEGER DEFAULT 0,
                created_by INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (created_by) REFERENCES users(id)
            );
            CREATE INDEX IF NOT EXISTS idx_task_templates_category ON task_templates(category);
            CREATE INDEX IF NOT EXISTS idx_task_templates_type ON task_templates(task_type);

            -- Task batches for grouping batch-created tasks
            CREATE TABLE IF NOT EXISTS task_batches (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                batch_id TEXT NOT NULL UNIQUE,
                name TEXT,
                description TEXT,
                template_id INTEGER,
                total_tasks INTEGER DEFAULT 0,
                created_count INTEGER DEFAULT 0,
                failed_count INTEGER DEFAULT 0,
                status TEXT DEFAULT 'pending',
                created_by TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (template_id) REFERENCES task_templates(id)
            );
            CREATE INDEX IF NOT EXISTS idx_task_batches_batch_id ON task_batches(batch_id);
            CREATE INDEX IF NOT EXISTS idx_task_batches_template ON task_batches(template_id);
            CREATE INDEX IF NOT EXISTS idx_task_batches_status ON task_batches(status);

            -- Task worklog entries for time tracking
            CREATE TABLE IF NOT EXISTS task_worklog (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_id INTEGER NOT NULL,
                user_id TEXT NOT NULL,
                description TEXT,
                time_spent_minutes INTEGER NOT NULL,
                work_date DATE NOT NULL,
                work_type TEXT DEFAULT 'general',
                billable BOOLEAN DEFAULT 1,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (task_id) REFERENCES task_queue(id) ON DELETE CASCADE
            );
            CREATE INDEX IF NOT EXISTS idx_worklog_task ON task_worklog(task_id);
            CREATE INDEX IF NOT EXISTS idx_worklog_user ON task_worklog(user_id);
            CREATE INDEX IF NOT EXISTS idx_worklog_date ON task_worklog(work_date);
            CREATE INDEX IF NOT EXISTS idx_worklog_type ON task_worklog(work_type);

            -- Task timers for active time tracking
            CREATE TABLE IF NOT EXISTS task_timers (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_id INTEGER NOT NULL,
                user_id TEXT NOT NULL,
                description TEXT,
                work_type TEXT DEFAULT 'general',
                start_time TIMESTAMP NOT NULL,
                end_time TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (task_id) REFERENCES task_queue(id) ON DELETE CASCADE
            );
            CREATE INDEX IF NOT EXISTS idx_timers_user ON task_timers(user_id);
            CREATE INDEX IF NOT EXISTS idx_timers_active ON task_timers(user_id, end_time) WHERE end_time IS NULL;

            -- Project budgets
            CREATE TABLE IF NOT EXISTS project_budgets (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                project_id INTEGER NOT NULL,
                budget_amount REAL NOT NULL,
                currency TEXT DEFAULT 'USD',
                fiscal_year INTEGER NOT NULL,
                notes TEXT,
                created_by TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE CASCADE,
                UNIQUE(project_id, fiscal_year)
            );
            CREATE INDEX IF NOT EXISTS idx_budgets_project ON project_budgets(project_id);
            CREATE INDEX IF NOT EXISTS idx_budgets_year ON project_budgets(fiscal_year);

            -- Project costs/expenses
            CREATE TABLE IF NOT EXISTS project_costs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                project_id INTEGER NOT NULL,
                amount REAL NOT NULL,
                category TEXT NOT NULL,
                description TEXT,
                cost_date DATE NOT NULL,
                status TEXT DEFAULT 'planned',
                vendor TEXT,
                invoice_ref TEXT,
                fiscal_year INTEGER NOT NULL,
                metadata TEXT,
                created_by TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE CASCADE
            );
            CREATE INDEX IF NOT EXISTS idx_costs_project ON project_costs(project_id);
            CREATE INDEX IF NOT EXISTS idx_costs_category ON project_costs(category);
            CREATE INDEX IF NOT EXISTS idx_costs_status ON project_costs(status);
            CREATE INDEX IF NOT EXISTS idx_costs_year ON project_costs(fiscal_year);
            CREATE INDEX IF NOT EXISTS idx_costs_date ON project_costs(cost_date);

            -- Notification rules
            CREATE TABLE IF NOT EXISTS notification_rules (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                event_type TEXT NOT NULL,
                channels TEXT NOT NULL,
                user_id TEXT,
                project_id INTEGER,
                conditions TEXT,
                frequency TEXT DEFAULT 'immediate',
                enabled BOOLEAN DEFAULT 1,
                quiet_hours_start TEXT,
                quiet_hours_end TEXT,
                description TEXT,
                created_by TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE CASCADE
            );
            CREATE INDEX IF NOT EXISTS idx_notif_rules_event ON notification_rules(event_type);
            CREATE INDEX IF NOT EXISTS idx_notif_rules_user ON notification_rules(user_id);
            CREATE INDEX IF NOT EXISTS idx_notif_rules_project ON notification_rules(project_id);
            CREATE INDEX IF NOT EXISTS idx_notif_rules_enabled ON notification_rules(enabled);

            -- Notification queue
            CREATE TABLE IF NOT EXISTS notification_queue (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                rule_id INTEGER,
                event_type TEXT NOT NULL,
                event_data TEXT,
                channels TEXT NOT NULL,
                user_id TEXT,
                status TEXT DEFAULT 'pending',
                sent_at TIMESTAMP,
                error_message TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (rule_id) REFERENCES notification_rules(id) ON DELETE SET NULL
            );
            CREATE INDEX IF NOT EXISTS idx_notif_queue_status ON notification_queue(status);
            CREATE INDEX IF NOT EXISTS idx_notif_queue_user ON notification_queue(user_id);

            -- Notification delivery log
            CREATE TABLE IF NOT EXISTS notification_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                notification_id INTEGER,
                channel TEXT NOT NULL,
                status TEXT NOT NULL,
                error_message TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (notification_id) REFERENCES notification_queue(id) ON DELETE CASCADE
            );
            CREATE INDEX IF NOT EXISTS idx_notif_log_notification ON notification_log(notification_id);
            CREATE INDEX IF NOT EXISTS idx_notif_log_channel ON notification_log(channel);

            -- Task conversion history
            CREATE TABLE IF NOT EXISTS task_conversions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_type TEXT NOT NULL,
                source_id INTEGER NOT NULL,
                target_type TEXT NOT NULL,
                target_id INTEGER NOT NULL,
                converted_by TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
            CREATE INDEX IF NOT EXISTS idx_conversions_source ON task_conversions(source_type, source_id);
            CREATE INDEX IF NOT EXISTS idx_conversions_target ON task_conversions(target_type, target_id);

            -- Dashboard layouts
            CREATE TABLE IF NOT EXISTS dashboard_layouts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT NOT NULL,
                name TEXT NOT NULL,
                description TEXT,
                layout_config TEXT NOT NULL,
                is_default BOOLEAN DEFAULT 0,
                is_public BOOLEAN DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
            CREATE INDEX IF NOT EXISTS idx_layouts_user ON dashboard_layouts(user_id);
            CREATE INDEX IF NOT EXISTS idx_layouts_default ON dashboard_layouts(user_id, is_default);
            CREATE INDEX IF NOT EXISTS idx_layouts_public ON dashboard_layouts(is_public);

            -- Worker registrations
            CREATE TABLE IF NOT EXISTS workers (
                id TEXT PRIMARY KEY,
                node_id TEXT NOT NULL,
                worker_type TEXT NOT NULL,
                status TEXT DEFAULT 'idle',
                current_task_id INTEGER,
                last_heartbeat TIMESTAMP,
                tasks_completed INTEGER DEFAULT 0,
                tasks_failed INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                deleted_at TIMESTAMP,
                deleted_by TEXT,
                FOREIGN KEY (node_id) REFERENCES nodes(id),
                FOREIGN KEY (current_task_id) REFERENCES task_queue(id)
            );
            CREATE INDEX IF NOT EXISTS idx_workers_deleted ON workers(deleted_at);

            -- Worker skills/expertise
            CREATE TABLE IF NOT EXISTS worker_skills (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                worker_id TEXT NOT NULL,
                skill_name TEXT NOT NULL,
                proficiency INTEGER DEFAULT 50,
                tasks_completed INTEGER DEFAULT 0,
                avg_duration_seconds REAL,
                last_used TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (worker_id) REFERENCES workers(id) ON DELETE CASCADE,
                UNIQUE(worker_id, skill_name)
            );
            CREATE INDEX IF NOT EXISTS idx_worker_skills_worker ON worker_skills(worker_id);
            CREATE INDEX IF NOT EXISTS idx_worker_skills_skill ON worker_skills(skill_name);

            -- Task skill requirements
            CREATE TABLE IF NOT EXISTS task_skill_requirements (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_type TEXT NOT NULL,
                skill_name TEXT NOT NULL,
                min_proficiency INTEGER DEFAULT 0,
                priority INTEGER DEFAULT 1,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(task_type, skill_name)
            );
            CREATE INDEX IF NOT EXISTS idx_task_skills_type ON task_skill_requirements(task_type);

            -- Worker alerts
            CREATE TABLE IF NOT EXISTS worker_alerts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                worker_id TEXT,
                worker_type TEXT NOT NULL,
                alert_type TEXT NOT NULL,
                severity TEXT DEFAULT 'warning',
                message TEXT NOT NULL,
                details TEXT,
                acknowledged BOOLEAN DEFAULT 0,
                acknowledged_by TEXT,
                acknowledged_at TIMESTAMP,
                resolved BOOLEAN DEFAULT 0,
                resolved_at TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );

            -- Worker configuration (capacity, weights for load balancing)
            CREATE TABLE IF NOT EXISTS worker_config (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                worker_id TEXT NOT NULL,
                key TEXT NOT NULL,
                value TEXT,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(worker_id, key)
            );
            CREATE INDEX IF NOT EXISTS idx_worker_config_worker ON worker_config(worker_id);

            -- Activity log
            CREATE TABLE IF NOT EXISTS activity_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id INTEGER,
                action TEXT NOT NULL,
                entity_type TEXT,
                entity_id INTEGER,
                details TEXT,
                ip_address TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (user_id) REFERENCES users(id)
            );

            -- Resource allocations
            CREATE TABLE IF NOT EXISTS resource_allocations (
                id TEXT PRIMARY KEY,
                resource_type TEXT NOT NULL,
                requester TEXT,
                node_id TEXT,
                priority TEXT DEFAULT 'normal',
                allocated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                released_at TIMESTAMP,
                metadata TEXT,
                FOREIGN KEY (node_id) REFERENCES nodes(id)
            );

            -- Deployments and deployment_gates tables are created by migration 002_add_testing.py

            -- Suggested tasks (Claude can suggest, user confirms)
            CREATE TABLE IF NOT EXISTS suggested_tasks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                description TEXT,
                task_type TEXT DEFAULT 'manual',
                -- Types: manual_test, browser_test, code_review, deployment, other
                priority TEXT DEFAULT 'normal',
                -- Priority: low, normal, high, critical
                status TEXT DEFAULT 'pending',
                -- Status: pending, confirmed, skipped, completed, expired
                source_session TEXT,
                -- The tmux session that suggested this task
                context TEXT,
                -- JSON with additional context (related feature, error, etc.)
                requires_confirmation BOOLEAN DEFAULT 1,
                confirmation_timeout INTEGER DEFAULT 300,
                -- Seconds to wait for confirmation (0 = no timeout)
                confirmed_by TEXT,
                confirmed_at TIMESTAMP,
                completed_at TIMESTAMP,
                result TEXT,
                -- JSON with task result/notes
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                expires_at TIMESTAMP
            );

            -- ============================================================
            -- AUTOPILOT TABLES (State Machine + MVP)
            -- ============================================================

            -- Autopilot Milestones (state machine)
            CREATE TABLE IF NOT EXISTS autopilot_milestones (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                project_id INTEGER NOT NULL,
                name TEXT NOT NULL,
                goal TEXT,
                scope TEXT,
                definition_of_done TEXT,
                status TEXT DEFAULT 'proposed',
                -- States: proposed, awaiting_approval, rework, approved, in_progress,
                --         validation, qa_promotion, prod_ready, released, abandoned, failed
                proposed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                approved_at TIMESTAMP,
                started_at TIMESTAMP,
                completed_at TIMESTAMP,
                approved_by TEXT,
                evidence_packet TEXT,
                risk_score REAL DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (project_id) REFERENCES projects(id)
            );

            -- Autopilot Runs (execution cycles)
            CREATE TABLE IF NOT EXISTS autopilot_runs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                milestone_id INTEGER NOT NULL,
                project_id INTEGER NOT NULL,
                run_number INTEGER DEFAULT 1,
                status TEXT DEFAULT 'created',
                -- States: created, planning, executing, testing, fixing, blocked,
                --         ready_for_review, promoting, monitoring, complete, failed, rolled_back
                tmux_session TEXT,
                plan_summary TEXT,
                blocked_reason TEXT,
                blocked_question TEXT,
                error_count INTEGER DEFAULT 0,
                test_pass_count INTEGER DEFAULT 0,
                test_fail_count INTEGER DEFAULT 0,
                started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                completed_at TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (milestone_id) REFERENCES autopilot_milestones(id),
                FOREIGN KEY (project_id) REFERENCES projects(id)
            );

            -- Autopilot Artifacts (ledger + evidence)
            CREATE TABLE IF NOT EXISTS autopilot_artifacts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                run_id INTEGER,
                milestone_id INTEGER,
                project_id INTEGER NOT NULL,
                artifact_type TEXT NOT NULL,
                -- Types: plan, command_trace, test_report, diff_summary, decision,
                --        evidence_packet, screenshot, metric_delta, pr_link
                title TEXT,
                content TEXT,
                file_path TEXT,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (run_id) REFERENCES autopilot_runs(id),
                FOREIGN KEY (milestone_id) REFERENCES autopilot_milestones(id),
                FOREIGN KEY (project_id) REFERENCES projects(id)
            );

            -- Autopilot Alerts (incidents + regressions)
            CREATE TABLE IF NOT EXISTS autopilot_alerts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                project_id INTEGER,
                run_id INTEGER,
                alert_type TEXT NOT NULL,
                -- Types: error, regression, test_failure, deploy_failure, blocked, incident
                severity TEXT DEFAULT 'medium',
                -- Severity: low, medium, high, critical
                status TEXT DEFAULT 'detected',
                -- States: detected, triaged, auto_remediation_running, needs_approval,
                --         manual_gated_remediation, resolved, blocked
                title TEXT NOT NULL,
                description TEXT,
                auto_fix_allowed BOOLEAN DEFAULT 0,
                remediation_plan TEXT,
                resolved_at TIMESTAMP,
                resolved_by TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (project_id) REFERENCES projects(id),
                FOREIGN KEY (run_id) REFERENCES autopilot_runs(id)
            );

            -- Web crawl results (for crawler service)
            CREATE TABLE IF NOT EXISTS crawl_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_id INTEGER NOT NULL,
                prompt TEXT NOT NULL,
                start_url TEXT,
                final_url TEXT,
                success BOOLEAN DEFAULT 0,
                extracted_data TEXT,
                action_history TEXT,
                screenshots TEXT,
                error_message TEXT,
                duration_seconds REAL,
                llm_provider TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (task_id) REFERENCES task_queue(id)
            );

            -- Dashboard widget configurations for customizable layouts
            CREATE TABLE IF NOT EXISTS dashboard_widgets (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id INTEGER,
                widget_type TEXT NOT NULL,
                title TEXT,
                position_x INTEGER DEFAULT 0,
                position_y INTEGER DEFAULT 0,
                width INTEGER DEFAULT 1,
                height INTEGER DEFAULT 1,
                settings TEXT DEFAULT '{}',
                is_visible BOOLEAN DEFAULT 1,
                is_collapsed BOOLEAN DEFAULT 0,
                refresh_interval INTEGER DEFAULT 30,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (user_id) REFERENCES users(id)
            );
            CREATE INDEX IF NOT EXISTS idx_dashboard_widgets_user ON dashboard_widgets(user_id);
            CREATE INDEX IF NOT EXISTS idx_dashboard_widgets_type ON dashboard_widgets(widget_type);

            -- User notifications for in-app messaging
            CREATE TABLE IF NOT EXISTS user_notifications (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id INTEGER,
                title TEXT NOT NULL,
                message TEXT,
                notification_type TEXT DEFAULT 'info',
                priority TEXT DEFAULT 'normal',
                category TEXT DEFAULT 'system',
                link TEXT,
                link_text TEXT,
                entity_type TEXT,
                entity_id INTEGER,
                is_read BOOLEAN DEFAULT 0,
                read_at TIMESTAMP,
                is_dismissed BOOLEAN DEFAULT 0,
                dismissed_at TIMESTAMP,
                expires_at TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (user_id) REFERENCES users(id)
            );
            CREATE INDEX IF NOT EXISTS idx_user_notifications_user ON user_notifications(user_id);
            CREATE INDEX IF NOT EXISTS idx_user_notifications_read ON user_notifications(user_id, is_read);
            CREATE INDEX IF NOT EXISTS idx_user_notifications_type ON user_notifications(notification_type);

            -- Team recognition kudos
            CREATE TABLE IF NOT EXISTS kudos (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                sender_user_id INTEGER NOT NULL,
                recipient_user_id INTEGER NOT NULL,
                project_id INTEGER,
                category TEXT DEFAULT 'general',
                message TEXT NOT NULL,
                points INTEGER DEFAULT 1,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (sender_user_id) REFERENCES users(id),
                FOREIGN KEY (recipient_user_id) REFERENCES users(id),
                FOREIGN KEY (project_id) REFERENCES projects(id)
            );
            CREATE INDEX IF NOT EXISTS idx_kudos_sender ON kudos(sender_user_id);
            CREATE INDEX IF NOT EXISTS idx_kudos_recipient ON kudos(recipient_user_id);
            CREATE INDEX IF NOT EXISTS idx_kudos_project ON kudos(project_id);
            CREATE INDEX IF NOT EXISTS idx_kudos_created_at ON kudos(created_at);

            -- User presence for real-time collaboration
            CREATE TABLE IF NOT EXISTS user_presence (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT NOT NULL,
                session_id TEXT NOT NULL UNIQUE,
                display_name TEXT,
                avatar_color TEXT,
                status TEXT DEFAULT 'online',
                current_entity_type TEXT,
                current_entity_id INTEGER,
                current_action TEXT DEFAULT 'viewing',
                last_heartbeat TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                connected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                user_agent TEXT,
                ip_address TEXT
            );
            CREATE INDEX IF NOT EXISTS idx_user_presence_user ON user_presence(user_id);
            CREATE INDEX IF NOT EXISTS idx_user_presence_entity ON user_presence(current_entity_type, current_entity_id);
            CREATE INDEX IF NOT EXISTS idx_user_presence_heartbeat ON user_presence(last_heartbeat);
            CREATE INDEX IF NOT EXISTS idx_user_presence_status ON user_presence(status);

            -- Custom metrics for user-defined tracking
            CREATE TABLE IF NOT EXISTS custom_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL UNIQUE,
                display_name TEXT,
                description TEXT,
                metric_type TEXT DEFAULT 'gauge',
                unit TEXT,
                value REAL DEFAULT 0,
                labels TEXT,
                min_value REAL,
                max_value REAL,
                alert_threshold_warning REAL,
                alert_threshold_critical REAL,
                is_enabled BOOLEAN DEFAULT 1,
                retention_days INTEGER DEFAULT 90,
                created_by INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (created_by) REFERENCES users(id)
            );
            CREATE INDEX IF NOT EXISTS idx_custom_metrics_name ON custom_metrics(name);
            CREATE INDEX IF NOT EXISTS idx_custom_metrics_type ON custom_metrics(metric_type);
            CREATE INDEX IF NOT EXISTS idx_custom_metrics_enabled ON custom_metrics(is_enabled);

            -- Custom metrics history for time-series data
            CREATE TABLE IF NOT EXISTS custom_metrics_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                metric_id INTEGER NOT NULL,
                value REAL NOT NULL,
                labels TEXT,
                recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (metric_id) REFERENCES custom_metrics(id) ON DELETE CASCADE
            );
            CREATE INDEX IF NOT EXISTS idx_custom_metrics_history_metric ON custom_metrics_history(metric_id);
            CREATE INDEX IF NOT EXISTS idx_custom_metrics_history_time ON custom_metrics_history(recorded_at);

            -- Custom reports for report builder
            CREATE TABLE IF NOT EXISTS custom_reports (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                description TEXT,
                data_source TEXT NOT NULL,
                columns TEXT,
                filters TEXT,
                config TEXT,
                schedule TEXT,
                owner_id INTEGER,
                is_public INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (owner_id) REFERENCES users(id) ON DELETE SET NULL
            );
            CREATE INDEX IF NOT EXISTS idx_custom_reports_owner ON custom_reports(owner_id);
            CREATE INDEX IF NOT EXISTS idx_custom_reports_public ON custom_reports(is_public);
            CREATE INDEX IF NOT EXISTS idx_custom_reports_source ON custom_reports(data_source);

            -- Report run history
            CREATE TABLE IF NOT EXISTS report_runs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                report_id INTEGER NOT NULL,
                row_count INTEGER DEFAULT 0,
                duration_seconds REAL,
                status TEXT DEFAULT 'pending',
                error TEXT,
                run_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (report_id) REFERENCES custom_reports(id) ON DELETE CASCADE
            );
            CREATE INDEX IF NOT EXISTS idx_report_runs_report ON report_runs(report_id);
            CREATE INDEX IF NOT EXISTS idx_report_runs_status ON report_runs(status);
            CREATE INDEX IF NOT EXISTS idx_report_runs_time ON report_runs(run_at);

            -- App-wide settings
            CREATE TABLE IF NOT EXISTS app_settings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                key TEXT NOT NULL UNIQUE,
                value TEXT,
                type TEXT DEFAULT 'string',
                category TEXT DEFAULT 'general',
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_by TEXT
            );
            CREATE INDEX IF NOT EXISTS idx_app_settings_key ON app_settings(key);
            CREATE INDEX IF NOT EXISTS idx_app_settings_category ON app_settings(category);

            -- Settings change history for audit
            CREATE TABLE IF NOT EXISTS settings_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                key TEXT NOT NULL,
                old_value TEXT,
                new_value TEXT,
                changed_by TEXT,
                changed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
            CREATE INDEX IF NOT EXISTS idx_settings_history_key ON settings_history(key);
            CREATE INDEX IF NOT EXISTS idx_settings_history_time ON settings_history(changed_at);

            -- Trigger to log settings changes
            CREATE TRIGGER IF NOT EXISTS log_settings_change
            AFTER UPDATE ON app_settings
            BEGIN
                INSERT INTO settings_history (key, old_value, new_value, changed_by)
                VALUES (OLD.key, OLD.value, NEW.value, NEW.updated_by);
            END;

            -- Secrets table for encrypted credential storage (Secure Vault)
            CREATE TABLE IF NOT EXISTS secrets (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE NOT NULL,
                encrypted_value BLOB NOT NULL,
                category TEXT NOT NULL DEFAULT 'general',
                description TEXT,
                project_id INTEGER,
                service TEXT,
                username TEXT,
                url TEXT,
                expires_at TIMESTAMP,
                last_accessed TIMESTAMP,
                access_count INTEGER DEFAULT 0,
                created_by TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE SET NULL
            );
            CREATE INDEX IF NOT EXISTS idx_secrets_category ON secrets(category);
            CREATE INDEX IF NOT EXISTS idx_secrets_service ON secrets(service);
            CREATE INDEX IF NOT EXISTS idx_secrets_project ON secrets(project_id);

            -- Secret access audit log
            CREATE TABLE IF NOT EXISTS secret_access_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                secret_id INTEGER NOT NULL,
                accessed_by TEXT,
                action TEXT NOT NULL,
                ip_address TEXT,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (secret_id) REFERENCES secrets(id) ON DELETE CASCADE
            );
            CREATE INDEX IF NOT EXISTS idx_secret_access_secret ON secret_access_log(secret_id);
            CREATE INDEX IF NOT EXISTS idx_secret_access_timestamp ON secret_access_log(timestamp DESC);

            -- Sprint retrospectives for milestone reviews
            CREATE TABLE IF NOT EXISTS sprint_retrospectives (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                milestone_id INTEGER NOT NULL,
                project_id INTEGER NOT NULL,
                sprint_name TEXT,
                start_date DATE,
                end_date DATE,
                status TEXT DEFAULT 'draft',
                facilitator TEXT,
                participants TEXT,
                summary TEXT,
                mood_score INTEGER,
                velocity_planned INTEGER,
                velocity_achieved INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                completed_at TIMESTAMP,
                FOREIGN KEY (milestone_id) REFERENCES milestones(id),
                FOREIGN KEY (project_id) REFERENCES projects(id)
            );
            CREATE INDEX IF NOT EXISTS idx_retrospectives_milestone ON sprint_retrospectives(milestone_id);
            CREATE INDEX IF NOT EXISTS idx_retrospectives_project ON sprint_retrospectives(project_id);
            CREATE INDEX IF NOT EXISTS idx_retrospectives_status ON sprint_retrospectives(status);

            -- Retrospective items (went well, needs improvement, action items, etc.)
            CREATE TABLE IF NOT EXISTS retrospective_items (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                retrospective_id INTEGER NOT NULL,
                category TEXT NOT NULL,
                content TEXT NOT NULL,
                votes INTEGER DEFAULT 0,
                is_action_item BOOLEAN DEFAULT 0,
                action_assignee TEXT,
                action_due_date DATE,
                action_status TEXT DEFAULT 'pending',
                created_by TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (retrospective_id) REFERENCES sprint_retrospectives(id) ON DELETE CASCADE
            );
            CREATE INDEX IF NOT EXISTS idx_retro_items_retro ON retrospective_items(retrospective_id);
            CREATE INDEX IF NOT EXISTS idx_retro_items_category ON retrospective_items(category);
            CREATE INDEX IF NOT EXISTS idx_retro_items_action ON retrospective_items(is_action_item, action_status);

            -- Create indexes for performance
            CREATE INDEX IF NOT EXISTS idx_features_project ON features(project_id);
            CREATE INDEX IF NOT EXISTS idx_features_milestone ON features(milestone_id);
            CREATE INDEX IF NOT EXISTS idx_features_status ON features(status);
            CREATE INDEX IF NOT EXISTS idx_bugs_project ON bugs(project_id);
            CREATE INDEX IF NOT EXISTS idx_bugs_status ON bugs(status);
            CREATE INDEX IF NOT EXISTS idx_errors_project ON errors(project_id);
            CREATE INDEX IF NOT EXISTS idx_errors_status ON errors(status);
            CREATE INDEX IF NOT EXISTS idx_errors_node ON errors(node_id);
            CREATE INDEX IF NOT EXISTS idx_task_queue_status ON task_queue(status);
            CREATE INDEX IF NOT EXISTS idx_activity_log_created ON activity_log(created_at);
            CREATE INDEX IF NOT EXISTS idx_autopilot_milestones_project ON autopilot_milestones(project_id);
            CREATE INDEX IF NOT EXISTS idx_autopilot_milestones_status ON autopilot_milestones(status);
            CREATE INDEX IF NOT EXISTS idx_autopilot_runs_milestone ON autopilot_runs(milestone_id);
            CREATE INDEX IF NOT EXISTS idx_autopilot_runs_status ON autopilot_runs(status);
            CREATE INDEX IF NOT EXISTS idx_autopilot_alerts_status ON autopilot_alerts(status);
            CREATE INDEX IF NOT EXISTS idx_crawl_results_task ON crawl_results(task_id);

            -- Optimized indexes for session-based queries
            CREATE INDEX IF NOT EXISTS idx_features_tmux_session ON features(tmux_session);
            CREATE INDEX IF NOT EXISTS idx_bugs_tmux_session ON bugs(tmux_session);
            CREATE INDEX IF NOT EXISTS idx_features_assigned ON features(assigned_to, assigned_node);
            CREATE INDEX IF NOT EXISTS idx_bugs_assigned ON bugs(assigned_to, assigned_node);

            -- Compound indexes for common filter+sort patterns
            CREATE INDEX IF NOT EXISTS idx_features_project_status ON features(project_id, status);
            CREATE INDEX IF NOT EXISTS idx_bugs_project_status ON bugs(project_id, status);
            CREATE INDEX IF NOT EXISTS idx_task_queue_status_priority ON task_queue(status, priority DESC, created_at);
            CREATE INDEX IF NOT EXISTS idx_errors_status_last_seen ON errors(status, last_seen DESC);

            -- Indexes for milestone aggregation queries
            CREATE INDEX IF NOT EXISTS idx_features_milestone_status ON features(milestone_id, status);
            CREATE INDEX IF NOT EXISTS idx_bugs_milestone_status ON bugs(milestone_id, status);
        """
            )
        except sqlite3.OperationalError as e:
            # Handle existing database with column mismatches gracefully
            if "no such column" in str(e):
                print(
                    f"[WARN] Database schema mismatch during init (expected with existing DB): {e}"
                )
            else:
                raise

        # Add autopilot columns to projects if not exists
        try:
            conn.execute(
                "ALTER TABLE projects ADD COLUMN autopilot_status TEXT DEFAULT 'inactive'"
            )
        except sqlite3.OperationalError:
            pass  # Column already exists
        try:
            conn.execute(
                "ALTER TABLE projects ADD COLUMN autopilot_policy TEXT DEFAULT '{}'"
            )
        except sqlite3.OperationalError:
            pass

        # Add hierarchy columns to task_queue for parent/child relationships
        for col, default in [
            ("parent_id", "NULL"),
            ("hierarchy_level", "0"),
            ("hierarchy_path", "'/'"),
            ("child_count", "0"),
        ]:
            try:
                conn.execute(
                    f"ALTER TABLE task_queue ADD COLUMN {col} {
                        'INTEGER' if col != 'hierarchy_path' else 'TEXT'} DEFAULT {default}"
                )
            except sqlite3.OperationalError:
                pass  # Column already exists

        # Add hierarchy columns to task_archive
        for col in [
            "parent_id",
            "hierarchy_level",
            "hierarchy_path",
            "child_count",
        ]:
            try:
                conn.execute(
                    f"ALTER TABLE task_archive ADD COLUMN {col} {
                        'INTEGER' if col != 'hierarchy_path' else 'TEXT'}"
                )
            except sqlite3.OperationalError:
                pass  # Column already exists

        # Create indexes for hierarchy queries
        try:
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_task_queue_parent ON task_queue(parent_id)"
            )
        except sqlite3.OperationalError:
            pass
        try:
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_task_queue_hierarchy_level ON task_queue(hierarchy_level)"
            )
        except sqlite3.OperationalError:
            pass

        # Add soft delete columns to all core entities
        soft_delete_tables = [
            "projects",
            "milestones",
            "features",
            "bugs",
            "devops_tasks",
            "nodes",
            "errors",
            "task_queue",
            "workers",
        ]
        for table in soft_delete_tables:
            for col in ["deleted_at", "deleted_by"]:
                try:
                    col_type = "TIMESTAMP" if col == "deleted_at" else "TEXT"
                    conn.execute(
                        f"ALTER TABLE {table} ADD COLUMN {col} {col_type}"
                    )
                except sqlite3.OperationalError:
                    pass  # Column already exists
            # Create index for soft delete queries
            try:
                conn.execute(
                    f"CREATE INDEX IF NOT EXISTS idx_{table}_deleted ON {table}(deleted_at)"
                )
            except sqlite3.OperationalError:
                pass

        # Create default admin user if not exists
        cursor = conn.execute(
            "SELECT id FROM users WHERE username = ?", (CONFIG["ADMIN_USER"],)
        )
        if not cursor.fetchone():
            password_hash = hashlib.sha256(
                CONFIG["ADMIN_PASSWORD"].encode()
            ).hexdigest()
            conn.execute(
                "INSERT INTO users (username, password_hash, role) VALUES (?, ?, ?)",
                (CONFIG["ADMIN_USER"], password_hash, "admin"),
            )

        conn.commit()


# Initialize database on module load
init_database()


# ============================================================================
# AUTHENTICATION
# ============================================================================


def require_auth(f):
    """Decorator to require authentication via session or API key."""

    @wraps(f)
    def decorated(*args, **kwargs):
        # Allow public access to monitor pages
        if request.path in ["/monitor.html", "/wrapper-live"]:
            return f(*args, **kwargs)

        # First, check for API key authentication
        if request.path.startswith("/api/"):
            try:
                from services.api_keys import extract_api_key, validate_api_key

                api_key = extract_api_key(request)
                if api_key:
                    result = validate_api_key(
                        api_key,
                        db_path=str(DB_PATH),
                        endpoint=request.path,
                        method=request.method,
                        ip_address=request.remote_addr,
                        user_agent=request.headers.get("User-Agent"),
                    )
                    if result["valid"]:
                        # Store API key info in g for access in route
                        from flask import g

                        g.api_key_auth = True
                        g.api_key_user = result["user_id"]
                        g.api_key_scopes = result["scopes"]
                        g.api_key_id = result["key_id"]
                        return f(*args, **kwargs)
                    else:
                        return (
                            jsonify(
                                {
                                    "error": result.get(
                                        "error", "Invalid API key"
                                    ),
                                    "code": "API_KEY_INVALID",
                                }
                            ),
                            401,
                        )
            except Exception as e:
                logger.debug(f"API key check failed: {e}")

        # Fall back to session authentication
        if not session.get("authenticated"):
            if request.is_json or request.path.startswith("/api/"):
                return jsonify({"error": "Authentication required"}), 401
            return redirect(url_for("login"))

        # No session timeout - sessions persist until explicit logout
        # Update last activity time for monitoring purposes only
        session["last_activity"] = datetime.now().isoformat()
        return f(*args, **kwargs)

    return decorated


def require_scope(scope: str):
    """Decorator to require a specific API key scope."""

    def decorator(f):
        @wraps(f)
        def decorated(*args, **kwargs):
            from flask import g

            if hasattr(g, "api_key_auth") and g.api_key_auth:
                scopes = getattr(g, "api_key_scopes", [])
                if scope not in scopes and "admin" not in scopes:
                    return (
                        jsonify(
                            {
                                "error": f"Missing required scope: {scope}",
                                "code": "INSUFFICIENT_SCOPE",
                            }
                        ),
                        403,
                    )
            return f(*args, **kwargs)

        return decorated

    return decorator


# Rate limiting storage: {ip:endpoint: [timestamps]}
_rate_limit_store: Dict[str, List[float]] = {}
_rate_limit_lock = threading.Lock()

# Rate limit violations tracking: {ip: [{timestamp, endpoint, limit}]}
_rate_limit_violations: Dict[str, List[dict]] = {}
_rate_limit_stats = {
    "total_requests": 0,
    "total_violations": 0,
    "start_time": time.time(),
}

# Configurable rate limits per endpoint (can be modified via API)
RATE_LIMIT_CONFIG = {
    "default": 1000,  # Increased from 60
    "login": 100,  # Increased from 10
    "create": 500,  # Increased from 30
    "upload": 200,  # Increased from 20
    "export": 100,  # Increased from 10
    "batch": 150,  # Increased from 15
}

# Trusted IPs exempt from rate limiting (local network and tailscale)
RATE_LIMIT_WHITELIST = [
    "127.0.0.1",
    "localhost",
    "192.168.1.",  # Local network
    "100.112.58.",  # Tailscale network
]


def rate_limit(requests_per_minute: int = 60, per_endpoint: bool = True, resource_type: str = "default"):
    """Enhanced rate limit decorator with database persistence and auto-throttling.

    Uses sliding window algorithm. When database service is available, also tracks
    violations to database for persistence and analytics. Supports auto-throttling
    based on system load.

    Args:
        requests_per_minute: Number of requests allowed per minute
        per_endpoint: If True, limit is per endpoint; if False, global limit
        resource_type: Type of resource (login, create, upload, etc.) for granular limits
    """

    def decorator(f):
        @wraps(f)
        def decorated(*args, **kwargs):
            ip = request.remote_addr or "unknown"

            # Check if IP is whitelisted
            for whitelist_pattern in RATE_LIMIT_WHITELIST:
                if ip.startswith(whitelist_pattern) or ip == whitelist_pattern:
                    # Whitelisted IP - skip rate limiting
                    return f(*args, **kwargs)

            # Try database-backed rate limiting first if service is available
            if app.rate_limiter is not None and app.resource_monitor is not None:
                try:
                    # Check auto-throttle based on system load
                    should_throttle, throttle_reason = app.resource_monitor.should_throttle()
                    if should_throttle:
                        logger.warning(f"Auto-throttling due to {throttle_reason}")
                        response = jsonify({
                            "error": "Service temporarily overloaded",
                            "reason": throttle_reason,
                            "retry_after": 60
                        })
                        response.status_code = 503
                        response.headers["Retry-After"] = "60"
                        return response

                    # Check database-backed limit
                    allowed, info = app.rate_limiter.check_limit(
                        scope="ip",
                        scope_value=ip,
                        resource_type=resource_type,
                        request_path=request.path,
                        user_agent=request.headers.get('User-Agent', '')
                    )

                    if not allowed:
                        logger.warning(
                            f"Rate limit exceeded: ip={ip}, "
                            f"type={resource_type}, limit={info['limit']}"
                        )
                        response = jsonify({
                            "error": "Rate limit exceeded",
                            "message": f"Too many requests. Limit: {info['limit']}/{info['limit_type']}",
                            "retry_after": info['retry_after'],
                            "limit": info['limit'],
                            "limit_type": info['limit_type']
                        })
                        response.status_code = 429
                        response.headers["Retry-After"] = str(info['retry_after'])
                        response.headers["X-RateLimit-Limit"] = str(info['limit'])
                        response.headers["X-RateLimit-Remaining"] = "0"
                        return response

                except Exception as e:
                    logger.error(f"Error in database-backed rate limiting: {e}")
                    # Fall through to in-memory limiter on error

            # Fall back to in-memory rate limiting
            endpoint = request.endpoint if per_endpoint else "global"
            key = f"{ip}:{endpoint}"
            now = time.time()
            window_start = now - 60

            with _rate_limit_lock:
                _rate_limit_stats["total_requests"] += 1
                if key not in _rate_limit_store:
                    _rate_limit_store[key] = []
                _rate_limit_store[key] = [
                    ts for ts in _rate_limit_store[key] if ts > window_start
                ]
                request_count = len(_rate_limit_store[key])

                if request_count >= requests_per_minute:
                    # Track violation
                    _rate_limit_stats["total_violations"] += 1
                    if ip not in _rate_limit_violations:
                        _rate_limit_violations[ip] = []
                    _rate_limit_violations[ip].append(
                        {
                            "timestamp": now,
                            "endpoint": endpoint,
                            "limit": requests_per_minute,
                            "request_count": request_count,
                        }
                    )
                    # Keep only last 100 violations per IP
                    _rate_limit_violations[ip] = _rate_limit_violations[ip][
                        -100:
                    ]

                    oldest = (
                        min(_rate_limit_store[key])
                        if _rate_limit_store[key]
                        else now
                    )
                    retry_after = int(oldest - window_start) + 1
                    response = jsonify(
                        {
                            "error": "Rate limit exceeded",
                            "message": f"Too many requests. Limit: {requests_per_minute}/minute",
                            "retry_after": retry_after,
                        }
                    )
                    response.status_code = 429
                    response.headers["Retry-After"] = str(retry_after)
                    response.headers["X-RateLimit-Limit"] = str(
                        requests_per_minute
                    )
                    response.headers["X-RateLimit-Remaining"] = "0"
                    response.headers["X-RateLimit-Reset"] = str(
                        int(oldest + 60)
                    )
                    return response

                _rate_limit_store[key].append(now)

            response = f(*args, **kwargs)
            if hasattr(response, "headers"):
                response.headers["X-RateLimit-Limit"] = str(
                    requests_per_minute
                )
                response.headers["X-RateLimit-Remaining"] = str(
                    requests_per_minute - request_count - 1
                )
            return response

        return decorated

    return decorator


def cleanup_rate_limits():
    """Periodic cleanup of old rate limit entries."""
    with _rate_limit_lock:
        cutoff = time.time() - 120  # Keep 2 minutes of history
        for key in list(_rate_limit_store.keys()):
            _rate_limit_store[key] = [
                ts for ts in _rate_limit_store[key] if ts > cutoff
            ]
            if not _rate_limit_store[key]:
                del _rate_limit_store[key]


def log_activity(
    action: str,
    entity_type: str = None,
    entity_id: int = None,
    details: str = None,
):
    """Log user activity with retry logic to prevent database lock hangs."""
    # Capture values outside the retry function to avoid request context issues
    user_id = session.get("user_id")
    ip_address = request.remote_addr if request else None

    def _do_log(conn):
        conn.execute(
            """
            INSERT INTO activity_log (user_id, action, entity_type, entity_id, details, ip_address)
            VALUES (?, ?, ?, ?, ?, ?)
        """,
            (user_id, action, entity_type, entity_id, details, ip_address),
        )
        return True

    try:
        with get_db_connection() as conn:
            conn.execute(
                """
                INSERT INTO activity_log (user_id, action, entity_type, entity_id, details, ip_address)
                VALUES (?, ?, ?, ?, ?, ?)
            """,
                (
                    session.get("user_id"),
                    action,
                    entity_type,
                    entity_id,
                    details,
                    request.remote_addr,
                ),
            )
    except Exception as e:
        # Log but don't fail - activity logging should never block the main
        # operation
        logger.warning(f"Failed to log activity after retries: {e}")


# ============================================================================
# WEBSOCKET EVENT HANDLERS
# ============================================================================


@socketio.on("connect")
def handle_connect():
    """Authenticate WebSocket connections using Flask session."""
    try:
        if not session.get("authenticated"):
            logger.warning("WebSocket connection rejected: not authenticated")
            return False  # Reject unauthenticated connections

        username = session.get("username", "unknown")
        emit("connected", {"status": "ok", "username": username})
        logger.info(f"WebSocket connected successfully: {username}")
        return True
    except Exception as e:
        logger.error(f"WebSocket connect error: {e}", exc_info=True)
        return False


@socketio.on("disconnect")
def handle_disconnect():
    """Handle client disconnect."""
    try:
        username = session.get("username", "unknown")
        logger.info(f"WebSocket disconnected: {username}")
    except Exception as e:
        logger.error(f"WebSocket disconnect error: {e}", exc_info=True)


@socketio.on_error_default
def handle_socketio_error(e):
    """Global error handler for SocketIO events."""
    logger.error(f"SocketIO error: {e}", exc_info=True)
    emit("error", {"message": "An error occurred", "type": "socketio_error"})


@socketio.on("join_room")
def handle_join_room(data):
    """Subscribe client to a data room for targeted updates."""
    room = data.get("room") if isinstance(data, dict) else data
    valid_rooms = [
        "stats",
        "errors",
        "tmux",
        "queue",
        "nodes",
        "activity",
        "alerts",
    ]
    if room in valid_rooms:
        join_room(room)
        emit("room_joined", {"room": room})
        logger.debug(f"Client joined room: {room}")


@socketio.on("join_user_alerts")
def handle_join_user_alerts():
    """Subscribe client to user-specific alert room for task assignments."""
    user = session.get("user")
    if user:
        user_room = f"user:{user}"
        join_room(user_room)
        emit("room_joined", {"room": user_room})
        try:
            from services.task_alerts import get_alert_service

            service = get_alert_service(str(DB_PATH))
            count = service.get_unread_count(user)
            emit("unread_alerts", {"count": count})
        except Exception:
            pass


@socketio.on("leave_room")
def handle_leave_room(data):
    """Unsubscribe client from a data room."""
    room = data.get("room") if isinstance(data, dict) else data
    leave_room(room)
    emit("room_left", {"room": room})


# ============================================================================
# TYPING INDICATORS WEBSOCKET HANDLERS
# ============================================================================


@socketio.on("typing_start")
def handle_typing_start(data):
    """Handle user starting to type in an entity field."""
    if not session.get("authenticated"):
        return

    user_id = str(session.get("user_id", session.get("username")))
    username = session.get("username", "Unknown")

    entity_type = data.get("entity_type")
    entity_id = str(data.get("entity_id"))
    field = data.get("field")

    if not entity_type or not entity_id:
        return

    manager = get_typing_manager()
    result = manager.set_typing(
        user_id=user_id,
        username=username,
        entity_type=entity_type,
        entity_id=entity_id,
        field=field,
    )

    # Notify others viewing this entity
    room = f"entity:{entity_type}:{entity_id}"
    emit(
        "user_typing",
        {
            "user_id": user_id,
            "username": username,
            "entity_type": entity_type,
            "entity_id": entity_id,
            "field": field,
            "action": "start",
        },
        room=room,
        include_self=False,
    )


@socketio.on("typing_stop")
def handle_typing_stop(data):
    """Handle user stopping typing."""
    if not session.get("authenticated"):
        return

    user_id = str(session.get("user_id", session.get("username")))
    username = session.get("username", "Unknown")

    entity_type = data.get("entity_type")
    entity_id = str(data.get("entity_id")) if data.get("entity_id") else None
    field = data.get("field")

    manager = get_typing_manager()
    manager.clear_typing(user_id, entity_type, entity_id, field)

    # Notify others
    if entity_type and entity_id:
        room = f"entity:{entity_type}:{entity_id}"
        emit(
            "user_typing",
            {
                "user_id": user_id,
                "username": username,
                "entity_type": entity_type,
                "entity_id": entity_id,
                "field": field,
                "action": "stop",
            },
            room=room,
            include_self=False,
        )


@socketio.on("view_entity")
def handle_view_entity(data):
    """Handle user viewing an entity."""
    if not session.get("authenticated"):
        return

    user_id = str(session.get("user_id", session.get("username")))
    username = session.get("username", "Unknown")

    entity_type = data.get("entity_type")
    entity_id = str(data.get("entity_id"))

    if not entity_type or not entity_id:
        return

    manager = get_typing_manager()
    result = manager.set_viewing(
        user_id=user_id,
        username=username,
        entity_type=entity_type,
        entity_id=entity_id,
    )

    # Join the entity room for real-time updates
    room = f"entity:{entity_type}:{entity_id}"
    join_room(room)

    # Notify others
    emit(
        "user_viewing",
        {
            "user_id": user_id,
            "username": username,
            "entity_type": entity_type,
            "entity_id": entity_id,
            "action": "join",
        },
        room=room,
        include_self=False,
    )

    # Send current activity to the joining user
    activity = manager.get_entity_activity(entity_type, entity_id)
    emit("entity_activity", activity)


@socketio.on("leave_entity")
def handle_leave_entity(data):
    """Handle user leaving an entity view."""
    if not session.get("authenticated"):
        return

    user_id = str(session.get("user_id", session.get("username")))
    username = session.get("username", "Unknown")

    entity_type = data.get("entity_type")
    entity_id = str(data.get("entity_id")) if data.get("entity_id") else None

    manager = get_typing_manager()
    manager.clear_viewing(user_id, entity_type, entity_id)
    manager.clear_typing(user_id, entity_type, entity_id)

    if entity_type and entity_id:
        room = f"entity:{entity_type}:{entity_id}"
        leave_room(room)

        emit(
            "user_viewing",
            {
                "user_id": user_id,
                "username": username,
                "entity_type": entity_type,
                "entity_id": entity_id,
                "action": "leave",
            },
            room=room,
            include_self=False,
        )


@socketio.on("presence_heartbeat")
def handle_presence_heartbeat(data=None):
    """Handle presence heartbeat from client."""
    if not session.get("authenticated"):
        return

    user_id = str(session.get("user_id", session.get("username")))
    username = session.get("username", "Unknown")

    manager = get_typing_manager()
    manager.heartbeat(user_id, username)

    emit("heartbeat_ack", {"status": "ok"})


@socketio.on("get_entity_activity")
def handle_get_entity_activity(data):
    """Get current activity for an entity."""
    if not session.get("authenticated"):
        return

    entity_type = data.get("entity_type")
    entity_id = str(data.get("entity_id"))

    if not entity_type or not entity_id:
        return

    manager = get_typing_manager()
    activity = manager.get_entity_activity(entity_type, entity_id)
    emit("entity_activity", activity)


# ============================================================================
# WEBSOCKET BROADCAST FUNCTIONS
# ============================================================================


def broadcast_stats():
    """Broadcast dashboard stats to all clients in the stats room."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            # Get project count
            projects = conn.execute(
                "SELECT COUNT(*) as count FROM projects WHERE status = 'active'"
            ).fetchone()["count"]

            # Get feature counts by status
            features = {}
            feature_rows = conn.execute(
                """
                SELECT status, COUNT(*) as count FROM features GROUP BY status
            """
            ).fetchall()
            for row in feature_rows:
                features[row["status"]] = row["count"]
            features["total"] = sum(features.values())

            # Get bug counts
            bugs = {}
            bug_rows = conn.execute(
                """
                SELECT status, COUNT(*) as count FROM bugs GROUP BY status
            """
            ).fetchall()
            for row in bug_rows:
                bugs[row["status"]] = row["count"]
            bugs["total"] = sum(bugs.values())

            # Get error counts
            errors = {}
            error_rows = conn.execute(
                """
                SELECT status, COUNT(*) as count, SUM(occurrence_count) as occurrences
                FROM errors GROUP BY status
            """
            ).fetchall()
            for row in error_rows:
                errors[row["status"]] = {
                    "count": row["count"],
                    "occurrences": row["occurrences"] or 0,
                }

            # Get node count
            nodes = {
                "online": conn.execute(
                    "SELECT COUNT(*) FROM nodes WHERE status = 'online'"
                ).fetchone()[0]
            }

            # Get tmux session count
            tmux_sessions = conn.execute(
                "SELECT COUNT(*) FROM tmux_sessions"
            ).fetchone()[0]

            stats = {
                "projects": projects,
                "features": features,
                "bugs": bugs,
                "errors": errors,
                "nodes": nodes,
                "tmux_sessions": tmux_sessions,
                "timestamp": datetime.now().isoformat(),
            }

        socketio.emit("stats_update", stats, room="stats")
    except Exception as e:
        logger.error(f"Error broadcasting stats: {e}")


def broadcast_errors():
    """Broadcast error list to clients in the errors room."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            errors = conn.execute(
                """
                SELECT id, error_type, message, source, status, occurrence_count,
                       first_seen, last_seen, node_id
                FROM errors
                WHERE status != 'resolved'
                ORDER BY last_seen DESC
                LIMIT 100
            """
            ).fetchall()

            error_list = [dict(e) for e in errors]
        socketio.emit("errors_update", error_list, room="errors")
    except Exception as e:
        logger.error(f"Error broadcasting errors: {e}")


def broadcast_tmux():
    """Broadcast tmux session list to clients in the tmux room."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            sessions = conn.execute(
                """
                SELECT id, session_name, node_id, window_count,
                       attached, purpose, assigned_task_type, assigned_task_id,
                       last_activity, created_at
                FROM tmux_sessions
                ORDER BY session_name
            """
            ).fetchall()

            session_list = [dict(s) for s in sessions]
        socketio.emit("tmux_update", session_list, room="tmux")
    except Exception as e:
        logger.error(f"Error broadcasting tmux sessions: {e}")


def broadcast_queue():
    """Broadcast autopilot queue status to clients in the queue room."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            # Get summary counts
            ready = conn.execute(
                """
                SELECT COUNT(*) FROM autopilot_milestones WHERE status = 'pending_approval'
            """
            ).fetchone()[0]

            blocked = conn.execute(
                """
                SELECT COUNT(*) FROM autopilot_runs WHERE status = 'blocked'
            """
            ).fetchone()[0]

            in_progress = conn.execute(
                """
                SELECT COUNT(*) FROM autopilot_runs WHERE status IN ('running', 'planning', 'executing', 'testing')
            """
            ).fetchone()[0]

            # Get in-progress runs
            runs = conn.execute(
                """
                SELECT id, project_id, status, current_step, tmux_session,
                       test_pass_count, test_fail_count, started_at
                FROM autopilot_runs
                WHERE status IN ('running', 'planning', 'executing', 'testing', 'blocked')
                ORDER BY started_at DESC
                LIMIT 20
            """
            ).fetchall()

            queue_data = {
                "summary": {
                    "ready_for_approval": ready,
                    "blocked": blocked,
                    "in_progress": in_progress,
                },
                "runs": [dict(r) for r in runs],
                "timestamp": datetime.now().isoformat(),
            }

        socketio.emit("queue_update", queue_data, room="queue")
    except Exception as e:
        logger.error(f"Error broadcasting queue: {e}")


def broadcast_nodes():
    """Broadcast node status to clients in the nodes room."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            nodes = conn.execute(
                """
                SELECT id, hostname, ip_address, status, cpu_usage, memory_usage,
                       disk_usage, session_count, worker_count, last_heartbeat
                FROM nodes
                ORDER BY hostname
            """
            ).fetchall()

            node_list = [dict(n) for n in nodes]
        socketio.emit("nodes_update", node_list, room="nodes")
    except Exception as e:
        logger.error(f"Error broadcasting nodes: {e}")


def broadcast_activity(action, entity_type=None, entity_id=None, details=None):
    """Broadcast a single activity event to activity room subscribers."""
    try:
        activity = {
            "action": action,
            "entity_type": entity_type,
            "entity_id": entity_id,
            "details": details,
            "username": session.get("username"),
            "timestamp": datetime.now().isoformat(),
        }
        socketio.emit("activity_update", activity, room="activity")
    except Exception as e:
        logger.error(f"Error broadcasting activity: {e}")


# ============================================================================
# SERVER-SENT EVENTS (SSE) FOR REAL-TIME UPDATES
# ============================================================================

# Store for SSE client queues - each client gets their own queue
_sse_clients: Dict[str, Dict] = {}
_sse_lock = threading.Lock()


def sse_publish(channel: str, data: dict):
    """Publish data to all SSE clients subscribed to a channel."""
    event_data = json.dumps(data)
    with _sse_lock:
        for client_id, client_info in list(_sse_clients.items()):
            if channel in client_info.get(
                "channels", []
            ) or "all" in client_info.get("channels", []):
                try:
                    client_info["queue"].put_nowait(
                        {"event": channel, "data": event_data}
                    )
                except Exception:
                    pass  # Queue full or client disconnected


def sse_register_client(client_id: str, channels: list):
    """Register a new SSE client."""
    import queue

    with _sse_lock:
        _sse_clients[client_id] = {
            "queue": queue.Queue(maxsize=100),
            "channels": channels,
            "connected_at": datetime.now().isoformat(),
        }


def sse_unregister_client(client_id: str):
    """Unregister an SSE client."""
    with _sse_lock:
        _sse_clients.pop(client_id, None)


def generate_sse_stream(client_id: str):
    """Generator function for SSE stream."""
    import queue

    # Send initial connection event
    yield f"event: connected\ndata: {json.dumps({'client_id': client_id, 'timestamp': datetime.now().isoformat()})}\n\n"

    # Send heartbeat every 30 seconds to keep connection alive
    last_heartbeat = time.time()

    while True:
        try:
            client_info = _sse_clients.get(client_id)
            if not client_info:
                break

            # Try to get an event from the queue
            try:
                event = client_info["queue"].get(timeout=1)
                yield f"event: {event['event']}\ndata: {event['data']}\n\n"
            except queue.Empty:
                pass

            # Send heartbeat if needed
            if time.time() - last_heartbeat > 30:
                yield f"event: heartbeat\ndata: {json.dumps({'timestamp': datetime.now().isoformat()})}\n\n"
                last_heartbeat = time.time()

        except GeneratorExit:
            break
        except Exception as e:
            logger.error(f"SSE stream error for {client_id}: {e}")
            break

    sse_unregister_client(client_id)


@app.route("/api/sse/stream", methods=["GET"])
@require_auth
def sse_stream():
    """
    Server-Sent Events stream for real-time updates.

    Query params:
        channels: Comma-separated list of channels to subscribe to
                  Options: stats, errors, tmux, queue, nodes, activity, all
                  Default: all

    Example:
        GET /api/sse/stream?channels=stats,errors

    JavaScript client example:
        const evtSource = new EventSource('/api/sse/stream?channels=stats,errors');
        evtSource.addEventListener('stats', (e) => console.log(JSON.parse(e.data)));
        evtSource.addEventListener('errors', (e) => console.log(JSON.parse(e.data)));
    """
    channels_param = request.args.get("channels", "all")
    channels = [c.strip() for c in channels_param.split(",")]

    valid_channels = {
        "stats",
        "errors",
        "tmux",
        "queue",
        "nodes",
        "activity",
        "all",
    }
    channels = [c for c in channels if c in valid_channels]
    if not channels:
        channels = ["all"]

    client_id = f"sse-{uuid.uuid4().hex[:12]}"
    sse_register_client(client_id, channels)

    response = make_response(generate_sse_stream(client_id))
    response.headers["Content-Type"] = "text/event-stream"
    response.headers["Cache-Control"] = "no-cache"
    response.headers["Connection"] = "keep-alive"
    response.headers["X-Accel-Buffering"] = "no"  # Disable nginx buffering
    return response


@app.route("/api/sse/clients", methods=["GET"])
@require_auth
def get_sse_clients():
    """Get list of connected SSE clients."""
    with _sse_lock:
        clients = []
        for client_id, info in _sse_clients.items():
            clients.append(
                {
                    "client_id": client_id,
                    "channels": info["channels"],
                    "connected_at": info["connected_at"],
                    "queue_size": info["queue"].qsize(),
                }
            )
    return jsonify({"clients": clients, "count": len(clients)})


@app.route("/api/sse/publish", methods=["POST"])
@require_auth
def sse_publish_endpoint():
    """
    Manually publish an event to SSE clients.

    Request body:
        channel: Channel to publish to
        data: Data to send (will be JSON encoded)
    """
    data = request.get_json()
    if not data:
        return api_error("Request body required", 400)

    channel = data.get("channel")
    event_data = data.get("data", {})

    if not channel:
        return api_error("Channel is required", 400)

    sse_publish(channel, event_data)
    return jsonify({"success": True, "channel": channel})


# Integrate SSE with existing broadcast functions
_original_broadcast_stats = broadcast_stats


def broadcast_stats_with_sse():
    """Enhanced broadcast_stats that also publishes to SSE."""
    _original_broadcast_stats()
    # Also publish to SSE
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            stats = {}
            for table in [
                "projects",
                "milestones",
                "features",
                "bugs",
                "errors",
                "nodes",
                "tmux_sessions",
            ]:
                try:
                    count = conn.execute(
                        f"SELECT COUNT(*) as c FROM {table}"
                    ).fetchone()["c"]
                    stats[table] = count
                except Exception:
                    stats[table] = 0
            sse_publish("stats", stats)
    except Exception as e:
        logger.debug(f"SSE stats publish error: {e}")


broadcast_stats = broadcast_stats_with_sse


_original_broadcast_errors = broadcast_errors


def broadcast_errors_with_sse():
    """Enhanced broadcast_errors that also publishes to SSE."""
    _original_broadcast_errors()
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            errors = conn.execute(
                """
                SELECT id, error_type, message, source, status, occurrence_count, last_seen
                FROM errors WHERE status != 'resolved'
                ORDER BY last_seen DESC LIMIT 50
            """
            ).fetchall()
            sse_publish(
                "errors",
                {"errors": [dict(e) for e in errors], "count": len(errors)},
            )
    except Exception as e:
        logger.debug(f"SSE errors publish error: {e}")


broadcast_errors = broadcast_errors_with_sse


_original_broadcast_nodes = broadcast_nodes


def broadcast_nodes_with_sse():
    """Enhanced broadcast_nodes that also publishes to SSE."""
    _original_broadcast_nodes()
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            nodes = conn.execute(
                """
                SELECT id, hostname, ip_address, role, status, cpu_usage, memory_usage, last_heartbeat
                FROM nodes ORDER BY hostname
            """
            ).fetchall()
            sse_publish(
                "nodes",
                {"nodes": [dict(n) for n in nodes], "count": len(nodes)},
            )
    except Exception as e:
        logger.debug(f"SSE nodes publish error: {e}")


broadcast_nodes = broadcast_nodes_with_sse


# ============================================================================
# AUTHENTICATION ROUTES
# ============================================================================


@app.route("/login/simple", methods=["GET"])
def login_simple():
    """Simple HTML-only login page (no JavaScript)."""
    if session.get("authenticated"):
        return redirect(url_for("dashboard"))
    return render_template("login_simple.html")


@app.route("/login/diagnostic", methods=["GET"])
def login_diagnostic():
    """Diagnostic page to test login functionality."""
    return render_template("login_diagnostic.html")


@app.route("/monitor.html")
def monitor_html():
    """Legacy route - redirect to clean /monitor path."""
    return redirect("/monitor", code=301)


@app.route("/wrapper-live")
def wrapper_live_monitor():
    """Public Go Wrapper live monitor - unique shareable URL."""
    response = send_from_directory("static", "monitor.html")
    # Prevent aggressive caching that causes page reloads
    response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
    response.headers["Pragma"] = "no-cache"
    response.headers["Expires"] = "0"
    return response


@app.route("/api/monitor/share", methods=["POST"])
@require_auth
def create_monitor_share():
    """Generate a unique shareable URL for the monitor dashboard."""
    try:
        # Generate unique token
        share_token = str(uuid.uuid4())[:12]  # Short unique ID
        timestamp = datetime.now().isoformat()

        # Return shareable URL
        share_url = f"http://100.112.58.92:8080/monitor/{share_token}"

        return jsonify({
            "success": True,
            "token": share_token,
            "url": share_url,
            "timestamp": timestamp,
            "type": "monitor_dashboard",
            "message": f"Share this URL: {share_url}"
        }), 200
    except Exception as e:
        logger.error(f"Error creating monitor share: {e}")
        return api_error(str(e), 500, "share_creation_failed")


@app.route("/monitor/<token>")
def monitor_share(token):
    """Access monitor dashboard via unique share token."""
    try:
        # Validate token format (simple UUID format check)
        if not token or len(token) < 8:
            return render_template("login.html"), 401

        response = send_from_directory("static", "monitor.html")
        # Prevent aggressive caching
        response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
        response.headers["Pragma"] = "no-cache"
        response.headers["Expires"] = "0"
        response.headers["X-Monitor-Token"] = token
        return response
    except Exception as e:
        logger.error(f"Error accessing monitor share {token}: {e}")
        return render_template("login.html"), 401


@app.route("/login", methods=["GET", "POST"])
@rate_limit(
    requests_per_minute=100
)  # Allow legitimate automated tools and health checks
def login():
    """Login page and authentication."""
    if request.method == "GET":
        if session.get("authenticated"):
            return redirect(url_for("dashboard"))
        return render_template("login.html")

    # POST - authenticate
    try:
        if request.is_json:
            data = request.get_json(silent=True) or {}
        else:
            data = request.form
    except Exception:
        data = {}

    username = (
        (data.get("username") or "").lower().strip()
    )  # Case-insensitive, trimmed
    password = data.get("password") or ""
    remember_me = data.get(
        "remember_me", True
    )  # Default to True for persistent sessions

    password_hash = hashlib.sha256(password.encode()).hexdigest()
    user_id_for_log = None  # Initialize to avoid UnboundLocalError

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        # Case-insensitive username comparison
        user = conn.execute(
            "SELECT id, username, role FROM users WHERE LOWER(username) = LOWER(?) AND password_hash = ?",
            (username, password_hash),
        ).fetchone()

        if user:
            # Set session as permanent (uses PERMANENT_SESSION_LIFETIME from
            # app config: 365 days)
            session.permanent = True
            logger.info(
                f"User {username} logged in with remember-me: {remember_me} (persistent session: 365 days)"
            )

            session["authenticated"] = True
            session["user_id"] = user["id"]
            session["username"] = user["username"]
            session["role"] = user["role"]
            session["login_time"] = datetime.now().isoformat()
            session["last_activity"] = datetime.now().isoformat()
            session["remember_me"] = remember_me

            conn.execute(
                "UPDATE users SET last_login = CURRENT_TIMESTAMP WHERE id = ?",
                (user["id"],),
            )
            # Note: log_activity called AFTER this with block exits to avoid DB
            # lock
            user_id_for_log = user["id"]

    # Log activity OUTSIDE the db connection block to avoid lock wait
    if session.get("authenticated") and user_id_for_log:
        log_activity("login", "user", user_id_for_log)

        if request.is_json:
            return jsonify({"success": True, "redirect": url_for("dashboard")})
        return redirect(url_for("dashboard"))

    if request.is_json:
        return jsonify({"error": "Invalid credentials"}), 401
    return render_template("login.html", error="Invalid credentials")


@app.route("/logout")
def logout():
    """Logout and clear session."""
    log_activity("logout")
    session.clear()
    return redirect(url_for("login"))


@app.route("/api/session/status")
@require_auth
def session_status():
    """Get current session status including time remaining."""
    last_activity = session.get("last_activity")
    timeout_seconds = CONFIG.get("SESSION_TIMEOUT", 3600)

    if last_activity:
        last_activity_time = datetime.fromisoformat(last_activity)
        elapsed = (datetime.now() - last_activity_time).total_seconds()
        remaining = max(0, timeout_seconds - elapsed)
    else:
        remaining = timeout_seconds

    return jsonify(
        {
            "authenticated": True,
            "username": session.get("username"),
            "role": session.get("role"),
            "login_time": session.get("login_time"),
            "last_activity": session.get("last_activity"),
            "timeout_seconds": timeout_seconds,
            "remaining_seconds": int(remaining),
            "warning_threshold": 300,  # Warn when 5 minutes remaining
        }
    )


@app.route("/api/session/keepalive", methods=["POST"])
@require_auth
def session_keepalive():
    """Keep session alive by updating last activity time."""
    session["last_activity"] = datetime.now().isoformat()
    return jsonify(
        {
            "success": True,
            "last_activity": session["last_activity"],
            "timeout_seconds": CONFIG.get("SESSION_TIMEOUT", 3600),
        }
    )


# ============================================================================
# CSRF TOKEN ENDPOINTS
# ============================================================================


@app.route("/api/csrf/token", methods=["GET"])
@require_auth
def get_csrf_token():
    """Get a fresh CSRF token for the current session.

    Returns:
        JSON with token and expiry info
    """
    token = generate_csrf_token()
    return jsonify(
        {
            "token": token,
            "header_name": "X-CSRF-Token",
            "form_field": "csrf_token",
            "lifetime_seconds": 3600,
        }
    )


@app.route("/api/csrf/validate", methods=["POST"])
@require_auth
def validate_csrf_endpoint():
    """Validate a CSRF token (for debugging/testing).

    Returns:
        JSON with validation result
    """
    from csrf_protection import get_csrf_token_from_request

    token = get_csrf_token_from_request()
    is_valid, error = validate_csrf_token(token)
    return jsonify(
        {"valid": is_valid, "error": error if not is_valid else None}
    )


# ============================================================================
# CORRELATION ID ENDPOINTS
# ============================================================================


@app.route("/api/correlation/current", methods=["GET"])
@require_auth
def get_current_correlation_id():
    """Get the current request's correlation ID.

    Returns:
        JSON with correlation ID and request context
    """
    return jsonify(
        {
            "correlation_id": get_correlation_id(),
            "header_name": CORRELATION_ID_HEADER,
            "context": get_request_context(),
        }
    )


# ============================================================================
# SECURITY HEADERS ENDPOINTS
# ============================================================================


@app.route("/api/security/headers", methods=["GET"])
@require_auth
def get_security_headers_config():
    """Get the current security headers configuration.

    Returns:
        JSON with security headers config (CSP, HSTS, etc.)
    """
    return jsonify(
        {
            "config": security_headers.get_config(),
            "enabled": security_headers.enabled,
            "headers_applied": list(security_headers.headers.keys()),
        }
    )


@app.route("/api/security/headers/test", methods=["GET"])
@require_auth
def test_security_headers():
    """Test endpoint to verify security headers are applied.

    Returns the security headers that would be applied to this response.
    """
    # Get the headers that will be applied
    applied_headers = {}

    # Basic security headers
    for header, value in security_headers.headers.items():
        applied_headers[header] = value

    # CSP header
    if security_headers.csp_enabled:
        csp_directives = []
        for directive, sources in security_headers.csp.items():
            if directive == "upgrade-insecure-requests":
                # Only include upgrade-insecure-requests when running HTTPS
                if sources is not None and request.is_secure:
                    csp_directives.append(directive)
            elif sources:
                csp_directives.append(f"{directive} {' '.join(sources)}")
        applied_headers["Content-Security-Policy"] = "; ".join(csp_directives)

    # HSTS header (if HTTPS)
    if security_headers.hsts_enabled:
        applied_headers["Strict-Transport-Security"] = (
            security_headers.hsts.get(
                "Strict-Transport-Security",
                "max-age=31536000; includeSubDomains",
            )
        )

    return jsonify(
        {
            "headers": applied_headers,
            "is_secure": request.is_secure,
            "protocol": request.headers.get("X-Forwarded-Proto", "http"),
        }
    )


# ============================================================================
# DISTRIBUTED TRACING ENDPOINTS
# ============================================================================


@app.route("/api/tracing/config", methods=["GET"])
@require_auth
def get_tracing_config():
    """Get the current distributed tracing configuration.

    Returns:
        JSON with tracing config (enabled, exporter, sample rate, etc.)
    """
    return jsonify(TracingConfig.get_config())


@app.route("/api/tracing/context", methods=["GET"])
@require_auth
def get_current_trace_context():
    """Get the current trace context for this request.

    Returns:
        JSON with trace context headers for propagation
    """
    context = get_trace_context()
    return jsonify(
        {
            "trace_context": context,
            "correlation_id": get_correlation_id(),
            "propagation_headers": list(context.keys()),
        }
    )


@app.route("/api/tracing/test", methods=["GET"])
@require_auth
def test_tracing():
    """Test endpoint to verify tracing is working.

    Creates a test span and returns trace information.
    """
    import time as time_module

    with trace_span("test_span", {"test": True}) as span:
        add_span_event("test_event", {"message": "Testing tracing"})

        # Simulate some work
        time_module.sleep(0.01)

        return jsonify(
            {
                "tracing_enabled": TracingConfig.is_enabled(),
                "config": TracingConfig.get_config(),
                "test_span_created": True,
                "correlation_id": get_correlation_id(),
            }
        )


# ============================================================================
# GLOBAL ERROR HANDLERS
# ============================================================================


def api_error(message, status_code=400, error_type=None, details=None):
    """Return a standardized API error response.

    Args:
        message: Human-readable error message
        status_code: HTTP status code (default 400)
        error_type: Optional error type for categorization
        details: Optional dict with additional error details

    Returns:
        Flask JSON response with error info and appropriate status code
    """
    response = {"error": message, "status_code": status_code, "success": False}
    if error_type:
        response["error_type"] = error_type
    if details:
        response["details"] = details

    # Include correlation ID for request tracking
    correlation_id = get_correlation_id()
    if correlation_id:
        response["correlation_id"] = correlation_id

    return jsonify(response), status_code


# ============================================================================
# XSS PREVENTION - INPUT SANITIZATION
# ============================================================================


def sanitize_request_data(data, fields=None, max_lengths=None):
    """Sanitize user input data to prevent XSS attacks.

    This function should be called on all user-provided data before
    processing or storing. It escapes HTML characters and enforces
    length limits on string fields.

    Args:
        data: Dict of user input data
        fields: List of field names to sanitize (None = all string fields)
        max_lengths: Dict mapping field names to max lengths

    Returns:
        Sanitized copy of the data dict
    """
    from utils import sanitize_dict

    default_max_lengths = {
        "name": 255,
        "title": 255,
        "description": 5000,
        "content": 10000,
        "message": 2000,
        "comment": 5000,
        "notes": 5000,
        "source": 500,
        "url": 2000,
        "path": 1000,
        "email": 255,
        "username": 100,
    }
    if max_lengths:
        default_max_lengths.update(max_lengths)
    return sanitize_dict(data, fields, default_max_lengths)


def get_sanitized_json():
    """Get sanitized JSON data from the current request.

    Convenience function that retrieves and sanitizes JSON request data.
    Should be used instead of request.get_json() for user-provided data.

    Returns:
        Sanitized dict of request data, or None if no JSON data
    """
    data = request.get_json(silent=True)
    if data and isinstance(data, dict):
        return sanitize_request_data(data)
    return data


# Register CLAUDE.md template routes after helpers are defined
if "_claude_templates_loaded" in dir() and _claude_templates_loaded:
    register_claude_template_routes(
        app, get_db_connection, require_auth, api_error, log_activity
    )


def paginate_query(query_result, page=1, per_page=50, max_per_page=100):
    """Paginate a list of query results.

    Args:
        query_result: List of items to paginate
        page: Current page number (1-indexed)
        per_page: Items per page (default 50)
        max_per_page: Maximum allowed items per page (default 100)

    Returns:
        Dict with paginated data and metadata
    """
    # Ensure valid parameters
    page = max(1, int(page) if page else 1)
    per_page = min(max(1, int(per_page) if per_page else 50), max_per_page)

    total = len(query_result)
    total_pages = (total + per_page - 1) // per_page if total > 0 else 1

    # Calculate slice indices
    start = (page - 1) * per_page
    end = start + per_page

    return {
        "items": query_result[start:end],
        "pagination": {
            "page": page,
            "per_page": per_page,
            "total": total,
            "total_pages": total_pages,
            "has_next": page < total_pages,
            "has_prev": page > 1,
        },
    }


def get_pagination_params():
    """Extract pagination parameters from request args.

    Returns:
        Tuple of (page, per_page)
    """
    page = request.args.get("page", 1, type=int)
    per_page = request.args.get("per_page", 50, type=int)
    # Also support 'limit' and 'offset' style
    if "limit" in request.args:
        per_page = request.args.get("limit", 50, type=int)
    if "offset" in request.args:
        offset = request.args.get("offset", 0, type=int)
        page = (offset // per_page) + 1 if per_page > 0 else 1
    return page, per_page


def log_error_to_db(
    error_type, message, source=None, stack_trace=None, auto_queue=True
):
    """Log an error to the database for tracking and optionally queue for auto-fix."""
    try:
        import json as json_module

        with get_db_connection() as conn:
            # Check for existing error
            existing = conn.execute(
                """
                SELECT id, occurrence_count, status FROM errors
                WHERE error_type = ? AND message = ? AND source = ? AND status IN ('open', 'queued')
            """,
                (error_type, message[:500], source or "backend"),
            ).fetchone()

            error_id = None
            if existing:
                error_id = existing[0]
                conn.execute(
                    """
                    UPDATE errors SET occurrence_count = occurrence_count + 1,
                    last_seen = CURRENT_TIMESTAMP WHERE id = ?
                """,
                    (error_id,),
                )
            else:
                cursor = conn.execute(
                    """
                    INSERT INTO errors (error_type, message, source, stack_trace, status, occurrence_count)
                    VALUES (?, ?, ?, ?, 'open', 1)
                """,
                    (
                        error_type,
                        message[:500],
                        source or "backend",
                        stack_trace or "",
                    ),
                )
                error_id = cursor.lastrowid

                # Auto-queue new errors for fixing (skip transient errors)
                skip_types = [
                    "transient",
                    "info",
                    "warning",
                    "database is locked",
                ]
                should_queue = auto_queue and not any(
                    skip in error_type.lower() or skip in message.lower()
                    for skip in skip_types
                )

                if should_queue and error_id:
                    # Queue for auto-fix
                    task_data = json_module.dumps(
                        {
                            "entity_type": "error",
                            "entity_id": error_id,
                            "error_type": error_type,
                            "message": message[:200],
                            "source": source or "backend",
                        }
                    )
                    conn.execute(
                        """
                        INSERT INTO task_queue (task_type, task_data, priority, max_retries, status)
                        VALUES ('error_fix', ?, 2, 2, 'pending')
                    """,
                        (task_data,),
                    )
                    conn.execute(
                        "UPDATE errors SET status = 'queued' WHERE id = ?",
                        (error_id,),
                    )
                    logger.info(
                        f"[AutoFix] Queued error {error_id} for auto-fix: {error_type}"
                    )

            conn.commit()
            return error_id
    except Exception as e:
        logging.error(f"Failed to log error to DB: {e}")
        return None


@app.errorhandler(Exception)
def handle_exception(e):
    """Catch all unhandled exceptions and log them."""
    import traceback

    stack = traceback.format_exc()

    # Log to database
    log_error_to_db(
        error_type="backend_exception",
        message=str(e),
        source=request.path if request else "unknown",
        stack_trace=stack,
    )

    logging.error(f"Unhandled exception: {e}\n{stack}")

    # Return appropriate response - hide details in production
    if request.path.startswith("/api/"):
        if APP_ENV == "prod":
            return (
                jsonify({"error": "Internal server error", "logged": True}),
                500,
            )
        return jsonify({"error": str(e), "logged": True}), 500
    return (
        render_template("login.html", error="An unexpected error occurred"),
        500,
    )


@app.errorhandler(404)
def handle_not_found(e):
    """Handle 404 errors and log broken links."""
    # Log 404 errors to catch broken links
    referrer = request.headers.get("Referer", "direct")
    user_agent = request.headers.get("User-Agent", "unknown")[:100]

    # Skip logging for common non-issues
    skip_paths = [
        "/favicon.ico",
        "/robots.txt",
        "/sitemap.xml",
        "/.well-known/",
    ]
    skip_extensions = [".map", ".wof", ".woff2", ".ttf"]

    should_log = not any(request.path.startswith(p) for p in skip_paths)
    should_log = should_log and not any(
        request.path.endswith(ext) for ext in skip_extensions
    )

    if should_log:
        log_error_to_db(
            error_type="broken_link_404",
            message=f"404 Not Found: {request.path}",
            source=f"Referrer: {referrer}",
            stack_trace=f"User-Agent: {user_agent}\nMethod: {
                request.method}\nPath: {
                request.path}",
            auto_queue=False,  # Don't auto-queue 404s for fixing
        )

    if request.path.startswith("/api/"):
        return jsonify({"error": "Not found", "path": request.path}), 404
    return redirect(url_for("login"))


@app.errorhandler(405)
def handle_method_not_allowed(e):
    """Handle 405 errors without logging (these are usually from bots/probes)."""
    if request.path.startswith("/api/"):
        return (
            jsonify(
                {
                    "error": "Method not allowed",
                    "path": request.path,
                    "method": request.method,
                }
            ),
            405,
        )
    return redirect(url_for("login"))


@app.errorhandler(500)
def handle_internal_error(e):
    """Handle 500 errors."""
    log_error_to_db(
        "internal_error", str(e), request.path if request else "unknown"
    )
    if request.path.startswith("/api/"):
        return jsonify({"error": "Internal server error", "logged": True}), 500
    return render_template("login.html", error="Internal server error"), 500


# ============================================================================
# MAIN DASHBOARD
# ============================================================================


@app.route("/", methods=["GET", "POST"])
@require_auth
def dashboard():
    """Main dashboard view."""
    response = make_response(render_template("dashboard.html"))
    # Prevent caching to ensure clients always get latest JavaScript
    response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
    response.headers["Pragma"] = "no-cache"
    response.headers["Expires"] = "0"
    return response


@app.route("/dashboard/print", methods=["GET"])
@require_auth
def dashboard_print_view():
    """Printable/exportable dashboard snapshot view."""
    response = make_response(render_template("dashboard_print.html"))
    response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
    response.headers["Pragma"] = "no-cache"
    response.headers["Expires"] = "0"
    return response


@app.route("/strategic", methods=["GET"])
@require_auth
def strategic_dashboard():
    """Strategic dashboard - Vision, Mission, Roadmap, Resources, Revenue."""
    response = make_response(render_template("strategic_dashboard.html"))
    response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
    response.headers["Pragma"] = "no-cache"
    response.headers["Expires"] = "0"
    return response


@app.route("/llm-metrics", methods=["GET"])
@require_auth
def llm_metrics_dashboard():
    """LLM Provider Metrics Dashboard - Usage, costs, failover stats, circuit breaker status."""
    response = make_response(render_template("llm_metrics_dashboard.html"))
    response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
    response.headers["Pragma"] = "no-cache"
    response.headers["Expires"] = "0"
    return response


@app.route("/go-wrapper-monitor", methods=["GET"])
@require_auth
def go_wrapper_monitor_dashboard():
    """Go Wrapper Monitoring Dashboard - Real-time agent status, tasks, SSE updates."""
    response = make_response(render_template("go_wrapper_monitor.html"))
    response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
    response.headers["Pragma"] = "no-cache"
    response.headers["Expires"] = "0"
    return response


# ============================================================================
# PROJECT API ENDPOINTS
# ============================================================================


@app.route("/api/projects", methods=["GET"])
@require_auth
def get_projects():
    """Get all projects with optional pagination.

    Returns a list of all projects with their feature, bug, and error counts.

    Query Parameters:
        page (int): Page number for pagination (default: 1)
        per_page (int): Items per page, max 100 (default: 50)
        paginate (str): Set to 'true' to enable pagination

    Returns:
        200: List of projects or paginated result
        500: Database error

    Example Request:
        GET /api/projects
        GET /api/projects?paginate=true&page=1&per_page=20

    Example Response:
        [
            {
                "id": 1,
                "name": "Architect Dashboard",
                "description": "Project management system",
                "source_path": "/path/to/project",
                "priority": 1,
                "feature_count": 15,
                "bug_count": 3,
                "error_count": 0,
                "created_at": "2024-01-15T10:30:00"
            }
        ]

    Example with Pagination:
        {
            "items": [...],
            "page": 1,
            "per_page": 20,
            "total": 45,
            "pages": 3
        }

    cURL Example:
        curl -X GET "http://localhost:8080/api/projects" \\
             -H "Cookie: session=<session_cookie>"
    """
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            # Optimized: Use subqueries instead of multiple LEFT JOINs to avoid Cartesian product
            # Check if we should include soft-deleted projects
            include_deleted = (
                request.args.get("include_deleted", "").lower() == "true"
            )
            deleted_filter = (
                "" if include_deleted else "WHERE p.deleted_at IS NULL"
            )

            projects = conn.execute(
                f"""
                SELECT p.*,
                       COALESCE(fc.cnt, 0) as feature_count,
                       COALESCE(bc.cnt, 0) as bug_count,
                       COALESCE(ec.cnt, 0) as error_count
                FROM projects p
                LEFT JOIN (SELECT project_id, COUNT(*) as cnt FROM features WHERE deleted_at IS NULL GROUP BY project_id) fc ON p.id = fc.project_id
                LEFT JOIN (SELECT project_id, COUNT(*) as cnt FROM bugs WHERE deleted_at IS NULL GROUP BY project_id) bc ON p.id = bc.project_id
                LEFT JOIN (SELECT project_id, COUNT(*) as cnt FROM errors WHERE deleted_at IS NULL GROUP BY project_id) ec ON p.id = ec.project_id
                {deleted_filter}
                ORDER BY p.priority DESC, p.name
            """
            ).fetchall()
            project_list = [dict(p) for p in projects]

            # Support pagination if requested
            if request.args.get("paginate", "").lower() == "true":
                page, per_page = get_pagination_params()
                result = paginate_query(project_list, page, per_page)
                return jsonify(result)

            return jsonify(project_list)
    except sqlite3.Error as e:
        logger.error(f"Database error fetching projects: {e}")
        return api_error("Failed to fetch projects", 500, "database_error")


@app.route("/api/projects", methods=["POST"])
@require_auth
@rate_limit(requests_per_minute=30)
def create_project():
    """Create a new project.

    Creates a new project and sets the authenticated user as owner.

    Request Body:
        name (str, required): Project name (must be unique)
        description (str, optional): Project description
        source_path (str, optional): Path to project source code
        priority (int, optional): Priority level 0-5 (default: 0)

    Returns:
        200: Project created successfully with ID
        400: Validation error (missing name)
        409: Project name already exists
        500: Database error

    Example Request:
        POST /api/projects
        Content-Type: application/json

        {
            "name": "My New Project",
            "description": "A cool new project",
            "source_path": "/home/user/projects/my-project",
            "priority": 2
        }

    Example Response:
        {
            "id": 42,
            "success": true
        }

    cURL Example:
        curl -X POST "http://localhost:8080/api/projects" \\
             -H "Content-Type: application/json" \\
             -H "Cookie: session=<session_cookie>" \\
             -d '{"name": "My Project", "description": "Project description"}'
    """
    data = request.get_json()

    if not data:
        return api_error("Request body is required", 400, "validation_error")

    name = data.get("name")
    if not name or not name.strip():
        return api_error("Project name is required", 400, "validation_error")

    try:
        with get_db_connection() as conn:
            cursor = conn.execute(
                """
                INSERT INTO projects (name, description, source_path, priority)
                VALUES (?, ?, ?, ?)
            """,
                (
                    name.strip(),
                    data.get("description"),
                    data.get("source_path"),
                    data.get("priority", 0),
                ),
            )
            project_id = cursor.lastrowid

            # Set creator as project owner
            user_id = session.get("user_id")
            if user_id:
                project_permissions.set_project_owner(
                    conn, project_id, user_id
                )

            log_activity("create_project", "project", project_id, name)

            return jsonify({"id": project_id, "success": True})
    except sqlite3.IntegrityError as e:
        if "UNIQUE" in str(e) or "projects.name" in str(e):
            return api_error(
                "A project with this name already exists",
                409,
                "duplicate_error",
            )
        logger.error(f"Integrity error creating project: {e}")
        return api_error("Failed to create project", 400, "database_error")
    except sqlite3.Error as e:
        logger.error(f"Database error creating project: {e}")
        return api_error("Failed to create project", 500, "database_error")


# ============================================================================
# PROJECT TEMPLATES API
# ============================================================================

BUILTIN_TEMPLATES = [
    {
        "name": "Web Application",
        "description": "Full-stack web application",
        "category": "web",
        "template_data": {
            "milestones": [
                {
                    "name": "MVP",
                    "description": "Minimum viable product",
                    "features": [
                        {
                            "title": "User authentication",
                            "description": "Login, signup, password reset",
                        },
                        {
                            "title": "Core functionality",
                            "description": "Main application features",
                        },
                        {
                            "title": "Basic UI/UX",
                            "description": "Initial user interface",
                        },
                    ],
                },
                {
                    "name": "Beta Release",
                    "description": "Feature-complete for testing",
                    "features": [
                        {
                            "title": "User dashboard",
                            "description": "User profile and settings",
                        },
                        {
                            "title": "Admin panel",
                            "description": "Administrative features",
                        },
                        {
                            "title": "Notifications",
                            "description": "Email and in-app notifications",
                        },
                    ],
                },
                {
                    "name": "Production",
                    "description": "Production-ready release",
                    "features": [
                        {
                            "title": "Performance optimization",
                            "description": "Caching, CDN, optimization",
                        },
                        {
                            "title": "Security hardening",
                            "description": "Security audit and fixes",
                        },
                        {
                            "title": "Documentation",
                            "description": "User and developer docs",
                        },
                    ],
                },
            ]
        },
    },
    {
        "name": "API Service",
        "description": "RESTful API backend service",
        "category": "backend",
        "template_data": {
            "milestones": [
                {
                    "name": "Core API",
                    "description": "Essential endpoints",
                    "features": [
                        {
                            "title": "API authentication",
                            "description": "JWT/OAuth implementation",
                        },
                        {
                            "title": "CRUD endpoints",
                            "description": "Basic resource operations",
                        },
                        {
                            "title": "Input validation",
                            "description": "Request validation",
                        },
                    ],
                },
                {
                    "name": "Advanced Features",
                    "description": "Extended functionality",
                    "features": [
                        {
                            "title": "Rate limiting",
                            "description": "API rate limiting",
                        },
                        {
                            "title": "Caching layer",
                            "description": "Response caching",
                        },
                        {"title": "Webhooks", "description": "Event webhooks"},
                    ],
                },
            ]
        },
    },
    {
        "name": "CLI Tool",
        "description": "Command-line interface application",
        "category": "tools",
        "template_data": {
            "milestones": [
                {
                    "name": "Core Commands",
                    "description": "Basic functionality",
                    "features": [
                        {
                            "title": "Argument parsing",
                            "description": "CLI argument handling",
                        },
                        {
                            "title": "Core commands",
                            "description": "Main command implementations",
                        },
                        {
                            "title": "Configuration",
                            "description": "Config file support",
                        },
                    ],
                },
                {
                    "name": "Polish",
                    "description": "UX improvements",
                    "features": [
                        {
                            "title": "Help documentation",
                            "description": "Built-in help",
                        },
                        {
                            "title": "Shell completion",
                            "description": "Bash/Zsh completion",
                        },
                        {
                            "title": "Error handling",
                            "description": "Friendly error messages",
                        },
                    ],
                },
            ]
        },
    },
    {
        "name": "Mobile App",
        "description": "Cross-platform mobile application",
        "category": "mobile",
        "template_data": {
            "milestones": [
                {
                    "name": "Core App",
                    "description": "Basic app functionality",
                    "features": [
                        {
                            "title": "App navigation",
                            "description": "Screen navigation and routing",
                        },
                        {
                            "title": "User authentication",
                            "description": "Login and signup flows",
                        },
                        {
                            "title": "Offline support",
                            "description": "Local data storage",
                        },
                    ],
                },
                {
                    "name": "Release",
                    "description": "App store release",
                    "features": [
                        {
                            "title": "Push notifications",
                            "description": "Remote notifications",
                        },
                        {
                            "title": "App store assets",
                            "description": "Screenshots, descriptions",
                        },
                        {
                            "title": "Analytics",
                            "description": "Usage tracking",
                        },
                    ],
                },
            ]
        },
    },
    {
        "name": "Data Pipeline",
        "description": "ETL/data processing pipeline",
        "category": "data",
        "template_data": {
            "milestones": [
                {
                    "name": "Pipeline Setup",
                    "description": "Core pipeline",
                    "features": [
                        {
                            "title": "Data ingestion",
                            "description": "Data source connectors",
                        },
                        {
                            "title": "Transformation logic",
                            "description": "Data cleaning",
                        },
                        {
                            "title": "Data validation",
                            "description": "Quality checks",
                        },
                    ],
                },
                {
                    "name": "Production",
                    "description": "Production deployment",
                    "features": [
                        {
                            "title": "Scheduling",
                            "description": "Automated scheduling",
                        },
                        {
                            "title": "Monitoring",
                            "description": "Pipeline monitoring",
                        },
                        {
                            "title": "Error recovery",
                            "description": "Failure handling",
                        },
                    ],
                },
            ]
        },
    },
]


@app.route("/api/templates", methods=["GET"])
@require_auth
def get_project_templates():
    """List all project templates."""
    category = request.args.get("category")
    include_builtin = (
        request.args.get("include_builtin", "true").lower() == "true"
    )
    templates = []
    if include_builtin:
        for idx, t in enumerate(BUILTIN_TEMPLATES):
            if category and t["category"] != category:
                continue
            templates.append(
                {
                    "id": f"builtin_{idx}",
                    "name": t["name"],
                    "description": t["description"],
                    "category": t["category"],
                    "is_builtin": True,
                    "template_data": t["template_data"],
                }
            )
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        q = (
            "SELECT * FROM project_templates"
            + (" WHERE category = ?" if category else "")
            + " ORDER BY usage_count DESC, name"
        )
        for r in conn.execute(q, [category] if category else []).fetchall():
            templates.append(
                {
                    "id": r["id"],
                    "name": r["name"],
                    "description": r["description"],
                    "category": r["category"],
                    "is_builtin": False,
                    "usage_count": r["usage_count"],
                    "template_data": json.loads(r["template_data"]),
                }
            )
    return jsonify({"templates": templates})


@app.route("/api/templates", methods=["POST"])
@require_auth
def create_project_template():
    """Create a new project template."""
    data = request.get_json() or {}
    if not data.get("name"):
        return jsonify({"error": "Template name is required"}), 400
    try:
        with get_db_connection() as conn:
            c = conn.execute(
                "INSERT INTO project_templates (name,description,category,template_data,created_by) VALUES (?,?,?,?,?)",
                (
                    data["name"].strip(),
                    data.get("description"),
                    data.get("category", "general"),
                    json.dumps(data.get("template_data", {})),
                    session.get("user", "unknown"),
                ),
            )
            log_activity(
                "create_template",
                "project_template",
                c.lastrowid,
                data["name"],
            )
            return jsonify({"id": c.lastrowid, "success": True})
    except sqlite3.IntegrityError:
        return (
            jsonify({"error": "A template with this name already exists"}),
            409,
        )


@app.route("/api/templates/<template_id>", methods=["GET"])
@require_auth
def get_project_template(template_id):
    """Get a single project template."""
    if str(template_id).startswith("builtin_"):
        idx = int(template_id.replace("builtin_", ""))
        if 0 <= idx < len(BUILTIN_TEMPLATES):
            t = BUILTIN_TEMPLATES[idx]
            return jsonify(
                {
                    "id": template_id,
                    "name": t["name"],
                    "description": t["description"],
                    "category": t["category"],
                    "is_builtin": True,
                    "template_data": t["template_data"],
                }
            )
        return jsonify({"error": "Template not found"}), 404
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        r = conn.execute(
            "SELECT * FROM project_templates WHERE id = ?", (template_id,)
        ).fetchone()
        return (
            jsonify(
                dict(r)
                | {
                    "template_data": json.loads(r["template_data"]),
                    "is_builtin": False,
                }
            )
            if r
            else (jsonify({"error": "Not found"}), 404)
        )


@app.route("/api/templates/<int:template_id>", methods=["PUT"])
@require_auth
def update_project_template(template_id):
    """Update a project template."""
    data = request.get_json() or {}
    with get_db_connection() as conn:
        if not conn.execute(
            "SELECT 1 FROM project_templates WHERE id=?", (template_id,)
        ).fetchone():
            return jsonify({"error": "Template not found"}), 404
        updates, params = [], []
        for k in ["name", "description", "category"]:
            if k in data:
                updates.append(f"{k}=?")
                params.append(data[k])
        if "template_data" in data:
            updates.append("template_data=?")
            params.append(json.dumps(data["template_data"]))
        if updates:
            conn.execute(
                f"UPDATE project_templates SET {
                    ','.join(updates)},updated_at=CURRENT_TIMESTAMP WHERE id=?",
                params + [template_id],
            )
            log_activity("update_template", "project_template", template_id)
        return jsonify({"success": True})


@app.route("/api/templates/<int:template_id>", methods=["DELETE"])
@require_auth
def delete_project_template(template_id):
    """Delete a project template."""
    with get_db_connection() as conn:
        r = conn.execute(
            "SELECT name FROM project_templates WHERE id=?", (template_id,)
        ).fetchone()
        if not r:
            return jsonify({"error": "Template not found"}), 404
        conn.execute(
            "DELETE FROM project_templates WHERE id=?", (template_id,)
        )
        log_activity("delete_template", "project_template", template_id, r[0])
        return jsonify({"success": True})


@app.route("/api/projects/from-template", methods=["POST"])
@require_auth
def create_project_from_template():
    """Create a new project from a template."""
    data = request.get_json() or {}
    template_id, project_name = data.get("template_id"), data.get("name")
    if not template_id:
        return jsonify({"error": "template_id is required"}), 400
    if not project_name:
        return jsonify({"error": "Project name is required"}), 400
    template_data, is_builtin = None, str(template_id).startswith("builtin_")
    if is_builtin:
        idx = int(str(template_id).replace("builtin_", ""))
        if 0 <= idx < len(BUILTIN_TEMPLATES):
            template_data = BUILTIN_TEMPLATES[idx]["template_data"]
    else:
        with get_db_connection() as conn:
            r = conn.execute(
                "SELECT template_data FROM project_templates WHERE id=?",
                (template_id,),
            ).fetchone()
            if r:
                template_data = json.loads(r[0])
    if not template_data:
        return jsonify({"error": "Template not found"}), 404
    try:
        with get_db_connection() as conn:
            c = conn.execute(
                "INSERT INTO projects (name,description,source_path,priority) VALUES (?,?,?,?)",
                (
                    project_name.strip(),
                    data.get("description", ""),
                    data.get("source_path"),
                    data.get("priority", 0),
                ),
            )
            project_id = c.lastrowid
            milestones_created, features_created = [], []
            for m in template_data.get("milestones", []):
                mc = conn.execute(
                    "INSERT INTO milestones (project_id,name,description,status) VALUES (?,?,?,'open')",
                    (
                        project_id,
                        m.get("name", "Milestone"),
                        m.get("description", ""),
                    ),
                )
                milestone_id = mc.lastrowid
                milestones_created.append(
                    {"id": milestone_id, "name": m.get("name")}
                )
                for f in m.get("features", []):
                    fc = conn.execute(
                        "INSERT INTO features (project_id,milestone_id,title,description,status,priority) VALUES (?,?,?,?,'open',?)",
                        (
                            project_id,
                            milestone_id,
                            f.get("title", "Feature"),
                            f.get("description", ""),
                            f.get("priority", 0),
                        ),
                    )
                    features_created.append(
                        {"id": fc.lastrowid, "title": f.get("title")}
                    )
            if not is_builtin:
                conn.execute(
                    "UPDATE project_templates SET usage_count=usage_count+1 WHERE id=?",
                    (template_id,),
                )
            log_activity(
                "create_from_template",
                "project",
                project_id,
                f"{project_name} from {template_id}",
            )
            return jsonify(
                {
                    "success": True,
                    "project_id": project_id,
                    "milestones_created": len(milestones_created),
                    "features_created": len(features_created),
                    "milestones": milestones_created,
                    "features": features_created,
                }
            )
    except sqlite3.IntegrityError as e:
        return (
            jsonify(
                {
                    "error": (
                        "A project with this name already exists"
                        if "UNIQUE" in str(e)
                        else str(e)
                    )
                }
            ),
            409,
        )


@app.route("/api/templates/categories", methods=["GET"])
@require_auth
def get_template_categories():
    """Get all template categories."""
    categories = {t["category"] for t in BUILTIN_TEMPLATES}
    with get_db_connection() as conn:
        for r in conn.execute(
            "SELECT DISTINCT category FROM project_templates"
        ).fetchall():
            if r[0]:
                categories.add(r[0])
    return jsonify({"categories": sorted(list(categories))})


@app.route("/api/projects/<int:project_id>/save-as-template", methods=["POST"])
@require_auth
def save_project_as_template(project_id):
    """Save an existing project structure as a template."""
    data = request.get_json() or {}
    if not data.get("name"):
        return jsonify({"error": "Template name is required"}), 400
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        project = conn.execute(
            "SELECT * FROM projects WHERE id=?", (project_id,)
        ).fetchone()
        if not project:
            return jsonify({"error": "Project not found"}), 404
        template_milestones = []
        for m in conn.execute(
            "SELECT id,name,description FROM milestones WHERE project_id=? ORDER BY id",
            (project_id,),
        ).fetchall():
            features = [
                {
                    "title": f["title"],
                    "description": f["description"],
                    "priority": f["priority"],
                }
                for f in conn.execute(
                    "SELECT title,description,priority FROM features WHERE milestone_id=?",
                    (m["id"],),
                ).fetchall()
            ]
            template_milestones.append(
                {
                    "name": m["name"],
                    "description": m["description"],
                    "features": features,
                }
            )
        try:
            c = conn.execute(
                "INSERT INTO project_templates (name,description,category,template_data,created_by) VALUES (?,?,?,?,?)",
                (
                    data["name"].strip(),
                    data.get(
                        "description",
                        f"From: {
                            project['name']}",
                    ),
                    data.get("category", "custom"),
                    json.dumps(
                        {
                            "milestones": template_milestones,
                            "source_project": project["name"],
                        }
                    ),
                    session.get("user", "unknown"),
                ),
            )
            log_activity(
                "save_as_template",
                "project_template",
                c.lastrowid,
                data["name"],
            )
            return jsonify(
                {
                    "success": True,
                    "template_id": c.lastrowid,
                    "milestones_saved": len(template_milestones),
                    "features_saved": sum(
                        len(m["features"]) for m in template_milestones
                    ),
                }
            )
        except sqlite3.IntegrityError:
            return (
                jsonify({"error": "A template with this name already exists"}),
                409,
            )


@app.route("/api/projects/<int:project_id>", methods=["GET"])
@require_auth
def get_project(project_id):
    """Get a single project by ID."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        project = conn.execute(
            "SELECT * FROM projects WHERE id = ?", (project_id,)
        ).fetchone()

        if not project:
            return jsonify({"error": "Project not found"}), 404

        return jsonify(dict(project))


@app.route("/api/projects/<int:project_id>", methods=["PUT"])
@require_auth
def update_project(project_id):
    """Update a project."""
    data = request.get_json()

    with get_db_connection() as conn:
        conn.execute(
            """
            UPDATE projects SET
                name = COALESCE(?, name),
                description = COALESCE(?, description),
                source_path = COALESCE(?, source_path),
                status = COALESCE(?, status),
                priority = COALESCE(?, priority),
                updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (
                data.get("name"),
                data.get("description"),
                data.get("source_path"),
                data.get("status"),
                data.get("priority"),
                project_id,
            ),
        )

        log_activity("update_project", "project", project_id)

        return jsonify({"success": True})


@app.route("/api/projects/<int:project_id>", methods=["DELETE"])
@require_auth
def delete_project(project_id):
    """Delete a project (soft delete).

    Query Parameters:
        hard (bool): If 'true', permanently delete instead of soft delete
        cascade (bool): If 'true', also soft delete related milestones, features, bugs
    """
    # Check if user has permission to delete
    user_id = session.get("user_id")
    username = session.get("username")
    hard_delete = request.args.get("hard", "").lower() == "true"
    cascade = request.args.get("cascade", "").lower() == "true"

    with get_db_connection() as conn:
        if not project_permissions.check_project_access(
            conn, user_id, project_id, "owner"
        ):
            return api_error(
                "Only project owners can delete projects",
                403,
                "permission_denied",
            )

        if hard_delete:
            # Permanent delete
            conn.execute("DELETE FROM projects WHERE id = ?", (project_id,))
            log_activity("hard_delete_project", "project", project_id)
        else:
            # Soft delete
            conn.execute(
                """
                UPDATE projects SET
                    deleted_at = CURRENT_TIMESTAMP,
                    deleted_by = ?,
                    status = 'archived',
                    updated_at = CURRENT_TIMESTAMP
                WHERE id = ? AND deleted_at IS NULL
            """,
                (username, project_id),
            )

            if cascade:
                # Cascade soft delete to related entities
                for table in ["milestones", "features", "bugs"]:
                    conn.execute(
                        """
                        UPDATE {table} SET
                            deleted_at = CURRENT_TIMESTAMP,
                            deleted_by = ?
                        WHERE project_id = ? AND deleted_at IS NULL
                    """,
                        (username, project_id),
                    )

            log_activity("soft_delete_project", "project", project_id)

        return jsonify(
            {
                "success": True,
                "soft_delete": not hard_delete,
                "cascade": cascade,
            }
        )


@app.route("/api/projects/<int:project_id>/restore", methods=["POST"])
@require_auth
def restore_project(project_id):
    """Restore a soft-deleted project.

    Query Parameters:
        cascade (bool): If 'true', also restore related milestones, features, bugs
    """
    user_id = session.get("user_id")
    cascade = request.args.get("cascade", "").lower() == "true"

    with get_db_connection() as conn:
        if not project_permissions.check_project_access(
            conn, user_id, project_id, "owner"
        ):
            return api_error(
                "Only project owners can restore projects",
                403,
                "permission_denied",
            )

        result = conn.execute(
            """
            UPDATE projects SET
                deleted_at = NULL,
                deleted_by = NULL,
                status = 'active',
                updated_at = CURRENT_TIMESTAMP
            WHERE id = ? AND deleted_at IS NOT NULL
        """,
            (project_id,),
        )

        if result.rowcount == 0:
            return api_error(
                "Project not found or not deleted", 404, "not_found"
            )

        restored_counts = {"projects": 1}

        if cascade:
            for table in ["milestones", "features", "bugs"]:
                count = conn.execute(
                    """
                    UPDATE {table} SET
                        deleted_at = NULL,
                        deleted_by = NULL
                    WHERE project_id = ? AND deleted_at IS NOT NULL
                """,
                    (project_id,),
                ).rowcount
                restored_counts[table] = count

        log_activity(
            "restore_project", "project", project_id, f"cascade={cascade}"
        )

    return jsonify({"success": True, "restored": restored_counts})


@app.route("/api/projects/deleted", methods=["GET"])
@require_auth
def get_deleted_projects():
    """Get all soft-deleted projects."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        projects = conn.execute(
            """
            SELECT * FROM projects
            WHERE deleted_at IS NOT NULL
            ORDER BY deleted_at DESC
        """
        ).fetchall()

    return jsonify([dict(p) for p in projects])


# ============================================================================
# USERS API ENDPOINTS
# ============================================================================


@app.route("/api/users", methods=["GET"])
@require_auth
def get_users():
    """Get list of users.

    Returns list of users (for admin/project permission management).
    Sensitive data (password_hash) is not returned.
    """
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        rows = conn.execute(
            """
            SELECT id, username, role, created_at, last_login
            FROM users
            ORDER BY username
        """
        ).fetchall()

        return jsonify([dict(row) for row in rows])


# ============================================================================
# PROJECT PERMISSIONS API ENDPOINTS
# ============================================================================


@app.route("/api/projects/<int:project_id>/access", methods=["GET"])
@require_auth
def get_project_access(project_id):
    """Get current user's access level for a project.

    Returns:
        access_level: User's access level (read, write, admin, owner) or null
        capabilities: What the user can do with this access level
    """
    user_id = session.get("user_id")
    with get_db_connection() as conn:
        access_level = project_permissions.get_user_project_access(
            conn, user_id, project_id
        )
        if access_level:
            capabilities = project_permissions.ACCESS_CAPABILITIES.get(
                access_level, []
            )
        else:
            capabilities = []

        return jsonify(
            {
                "project_id": project_id,
                "user_id": user_id,
                "access_level": access_level,
                "capabilities": capabilities,
            }
        )


@app.route("/api/projects/<int:project_id>/members", methods=["GET"])
@require_auth
def get_project_members(project_id):
    """Get all members of a project.

    Query params:
        include_summary: Include access level summary (default: false)

    Returns:
        members: List of project members with their access levels
        summary: (optional) Counts by access level
    """
    user_id = session.get("user_id")
    with get_db_connection() as conn:
        # Check if user has at least read access
        if not project_permissions.check_project_access(
            conn, user_id, project_id, "read"
        ):
            return api_error("Access denied", 403, "permission_denied")

        members = project_permissions.get_project_members(conn, project_id)

        result = {"members": members}

        if request.args.get("include_summary", "").lower() == "true":
            result["summary"] = project_permissions.get_project_access_summary(
                conn, project_id
            )

        return jsonify(result)


@app.route("/api/projects/<int:project_id>/members", methods=["POST"])
@require_auth
def add_project_member(project_id):
    """Add a member to a project.

    Request body:
        user_id: User ID to add
        access_level: Access level to grant (read, write, admin, owner)

    Returns:
        success: True if added
        member: Added member info
    """
    data = request.get_json()
    if not data:
        return api_error("Request body is required", 400, "validation_error")

    target_user_id = data.get("user_id")
    access_level = data.get("access_level", "read")

    if not target_user_id:
        return api_error("user_id is required", 400, "validation_error")

    added_by = session.get("user_id")

    try:
        with get_db_connection() as conn:
            result = project_permissions.add_project_member(
                conn, project_id, target_user_id, access_level, added_by
            )
            log_activity(
                "add_project_member",
                "project",
                project_id,
                f"Added user {target_user_id} as {access_level}",
            )
            return jsonify(result)
    except ValueError as e:
        return api_error(str(e), 400, "validation_error")
    except sqlite3.Error as e:
        logger.error(f"Database error adding project member: {e}")
        return api_error("Failed to add member", 500, "database_error")


@app.route(
    "/api/projects/<int:project_id>/members/<int:member_user_id>",
    methods=["PUT"],
)
@require_auth
def update_project_member(project_id, member_user_id):
    """Update a project member's access level.

    Request body:
        access_level: New access level (read, write, admin, owner)

    Returns:
        success: True if updated
        old_level: Previous access level
        new_level: New access level
    """
    data = request.get_json()
    if not data:
        return api_error("Request body is required", 400, "validation_error")

    new_level = data.get("access_level")
    if not new_level:
        return api_error("access_level is required", 400, "validation_error")

    updated_by = session.get("user_id")

    try:
        with get_db_connection() as conn:
            result = project_permissions.update_project_member(
                conn, project_id, member_user_id, new_level, updated_by
            )
            log_activity(
                "update_project_member",
                "project",
                project_id,
                f"Updated user {member_user_id} to {new_level}",
            )
            return jsonify(result)
    except ValueError as e:
        return api_error(str(e), 400, "validation_error")
    except sqlite3.Error as e:
        logger.error(f"Database error updating project member: {e}")
        return api_error("Failed to update member", 500, "database_error")


@app.route(
    "/api/projects/<int:project_id>/members/<int:member_user_id>",
    methods=["DELETE"],
)
@require_auth
def remove_project_member(project_id, member_user_id):
    """Remove a member from a project.

    Returns:
        success: True if removed
    """
    removed_by = session.get("user_id")

    try:
        with get_db_connection() as conn:
            result = project_permissions.remove_project_member(
                conn, project_id, member_user_id, removed_by
            )
            log_activity(
                "remove_project_member",
                "project",
                project_id,
                f"Removed user {member_user_id}",
            )
            return jsonify(result)
    except ValueError as e:
        return api_error(str(e), 400, "validation_error")
    except sqlite3.Error as e:
        logger.error(f"Database error removing project member: {e}")
        return api_error("Failed to remove member", 500, "database_error")


@app.route("/api/projects/<int:project_id>/invitations", methods=["GET"])
@require_auth
def get_project_invitations(project_id):
    """Get pending invitations for a project.

    Query params:
        status: Filter by status (pending, accepted, declined, expired)

    Returns:
        invitations: List of invitations
    """
    user_id = session.get("user_id")
    status = request.args.get("status")

    with get_db_connection() as conn:
        # Check if user has admin access
        if not project_permissions.check_project_access(
            conn, user_id, project_id, "admin"
        ):
            return api_error("Admin access required", 403, "permission_denied")

        invitations = project_permissions.get_project_invitations(
            conn, project_id=project_id, status=status
        )
        return jsonify({"invitations": invitations})


@app.route("/api/projects/<int:project_id>/invitations", methods=["POST"])
@require_auth
def create_project_invitation(project_id):
    """Create an invitation to join a project.

    Request body:
        user_id: User ID to invite (optional if email provided)
        email: Email to invite (optional if user_id provided)
        access_level: Access level to grant on acceptance
        message: Optional message to include

    Returns:
        invitation: Created invitation details
    """
    data = request.get_json()
    if not data:
        return api_error("Request body is required", 400, "validation_error")

    invited_by = session.get("user_id")

    try:
        with get_db_connection() as conn:
            invitation = project_permissions.create_project_invitation(
                conn,
                project_id,
                invited_by,
                user_id=data.get("user_id"),
                email=data.get("email"),
                access_level=data.get("access_level", "read"),
                message=data.get("message"),
            )
            log_activity(
                "create_invitation",
                "project",
                project_id,
                "Invited user to project",
            )
            return jsonify(invitation)
    except ValueError as e:
        return api_error(str(e), 400, "validation_error")
    except sqlite3.Error as e:
        logger.error(f"Database error creating invitation: {e}")
        return api_error("Failed to create invitation", 500, "database_error")


@app.route("/api/invitations/pending", methods=["GET"])
@require_auth
def get_my_pending_invitations():
    """Get pending invitations for the current user.

    Returns:
        invitations: List of pending invitations
    """
    user_id = session.get("user_id")

    with get_db_connection() as conn:
        invitations = project_permissions.get_project_invitations(
            conn, user_id=user_id, status="pending"
        )
        return jsonify({"invitations": invitations})


@app.route("/api/invitations/<int:invitation_id>/accept", methods=["POST"])
@require_auth
def accept_invitation(invitation_id):
    """Accept a project invitation.

    Returns:
        success: True if accepted
        project_id: ID of the project joined
        access_level: Access level granted
    """
    user_id = session.get("user_id")

    try:
        with get_db_connection() as conn:
            result = project_permissions.respond_to_invitation(
                conn, invitation_id=invitation_id, user_id=user_id, accept=True
            )
            log_activity(
                "accept_invitation",
                "project",
                result.get("project_id"),
                "Accepted invitation",
            )
            return jsonify(result)
    except ValueError as e:
        return api_error(str(e), 400, "validation_error")
    except sqlite3.Error as e:
        logger.error(f"Database error accepting invitation: {e}")
        return api_error("Failed to accept invitation", 500, "database_error")


@app.route("/api/invitations/<int:invitation_id>/decline", methods=["POST"])
@require_auth
def decline_invitation(invitation_id):
    """Decline a project invitation.

    Returns:
        success: True if declined
    """
    user_id = session.get("user_id")

    try:
        with get_db_connection() as conn:
            result = project_permissions.respond_to_invitation(
                conn,
                invitation_id=invitation_id,
                user_id=user_id,
                accept=False,
            )
            return jsonify(result)
    except ValueError as e:
        return api_error(str(e), 400, "validation_error")
    except sqlite3.Error as e:
        logger.error(f"Database error declining invitation: {e}")
        return api_error("Failed to decline invitation", 500, "database_error")


@app.route("/api/invitations/token/<token>", methods=["POST"])
def respond_to_invitation_by_token(token):
    """Accept or decline an invitation using the token (for email links).

    Query params:
        accept: 'true' to accept, 'false' to decline

    Returns:
        success: True if responded
    """
    accept = request.args.get("accept", "true").lower() == "true"

    # User must be logged in
    if not session.get("authenticated"):
        return api_error(
            "Please log in to respond to this invitation", 401, "auth_required"
        )

    user_id = session.get("user_id")

    try:
        with get_db_connection() as conn:
            result = project_permissions.respond_to_invitation(
                conn, token=token, user_id=user_id, accept=accept
            )
            if accept:
                log_activity(
                    "accept_invitation",
                    "project",
                    result.get("project_id"),
                    "Accepted invitation via token",
                )
            return jsonify(result)
    except ValueError as e:
        return api_error(str(e), 400, "validation_error")
    except sqlite3.Error as e:
        logger.error(f"Database error responding to invitation: {e}")
        return api_error(
            "Failed to respond to invitation", 500, "database_error"
        )


@app.route("/api/projects/<int:project_id>/permissions/log", methods=["GET"])
@require_auth
def get_project_permissions_log(project_id):
    """Get the permission change audit log for a project.

    Query params:
        action: Filter by action type
        limit: Max entries (default 50)
        offset: Pagination offset

    Returns:
        entries: List of log entries
    """
    user_id = session.get("user_id")
    action = request.args.get("action")
    limit = request.args.get("limit", 50, type=int)
    offset = request.args.get("offset", 0, type=int)

    with get_db_connection() as conn:
        # Check if user has admin access
        if not project_permissions.check_project_access(
            conn, user_id, project_id, "admin"
        ):
            return api_error("Admin access required", 403, "permission_denied")

        entries = project_permissions.get_permission_log(
            conn,
            project_id=project_id,
            action=action,
            limit=limit,
            offset=offset,
        )
        return jsonify({"entries": entries})


@app.route("/api/projects/accessible", methods=["GET"])
@require_auth
def get_accessible_projects():
    """Get all projects the current user has access to.

    Returns:
        projects: List of projects with access level info
    """
    user_id = session.get("user_id")

    with get_db_connection() as conn:
        projects = project_permissions.get_user_accessible_projects(
            conn, user_id
        )
        return jsonify({"projects": projects})


@app.route("/api/projects/health", methods=["GET"])
@require_auth
def get_projects_health():
    """
    Get health scores for all active projects.

    Health score (0-100) based on:
    - Open bugs (critical/high severity) - reduces score
    - Feature completion rate - increases score
    - Milestone progress - increases score
    - Recent errors - reduces score
    - Stale items (no updates in 7+ days) - reduces score
    """
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        projects = conn.execute(
            """
            SELECT id, name, status, priority FROM projects
            WHERE status = 'active' ORDER BY priority DESC, name
        """
        ).fetchall()

        health_results = []
        for project in projects:
            project_id = project["id"]
            health_score = 100
            breakdown = {}

            # Bug impact (-40 max)
            bugs = conn.execute(
                """
                SELECT COUNT(*) as total,
                    SUM(CASE WHEN status = 'open' THEN 1 ELSE 0 END) as open_bugs,
                    SUM(CASE WHEN status = 'open' AND severity = 'critical' THEN 1 ELSE 0 END) as critical,
                    SUM(CASE WHEN status = 'open' AND severity = 'high' THEN 1 ELSE 0 END) as high,
                    SUM(CASE WHEN status = 'open' AND severity = 'medium' THEN 1 ELSE 0 END) as medium
                FROM bugs WHERE project_id = ?
            """,
                (project_id,),
            ).fetchone()

            bug_penalty = (
                (bugs["critical"] or 0) * 10
                + (bugs["high"] or 0) * 5
                + (bugs["medium"] or 0) * 2
            )
            bug_penalty = min(bug_penalty, 40)
            health_score -= bug_penalty
            breakdown["bugs"] = {
                "impact": -bug_penalty,
                "open": bugs["open_bugs"] or 0,
                "critical": bugs["critical"] or 0,
                "high": bugs["high"] or 0,
            }

            # Feature completion (+20 max)
            features = conn.execute(
                """
                SELECT COUNT(*) as total,
                    SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completed
                FROM features WHERE project_id = ?
            """,
                (project_id,),
            ).fetchone()

            total_f = features["total"] or 0
            completed_f = features["completed"] or 0
            completion_rate = (
                (completed_f / total_f * 100) if total_f > 0 else 0
            )
            feature_bonus = (
                20
                if completion_rate >= 80
                else (10 if completion_rate >= 50 else 0)
            )
            health_score += feature_bonus
            breakdown["features"] = {
                "impact": feature_bonus,
                "total": total_f,
                "completed": completed_f,
                "rate": round(completion_rate, 1),
            }

            # Milestone progress (+15 max)
            milestones = conn.execute(
                """
                SELECT COUNT(*) as total, AVG(progress) as avg_progress,
                    SUM(CASE WHEN target_date < date('now') AND status != 'completed' THEN 1 ELSE 0 END) as overdue
                FROM milestones WHERE project_id = ?
            """,
                (project_id,),
            ).fetchone()

            avg_prog = milestones["avg_progress"] or 0
            overdue = milestones["overdue"] or 0
            milestone_bonus = (avg_prog / 100) * 15 - overdue * 5
            health_score += milestone_bonus
            breakdown["milestones"] = {
                "impact": round(milestone_bonus, 1),
                "avg_progress": round(avg_prog, 1),
                "overdue": overdue,
            }

            # Recent errors (-15 max)
            errors = conn.execute(
                """
                SELECT SUM(CASE WHEN created_at > datetime('now', '-7 days') THEN 1 ELSE 0 END) as recent,
                    SUM(CASE WHEN resolved = 0 THEN 1 ELSE 0 END) as unresolved
                FROM errors WHERE project_id = ?
            """,
                (project_id,),
            ).fetchone()

            error_penalty = min(
                (errors["recent"] or 0) * 1.5
                + (errors["unresolved"] or 0) * 0.5,
                15,
            )
            health_score -= error_penalty
            breakdown["errors"] = {
                "impact": -round(error_penalty, 1),
                "recent": errors["recent"] or 0,
                "unresolved": errors["unresolved"] or 0,
            }

            health_score = max(0, min(100, health_score))
            status = (
                "healthy"
                if health_score >= 80
                else (
                    "warning"
                    if health_score >= 60
                    else ("at_risk" if health_score >= 40 else "critical")
                )
            )

            health_results.append(
                {
                    "project_id": project_id,
                    "name": project["name"],
                    "health_score": round(health_score, 1),
                    "status": status,
                    "priority": project["priority"],
                    "breakdown": breakdown,
                }
            )

        health_results.sort(key=lambda x: x["health_score"])
        total = len(health_results)
        summary = {
            "total": total,
            "healthy": len(
                [p for p in health_results if p["status"] == "healthy"]
            ),
            "warning": len(
                [p for p in health_results if p["status"] == "warning"]
            ),
            "at_risk": len(
                [p for p in health_results if p["status"] == "at_risk"]
            ),
            "critical": len(
                [p for p in health_results if p["status"] == "critical"]
            ),
            "avg_score": (
                round(
                    sum(p["health_score"] for p in health_results) / total, 1
                )
                if total
                else 0
            ),
        }
        return jsonify({"projects": health_results, "summary": summary})


@app.route("/api/projects/<int:project_id>/health", methods=["GET"])
@require_auth
def get_project_health(project_id):
    """Get health score for a specific project."""
    response = get_projects_health()
    data = response.get_json()
    for p in data["projects"]:
        if p["project_id"] == project_id:
            return jsonify(p)
    return api_error("Project not found", 404)


@app.route("/api/projects/<int:project_id>/health/detailed", methods=["GET"])
@require_auth
def get_project_health_detailed(project_id):
    """Get comprehensive health score with detailed breakdown and recommendations."""
    include_recommendations = (
        request.args.get("recommendations", "true").lower() == "true"
    )

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        project = conn.execute(
            "SELECT * FROM projects WHERE id = ?", (project_id,)
        ).fetchone()
        if not project:
            return api_error("Project not found", 404, "not_found")

        score = 100
        breakdown = {}
        recommendations = []

        # 1. Bug Impact (-40 max)
        bugs = conn.execute(
            """
            SELECT COUNT(*) as total, SUM(CASE WHEN status='open' THEN 1 ELSE 0 END) as open_count,
                SUM(CASE WHEN status='open' AND severity='critical' THEN 1 ELSE 0 END) as critical,
                SUM(CASE WHEN status='open' AND severity='high' THEN 1 ELSE 0 END) as high,
                SUM(CASE WHEN status='open' AND severity='medium' THEN 1 ELSE 0 END) as medium,
                SUM(CASE WHEN status='resolved' AND updated_at > datetime('now','-7 days') THEN 1 ELSE 0 END) as resolved_week
            FROM bugs WHERE project_id = ?
        """,
            (project_id,),
        ).fetchone()
        crit, high, med = (
            bugs["critical"] or 0,
            bugs["high"] or 0,
            bugs["medium"] or 0,
        )
        bug_penalty = min(crit * 12 + high * 6 + med * 2, 40)
        score -= bug_penalty
        breakdown["bugs"] = {
            "impact": -bug_penalty,
            "weight": 40,
            "open": bugs["open_count"] or 0,
            "critical": crit,
            "high": high,
            "medium": med,
            "resolved_this_week": bugs["resolved_week"] or 0,
        }
        if crit > 0:
            recommendations.append(
                {
                    "priority": "critical",
                    "category": "bugs",
                    "message": f"Fix {crit} critical bug(s) immediately",
                }
            )
        if high > 2:
            recommendations.append(
                {
                    "priority": "high",
                    "category": "bugs",
                    "message": f"Address {high} high-severity bugs",
                }
            )

        # 2. Feature Completion (+20 max)
        features = conn.execute(
            """
            SELECT COUNT(*) as total, SUM(CASE WHEN status='completed' THEN 1 ELSE 0 END) as completed,
                SUM(CASE WHEN status='in_progress' THEN 1 ELSE 0 END) as in_progress,
                SUM(CASE WHEN status='blocked' THEN 1 ELSE 0 END) as blocked
            FROM features WHERE project_id = ?
        """,
            (project_id,),
        ).fetchone()
        total_f, completed_f, blocked_f = (
            features["total"] or 0,
            features["completed"] or 0,
            features["blocked"] or 0,
        )
        completion_rate = (completed_f / total_f * 100) if total_f > 0 else 0
        feature_bonus = max(
            0, min((completion_rate / 100) * 20, 20) - blocked_f * 2
        )
        score += feature_bonus
        breakdown["features"] = {
            "impact": round(feature_bonus, 1),
            "weight": 20,
            "total": total_f,
            "completed": completed_f,
            "in_progress": features["in_progress"] or 0,
            "blocked": blocked_f,
            "completion_rate": round(completion_rate, 1),
        }
        if blocked_f > 0:
            recommendations.append(
                {
                    "priority": "high",
                    "category": "features",
                    "message": f"Unblock {blocked_f} feature(s)",
                }
            )

        # 3. Milestone Progress (+15/-15)
        milestones = conn.execute(
            """
            SELECT COUNT(*) as total, SUM(CASE WHEN status='completed' THEN 1 ELSE 0 END) as completed,
                SUM(CASE WHEN target_date < date('now') AND status != 'completed' THEN 1 ELSE 0 END) as overdue,
                SUM(CASE WHEN target_date BETWEEN date('now') AND date('now','+7 days') AND status != 'completed' THEN 1 ELSE 0 END) as due_soon,
                AVG(CASE WHEN status != 'completed' THEN progress ELSE NULL END) as avg_progress
            FROM milestones WHERE project_id = ?
        """,
            (project_id,),
        ).fetchone()
        total_ms, overdue_ms, due_soon_ms = (
            milestones["total"] or 0,
            milestones["overdue"] or 0,
            milestones["due_soon"] or 0,
        )
        avg_progress = milestones["avg_progress"] or 0
        ms_score = (
            max(-15, min(15, (avg_progress / 100) * 15 - overdue_ms * 5))
            if total_ms > 0
            else 0
        )
        score += ms_score
        breakdown["milestones"] = {
            "impact": round(ms_score, 1),
            "weight": 15,
            "total": total_ms,
            "completed": milestones["completed"] or 0,
            "overdue": overdue_ms,
            "due_soon": due_soon_ms,
            "avg_progress": round(avg_progress, 1),
        }
        if overdue_ms > 0:
            recommendations.append(
                {
                    "priority": "critical",
                    "category": "milestones",
                    "message": f"{overdue_ms} milestone(s) overdue",
                }
            )
        if due_soon_ms > 0:
            recommendations.append(
                {
                    "priority": "medium",
                    "category": "milestones",
                    "message": f"{due_soon_ms} milestone(s) due within 7 days",
                }
            )

        # 4. Task Queue Health (-20 max)
        tasks = conn.execute(
            """
            SELECT COUNT(*) as total, SUM(CASE WHEN status='failed' THEN 1 ELSE 0 END) as failed,
                SUM(CASE WHEN status='pending' AND created_at < datetime('now','-3 days') THEN 1 ELSE 0 END) as stale,
                SUM(CASE WHEN completed_at > datetime('now','-7 days') THEN 1 ELSE 0 END) as completed_week
            FROM task_queue WHERE task_data LIKE ?
        """,
            (f'%"project_id":{project_id}%',),
        ).fetchone()
        failed, stale = tasks["failed"] or 0, tasks["stale"] or 0
        task_penalty = min(failed * 3 + stale * 2, 20)
        score -= task_penalty
        breakdown["tasks"] = {
            "impact": -task_penalty,
            "weight": 20,
            "total": tasks["total"] or 0,
            "failed": failed,
            "stale": stale,
            "completed_this_week": tasks["completed_week"] or 0,
        }
        if failed > 0:
            recommendations.append(
                {
                    "priority": "high",
                    "category": "tasks",
                    "message": f"Investigate {failed} failed task(s)",
                }
            )

        # 5. Recent Activity (+10 max)
        activity = conn.execute(
            "SELECT COUNT(*) as week FROM activity_log WHERE entity_type='project' AND entity_id=? AND created_at > datetime('now','-7 days')",
            (project_id,),
        ).fetchone()
        weekly = activity["week"] or 0
        activity_bonus = min(weekly * 0.5, 10)
        score += activity_bonus
        breakdown["activity"] = {
            "impact": round(activity_bonus, 1),
            "weight": 10,
            "this_week": weekly,
        }
        if weekly == 0:
            recommendations.append(
                {
                    "priority": "low",
                    "category": "activity",
                    "message": "No activity in past week",
                }
            )

        # 6. Error Frequency (-15 max)
        errors = conn.execute(
            "SELECT SUM(CASE WHEN created_at > datetime('now','-7 days') THEN 1 ELSE 0 END) as recent, SUM(CASE WHEN resolved=0 THEN 1 ELSE 0 END) as unresolved FROM errors WHERE project_id=?",
            (project_id,),
        ).fetchone()
        recent_err, unresolved = (
            errors["recent"] or 0,
            errors["unresolved"] or 0,
        )
        error_penalty = min(recent_err + unresolved * 0.5, 15)
        score -= error_penalty
        breakdown["errors"] = {
            "impact": -round(error_penalty, 1),
            "weight": 15,
            "recent": recent_err,
            "unresolved": unresolved,
        }

        # Final score
        score = max(0, min(100, score))
        status = (
            "excellent"
            if score >= 85
            else (
                "healthy"
                if score >= 70
                else (
                    "warning"
                    if score >= 55
                    else ("at_risk" if score >= 40 else "critical")
                )
            )
        )
        emoji = {
            "excellent": "",
            "healthy": "",
            "warning": "",
            "at_risk": "",
            "critical": "",
        }[status]

        recommendations.sort(
            key=lambda x: {
                "critical": 0,
                "high": 1,
                "medium": 2,
                "low": 3,
            }.get(x["priority"], 4)
        )

        result = {
            "project_id": project_id,
            "project_name": project["name"],
            "health_score": round(score, 1),
            "status": status,
            "status_emoji": emoji,
            "breakdown": breakdown,
            "calculated_at": datetime.utcnow().isoformat(),
        }
        if include_recommendations:
            result["recommendations"] = recommendations[:10]
        return jsonify(result)


@app.route("/api/projects/<int:project_id>/health/trend", methods=["GET"])
@require_auth
def get_project_health_trend(project_id):
    """Get health score trend over time."""
    days = min(request.args.get("days", 30, type=int), 90)
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        project = conn.execute(
            "SELECT name FROM projects WHERE id = ?", (project_id,)
        ).fetchone()
        if not project:
            return api_error("Project not found", 404, "not_found")

        # Try historical data (if project_health_history table exists)
        try:
            history = conn.execute(
                "SELECT health_score, calculated_at FROM project_health_history WHERE project_id = ? AND calculated_at > datetime('now', '-' || ? || ' days') ORDER BY calculated_at",
                (project_id, days),
            ).fetchall()
        except Exception:
            history = []

        if not history:
            current = get_project_health_detailed(project_id).get_json()
            return jsonify(
                {
                    "project_id": project_id,
                    "project_name": project["name"],
                    "trend": "stable",
                    "current_score": current.get("health_score", 0),
                    "data_points": [
                        {
                            "score": current.get("health_score", 0),
                            "date": datetime.utcnow().isoformat(),
                        }
                    ],
                    "period_days": days,
                }
            )

        scores = [h["health_score"] for h in history]
        trend = "stable"
        if len(scores) >= 2:
            first_avg = sum(scores[: len(scores) // 2]) / max(
                len(scores) // 2, 1
            )
            second_avg = sum(scores[len(scores) // 2:]) / max(
                len(scores) - len(scores) // 2, 1
            )
            trend = (
                "improving"
                if second_avg - first_avg > 5
                else ("declining" if first_avg - second_avg > 5 else "stable")
            )

        return jsonify(
            {
                "project_id": project_id,
                "project_name": project["name"],
                "trend": trend,
                "current_score": scores[-1],
                "min_score": min(scores),
                "max_score": max(scores),
                "avg_score": round(sum(scores) / len(scores), 1),
                "data_points": [
                    {"score": h["health_score"], "date": h["calculated_at"]}
                    for h in history
                ],
                "period_days": days,
            }
        )


@app.route("/api/projects/health/summary", methods=["GET"])
@require_auth
def get_all_projects_health_summary():
    """Get health summary across all projects."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        projects = conn.execute(
            "SELECT id, name, status, priority FROM projects WHERE status = 'active' ORDER BY priority DESC, name"
        ).fetchall()
        if not projects:
            return jsonify({"projects": [], "summary": {"total": 0}})

        health_data = []
        for p in projects:
            pid = p["id"]
            score = 100

            bugs = conn.execute(
                "SELECT SUM(CASE WHEN status='open' AND severity='critical' THEN 12 WHEN status='open' AND severity='high' THEN 6 ELSE 0 END) as penalty FROM bugs WHERE project_id=?",
                (pid,),
            ).fetchone()
            score -= min(bugs["penalty"] or 0, 40)

            features = conn.execute(
                "SELECT COUNT(*) as t, SUM(CASE WHEN status='completed' THEN 1 ELSE 0 END) as c FROM features WHERE project_id=?",
                (pid,),
            ).fetchone()
            if features["t"]:
                score += min((features["c"] or 0) / features["t"] * 20, 20)

            milestones = conn.execute(
                "SELECT SUM(CASE WHEN target_date < date('now') AND status != 'completed' THEN 5 ELSE 0 END) as penalty FROM milestones WHERE project_id=?",
                (pid,),
            ).fetchone()
            score -= milestones["penalty"] or 0

            score = max(0, min(100, score))
            status = (
                "excellent"
                if score >= 85
                else (
                    "healthy"
                    if score >= 70
                    else (
                        "warning"
                        if score >= 55
                        else ("at_risk" if score >= 40 else "critical")
                    )
                )
            )
            health_data.append(
                {
                    "project_id": pid,
                    "name": p["name"],
                    "health_score": round(score, 1),
                    "status": status,
                    "priority": p["priority"],
                }
            )

        scores = [h["health_score"] for h in health_data]
        by_status = {}
        for h in health_data:
            by_status[h["status"]] = by_status.get(h["status"], 0) + 1

        return jsonify(
            {
                "projects": sorted(
                    health_data, key=lambda x: x["health_score"]
                ),
                "summary": {
                    "total": len(health_data),
                    "avg_score": round(sum(scores) / len(scores), 1),
                    "min_score": min(scores),
                    "max_score": max(scores),
                    "by_status": by_status,
                    "needs_attention": len(
                        [
                            h
                            for h in health_data
                            if h["status"] in ["critical", "at_risk"]
                        ]
                    ),
                },
            }
        )


# ============================================================================
# PROJECT STATUS BADGES AND INDICATORS
# ============================================================================

PROJECT_BADGE_TYPES = {
    "status": {
        "colors": {
            "active": "#4CAF50",
            "paused": "#FF9800",
            "completed": "#2196F3",
            "archived": "#9E9E9E",
            "blocked": "#F44336",
        }
    },
    "health": {
        "colors": {
            "excellent": "#4CAF50",
            "healthy": "#8BC34A",
            "warning": "#FF9800",
            "at_risk": "#FF5722",
            "critical": "#F44336",
        }
    },
    "priority": {
        "colors": {
            "critical": "#9C27B0",
            "high": "#F44336",
            "medium": "#FF9800",
            "low": "#4CAF50",
            "none": "#9E9E9E",
        }
    },
    "progress": {
        "colors": {
            "complete": "#4CAF50",
            "on_track": "#8BC34A",
            "behind": "#FF9800",
            "at_risk": "#F44336",
        }
    },
    "activity": {
        "colors": {
            "active": "#4CAF50",
            "moderate": "#8BC34A",
            "slow": "#FF9800",
            "stale": "#F44336",
            "dormant": "#9E9E9E",
        }
    },
    "bugs": {
        "colors": {
            "none": "#4CAF50",
            "low": "#8BC34A",
            "moderate": "#FF9800",
            "high": "#FF5722",
            "critical": "#F44336",
        }
    },
}


@app.route("/api/projects/<int:project_id>/badges", methods=["GET"])
@require_auth
def get_project_badges(project_id):
    """Get status badges for a specific project.

    Returns badges for: status, health, priority, progress, activity, bugs.
    Each badge has: type, value, label, color, description.
    """
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        project = conn.execute(
            "SELECT * FROM projects WHERE id = ?", (project_id,)
        ).fetchone()
        if not project:
            return api_error("Project not found", 404, "not_found")

        badges = []
        p = dict(project)

        # Status badge
        status = p.get("status", "active")
        badges.append(
            {
                "type": "status",
                "value": status,
                "label": status.replace("_", " ").title(),
                "color": PROJECT_BADGE_TYPES["status"]["colors"].get(
                    status, "#9E9E9E"
                ),
                "description": f"Project is {status}",
            }
        )

        # Priority badge
        priority = p.get("priority", "medium")
        if isinstance(priority, int):
            priority = (
                "critical"
                if priority >= 5
                else (
                    "high"
                    if priority >= 4
                    else ("medium" if priority >= 2 else "low")
                )
            )
        badges.append(
            {
                "type": "priority",
                "value": priority,
                "label": priority.title(),
                "color": PROJECT_BADGE_TYPES["priority"]["colors"].get(
                    priority, "#9E9E9E"
                ),
                "description": f"{priority.title()} priority project",
            }
        )

        # Health badge (calculate score)
        score = 100
        bugs = conn.execute(
            "SELECT SUM(CASE WHEN status='open' AND severity='critical' THEN 12 WHEN status='open' AND severity='high' THEN 6 ELSE 0 END) as penalty FROM bugs WHERE project_id=?",
            (project_id,),
        ).fetchone()
        score -= min(bugs["penalty"] or 0, 40)
        features = conn.execute(
            "SELECT COUNT(*) as t, SUM(CASE WHEN status='completed' THEN 1 ELSE 0 END) as c FROM features WHERE project_id=?",
            (project_id,),
        ).fetchone()
        if features["t"]:
            score += min((features["c"] or 0) / features["t"] * 20, 20)
        milestones = conn.execute(
            "SELECT SUM(CASE WHEN target_date < date('now') AND status != 'completed' THEN 5 ELSE 0 END) as penalty FROM milestones WHERE project_id=?",
            (project_id,),
        ).fetchone()
        score -= milestones["penalty"] or 0
        score = max(0, min(100, score))
        health = (
            "excellent"
            if score >= 85
            else (
                "healthy"
                if score >= 70
                else (
                    "warning"
                    if score >= 55
                    else ("at_risk" if score >= 40 else "critical")
                )
            )
        )
        badges.append(
            {
                "type": "health",
                "value": health,
                "label": f"{health.replace('_', ' ').title()} ({int(score)}%)",
                "color": PROJECT_BADGE_TYPES["health"]["colors"].get(
                    health, "#9E9E9E"
                ),
                "description": f"Health score: {int(score)}%",
                "score": int(score),
            }
        )

        # Progress badge
        total_features = features["t"] or 0
        completed_features = features["c"] or 0
        progress_pct = (
            (completed_features / total_features * 100)
            if total_features > 0
            else 0
        )
        progress_status = (
            "complete"
            if progress_pct >= 100
            else (
                "on_track"
                if progress_pct >= 60
                else ("behind" if progress_pct >= 30 else "at_risk")
            )
        )
        badges.append(
            {
                "type": "progress",
                "value": progress_status,
                "label": f"{
                    int(progress_pct)}% Complete",
                "color": PROJECT_BADGE_TYPES["progress"]["colors"].get(
                    progress_status, "#9E9E9E"
                ),
                "description": f"{completed_features}/{total_features} features completed",
                "percentage": int(progress_pct),
            }
        )

        # Bug severity badge
        bug_counts = conn.execute(
            "SELECT severity, COUNT(*) as cnt FROM bugs WHERE project_id=? AND status='open' GROUP BY severity",
            (project_id,),
        ).fetchall()
        bug_map = {r["severity"]: r["cnt"] for r in bug_counts}
        critical_bugs = bug_map.get("critical", 0)
        high_bugs = bug_map.get("high", 0)
        total_open_bugs = sum(bug_map.values())
        bug_status = (
            "critical"
            if critical_bugs > 0
            else (
                "high"
                if high_bugs >= 3
                else (
                    "moderate"
                    if total_open_bugs >= 5
                    else ("low" if total_open_bugs > 0 else "none")
                )
            )
        )
        badges.append(
            {
                "type": "bugs",
                "value": bug_status,
                "label": f"{total_open_bugs} Open Bugs",
                "color": PROJECT_BADGE_TYPES["bugs"]["colors"].get(
                    bug_status, "#9E9E9E"
                ),
                "description": f"{critical_bugs} critical, {high_bugs} high severity",
                "count": total_open_bugs,
            }
        )

        # Activity badge (based on recent updates)
        recent_activity = conn.execute(
            """
            SELECT COUNT(*) as cnt FROM (
                SELECT updated_at FROM features WHERE project_id=? AND updated_at > datetime('now', '-7 days')
                UNION ALL SELECT updated_at FROM bugs WHERE project_id=? AND updated_at > datetime('now', '-7 days')
                UNION ALL SELECT updated_at FROM milestones WHERE project_id=? AND updated_at > datetime('now', '-7 days')
            )
        """,
            (project_id, project_id, project_id),
        ).fetchone()["cnt"]
        activity = (
            "active"
            if recent_activity >= 10
            else (
                "moderate"
                if recent_activity >= 5
                else ("slow" if recent_activity >= 1 else "stale")
            )
        )
        badges.append(
            {
                "type": "activity",
                "value": activity,
                "label": activity.title(),
                "color": PROJECT_BADGE_TYPES["activity"]["colors"].get(
                    activity, "#9E9E9E"
                ),
                "description": f"{recent_activity} updates in last 7 days",
                "recent_count": recent_activity,
            }
        )

        return jsonify(
            {
                "project_id": project_id,
                "project_name": p.get("name", ""),
                "badges": badges,
                "badge_types": PROJECT_BADGE_TYPES,
            }
        )


@app.route("/api/projects/badges", methods=["GET"])
@require_auth
def get_all_project_badges():
    """Get status badges for all active projects.

    Query params:
        status: Filter by project status (default: active)
        badge_type: Filter to specific badge type(s) (comma-separated)

    Returns summary with all projects and their badges.
    """
    status_filter = request.args.get("status", "active")
    badge_types = (
        request.args.get("badge_type", "").split(",")
        if request.args.get("badge_type")
        else None
    )

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        query = "SELECT id, name, status, priority FROM projects"
        params = []
        if status_filter:
            query += " WHERE status = ?"
            params.append(status_filter)
        query += " ORDER BY priority DESC, name"

        projects = conn.execute(query, params).fetchall()
        results = []

        for project in projects:
            pid = project["id"]
            project_badges = []

            # Calculate all badges (reusing logic from single project endpoint)
            p_status = project["status"] or "active"
            p_priority = project["priority"]
            if isinstance(p_priority, int):
                p_priority = (
                    "critical"
                    if p_priority >= 5
                    else (
                        "high"
                        if p_priority >= 4
                        else ("medium" if p_priority >= 2 else "low")
                    )
                )

            if not badge_types or "status" in badge_types:
                project_badges.append(
                    {
                        "type": "status",
                        "value": p_status,
                        "color": PROJECT_BADGE_TYPES["status"]["colors"].get(
                            p_status, "#9E9E9E"
                        ),
                    }
                )
            if not badge_types or "priority" in badge_types:
                project_badges.append(
                    {
                        "type": "priority",
                        "value": p_priority,
                        "color": PROJECT_BADGE_TYPES["priority"]["colors"].get(
                            p_priority, "#9E9E9E"
                        ),
                    }
                )

            # Health
            score = 100
            bugs = conn.execute(
                "SELECT SUM(CASE WHEN status='open' AND severity='critical' THEN 12 WHEN status='open' AND severity='high' THEN 6 ELSE 0 END) as penalty FROM bugs WHERE project_id=?",
                (pid,),
            ).fetchone()
            score -= min(bugs["penalty"] or 0, 40)
            features = conn.execute(
                "SELECT COUNT(*) as t, SUM(CASE WHEN status='completed' THEN 1 ELSE 0 END) as c FROM features WHERE project_id=?",
                (pid,),
            ).fetchone()
            if features["t"]:
                score += min((features["c"] or 0) / features["t"] * 20, 20)
            score = max(0, min(100, score))
            health = (
                "excellent"
                if score >= 85
                else (
                    "healthy"
                    if score >= 70
                    else (
                        "warning"
                        if score >= 55
                        else ("at_risk" if score >= 40 else "critical")
                    )
                )
            )

            if not badge_types or "health" in badge_types:
                project_badges.append(
                    {
                        "type": "health",
                        "value": health,
                        "score": int(score),
                        "color": PROJECT_BADGE_TYPES["health"]["colors"].get(
                            health, "#9E9E9E"
                        ),
                    }
                )

            # Progress
            if not badge_types or "progress" in badge_types:
                progress_pct = (
                    (features["c"] or 0) / features["t"] * 100
                    if features["t"]
                    else 0
                )
                progress_status = (
                    "complete"
                    if progress_pct >= 100
                    else (
                        "on_track"
                        if progress_pct >= 60
                        else ("behind" if progress_pct >= 30 else "at_risk")
                    )
                )
                project_badges.append(
                    {
                        "type": "progress",
                        "value": progress_status,
                        "percentage": int(progress_pct),
                        "color": PROJECT_BADGE_TYPES["progress"]["colors"].get(
                            progress_status, "#9E9E9E"
                        ),
                    }
                )

            # Bugs
            if not badge_types or "bugs" in badge_types:
                bug_counts = conn.execute(
                    "SELECT severity, COUNT(*) as cnt FROM bugs WHERE project_id=? AND status='open' GROUP BY severity",
                    (pid,),
                ).fetchall()
                bug_map = {r["severity"]: r["cnt"] for r in bug_counts}
                total_open = sum(bug_map.values())
                bug_status = (
                    "critical"
                    if bug_map.get("critical", 0) > 0
                    else (
                        "high"
                        if bug_map.get("high", 0) >= 3
                        else (
                            "moderate"
                            if total_open >= 5
                            else ("low" if total_open > 0 else "none")
                        )
                    )
                )
                project_badges.append(
                    {
                        "type": "bugs",
                        "value": bug_status,
                        "count": total_open,
                        "color": PROJECT_BADGE_TYPES["bugs"]["colors"].get(
                            bug_status, "#9E9E9E"
                        ),
                    }
                )

            results.append(
                {
                    "project_id": pid,
                    "name": project["name"],
                    "badges": project_badges,
                }
            )

        return jsonify(
            {
                "projects": results,
                "total": len(results),
                "badge_types": PROJECT_BADGE_TYPES,
            }
        )


@app.route("/api/projects/<int:project_id>/indicators", methods=["GET"])
@require_auth
def get_project_indicators(project_id):
    """Get detailed status indicators for a project.

    Returns comprehensive metrics including:
    - velocity: Feature completion rate over time
    - burndown: Remaining work vs time
    - risks: Identified project risks
    - blockers: Current blocking issues
    - team_activity: Recent contributor activity
    """
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        project = conn.execute(
            "SELECT * FROM projects WHERE id = ?", (project_id,)
        ).fetchone()
        if not project:
            return api_error("Project not found", 404, "not_found")

        indicators = {}

        # Velocity: features completed per week over last 4 weeks
        velocity_data = []
        for weeks_ago in range(4, 0, -1):
            count = conn.execute(
                """
                SELECT COUNT(*) FROM features
                WHERE project_id = ? AND status = 'completed'
                AND updated_at BETWEEN datetime('now', ? || ' days') AND datetime('now', ? || ' days')
            """,
                (project_id, f"-{weeks_ago * 7}", f"-{(weeks_ago - 1) * 7}"),
            ).fetchone()[0]
            velocity_data.append(
                {"week": f"-{weeks_ago}w", "completed": count}
            )
        avg_velocity = (
            sum(v["completed"] for v in velocity_data) / len(velocity_data)
            if velocity_data
            else 0
        )
        indicators["velocity"] = {
            "weekly_data": velocity_data,
            "average": round(avg_velocity, 1),
            "trend": (
                "up"
                if velocity_data[-1]["completed"] > avg_velocity
                else "down"
            ),
        }

        # Burndown: remaining features/bugs
        remaining_features = conn.execute(
            "SELECT COUNT(*) FROM features WHERE project_id = ? AND status != 'completed'",
            (project_id,),
        ).fetchone()[0]
        open_bugs = conn.execute(
            "SELECT COUNT(*) FROM bugs WHERE project_id = ? AND status = 'open'",
            (project_id,),
        ).fetchone()[0]
        indicators["burndown"] = {
            "remaining_features": remaining_features,
            "open_bugs": open_bugs,
            "total_remaining": remaining_features + open_bugs,
        }

        # Risks: overdue milestones, stale features, high bug count
        risks = []
        overdue = conn.execute(
            "SELECT COUNT(*) FROM milestones WHERE project_id = ? AND target_date < date('now') AND status != 'completed'",
            (project_id,),
        ).fetchone()[0]
        if overdue > 0:
            risks.append(
                {
                    "type": "overdue_milestones",
                    "severity": "high",
                    "count": overdue,
                    "message": f"{overdue} milestone(s) past due date",
                }
            )
        stale = conn.execute(
            "SELECT COUNT(*) FROM features WHERE project_id = ? AND status = 'in_progress' AND updated_at < datetime('now', '-14 days')",
            (project_id,),
        ).fetchone()[0]
        if stale > 0:
            risks.append(
                {
                    "type": "stale_features",
                    "severity": "medium",
                    "count": stale,
                    "message": f"{stale} in-progress feature(s) not updated in 2+ weeks",
                }
            )
        critical_bugs = conn.execute(
            "SELECT COUNT(*) FROM bugs WHERE project_id = ? AND status = 'open' AND severity = 'critical'",
            (project_id,),
        ).fetchone()[0]
        if critical_bugs > 0:
            risks.append(
                {
                    "type": "critical_bugs",
                    "severity": "critical",
                    "count": critical_bugs,
                    "message": f"{critical_bugs} critical bug(s) open",
                }
            )
        indicators["risks"] = risks

        # Blockers: features/bugs marked as blocked
        blockers = conn.execute(
            """
            SELECT 'feature' as item_type, id, title, status FROM features WHERE project_id = ? AND status = 'blocked'
            UNION ALL SELECT 'bug' as item_type, id, title, status FROM bugs WHERE project_id = ? AND status = 'blocked'
        """,
            (project_id, project_id),
        ).fetchall()
        indicators["blockers"] = [dict(b) for b in blockers]

        # Recent activity summary
        recent_features = conn.execute(
            "SELECT COUNT(*) FROM features WHERE project_id = ? AND updated_at > datetime('now', '-7 days')",
            (project_id,),
        ).fetchone()[0]
        recent_bugs = conn.execute(
            "SELECT COUNT(*) FROM bugs WHERE project_id = ? AND updated_at > datetime('now', '-7 days')",
            (project_id,),
        ).fetchone()[0]
        indicators["activity"] = {
            "features_updated_7d": recent_features,
            "bugs_updated_7d": recent_bugs,
            "total_7d": recent_features + recent_bugs,
        }

        return jsonify(
            {
                "project_id": project_id,
                "project_name": project["name"],
                "indicators": indicators,
            }
        )


@app.route("/api/projects/<int:project_id>/badge-svg", methods=["GET"])
@require_auth
def get_project_badge_svg(project_id):
    """Generate an SVG badge for a project.

    Query params:
        type: Badge type (status, health, priority, progress, bugs, activity)
        style: Badge style (flat, flat-square, plastic) - default: flat

    Returns SVG image suitable for embedding in README files.
    """
    badge_type = request.args.get("type", "health")
    style = request.args.get("style", "flat")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        project = conn.execute(
            "SELECT * FROM projects WHERE id = ?", (project_id,)
        ).fetchone()
        if not project:
            return api_error("Project not found", 404, "not_found")

        label = badge_type.replace("_", " ").title()
        value = ""
        color = "#9E9E9E"

        if badge_type == "status":
            value = project["status"] or "active"
            color = PROJECT_BADGE_TYPES["status"]["colors"].get(value, color)
        elif badge_type == "priority":
            p = project["priority"]
            value = (
                "critical"
                if p >= 5
                else (
                    ("high" if p >= 4 else ("medium" if p >= 2 else "low"))
                    if isinstance(p, int)
                    else (p or "medium")
                )
            )
            color = PROJECT_BADGE_TYPES["priority"]["colors"].get(value, color)
        elif badge_type == "health":
            score = 100
            bugs = conn.execute(
                "SELECT SUM(CASE WHEN status='open' AND severity='critical' THEN 12 WHEN status='open' AND severity='high' THEN 6 ELSE 0 END) as penalty FROM bugs WHERE project_id=?",
                (project_id,),
            ).fetchone()
            score -= min(bugs["penalty"] or 0, 40)
            features = conn.execute(
                "SELECT COUNT(*) as t, SUM(CASE WHEN status='completed' THEN 1 ELSE 0 END) as c FROM features WHERE project_id=?",
                (project_id,),
            ).fetchone()
            if features["t"]:
                score += min((features["c"] or 0) / features["t"] * 20, 20)
            score = max(0, min(100, int(score)))
            value = f"{score}%"
            health = (
                "excellent"
                if score >= 85
                else (
                    "healthy"
                    if score >= 70
                    else (
                        "warning"
                        if score >= 55
                        else ("at_risk" if score >= 40 else "critical")
                    )
                )
            )
            color = PROJECT_BADGE_TYPES["health"]["colors"].get(health, color)
        elif badge_type == "progress":
            features = conn.execute(
                "SELECT COUNT(*) as t, SUM(CASE WHEN status='completed' THEN 1 ELSE 0 END) as c FROM features WHERE project_id=?",
                (project_id,),
            ).fetchone()
            pct = (
                int((features["c"] or 0) / features["t"] * 100)
                if features["t"]
                else 0
            )
            value = f"{pct}%"
            progress_status = (
                "complete"
                if pct >= 100
                else (
                    "on_track"
                    if pct >= 60
                    else ("behind" if pct >= 30 else "at_risk")
                )
            )
            color = PROJECT_BADGE_TYPES["progress"]["colors"].get(
                progress_status, color
            )
        elif badge_type == "bugs":
            bug_counts = conn.execute(
                "SELECT severity, COUNT(*) as cnt FROM bugs WHERE project_id=? AND status='open' GROUP BY severity",
                (project_id,),
            ).fetchall()
            bug_map = {r["severity"]: r["cnt"] for r in bug_counts}
            total = sum(bug_map.values())
            value = str(total)
            bug_status = (
                "critical"
                if bug_map.get("critical", 0) > 0
                else (
                    "high"
                    if bug_map.get("high", 0) >= 3
                    else (
                        "moderate"
                        if total >= 5
                        else ("low" if total > 0 else "none")
                    )
                )
            )
            color = PROJECT_BADGE_TYPES["bugs"]["colors"].get(
                bug_status, color
            )

        # Calculate widths
        label_width = len(label) * 7 + 10
        value_width = len(value) * 7 + 10
        total_width = label_width + value_width

        # Generate SVG
        svg = """<svg xmlns="http://www.w3.org/2000/svg" width="{total_width}" height="20">
  <linearGradient id="b" x2="0" y2="100%">
    <stop offset="0" stop-color="#bbb" stop-opacity=".1"/>
    <stop offset="1" stop-opacity=".1"/>
  </linearGradient>
  <clipPath id="a"><rect width="{total_width}" height="20" rx="3" fill="#fff"/></clipPath>
  <g clip-path="url(#a)">
    <path fill="#555" d="M0 0h{label_width}v20H0z"/>
    <path fill="{color}" d="M{label_width} 0h{value_width}v20H{label_width}z"/>
    <path fill="url(#b)" d="M0 0h{total_width}v20H0z"/>
  </g>
  <g fill="#ff" text-anchor="middle" font-family="DejaVu Sans,Verdana,Geneva,sans-seri" font-size="11">
    <text x="{label_width/2}" y="15" fill="#010101" fill-opacity=".3">{label}</text>
    <text x="{label_width/2}" y="14">{label}</text>
    <text x="{label_width + value_width/2}" y="15" fill="#010101" fill-opacity=".3">{value}</text>
    <text x="{label_width + value_width/2}" y="14">{value}</text>
  </g>
</svg>"""

        from flask import Response

        return Response(
            svg,
            mimetype="image/svg+xml",
            headers={"Cache-Control": "no-cache, no-store, must-revalidate"},
        )


# ============================================================================
# PROJECT PORTFOLIO VIEW API
# ============================================================================


@app.route("/api/portfolio", methods=["GET"])
@require_auth
def api_get_portfolio_overview():
    """Get portfolio overview with all projects and aggregated metrics.

    Query params:
        group_by: Grouping method (category, status, priority, health) - default: category
        include_archived: Include archived projects (default: false)
        sort_by: Sort field (name, priority, health, progress, updated) - default: priority
        sort_order: asc or desc (default: desc)
    """
    with get_db_connection() as conn:
        result = portfolio.get_portfolio_overview(
            conn,
            group_by=request.args.get("group_by", "category"),
            include_archived=request.args.get("include_archived", "").lower()
            == "true",
            sort_by=request.args.get("sort_by", "priority"),
            sort_order=request.args.get("sort_order", "desc"),
        )
        return jsonify(result)


@app.route("/api/portfolio/summary", methods=["GET"])
@require_auth
def api_get_portfolio_summary():
    """Get compact portfolio summary for dashboard widgets."""
    with get_db_connection() as conn:
        return jsonify(portfolio.get_portfolio_summary(conn))


@app.route("/api/portfolio/timeline", methods=["GET"])
@require_auth
def api_get_portfolio_timeline():
    """Get portfolio timeline/roadmap view.

    Query params:
        start_date: Start of timeline (default: today - 30 days)
        end_date: End of timeline (default: today + 90 days)
        include_completed: Include completed milestones (default: false)
    """
    with get_db_connection() as conn:
        result = portfolio.get_portfolio_timeline(
            conn,
            start_date=request.args.get("start_date"),
            end_date=request.args.get("end_date"),
            include_completed=request.args.get("include_completed", "").lower()
            == "true",
        )
        return jsonify(result)


@app.route("/api/portfolio/comparison", methods=["GET"])
@require_auth
def api_get_portfolio_comparison():
    """Compare multiple projects side by side.

    Query params:
        project_ids: Comma-separated list of project IDs to compare (required)
        metrics: Comma-separated metrics to include (health, progress, bugs, milestones, activity)
    """
    project_ids_str = request.args.get("project_ids", "")
    if not project_ids_str:
        return api_error(
            "project_ids parameter is required", 400, "validation_error"
        )
    try:
        project_ids = [int(pid.strip()) for pid in project_ids_str.split(",")]
    except ValueError:
        return api_error("Invalid project_ids format", 400, "validation_error")

    metrics_filter = (
        request.args.get("metrics", "").split(",")
        if request.args.get("metrics")
        else None
    )

    with get_db_connection() as conn:
        return jsonify(
            portfolio.get_portfolio_comparison(
                conn, project_ids, metrics_filter
            )
        )


@app.route("/api/portfolio/risks", methods=["GET"])
@require_auth
def api_get_portfolio_risks():
    """Get portfolio-wide risk assessment.

    Returns aggregated risks: overdue milestones, critical bugs, stale projects, poor health.
    """
    with get_db_connection() as conn:
        return jsonify(portfolio.get_portfolio_risks(conn))


@app.route("/api/portfolio/allocation", methods=["GET"])
@require_auth
def api_get_portfolio_allocation():
    """Get resource allocation view across portfolio.

    Shows distribution of work items across projects.
    """
    with get_db_connection() as conn:
        return jsonify(portfolio.get_portfolio_allocation(conn))


@app.route("/api/portfolio/categories", methods=["GET"])
@require_auth
def api_get_portfolio_categories():
    """Get available portfolio categories and their usage."""
    with get_db_connection() as conn:
        return jsonify(portfolio.get_portfolio_categories_usage(conn))


@app.route("/api/projects/<int:project_id>/category", methods=["PUT"])
@require_auth
def api_set_project_category(project_id):
    """Set the portfolio category for a project.

    Request body:
        category: Category key (strategic, operational, maintenance, innovation, etc.)
    """
    data = request.get_json()
    if not data:
        return api_error("Request body required", 400, "validation_error")

    try:
        with get_db_connection() as conn:
            result = portfolio.set_project_category(
                conn, project_id, data.get("category")
            )
            if result is None:
                return api_error("Project not found", 404, "not_found")
            log_activity("set_project_category", "project", project_id, result)
            return jsonify({"success": True, "category": result})
    except ValueError as e:
        return api_error(str(e), 400, "validation_error")


@app.route("/api/projects/<int:project_id>/git", methods=["GET"])
@require_auth
def get_project_git_info(project_id):
    """Get git repository information for a project.

    Returns:
    - branch: Current branch name
    - remote_url: Git remote URL (for generating links)
    - web_url: HTTPS URL for web viewing (GitHub, GitLab, etc.)
    - commits: Recent commits with hash, message, author, date
    - is_dirty: Whether there are uncommitted changes
    """
    import subprocess

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        project = conn.execute(
            "SELECT source_path FROM projects WHERE id = ?", (project_id,)
        ).fetchone()

        if not project or not project["source_path"]:
            return jsonify({"error": "Project has no source path"}), 404

        source_path = project["source_path"]
        if not Path(source_path).exists():
            return (
                jsonify(
                    {"error": f"Source path does not exist: {source_path}"}
                ),
                404,
            )

        git_dir = Path(source_path) / ".git"
        if not git_dir.exists():
            return jsonify({"error": "Not a git repository"}), 404

        try:
            # Get current branch
            branch = subprocess.run(
                ["git", "-C", source_path, "rev-parse", "--abbrev-re", "HEAD"],
                capture_output=True,
                text=True,
                timeout=5,
            ).stdout.strip()

            # Get remote URL
            remote_url = subprocess.run(
                ["git", "-C", source_path, "remote", "get-url", "origin"],
                capture_output=True,
                text=True,
                timeout=5,
            ).stdout.strip()

            # Convert SSH URL to HTTPS for web viewing
            web_url = remote_url
            if remote_url.startswith("git@"):
                # git@github.com:user/repo.git -> https://github.com/user/repo
                web_url = (
                    remote_url.replace(":", "/")
                    .replace("git@", "https://")
                    .replace(".git", "")
                )
            elif remote_url.endswith(".git"):
                web_url = remote_url[:-4]

            # Get recent commits (last 10)
            commit_log = subprocess.run(
                [
                    "git",
                    "-C",
                    source_path,
                    "log",
                    "--oneline",
                    "-10",
                    "--format=%H|%h|%s|%an|%ar",
                ],
                capture_output=True,
                text=True,
                timeout=5,
            ).stdout.strip()

            commits = []
            for line in commit_log.split("\n"):
                if line:
                    parts = line.split("|", 4)
                    if len(parts) >= 5:
                        commits.append(
                            {
                                "hash": parts[0],
                                "short_hash": parts[1],
                                "message": parts[2],
                                "author": parts[3],
                                "date": parts[4],
                                "url": (
                                    f"{web_url}/commit/{parts[0]}"
                                    if web_url
                                    else None
                                ),
                            }
                        )

            # Check for uncommitted changes
            status = subprocess.run(
                ["git", "-C", source_path, "status", "--porcelain"],
                capture_output=True,
                text=True,
                timeout=5,
            ).stdout.strip()
            is_dirty = bool(status)

            # Get branch URL
            branch_url = f"{web_url}/tree/{branch}" if web_url else None

            return jsonify(
                {
                    "branch": branch,
                    "branch_url": branch_url,
                    "remote_url": remote_url,
                    "web_url": web_url,
                    "commits": commits,
                    "is_dirty": is_dirty,
                    "uncommitted_count": (
                        len(status.split("\n")) if status else 0
                    ),
                }
            )

        except subprocess.TimeoutExpired:
            return jsonify({"error": "Git command timed out"}), 500
        except Exception as e:
            return jsonify({"error": str(e)}), 500


# ============================================================================
# QUICK ACCESS SIDEBAR API
# ============================================================================


@app.route("/api/quick-access", methods=["GET"])
@require_auth
def get_quick_access():
    """Get quick access sidebar data including pinned and recent projects."""
    user_id = session.get("user", "anonymous")
    with get_db_connection() as conn:
        return jsonify(get_quick_access_data(conn, user_id))


@app.route("/api/projects/<int:project_id>/pin", methods=["POST"])
@require_auth
def pin_project(project_id):
    """Pin a project to quick access sidebar."""
    user_id = session.get("user", "anonymous")
    data = request.get_json() or {}
    position = data.get("position")
    with get_db_connection() as conn:
        result = pin_project_for_user(conn, user_id, project_id, position)
        if not result.get("success") and result.get("error"):
            return jsonify(result), 404
        log_activity("pin_project", "project", project_id, f"user={user_id}")
        return jsonify(result)


@app.route("/api/projects/<int:project_id>/pin", methods=["DELETE"])
@require_auth
def unpin_project(project_id):
    """Unpin a project from quick access sidebar."""
    user_id = session.get("user", "anonymous")
    with get_db_connection() as conn:
        result = unpin_project_for_user(conn, user_id, project_id)
        log_activity("unpin_project", "project", project_id, f"user={user_id}")
        return jsonify(result)


@app.route("/api/quick-access/pinned", methods=["GET"])
@require_auth
def get_pinned_projects():
    """Get only pinned projects for quick access."""
    user_id = session.get("user", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        pinned_ids = get_pinned_ids(conn, user_id)
        if not pinned_ids:
            return jsonify({"projects": [], "count": 0})
        placeholders = ",".join("?" * len(pinned_ids))
        rows = conn.execute(
            f"SELECT id,name,description,status,priority,created_at FROM projects WHERE id IN ({placeholders})",
            pinned_ids,
        ).fetchall()
        proj_map = {r["id"]: dict(r) for r in rows}
        projects = [proj_map[pid] for pid in pinned_ids if pid in proj_map]
    return jsonify({"projects": projects, "count": len(projects)})


@app.route("/api/quick-access/pinned/reorder", methods=["PUT"])
@require_auth
def reorder_pinned_projects():
    """Reorder pinned projects list."""
    user_id = session.get("user", "anonymous")
    data = request.get_json() or {}
    new_order = data.get("order", [])
    with get_db_connection() as conn:
        result = reorder_pinned_for_user(conn, user_id, new_order)
        if not result.get("success"):
            return jsonify(result), 400
        return jsonify(result)


@app.route("/api/quick-access/recent", methods=["GET"])
@require_auth
def get_recent_projects():
    """Get recently accessed projects."""
    user_id = session.get("user", "anonymous")
    limit = min(int(request.args.get("limit", 10)), 50)
    with get_db_connection() as conn:
        projects = get_recent_projects_for_user(conn, user_id, limit)
    return jsonify({"projects": projects, "count": len(projects)})


@app.route("/api/quick-access/track", methods=["POST"])
@require_auth
def track_project_access():
    """Track project access for recent projects list."""
    data = request.get_json() or {}
    project_id = data.get("project_id")
    if not project_id:
        return jsonify({"error": "project_id required"}), 400
    log_activity("view_project", "project", project_id)
    return jsonify({"success": True})


# ============================================================================
# PROJECT DEPENDENCIES API
# ============================================================================


@app.route("/api/projects/dependencies", methods=["GET"])
@require_auth
@api_error_handler
def get_all_project_dependencies():
    """Get all project dependencies.

    Query params:
        project_id: Filter by project
        type: Filter by dependency type
        include_transitive: Include transitive dependencies
    """
    project_id = request.args.get("project_id", type=int)
    dep_type = request.args.get("type")
    include_transitive = (
        request.args.get("include_transitive", "false").lower() == "true"
    )

    with get_db_connection() as conn:
        dependencies = project_dependencies.get_dependencies(
            conn,
            project_id=project_id,
            dependency_type=dep_type,
            include_transitive=include_transitive,
        )

        return jsonify(
            {"dependencies": dependencies, "count": len(dependencies)}
        )


@app.route("/api/projects/dependencies/types", methods=["GET"])
@require_auth
@api_error_handler
def get_dependency_types():
    """Get available dependency types."""
    types = project_dependencies.get_dependency_types()
    return jsonify({"types": types})


@app.route("/api/projects/dependencies", methods=["POST"])
@require_auth
@api_error_handler
def add_project_dependency():
    """Add a dependency between two projects.

    Request body:
        source_project_id: Source project ID
        target_project_id: Target project ID
        dependency_type: Type of dependency
        description: Optional description
    """
    data = request.get_json()
    validate_required_fields(
        data, ["source_project_id", "target_project_id", "dependency_type"]
    )

    user_id = session.get("user_id")
    username = session.get("username", "unknown")

    with get_db_connection() as conn:
        try:
            result = project_dependencies.add_dependency(
                conn,
                source_project_id=data["source_project_id"],
                target_project_id=data["target_project_id"],
                dependency_type=data["dependency_type"],
                description=data.get("description"),
                created_by=username,
                metadata=data.get("metadata"),
            )
            conn.commit()

            log_activity(
                conn,
                user_id,
                "add_dependency",
                "project_dependency",
                result["id"],
                {
                    "source": data["source_project_id"],
                    "target": data["target_project_id"],
                    "type": data["dependency_type"],
                },
            )

            return (
                jsonify(
                    {
                        "success": True,
                        "dependency": result,
                        "message": "Dependency added successfully",
                    }
                ),
                201,
            )

        except ValueError as e:
            raise APIError(str(e), 400)


@app.route(
    "/api/projects/dependencies/<int:dependency_id>", methods=["DELETE"]
)
@require_auth
@api_error_handler
def remove_project_dependency(dependency_id):
    """Remove a project dependency."""
    user_id = session.get("user_id")

    with get_db_connection() as conn:
        success = project_dependencies.remove_dependency(conn, dependency_id)
        conn.commit()

        if success:
            log_activity(
                conn,
                user_id,
                "remove_dependency",
                "project_dependency",
                dependency_id,
                {},
            )
            return jsonify({"success": True, "message": "Dependency removed"})
        else:
            raise APIError("Dependency not found", 404)


@app.route("/api/projects/dependencies/<int:dependency_id>", methods=["PUT"])
@require_auth
@api_error_handler
def update_project_dependency(dependency_id):
    """Update a project dependency."""
    data = request.get_json()
    user_id = session.get("user_id")

    with get_db_connection() as conn:
        success = project_dependencies.update_dependency(
            conn,
            dependency_id,
            description=data.get("description"),
            metadata=data.get("metadata"),
        )
        conn.commit()

        if success:
            log_activity(
                conn,
                user_id,
                "update_dependency",
                "project_dependency",
                dependency_id,
                data,
            )
            return jsonify({"success": True, "message": "Dependency updated"})
        else:
            return jsonify(
                {
                    "success": False,
                    "message": "No changes or dependency not found",
                }
            )


@app.route("/api/projects/<int:project_id>/dependencies", methods=["GET"])
@require_auth
@api_error_handler
def get_project_deps(project_id):
    """Get all dependencies for a specific project."""
    direction = request.args.get("direction", "both")

    with get_db_connection() as conn:
        result = project_dependencies.get_project_dependencies(
            conn, project_id, direction
        )
        return jsonify(result)


@app.route("/api/projects/<int:project_id>/dependencies/tree", methods=["GET"])
@require_auth
@api_error_handler
def get_project_dependency_tree(project_id):
    """Get dependency tree for a project."""
    direction = request.args.get("direction", "downstream")
    max_depth = request.args.get("max_depth", 10, type=int)

    with get_db_connection() as conn:
        tree = project_dependencies.get_dependency_tree(
            conn, project_id, direction, max_depth
        )

        if tree is None:
            raise APIError("Project not found", 404)

        return jsonify({"success": True, "tree": tree})


@app.route(
    "/api/projects/<int:project_id>/dependencies/impact", methods=["GET"]
)
@require_auth
@api_error_handler
def get_project_impact_analysis(project_id):
    """Get impact analysis for changes to a project."""
    with get_db_connection() as conn:
        analysis = project_dependencies.get_impact_analysis(conn, project_id)

        if analysis is None:
            raise APIError("Project not found", 404)

        return jsonify({"success": True, **analysis})


@app.route("/api/projects/dependencies/stats", methods=["GET"])
@require_auth
@api_error_handler
def get_dependency_stats():
    """Get statistics about project dependencies."""
    with get_db_connection() as conn:
        stats = project_dependencies.get_dependency_stats(conn)
        return jsonify({"success": True, "stats": stats})


@app.route("/api/projects/dependencies/validate", methods=["GET"])
@require_auth
@api_error_handler
def validate_dependencies():
    """Validate all project dependencies for issues."""
    with get_db_connection() as conn:
        result = project_dependencies.validate_all_dependencies(conn)
        return jsonify({"success": True, **result})


@app.route(
    "/api/projects/<int:project_id>/dependencies/transitive", methods=["GET"]
)
@require_auth
@api_error_handler
def get_transitive_deps(project_id):
    """Get transitive dependencies for a project."""
    dep_types = request.args.getlist("types") or None

    with get_db_connection() as conn:
        deps = project_dependencies.get_transitive_dependencies(
            conn, project_id, dep_types
        )
        return jsonify(
            {
                "project_id": project_id,
                "transitive_dependencies": deps,
                "count": len(deps),
            }
        )


# ============================================================================
# AUTOPILOT API ENDPOINTS
# ============================================================================


@app.route("/api/projects/<int:project_id>/autopilot", methods=["POST"])
@require_auth
def toggle_autopilot(project_id):
    """Toggle autopilot on/off for a project."""
    data = request.get_json()
    action = data.get("action")  # 'activate', 'pause', 'deactivate'

    status_map = {
        "activate": "active",
        "pause": "paused",
        "deactivate": "inactive",
    }

    if action not in status_map:
        return (
            jsonify(
                {"error": "Invalid action. Use: activate, pause, deactivate"}
            ),
            400,
        )

    new_status = status_map[action]

    with get_db_connection() as conn:
        conn.execute(
            "UPDATE projects SET autopilot_status = ?, updated_at = CURRENT_TIMESTAMP WHERE id = ?",
            (new_status, project_id),
        )
        log_activity(f"autopilot_{action}", "project", project_id)

        # If activating, propose first milestone if none exists
        if action == "activate":
            existing = conn.execute(
                "SELECT id FROM autopilot_milestones WHERE project_id = ? AND status NOT IN ('released', 'abandoned', 'failed')",
                (project_id,),
            ).fetchone()

            if not existing:
                # Get project info for milestone proposal
                project = conn.execute(
                    "SELECT name, description FROM projects WHERE id = ?",
                    (project_id,),
                ).fetchone()
                conn.execute(
                    """
                    INSERT INTO autopilot_milestones (project_id, name, goal, status)
                    VALUES (?, ?, ?, 'proposed')
                """,
                    (
                        project_id,
                        "Milestone 1: Initial Setup",
                        f"Set up and verify {
                            project[0] if project else 'project'} foundation",
                    ),
                )

        return jsonify({"success": True, "autopilot_status": new_status})


@app.route("/api/autopilot/queue", methods=["GET"])
@require_auth
def get_autopilot_queue():
    """Get autopilot queue: ready for approval, blocked, incidents, needs input."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Ready for approval (milestones awaiting approval + runs ready for
        # review)
        ready_for_approval = conn.execute(
            """
            SELECT 'milestone' as item_type, m.id, m.id as item_id, m.name as title, m.goal as description,
                   p.name as project_name, m.status, m.created_at as created, m.project_id
            FROM autopilot_milestones m
            JOIN projects p ON m.project_id = p.id
            WHERE m.status IN ('proposed', 'awaiting_approval', 'qa_promotion', 'prod_ready')
            UNION ALL
            SELECT 'run' as item_type, r.id, r.id as item_id, m.name || ' - Run #' || r.run_number as title,
                   COALESCE(r.plan_summary, 'Ready to start') as description,
                   p.name as project_name, r.status, r.created_at as created, r.project_id
            FROM autopilot_runs r
            JOIN autopilot_milestones m ON r.milestone_id = m.id
            JOIN projects p ON r.project_id = p.id
            WHERE r.status IN ('created', 'ready_for_review')
            ORDER BY created DESC
        """
        ).fetchall()

        # Blocked items
        blocked = conn.execute(
            """
            SELECT 'run' as item_type, r.id, r.id as item_id, m.name || ' - Run #' || r.run_number as title,
                   r.blocked_reason as description, r.blocked_question as question,
                   p.name as project_name, r.status, r.created_at, r.project_id
            FROM autopilot_runs r
            JOIN autopilot_milestones m ON r.milestone_id = m.id
            JOIN projects p ON r.project_id = p.id
            WHERE r.status = 'blocked'
            ORDER BY r.created_at DESC
        """
        ).fetchall()

        # In-progress runs (planning, executing, testing, fixing)
        in_progress = conn.execute(
            """
            SELECT r.id, r.id as item_id, m.name || ' - Run #' || r.run_number as title,
                   r.plan_summary as description, r.tmux_session,
                   p.name as project_name, r.status, r.started_at,
                   r.test_pass_count, r.test_fail_count, r.error_count, r.project_id
            FROM autopilot_runs r
            JOIN autopilot_milestones m ON r.milestone_id = m.id
            JOIN projects p ON r.project_id = p.id
            WHERE r.status IN ('planning', 'executing', 'testing', 'fixing', 'promoting', 'monitoring')
            ORDER BY r.started_at DESC
        """
        ).fetchall()

        # Incidents/Alerts
        incidents = conn.execute(
            """
            SELECT a.id, a.id as item_id, a.alert_type, a.severity, a.title, a.description,
                   p.name as project_name, a.status, a.created_at, a.project_id
            FROM autopilot_alerts a
            LEFT JOIN projects p ON a.project_id = p.id
            WHERE a.status NOT IN ('resolved')
            ORDER BY
                CASE a.severity WHEN 'critical' THEN 0 WHEN 'high' THEN 1 WHEN 'medium' THEN 2 ELSE 3 END,
                a.created_at DESC
        """
        ).fetchall()

        # Summary counts
        summary = {
            "ready_for_approval": len(ready_for_approval),
            "blocked": len(blocked),
            "incidents": len(
                [i for i in incidents if i["alert_type"] == "incident"]
            ),
            "needs_input": len(blocked),
            "in_progress": len(in_progress),
        }

        return jsonify(
            {
                "summary": summary,
                "items": [dict(r) for r in ready_for_approval],
                "blocked": [dict(b) for b in blocked],
                "in_progress": [dict(ip) for ip in in_progress],
                "incidents": [dict(i) for i in incidents],
            }
        )


@app.route("/api/autopilot/milestones", methods=["GET"])
@require_auth
def get_autopilot_milestones():
    """Get autopilot milestones."""
    project_id = request.args.get("project_id")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Optimized: Use LEFT JOIN with GROUP BY instead of correlated subquery
        query = """
            SELECT m.*, p.name as project_name,
                   COALESCE(rc.run_count, 0) as run_count
            FROM autopilot_milestones m
            JOIN projects p ON m.project_id = p.id
            LEFT JOIN (
                SELECT milestone_id, COUNT(*) as run_count
                FROM autopilot_runs
                GROUP BY milestone_id
            ) rc ON rc.milestone_id = m.id
        """
        params = []

        if project_id:
            query += " WHERE m.project_id = ?"
            params.append(project_id)

        query += " ORDER BY m.created_at DESC"
        milestones = conn.execute(query, params).fetchall()

        return jsonify([dict(m) for m in milestones])


@app.route(
    "/api/autopilot/milestones/<int:milestone_id>/approve", methods=["POST"]
)
@require_auth
def approve_milestone(milestone_id):
    """Approve a milestone to start execution."""
    with get_db_connection() as conn:
        conn.execute(
            """
            UPDATE autopilot_milestones SET
                status = 'approved',
                approved_at = CURRENT_TIMESTAMP,
                approved_by = ?
            WHERE id = ? AND status IN ('proposed', 'awaiting_approval', 'rework')
        """,
            (session.get("username", "user"), milestone_id),
        )

        log_activity("approve_milestone", "autopilot_milestone", milestone_id)

        # Create initial run
        milestone = conn.execute(
            "SELECT project_id FROM autopilot_milestones WHERE id = ?",
            (milestone_id,),
        ).fetchone()
        if milestone:
            conn.execute(
                """
                INSERT INTO autopilot_runs (milestone_id, project_id, status)
                VALUES (?, ?, 'created')
            """,
                (milestone_id, milestone[0]),
            )

        return jsonify({"success": True})


@app.route("/api/autopilot/milestones/<int:milestone_id>", methods=["GET"])
@require_auth
def get_autopilot_milestone(milestone_id):
    """Get a single autopilot milestone."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        milestone = conn.execute(
            """
            SELECT m.*, p.name as project_name
            FROM autopilot_milestones m
            JOIN projects p ON m.project_id = p.id
            WHERE m.id = ?
        """,
            (milestone_id,),
        ).fetchone()

        if not milestone:
            return jsonify({"error": "Milestone not found"}), 404

        return jsonify(dict(milestone))


@app.route(
    "/api/autopilot/milestones/<int:milestone_id>/reject", methods=["POST"]
)
@require_auth
def reject_milestone(milestone_id):
    """Reject a milestone and send back for rework."""
    data = request.get_json() or {}
    reason = data.get("reason", "No reason provided")

    with get_db_connection() as conn:
        conn.execute(
            """
            UPDATE autopilot_milestones SET
                status = 'rework',
                evidence_packet = json_set(
                    COALESCE(evidence_packet, '{}'),
                    '$.rejection_reason', ?,
                    '$.rejected_at', datetime('now'),
                    '$.rejected_by', ?
                )
            WHERE id = ? AND status IN ('proposed', 'awaiting_approval')
        """,
            (reason, session.get("username", "user"), milestone_id),
        )

        log_activity(
            "reject_milestone",
            "autopilot_milestone",
            milestone_id,
            {"reason": reason},
        )

        return jsonify({"success": True})


@app.route("/api/autopilot/runs", methods=["GET"])
@require_auth
def get_autopilot_runs():
    """Get autopilot runs."""
    milestone_id = request.args.get("milestone_id")
    project_id = request.args.get("project_id")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        query = """
            SELECT r.*, m.name as milestone_name, p.name as project_name
            FROM autopilot_runs r
            JOIN autopilot_milestones m ON r.milestone_id = m.id
            JOIN projects p ON r.project_id = p.id
            WHERE 1=1
        """
        params = []

        if milestone_id:
            query += " AND r.milestone_id = ?"
            params.append(milestone_id)
        if project_id:
            query += " AND r.project_id = ?"
            params.append(project_id)

        query += " ORDER BY r.created_at DESC"
        runs = conn.execute(query, params).fetchall()

        return jsonify([dict(r) for r in runs])


@app.route("/api/autopilot/runs/<int:run_id>", methods=["GET"])
@require_auth
def get_autopilot_run(run_id):
    """Get a single autopilot run."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        run = conn.execute(
            """
            SELECT r.*, m.name as milestone_name, m.goal, m.scope, m.definition_of_done,
                   p.name as project_name, p.source_path
            FROM autopilot_runs r
            JOIN autopilot_milestones m ON r.milestone_id = m.id
            JOIN projects p ON r.project_id = p.id
            WHERE r.id = ?
        """,
            (run_id,),
        ).fetchone()

        if not run:
            return jsonify({"error": "Run not found"}), 404

        return jsonify(dict(run))


@app.route("/api/autopilot/runs/<int:run_id>/start", methods=["POST"])
@require_auth
def start_autopilot_run(run_id):
    """Start an autopilot run - sends work to selected tmux session."""
    try:
        data = request.get_json(force=True, silent=True) or {}
    except (TypeError, ValueError):
        data = {}
    session_name = data.get("tmux_session")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        run = conn.execute(
            """
            SELECT r.*, m.name as milestone_name, m.goal, m.scope, m.definition_of_done,
                   p.name as project_name, p.source_path
            FROM autopilot_runs r
            JOIN autopilot_milestones m ON r.milestone_id = m.id
            JOIN projects p ON r.project_id = p.id
            WHERE r.id = ?
        """,
            (run_id,),
        ).fetchone()

        if not run:
            return jsonify({"error": "Run not found"}), 404

        # Use provided session or create new one
        if not session_name:
            session_name = f"autopilot_run_{run_id}"
            try:
                subprocess.run(
                    ["tmux", "new-session", "-d", "-s", session_name],
                    check=True,
                    timeout=5,
                )
            except (subprocess.SubprocessError, OSError):
                pass  # Session might already exist
        else:
            # Verify the session exists
            result = subprocess.run(
                ["tmux", "has-session", "-t", session_name],
                capture_output=True,
                timeout=5,
            )
            if result.returncode != 0:
                # Session doesn't exist, create it
                try:
                    subprocess.run(
                        ["tmux", "new-session", "-d", "-s", session_name],
                        check=True,
                        timeout=5,
                    )
                except (subprocess.SubprocessError, OSError):
                    pass

        # Update run status
        conn.execute(
            """
            UPDATE autopilot_runs SET
                status = 'planning',
                tmux_session = ?,
                started_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (session_name, run_id),
        )

        # Update milestone status
        conn.execute(
            """
            UPDATE autopilot_milestones SET status = 'in_progress', started_at = CURRENT_TIMESTAMP
            WHERE id = ? AND status = 'approved'
        """,
            (run["milestone_id"],),
        )

        # Link session to this run, milestone, and entity
        conn.execute(
            """
            UPDATE tmux_sessions SET
                autopilot_run_id = ?,
                milestone_id = ?,
                assigned_task_type = 'autopilot_run',
                assigned_task_id = ?,
                last_activity = CURRENT_TIMESTAMP
            WHERE session_name = ?
        """,
            (run_id, run["milestone_id"], run_id, session_name),
        )

        # Queue task to Claude
        import json as json_module

        message = """Execute this autopilot run for project improvement:

MILESTONE: {run['milestone_name']}
GOAL: {run['goal'] or 'Improve the project'}
SCOPE: {run['scope'] or 'As defined in milestone'}
DEFINITION OF DONE: {run['definition_of_done'] or 'All tasks completed and tests passing'}

PROJECT: {run['project_name']}
PATH: {run['source_path'] or 'N/A'}

AUTOPILOT RUN #{run['run_number']}

WORKFLOW:
1. PLANNING: Analyze the goal and create a plan
2. EXECUTING: Implement the changes
3. TESTING: Run tests to verify
4. If tests fail, enter FIXING mode and retry
5. When complete, generate evidence packet

UPDATE STATUS via API:
- Planning complete: curl -X POST "http://localhost:8085/api/autopilot/runs/{run_id}/status" -H "Content-Type: application/json" -d '{{"status": "executing"}}'
- Executing complete: curl -X POST "http://localhost:8085/api/autopilot/runs/{run_id}/status" -d '{{"status": "testing"}}'
- Tests passed: curl -X POST "http://localhost:8085/api/autopilot/runs/{run_id}/status" -d '{{"status": "ready_for_review"}}'
- Blocked: curl -X POST "http://localhost:8085/api/autopilot/runs/{run_id}/status" -d '{{"status": "blocked", "reason": "...", "question": "..."}}'

Start with the PLANNING phase."""

        task_data = json_module.dumps(
            {
                "entity_type": "autopilot_run",
                "entity_id": run_id,
                "session": session_name,
                "message": message,
            }
        )

        conn.execute(
            """
            INSERT INTO task_queue (task_type, task_data, priority, max_retries)
            VALUES ('claude_task', ?, 3, 3)
        """,
            (task_data,),
        )

        log_activity("start_autopilot_run", "autopilot_run", run_id)

        return jsonify({"success": True, "session": session_name})


@app.route("/api/autopilot/runs/<int:run_id>/resume", methods=["POST"])
@require_auth
def resume_autopilot_run(run_id):
    """Resume a blocked autopilot run with optional answer/guidance."""
    data = request.get_json() or {}
    answer = data.get("answer")
    resume_to = data.get("status", "executing")  # Default to executing phase

    valid_resume_statuses = ["planning", "executing", "testing", "fixing"]
    if resume_to not in valid_resume_statuses:
        return (
            jsonify(
                {
                    "error": f'Invalid resume status. Use: {", ".join(valid_resume_statuses)}'
                }
            ),
            400,
        )

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        run = conn.execute(
            """
            SELECT r.*, m.name as milestone_name
            FROM autopilot_runs r
            JOIN autopilot_milestones m ON r.milestone_id = m.id
            WHERE r.id = ?
        """,
            (run_id,),
        ).fetchone()

        if not run:
            return jsonify({"error": "Run not found"}), 404

        if run["status"] != "blocked":
            return (
                jsonify(
                    {
                        "error": f'Run is not blocked (current status: {run["status"]})'
                    }
                ),
                400,
            )

        # Update run status and clear blocked fields
        conn.execute(
            """
            UPDATE autopilot_runs SET
                status = ?,
                blocked_reason = NULL,
                blocked_question = NULL
            WHERE id = ?
        """,
            (resume_to, run_id),
        )

        # If answer provided, send it to the tmux session
        if answer and run["tmux_session"]:
            import json as json_module

            message = """RESUME from blocked state:

ANSWER: {answer}

Continue with the {resume_to.upper()} phase."""

            task_data = json_module.dumps(
                {
                    "entity_type": "autopilot_run",
                    "entity_id": run_id,
                    "session": run["tmux_session"],
                    "message": message,
                }
            )

            conn.execute(
                """
                INSERT INTO task_queue (task_type, task_data, priority, max_retries)
                VALUES ('claude_task', ?, 3, 3)
            """,
                (task_data,),
            )

        log_activity(
            "resume_autopilot_run",
            "autopilot_run",
            run_id,
            f'answer: {answer[:50] if answer else "none"}...',
        )

        return jsonify(
            {
                "success": True,
                "run_id": run_id,
                "new_status": resume_to,
                "session": run["tmux_session"],
            }
        )


@app.route("/api/autopilot/runs/<int:run_id>/status", methods=["POST"])
@require_auth
def update_run_status(run_id):
    """Update autopilot run status."""
    data = request.get_json()
    new_status = data.get("status")
    reason = data.get("reason")
    question = data.get("question")

    valid_statuses = [
        "planning",
        "executing",
        "testing",
        "fixing",
        "blocked",
        "ready_for_review",
        "promoting",
        "monitoring",
        "complete",
        "failed",
    ]

    if new_status not in valid_statuses:
        return (
            jsonify(
                {"error": f'Invalid status. Use: {", ".join(valid_statuses)}'}
            ),
            400,
        )

    with get_db_connection() as conn:
        update_fields = "status = ?"
        params = [new_status]

        if new_status == "blocked":
            update_fields += ", blocked_reason = ?, blocked_question = ?"
            params.extend([reason, question])
        elif new_status != "blocked":
            # Clear blocked fields when unblocking
            update_fields += ", blocked_reason = NULL, blocked_question = NULL"

        if new_status == "complete":
            update_fields += ", completed_at = CURRENT_TIMESTAMP"

        params.append(run_id)
        conn.execute(
            f"UPDATE autopilot_runs SET {update_fields} WHERE id = ?", params
        )

        # If run complete or failed, update milestone and clear session
        # association
        if new_status in ("complete", "failed", "rolled_back"):
            run = conn.execute(
                "SELECT milestone_id, tmux_session FROM autopilot_runs WHERE id = ?",
                (run_id,),
            ).fetchone()
            if run:
                conn.execute(
                    """
                    UPDATE autopilot_milestones SET status = 'validation'
                    WHERE id = ? AND status = 'in_progress'
                """,
                    (run[0],),
                )

                # Clear session association
                if run[1]:  # tmux_session
                    conn.execute(
                        """
                        UPDATE tmux_sessions SET
                            autopilot_run_id = NULL,
                            milestone_id = NULL,
                            assigned_task_type = NULL,
                            assigned_task_id = NULL
                        WHERE session_name = ?
                    """,
                        (run[1],),
                    )

        log_activity(f"run_status_{new_status}", "autopilot_run", run_id)

    # Broadcast queue update via WebSocket
    broadcast_queue()
    broadcast_stats()
    return jsonify({"success": True})


# ============================================================================
# MILESTONE API ENDPOINTS
# ============================================================================


@app.route("/api/milestones", methods=["GET"])
@require_auth
def get_milestones():
    """Get milestones with feature and bug counts.

    Returns milestones sorted by target date, including progress metrics.

    Query Parameters:
        project_id (int): Filter by project ID
        page (int): Page number (default: 1)
        per_page (int): Items per page, max 100 (default: 50)
        paginate (str): Set to 'true' for pagination response

    Returns:
        200: List of milestones or paginated result

    Example Request:
        GET /api/milestones
        GET /api/milestones?project_id=1
        GET /api/milestones?paginate=true

    Example Response:
        [
            {
                "id": 1,
                "name": "MVP Release",
                "description": "Minimum viable product",
                "project_id": 1,
                "project_name": "My App",
                "target_date": "2024-03-01",
                "status": "in_progress",
                "feature_count": 12,
                "bug_count": 3,
                "completed_features": 8,
                "resolved_bugs": 2,
                "created_at": "2024-01-01T10:00:00"
            }
        ]

    cURL Example:
        curl -X GET "http://localhost:8080/api/milestones?project_id=1" \\
             -H "Cookie: session=<session_cookie>"
    """
    project_id = request.args.get("project_id")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Optimized: Use subqueries to avoid Cartesian product from multiple
        # LEFT JOINs
        if project_id:
            milestones = conn.execute(
                """
                SELECT m.*, p.name as project_name,
                       COALESCE(fc.total, 0) as feature_count,
                       COALESCE(bc.total, 0) as bug_count,
                       COALESCE(fc.completed, 0) as completed_features,
                       COALESCE(bc.resolved, 0) as resolved_bugs
                FROM milestones m
                LEFT JOIN projects p ON m.project_id = p.id
                LEFT JOIN (
                    SELECT milestone_id, COUNT(*) as total,
                           SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completed
                    FROM features GROUP BY milestone_id
                ) fc ON m.id = fc.milestone_id
                LEFT JOIN (
                    SELECT milestone_id, COUNT(*) as total,
                           SUM(CASE WHEN status = 'resolved' THEN 1 ELSE 0 END) as resolved
                    FROM bugs GROUP BY milestone_id
                ) bc ON m.id = bc.milestone_id
                WHERE m.project_id = ?
                ORDER BY m.target_date, m.name
            """,
                (project_id,),
            ).fetchall()
        else:
            milestones = conn.execute(
                """
                SELECT m.*, p.name as project_name,
                       COALESCE(fc.cnt, 0) as feature_count,
                       COALESCE(bc.cnt, 0) as bug_count
                FROM milestones m
                LEFT JOIN projects p ON m.project_id = p.id
                LEFT JOIN (SELECT milestone_id, COUNT(*) as cnt FROM features GROUP BY milestone_id) fc ON m.id = fc.milestone_id
                LEFT JOIN (SELECT milestone_id, COUNT(*) as cnt FROM bugs GROUP BY milestone_id) bc ON m.id = bc.milestone_id
                ORDER BY m.target_date, m.name
            """
            ).fetchall()

        milestone_list = [dict(m) for m in milestones]

        # Support pagination if requested
        if request.args.get("paginate", "").lower() == "true":
            page, per_page = get_pagination_params()
            result = paginate_query(milestone_list, page, per_page)
            return jsonify(result)

        return jsonify(milestone_list)


@app.route("/api/milestones", methods=["POST"])
@require_auth
def create_milestone():
    """Create a new milestone."""
    data = request.get_json()

    with get_db_connection() as conn:
        cursor = conn.execute(
            """
            INSERT INTO milestones (project_id, name, description, target_date)
            VALUES (?, ?, ?, ?)
        """,
            (
                data.get("project_id"),
                data.get("name"),
                data.get("description"),
                data.get("target_date"),
            ),
        )
        milestone_id = cursor.lastrowid

        log_activity(
            "create_milestone", "milestone", milestone_id, data.get("name")
        )

        return jsonify({"id": milestone_id, "success": True})


@app.route("/api/milestones/<int:milestone_id>", methods=["GET", "PUT"])
@require_auth
def get_or_update_milestone(milestone_id):
    """Get or update a milestone."""
    if request.method == "GET":
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            milestone = conn.execute(
                "SELECT * FROM milestones WHERE id = ?", (milestone_id,)
            ).fetchone()
            if not milestone:
                return jsonify({"error": "Milestone not found"}), 404
            return jsonify(dict(milestone))

    # PUT - update milestone
    data = request.get_json()

    with get_db_connection() as conn:
        conn.execute(
            """
            UPDATE milestones SET
                name = COALESCE(?, name),
                description = COALESCE(?, description),
                target_date = COALESCE(?, target_date),
                status = COALESCE(?, status),
                progress = COALESCE(?, progress),
                updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (
                data.get("name"),
                data.get("description"),
                data.get("target_date"),
                data.get("status"),
                data.get("progress"),
                milestone_id,
            ),
        )

        log_activity("update_milestone", "milestone", milestone_id)

        return jsonify({"success": True})


# ============================================================================
# SPRINT RETROSPECTIVES API
# ============================================================================


@app.route("/api/retrospectives", methods=["GET"])
@require_auth
def get_retrospectives():
    """Get retrospectives with optional filters.

    Query params:
        project_id: Filter by project
        milestone_id: Filter by milestone
        status: Filter by status (draft, active, completed)
        limit: Max results (default 50)
        offset: Pagination offset
    """
    project_id = request.args.get("project_id", type=int)
    milestone_id = request.args.get("milestone_id", type=int)
    status = request.args.get("status")
    limit = min(request.args.get("limit", 50, type=int), 200)
    offset = request.args.get("offset", 0, type=int)

    with get_db_connection() as conn:
        retros = retrospectives.get_retrospectives(
            conn,
            project_id=project_id,
            milestone_id=milestone_id,
            status=status,
            limit=limit,
            offset=offset,
        )
        return jsonify(
            {
                "retrospectives": retros,
                "categories": retrospectives.RETRO_CATEGORIES,
            }
        )


@app.route("/api/retrospectives", methods=["POST"])
@require_auth
def create_retrospective():
    """Create a new retrospective.

    Request body:
        milestone_id: Associated milestone (required)
        project_id: Associated project (required)
        sprint_name: Name of the sprint (optional)
        start_date: Sprint start date (optional)
        end_date: Sprint end date (optional)
        facilitator: Facilitator name (optional)
        participants: List of participant names (optional)
    """
    data = request.get_json()
    if not data:
        return api_error("Request body is required", 400, "validation_error")

    milestone_id = data.get("milestone_id")
    project_id = data.get("project_id")

    if not milestone_id:
        return api_error("milestone_id is required", 400, "validation_error")
    if not project_id:
        return api_error("project_id is required", 400, "validation_error")

    try:
        with get_db_connection() as conn:
            retro_id = retrospectives.create_retrospective(
                conn,
                milestone_id=milestone_id,
                project_id=project_id,
                sprint_name=data.get("sprint_name"),
                start_date=data.get("start_date"),
                end_date=data.get("end_date"),
                facilitator=data.get("facilitator"),
                participants=data.get("participants"),
            )
            log_activity("create_retrospective", "retrospective", retro_id)
            return jsonify({"id": retro_id, "success": True})
    except sqlite3.Error as e:
        logger.error(f"Database error creating retrospective: {e}")
        return api_error(
            "Failed to create retrospective", 500, "database_error"
        )


@app.route("/api/retrospectives/<int:retro_id>", methods=["GET"])
@require_auth
def get_retrospective(retro_id):
    """Get a single retrospective with all items."""
    with get_db_connection() as conn:
        retro = retrospectives.get_retrospective(conn, retro_id)
        if not retro:
            return api_error("Retrospective not found", 404, "not_found")
        return jsonify(retro)


@app.route("/api/retrospectives/<int:retro_id>", methods=["PUT"])
@require_auth
def update_retrospective(retro_id):
    """Update a retrospective.

    Request body (all optional):
        sprint_name, start_date, end_date, status, facilitator,
        participants, summary, mood_score, velocity_planned, velocity_achieved
    """
    data = request.get_json()
    if not data:
        return api_error("Request body is required", 400, "validation_error")

    try:
        with get_db_connection() as conn:
            updated = retrospectives.update_retrospective(
                conn, retro_id, **data
            )
            if updated:
                log_activity("update_retrospective", "retrospective", retro_id)
            return jsonify({"success": updated})
    except sqlite3.Error as e:
        logger.error(f"Database error updating retrospective: {e}")
        return api_error(
            "Failed to update retrospective", 500, "database_error"
        )


@app.route("/api/retrospectives/<int:retro_id>", methods=["DELETE"])
@require_auth
def delete_retrospective(retro_id):
    """Delete a retrospective and all its items."""
    try:
        with get_db_connection() as conn:
            deleted = retrospectives.delete_retrospective(conn, retro_id)
            if deleted:
                log_activity("delete_retrospective", "retrospective", retro_id)
            return jsonify({"success": deleted})
    except sqlite3.Error as e:
        logger.error(f"Database error deleting retrospective: {e}")
        return api_error(
            "Failed to delete retrospective", 500, "database_error"
        )


@app.route("/api/retrospectives/<int:retro_id>/items", methods=["GET"])
@require_auth
def get_retrospective_items(retro_id):
    """Get items for a retrospective.

    Query params:
        category: Filter by category (optional)
    """
    category = request.args.get("category")
    with get_db_connection() as conn:
        items = retrospectives.get_retrospective_items(
            conn, retro_id, category=category
        )
        return jsonify(
            {"items": items, "categories": retrospectives.RETRO_CATEGORIES}
        )


@app.route("/api/retrospectives/<int:retro_id>/items", methods=["POST"])
@require_auth
def add_retrospective_item(retro_id):
    """Add an item to a retrospective.

    Request body:
        category: Item category (required) - went_well, needs_improvement, action_item, kudos, question, blocker
        content: Item content (required)
        is_action_item: Mark as action item (optional)
        action_assignee: Assignee for action item (optional)
        action_due_date: Due date for action item (optional)
    """
    data = request.get_json()
    if not data:
        return api_error("Request body is required", 400, "validation_error")

    category = data.get("category")
    content = data.get("content", "").strip()

    if not category:
        return api_error("category is required", 400, "validation_error")
    if not content:
        return api_error("content is required", 400, "validation_error")

    try:
        with get_db_connection() as conn:
            item_id = retrospectives.add_retrospective_item(
                conn,
                retro_id,
                category=category,
                content=content,
                is_action_item=data.get("is_action_item", False),
                action_assignee=data.get("action_assignee"),
                action_due_date=data.get("action_due_date"),
                created_by=session.get("user_id"),
            )
            log_activity("add_retro_item", "retrospective_item", item_id)
            return jsonify({"id": item_id, "success": True})
    except ValueError as e:
        return api_error(str(e), 400, "validation_error")
    except sqlite3.Error as e:
        logger.error(f"Database error adding retrospective item: {e}")
        return api_error("Failed to add item", 500, "database_error")


@app.route("/api/retrospectives/items/<int:item_id>", methods=["PUT"])
@require_auth
def update_retrospective_item(item_id):
    """Update a retrospective item.

    Request body (all optional):
        category, content, is_action_item, action_assignee, action_due_date, action_status
    """
    data = request.get_json()
    if not data:
        return api_error("Request body is required", 400, "validation_error")

    try:
        with get_db_connection() as conn:
            updated = retrospectives.update_retrospective_item(
                conn, item_id, **data
            )
            if updated:
                log_activity(
                    "update_retro_item", "retrospective_item", item_id
                )
            return jsonify({"success": updated})
    except ValueError as e:
        return api_error(str(e), 400, "validation_error")
    except sqlite3.Error as e:
        logger.error(f"Database error updating item: {e}")
        return api_error("Failed to update item", 500, "database_error")


@app.route("/api/retrospectives/items/<int:item_id>", methods=["DELETE"])
@require_auth
def delete_retrospective_item(item_id):
    """Delete a retrospective item."""
    try:
        with get_db_connection() as conn:
            deleted = retrospectives.delete_retrospective_item(conn, item_id)
            if deleted:
                log_activity(
                    "delete_retro_item", "retrospective_item", item_id
                )
            return jsonify({"success": deleted})
    except sqlite3.Error as e:
        logger.error(f"Database error deleting item: {e}")
        return api_error("Failed to delete item", 500, "database_error")


@app.route("/api/retrospectives/items/<int:item_id>/vote", methods=["POST"])
@require_auth
def vote_retrospective_item(item_id):
    """Vote on a retrospective item.

    Request body (optional):
        increment: Vote change (default +1, use -1 to downvote)
    """
    data = request.get_json() or {}
    increment = data.get("increment", 1)

    try:
        with get_db_connection() as conn:
            new_votes = retrospectives.vote_item(conn, item_id, increment)
            if new_votes is None:
                return api_error("Item not found", 404, "not_found")
            return jsonify({"success": True, "votes": new_votes})
    except sqlite3.Error as e:
        logger.error(f"Database error voting on item: {e}")
        return api_error("Failed to vote", 500, "database_error")


@app.route("/api/retrospectives/<int:retro_id>/action-items", methods=["GET"])
@require_auth
def get_retro_action_items(retro_id):
    """Get action items for a specific retrospective."""
    status = request.args.get("status")
    with get_db_connection() as conn:
        items = retrospectives.get_action_items(
            conn, retro_id=retro_id, status=status
        )
        return jsonify({"action_items": items})


@app.route("/api/retrospectives/action-items/pending", methods=["GET"])
@require_auth
def get_pending_action_items():
    """Get all pending action items across retrospectives.

    Query params:
        project_id: Filter by project (optional)
    """
    project_id = request.args.get("project_id", type=int)
    with get_db_connection() as conn:
        items = retrospectives.get_pending_action_items(
            conn, project_id=project_id
        )
        return jsonify({"action_items": items, "count": len(items)})


@app.route("/api/retrospectives/summary", methods=["GET"])
@require_auth
def get_retrospective_summary():
    """Get summary statistics for retrospectives.

    Query params:
        project_id: Filter by project (optional)
        days: Number of days to analyze (default 90)
    """
    project_id = request.args.get("project_id", type=int)
    days = request.args.get("days", 90, type=int)

    with get_db_connection() as conn:
        summary = retrospectives.get_retrospective_summary(
            conn, project_id=project_id, days=days
        )
        return jsonify(summary)


@app.route("/api/milestones/<int:milestone_id>/retrospective", methods=["GET"])
@require_auth
def get_milestone_retrospective(milestone_id):
    """Get the retrospective for a specific milestone."""
    with get_db_connection() as conn:
        retro = retrospectives.get_milestone_retrospective(conn, milestone_id)
        if not retro:
            return jsonify(
                {
                    "retrospective": None,
                    "categories": retrospectives.RETRO_CATEGORIES,
                }
            )
        return jsonify(
            {
                "retrospective": retro,
                "categories": retrospectives.RETRO_CATEGORIES,
            }
        )


@app.route("/api/milestones/progress", methods=["GET"])
@require_auth
def get_milestones_progress():
    """Get comprehensive progress tracking for milestones.

    Query params:
        project_id: Filter by project (optional)
        include_items: Include feature/bug lists per milestone (default false)

    Returns:
        summary: Overall progress statistics
        by_status: Breakdown by milestone status
        by_project: Progress grouped by project
        milestones: Individual milestone progress details
        timeline: Upcoming and overdue milestones
    """
    project_id = request.args.get("project_id", type=int)
    include_items = request.args.get("include_items", "").lower() == "true"

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Base filter
        where_clause = "WHERE m.project_id = ?" if project_id else ""
        params = (project_id,) if project_id else ()

        # Get milestones with progress data
        milestones = conn.execute(
            """
            SELECT m.*, p.name as project_name,
                   COALESCE(fc.total, 0) as total_features,
                   COALESCE(fc.completed, 0) as completed_features,
                   COALESCE(fc.in_progress, 0) as in_progress_features,
                   COALESCE(bc.total, 0) as total_bugs,
                   COALESCE(bc.resolved, 0) as resolved_bugs,
                   COALESCE(bc.open_bugs, 0) as open_bugs,
                   COALESCE(fc.estimated_hours, 0) as estimated_hours,
                   COALESCE(fc.actual_hours, 0) as actual_hours
            FROM milestones m
            LEFT JOIN projects p ON m.project_id = p.id
            LEFT JOIN (
                SELECT milestone_id,
                       COUNT(*) as total,
                       SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completed,
                       SUM(CASE WHEN status = 'in_progress' THEN 1 ELSE 0 END) as in_progress,
                       SUM(COALESCE(estimated_hours, 0)) as estimated_hours,
                       SUM(COALESCE(actual_hours, 0)) as actual_hours
                FROM features GROUP BY milestone_id
            ) fc ON m.id = fc.milestone_id
            LEFT JOIN (
                SELECT milestone_id,
                       COUNT(*) as total,
                       SUM(CASE WHEN status = 'resolved' THEN 1 ELSE 0 END) as resolved,
                       SUM(CASE WHEN status IN ('open', 'in_progress') THEN 1 ELSE 0 END) as open_bugs
                FROM bugs GROUP BY milestone_id
            ) bc ON m.id = bc.milestone_id
            {where_clause}
            ORDER BY m.target_date, m.name
        """,
            params,
        ).fetchall()

        milestone_list = []
        by_status = {
            "open": [],
            "in_progress": [],
            "completed": [],
            "other": [],
        }
        by_project = {}
        upcoming = []
        overdue = []

        total_features = 0
        total_completed_features = 0
        total_bugs = 0
        total_resolved_bugs = 0
        total_estimated = 0
        total_actual = 0

        for m in milestones:
            m_dict = dict(m)

            # Calculate completion percentage
            items_total = m_dict["total_features"] + m_dict["total_bugs"]
            items_done = m_dict["completed_features"] + m_dict["resolved_bugs"]
            m_dict["calculated_progress"] = round(
                (items_done / items_total * 100) if items_total > 0 else 0, 1
            )

            # Determine if overdue
            if m_dict["target_date"]:
                from datetime import date, datetime

                try:
                    target = (
                        datetime.strptime(
                            m_dict["target_date"], "%Y-%m-%d"
                        ).date()
                        if isinstance(m_dict["target_date"], str)
                        else m_dict["target_date"]
                    )
                    today = date.today()
                    m_dict["days_until_due"] = (target - today).days
                    m_dict["is_overdue"] = (
                        target < today and m_dict["status"] != "completed"
                    )

                    if m_dict["is_overdue"]:
                        overdue.append(
                            {
                                "id": m_dict["id"],
                                "name": m_dict["name"],
                                "project": m_dict["project_name"],
                                "target_date": m_dict["target_date"],
                                "days_overdue": -m_dict["days_until_due"],
                                "progress": m_dict["calculated_progress"],
                            }
                        )
                    elif (
                        m_dict["days_until_due"] <= 14
                        and m_dict["status"] != "completed"
                    ):
                        upcoming.append(
                            {
                                "id": m_dict["id"],
                                "name": m_dict["name"],
                                "project": m_dict["project_name"],
                                "target_date": m_dict["target_date"],
                                "days_until_due": m_dict["days_until_due"],
                                "progress": m_dict["calculated_progress"],
                            }
                        )
                except (ValueError, TypeError):
                    m_dict["days_until_due"] = None
                    m_dict["is_overdue"] = False

            # Aggregate totals
            total_features += m_dict["total_features"]
            total_completed_features += m_dict["completed_features"]
            total_bugs += m_dict["total_bugs"]
            total_resolved_bugs += m_dict["resolved_bugs"]
            total_estimated += m_dict["estimated_hours"] or 0
            total_actual += m_dict["actual_hours"] or 0

            # Group by status
            status = m_dict["status"] or "other"
            if status in by_status:
                by_status[status].append(m_dict["id"])
            else:
                by_status["other"].append(m_dict["id"])

            # Group by project
            proj_name = m_dict["project_name"] or "Unassigned"
            if proj_name not in by_project:
                by_project[proj_name] = {
                    "project_id": m_dict["project_id"],
                    "milestones": 0,
                    "completed": 0,
                    "features": 0,
                    "bugs": 0,
                }
            by_project[proj_name]["milestones"] += 1
            by_project[proj_name]["features"] += m_dict["total_features"]
            by_project[proj_name]["bugs"] += m_dict["total_bugs"]
            if m_dict["status"] == "completed":
                by_project[proj_name]["completed"] += 1

            # Include feature/bug items if requested
            if include_items:
                features = conn.execute(
                    "SELECT id, name, status FROM features WHERE milestone_id = ?",
                    (m_dict["id"],),
                ).fetchall()
                bugs = conn.execute(
                    "SELECT id, title, status, severity FROM bugs WHERE milestone_id = ?",
                    (m_dict["id"],),
                ).fetchall()
                m_dict["features"] = [dict(f) for f in features]
                m_dict["bugs"] = [dict(b) for b in bugs]

            milestone_list.append(m_dict)

        # Build summary
        total_milestones = len(milestone_list)
        completed_milestones = len(by_status["completed"])

        summary = {
            "total_milestones": total_milestones,
            "completed_milestones": completed_milestones,
            "in_progress_milestones": len(by_status["in_progress"]),
            "open_milestones": len(by_status["open"]),
            "completion_rate": round(
                (
                    (completed_milestones / total_milestones * 100)
                    if total_milestones > 0
                    else 0
                ),
                1,
            ),
            "total_features": total_features,
            "completed_features": total_completed_features,
            "feature_completion_rate": round(
                (
                    (total_completed_features / total_features * 100)
                    if total_features > 0
                    else 0
                ),
                1,
            ),
            "total_bugs": total_bugs,
            "resolved_bugs": total_resolved_bugs,
            "bug_resolution_rate": round(
                (
                    (total_resolved_bugs / total_bugs * 100)
                    if total_bugs > 0
                    else 0
                ),
                1,
            ),
            "total_estimated_hours": round(total_estimated, 1),
            "total_actual_hours": round(total_actual, 1),
            "overdue_count": len(overdue),
            "upcoming_count": len(upcoming),
        }

        return jsonify(
            {
                "summary": summary,
                "by_status": {
                    k: {"count": len(v), "milestone_ids": v}
                    for k, v in by_status.items()
                },
                "by_project": by_project,
                "milestones": milestone_list,
                "timeline": {
                    "overdue": sorted(
                        overdue, key=lambda x: x["days_overdue"], reverse=True
                    ),
                    "upcoming": sorted(
                        upcoming, key=lambda x: x["days_until_due"]
                    ),
                },
            }
        )


@app.route("/api/milestones/critical-path", methods=["GET"])
@require_auth
def get_milestones_critical_path():
    """Calculate the critical path through project milestones.

    The critical path identifies the longest sequence of dependent milestones
    that determines the minimum project duration.

    Query params:
        project_id: Filter by project (optional)
        include_completed: Include completed milestones (default false)
        include_features: Include feature breakdown (default false)
    """
    project_id = request.args.get("project_id", type=int)
    include_completed = (
        request.args.get("include_completed", "false").lower() == "true"
    )
    include_features = (
        request.args.get("include_features", "false").lower() == "true"
    )

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        conditions = []
        params = []
        if project_id:
            conditions.append("m.project_id = ?")
            params.append(project_id)
        if not include_completed:
            conditions.append("m.status NOT IN ('completed', 'cancelled')")

        where_clause = (
            f"WHERE {' AND '.join(conditions)}" if conditions else ""
        )

        milestones = conn.execute(
            """
            SELECT m.id, m.project_id, m.name, m.description, m.target_date, m.status, m.progress,
                   p.name as project_name,
                   COALESCE(fc.total, 0) as total_features,
                   COALESCE(fc.completed, 0) as completed_features,
                   COALESCE(fc.in_progress, 0) as in_progress_features,
                   COALESCE(fc.pending, 0) as pending_features,
                   COALESCE(fc.estimated_hours, 0) as estimated_hours,
                   COALESCE(fc.actual_hours, 0) as actual_hours,
                   COALESCE(bc.total, 0) as total_bugs,
                   COALESCE(bc.open_bugs, 0) as open_bugs,
                   julianday(m.target_date) - julianday('now') as days_remaining,
                   CASE WHEN m.target_date < DATE('now') AND m.status != 'completed' THEN 1 ELSE 0 END as is_overdue
            FROM milestones m
            LEFT JOIN projects p ON m.project_id = p.id
            LEFT JOIN (
                SELECT milestone_id, COUNT(*) as total,
                       SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completed,
                       SUM(CASE WHEN status = 'in_progress' THEN 1 ELSE 0 END) as in_progress,
                       SUM(CASE WHEN status NOT IN ('completed', 'in_progress') THEN 1 ELSE 0 END) as pending,
                       SUM(COALESCE(estimated_hours, 0)) as estimated_hours,
                       SUM(COALESCE(actual_hours, 0)) as actual_hours
                FROM features GROUP BY milestone_id
            ) fc ON m.id = fc.milestone_id
            LEFT JOIN (
                SELECT milestone_id, COUNT(*) as total,
                       SUM(CASE WHEN status IN ('open', 'in_progress') THEN 1 ELSE 0 END) as open_bugs
                FROM bugs GROUP BY milestone_id
            ) bc ON m.id = bc.milestone_id
            {where_clause}
            ORDER BY m.target_date ASC, m.id
        """,
            params,
        ).fetchall()

        if not milestones:
            return jsonify(
                {
                    "critical_path": [],
                    "total_duration_days": 0,
                    "message": "No milestones found",
                }
            )

        # Build milestone nodes with work weights
        nodes = {}
        for m in milestones:
            md = dict(m)
            total_items = md["total_features"] + md["total_bugs"]
            completed = md["completed_features"]
            remaining = total_items - completed
            completion_pct = (
                (completed / total_items * 100) if total_items > 0 else 0
            )

            # Work weight based on remaining work
            work_weight = (
                remaining * 2
                + md["open_bugs"] * 3
                + max(0, md["estimated_hours"] - md["actual_hours"]) * 0.5
            )

            md["work_weight"] = round(work_weight, 1)
            md["completion_pct"] = round(completion_pct, 1)
            md["remaining_items"] = remaining
            md["is_critical"] = False
            md["slack"] = 0
            nodes[md["id"]] = md

        # Group by project and create dependency chains
        projects = {}
        for mid, node in nodes.items():
            pid = node["project_id"]
            if pid not in projects:
                projects[pid] = []
            projects[pid].append(node)

        # Sort by target date within each project
        dependencies = []
        for pid in projects:
            projects[pid].sort(
                key=lambda x: (x["target_date"] or "9999-12-31", x["id"])
            )
            for i in range(1, len(projects[pid])):
                prev, curr = projects[pid][i - 1], projects[pid][i]
                dependencies.append(
                    {
                        "from_id": prev["id"],
                        "to_id": curr["id"],
                        "from_name": prev["name"],
                        "to_name": curr["name"],
                    }
                )

        # Find critical path using longest path algorithm
        def find_critical_path():
            successors = {mid: [] for mid in nodes}
            predecessors = {mid: [] for mid in nodes}
            for dep in dependencies:
                if dep["from_id"] in successors and dep["to_id"] in nodes:
                    successors[dep["from_id"]].append(dep["to_id"])
                    predecessors[dep["to_id"]].append(dep["from_id"])

            start_nodes = [mid for mid in nodes if not predecessors[mid]]
            if not start_nodes:
                start_nodes = list(nodes.keys())[:1] if nodes else []

            def dfs(nid, path, weight):
                new_path = path + [nid]
                new_weight = weight + nodes[nid]["work_weight"]
                if not successors[nid]:
                    return new_path, new_weight
                best_path, best_weight = new_path, new_weight
                for succ in successors[nid]:
                    sp, sw = dfs(succ, new_path, new_weight)
                    if sw > best_weight:
                        best_path, best_weight = sp, sw
                return best_path, best_weight

            longest_path, max_weight = [], 0
            for start in start_nodes:
                p, w = dfs(start, [], 0)
                if w > max_weight:
                    longest_path, max_weight = p, w
            return longest_path, max_weight

        critical_ids, total_weight = find_critical_path()

        # Mark critical nodes and calculate slack
        for mid in critical_ids:
            nodes[mid]["is_critical"] = True
        for mid, node in nodes.items():
            if not node["is_critical"]:
                node["slack"] = round(total_weight - node["work_weight"], 1)

        # Build critical path result
        critical_path = []
        cumulative = 0
        for mid in critical_ids:
            n = nodes[mid]
            cumulative += n["work_weight"]
            cp = {
                "id": n["id"],
                "name": n["name"],
                "project_id": n["project_id"],
                "project_name": n["project_name"],
                "target_date": n["target_date"],
                "status": n["status"],
                "progress": n["progress"],
                "completion_pct": n["completion_pct"],
                "work_weight": n["work_weight"],
                "cumulative_weight": round(cumulative, 1),
                "remaining_items": n["remaining_items"],
                "total_features": n["total_features"],
                "open_bugs": n["open_bugs"],
                "days_remaining": n["days_remaining"],
                "is_overdue": bool(n["is_overdue"]),
            }
            if include_features:
                feats = conn.execute(
                    "SELECT id, name, status, priority FROM features WHERE milestone_id = ?",
                    (mid,),
                ).fetchall()
                cp["features"] = [dict(f) for f in feats]
            critical_path.append(cp)

        # Calculate total duration
        total_duration = 0
        if (
            critical_path
            and critical_path[0].get("target_date")
            and critical_path[-1].get("target_date")
        ):
            try:
                start = datetime.strptime(
                    critical_path[0]["target_date"], "%Y-%m-%d"
                )
                end = datetime.strptime(
                    critical_path[-1]["target_date"], "%Y-%m-%d"
                )
                total_duration = (end - start).days
            except Exception:
                total_duration = int(total_weight)

        # Slack analysis
        slack_milestones = [
            {
                "id": n["id"],
                "name": n["name"],
                "project_name": n["project_name"],
                "target_date": n["target_date"],
                "slack": n["slack"],
                "completion_pct": n["completion_pct"],
            }
            for mid, n in nodes.items()
            if not n["is_critical"]
        ]
        slack_milestones.sort(key=lambda x: x["slack"], reverse=True)

        # Risk factors
        risks = []
        overdue_critical = [m for m in critical_path if m["is_overdue"]]
        if overdue_critical:
            risks.append(
                {
                    "type": "critical_overdue",
                    "severity": "high",
                    "message": f"{
                        len(overdue_critical)} critical milestone(s) overdue",
                    "milestones": [m["name"] for m in overdue_critical],
                }
            )

        tight = [
            m
            for m in critical_path
            if m["days_remaining"] and 0 < m["days_remaining"] < 7
        ]
        if tight:
            risks.append(
                {
                    "type": "tight_deadline",
                    "severity": "medium",
                    "message": f"{len(tight)} milestone(s) due within 7 days",
                    "milestones": [m["name"] for m in tight],
                }
            )

        low_progress = [
            m
            for m in critical_path
            if m["completion_pct"] < 25 and m["total_features"] > 0
        ]
        if low_progress:
            risks.append(
                {
                    "type": "low_progress",
                    "severity": "medium",
                    "message": f"{
                        len(low_progress)} critical milestone(s) < 25% complete",
                    "milestones": [m["name"] for m in low_progress],
                }
            )

        return jsonify(
            {
                "critical_path": critical_path,
                "critical_path_length": len(critical_path),
                "total_duration_days": total_duration,
                "total_work_weight": round(total_weight, 1),
                "dependencies": dependencies,
                "slack_analysis": slack_milestones,
                "risk_factors": risks,
                "summary": {
                    "total_milestones": len(nodes),
                    "critical_milestones": len(critical_path),
                    "milestones_with_slack": len(slack_milestones),
                    "risk_count": len(risks),
                    "overall_risk": (
                        "high"
                        if any(r["severity"] == "high" for r in risks)
                        else "medium" if risks else "low"
                    ),
                },
            }
        )


# ============================================================================
# SPRINT MANAGEMENT API
# ============================================================================


@app.route("/api/sprints", methods=["GET"])
@require_auth
def get_sprints():
    """Get all sprints with optional filtering.

    Query params:
        project_id: Filter by project
        status: Filter by status (planning, active, completed, cancelled)
        limit: Max results (default 50)
        include_stats: Include task/feature counts (default true)
    """
    project_id = request.args.get("project_id", type=int)
    status = request.args.get("status")
    limit = min(int(request.args.get("limit", 50)), 200)
    include_stats = request.args.get("include_stats", "true").lower() == "true"

    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            query = """
                SELECT s.*, p.name as project_name
                FROM sprints s
                LEFT JOIN projects p ON s.project_id = p.id
                WHERE 1=1
            """
            params = []
            if project_id:
                query += " AND s.project_id = ?"
                params.append(project_id)
            if status:
                query += " AND s.status = ?"
                params.append(status)
            query += " ORDER BY s.start_date DESC LIMIT ?"
            params.append(limit)

            sprints = conn.execute(query, params).fetchall()
            result = []

            for sprint in sprints:
                sprint_dict = dict(sprint)
                if include_stats:
                    # Get task counts
                    task_stats = conn.execute(
                        """
                        SELECT COUNT(*) as total,
                            SUM(CASE WHEN t.status = 'completed' THEN 1 ELSE 0 END) as completed,
                            SUM(CASE WHEN t.status = 'in_progress' THEN 1 ELSE 0 END) as in_progress,
                            SUM(st.story_points) as total_points,
                            SUM(CASE WHEN t.status = 'completed' THEN st.story_points ELSE 0 END) as completed_points
                        FROM sprint_tasks st
                        JOIN task_queue t ON st.task_id = t.id
                        WHERE st.sprint_id = ? AND st.removed_at IS NULL
                    """,
                        (sprint["id"],),
                    ).fetchone()

                    # Get feature counts
                    feature_stats = conn.execute(
                        """
                        SELECT COUNT(*) as total,
                            SUM(CASE WHEN f.status = 'completed' THEN 1 ELSE 0 END) as completed,
                            SUM(sf.story_points) as total_points,
                            SUM(CASE WHEN f.status = 'completed' THEN sf.story_points ELSE 0 END) as completed_points
                        FROM sprint_features sf
                        JOIN features f ON sf.feature_id = f.id
                        WHERE sf.sprint_id = ?
                    """,
                        (sprint["id"],),
                    ).fetchone()

                    # Get bug counts
                    bug_stats = conn.execute(
                        """
                        SELECT COUNT(*) as total,
                            SUM(CASE WHEN b.status = 'resolved' THEN 1 ELSE 0 END) as resolved
                        FROM sprint_bugs sb
                        JOIN bugs b ON sb.bug_id = b.id
                        WHERE sb.sprint_id = ?
                    """,
                        (sprint["id"],),
                    ).fetchone()

                    sprint_dict["stats"] = {
                        "tasks": {
                            "total": task_stats["total"] or 0,
                            "completed": task_stats["completed"] or 0,
                            "in_progress": task_stats["in_progress"] or 0,
                        },
                        "features": {
                            "total": feature_stats["total"] or 0,
                            "completed": feature_stats["completed"] or 0,
                        },
                        "bugs": {
                            "total": bug_stats["total"] or 0,
                            "resolved": bug_stats["resolved"] or 0,
                        },
                        "points": {
                            "planned": (task_stats["total_points"] or 0)
                            + (feature_stats["total_points"] or 0),
                            "completed": (task_stats["completed_points"] or 0)
                            + (feature_stats["completed_points"] or 0),
                        },
                    }

                    # Calculate progress
                    total_points = sprint_dict["stats"]["points"]["planned"]
                    completed_points = sprint_dict["stats"]["points"][
                        "completed"
                    ]
                    sprint_dict["progress"] = (
                        round(completed_points / total_points * 100, 1)
                        if total_points > 0
                        else 0
                    )

                result.append(sprint_dict)

            return jsonify({"sprints": result, "count": len(result)})
    except Exception as e:
        logger.error(f"Failed to get sprints: {e}")
        return jsonify({"error": str(e)}), 500


# DUPLICATE REMOVED: @app.route('/api/sprints', methods=['POST'])
# DUPLICATE REMOVED: @require_auth
# DUPLICATE REMOVED: def create_sprint():
# DUPLICATE REMOVED:     """Create a new sprint.

# DUPLICATE REMOVED:     Request body:
# DUPLICATE REMOVED:         name: Sprint name (required)
# DUPLICATE REMOVED:         goal: Sprint goal/objective
# DUPLICATE REMOVED:         project_id: Associated project ID
# DUPLICATE REMOVED:         start_date: Sprint start date (YYYY-MM-DD)
# DUPLICATE REMOVED:         end_date: Sprint end date (YYYY-MM-DD)
# DUPLICATE REMOVED:         capacity_hours: Team capacity in hours
# DUPLICATE REMOVED:     """
# DUPLICATE REMOVED:     data = request.get_json() or {}
# DUPLICATE REMOVED:     name = data.get('name')
# DUPLICATE REMOVED:     if not name:
# DUPLICATE REMOVED:         return jsonify({'error': 'Sprint name is
# required'}), 400

# DUPLICATE REMOVED:     start_date = data.get('start_date')
# DUPLICATE REMOVED:     end_date = data.get('end_date')
# DUPLICATE REMOVED:     if not start_date or not end_date:
# DUPLICATE REMOVED:         return jsonify({'error': 'Start and end dates
# are required'}), 400

# DUPLICATE REMOVED:     try:
# DUPLICATE REMOVED:         with get_db_connection() as conn:
# DUPLICATE REMOVED:             cursor = conn.execute("""
# DUPLICATE REMOVED:                 INSERT INTO sprints (name, goal, project_id, start_date, end_date, capacity_hours, status, created_by)
# DUPLICATE REMOVED:                 VALUES (?, ?, ?, ?, ?, ?, 'planning', ?)
# DUPLICATE REMOVED:             """, (name, data.get('goal'), data.get('project_id'), start_date, end_date,
# DUPLICATE REMOVED:                   data.get('capacity_hours', 0), session.get('user', 'unknown')))
# DUPLICATE REMOVED:             sprint_id = cursor.lastrowid
# DUPLICATE REMOVED:             conn.commit()
# DUPLICATE REMOVED:             log_activity('create', 'sprint', sprint_id, f"name={name}")
# DUPLICATE REMOVED:             return jsonify({'success': True, 'sprint_id': sprint_id, 'name': name})
# DUPLICATE REMOVED:     except Exception as e:
# DUPLICATE REMOVED:         logger.error(f"Failed to create sprint: {e}")
# DUPLICATE REMOVED:         return jsonify({'error': str(e)}), 500


@app.route("/api/sprints/<int:sprint_id>", methods=["GET"])
@require_auth
def get_sprint(sprint_id):
    """Get a single sprint with full details."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            sprint = conn.execute(
                """
                SELECT s.*, p.name as project_name
                FROM sprints s
                LEFT JOIN projects p ON s.project_id = p.id
                WHERE s.id = ?
            """,
                (sprint_id,),
            ).fetchone()

            if not sprint:
                return jsonify({"error": "Sprint not found"}), 404

            sprint_dict = dict(sprint)

            # Get tasks
            tasks = conn.execute(
                """
                SELECT t.*, st.story_points, st.added_at
                FROM sprint_tasks st
                JOIN task_queue t ON st.task_id = t.id
                WHERE st.sprint_id = ? AND st.removed_at IS NULL
                ORDER BY t.priority DESC
            """,
                (sprint_id,),
            ).fetchall()
            sprint_dict["tasks"] = [dict(t) for t in tasks]

            # Get features
            features = conn.execute(
                """
                SELECT f.*, sf.story_points, sf.added_at
                FROM sprint_features sf
                JOIN features f ON sf.feature_id = f.id
                WHERE sf.sprint_id = ?
                ORDER BY f.priority DESC
            """,
                (sprint_id,),
            ).fetchall()
            sprint_dict["features"] = [dict(f) for f in features]

            # Get bugs
            bugs = conn.execute(
                """
                SELECT b.*, sb.story_points, sb.added_at
                FROM sprint_bugs sb
                JOIN bugs b ON sb.bug_id = b.id
                WHERE sb.sprint_id = ?
                ORDER BY b.severity DESC
            """,
                (sprint_id,),
            ).fetchall()
            sprint_dict["bugs"] = [dict(b) for b in bugs]

            # Calculate stats
            total_points = sum(
                t.get("story_points", 0) or 0 for t in sprint_dict["tasks"]
            )
            total_points += sum(
                f.get("story_points", 0) or 0 for f in sprint_dict["features"]
            )
            completed_points = sum(
                (t.get("story_points", 0) or 0)
                for t in sprint_dict["tasks"]
                if t["status"] == "completed"
            )
            completed_points += sum(
                (f.get("story_points", 0) or 0)
                for f in sprint_dict["features"]
                if f["status"] == "completed"
            )

            sprint_dict["stats"] = {
                "total_points": total_points,
                "completed_points": completed_points,
                "progress": (
                    round(completed_points / total_points * 100, 1)
                    if total_points > 0
                    else 0
                ),
                "task_count": len(sprint_dict["tasks"]),
                "feature_count": len(sprint_dict["features"]),
                "bug_count": len(sprint_dict["bugs"]),
            }

            return jsonify(sprint_dict)
    except Exception as e:
        logger.error(f"Failed to get sprint: {e}")
        return jsonify({"error": str(e)}), 500


# DUPLICATE REMOVED: @app.route('/api/sprints/<int:sprint_id>', methods=['PUT'])
# DUPLICATE REMOVED: @require_auth
# DUPLICATE REMOVED: def update_sprint(sprint_id):
# DUPLICATE REMOVED:     """Update a sprint."""
# DUPLICATE REMOVED:     data = request.get_json() or {}
# DUPLICATE REMOVED:     try:
# DUPLICATE REMOVED:         with get_db_connection() as conn:
# DUPLICATE REMOVED:             sprint = conn.execute("SELECT * FROM sprints WHERE id = ?", (sprint_id,)).fetchone()
# DUPLICATE REMOVED:             if not sprint:
# DUPLICATE REMOVED:                 return jsonify({'error': 'Sprint not
# found'}), 404

# DUPLICATE REMOVED:             updates = []
# DUPLICATE REMOVED:             params = []
# DUPLICATE REMOVED:             for field in ['name', 'goal', 'project_id', 'start_date', 'end_date', 'status', 'capacity_hours', 'notes', 'retrospective']:
# DUPLICATE REMOVED:                 if field in data:
# DUPLICATE REMOVED:                     updates.append(f"{field} = ?")
# DUPLICATE REMOVED:                     params.append(data[field])

# DUPLICATE REMOVED:             if not updates:
# DUPLICATE REMOVED:                 return jsonify({'error': 'No fields
# to update'}), 400

# Handle status transitions
# DUPLICATE REMOVED:             if 'status' in data:
# DUPLICATE REMOVED:                 new_status = data['status']
# DUPLICATE REMOVED:                 if new_status == 'completed':
# DUPLICATE REMOVED:                     updates.append("completed_at = CURRENT_TIMESTAMP")
# Calculate actual velocity
# DUPLICATE REMOVED:                     result = conn.execute("""
# DUPLICATE REMOVED:                         SELECT SUM(CASE WHEN t.status = 'completed' THEN st.story_points ELSE 0 END) as velocity
# DUPLICATE REMOVED:                         FROM sprint_tasks st
# DUPLICATE REMOVED:                         JOIN task_queue t ON st.task_id = t.id
# DUPLICATE REMOVED:                         WHERE st.sprint_id = ?
# DUPLICATE REMOVED:                     """, (sprint_id,)).fetchone()
# DUPLICATE REMOVED:                     updates.append("velocity_actual = ?")
# DUPLICATE REMOVED:                     params.append(result['velocity'] or 0)

# DUPLICATE REMOVED:             updates.append("updated_at = CURRENT_TIMESTAMP")
# DUPLICATE REMOVED:             params.append(sprint_id)
# DUPLICATE REMOVED:             conn.execute(f"UPDATE sprints SET {', '.join(updates)} WHERE id = ?", params)
# DUPLICATE REMOVED:             conn.commit()
# DUPLICATE REMOVED:             log_activity('update', 'sprint', sprint_id, f"fields={list(data.keys())}")
# DUPLICATE REMOVED:             return jsonify({'success': True, 'sprint_id': sprint_id})
# DUPLICATE REMOVED:     except Exception as e:
# DUPLICATE REMOVED:         logger.error(f"Failed to update sprint: {e}")
# DUPLICATE REMOVED:         return jsonify({'error': str(e)}), 500


# DUPLICATE REMOVED: @app.route('/api/sprints/<int:sprint_id>', methods=['DELETE'])
# DUPLICATE REMOVED: @require_auth
# DUPLICATE REMOVED: def delete_sprint(sprint_id):
# DUPLICATE REMOVED:     """Delete a sprint."""
# DUPLICATE REMOVED:     try:
# DUPLICATE REMOVED:         with get_db_connection() as conn:
# DUPLICATE REMOVED:             result = conn.execute("DELETE FROM sprints WHERE id = ?", (sprint_id,))
# DUPLICATE REMOVED:             if result.rowcount == 0:
# DUPLICATE REMOVED:                 return jsonify({'error': 'Sprint not found'}), 404
# Clean up associations
# DUPLICATE REMOVED:             conn.execute("DELETE FROM sprint_tasks WHERE sprint_id = ?", (sprint_id,))
# DUPLICATE REMOVED:             conn.execute("DELETE FROM sprint_features WHERE sprint_id = ?", (sprint_id,))
# DUPLICATE REMOVED:             conn.execute("DELETE FROM sprint_bugs WHERE sprint_id = ?", (sprint_id,))
# DUPLICATE REMOVED:             conn.execute("DELETE FROM sprint_daily_snapshots WHERE sprint_id = ?", (sprint_id,))
# DUPLICATE REMOVED:             conn.commit()
# DUPLICATE REMOVED:             log_activity('delete', 'sprint', sprint_id)
# DUPLICATE REMOVED:             return jsonify({'success': True, 'sprint_id': sprint_id})
# DUPLICATE REMOVED:     except Exception as e:
# DUPLICATE REMOVED:         logger.error(f"Failed to delete sprint: {e}")
# DUPLICATE REMOVED:         return jsonify({'error': str(e)}), 500


# DUPLICATE REMOVED: @app.route('/api/sprints/<int:sprint_id>/start', methods=['POST'])
# DUPLICATE REMOVED: @require_auth
# DUPLICATE REMOVED: def start_sprint(sprint_id):
# DUPLICATE REMOVED:     """Start a sprint (change status to active)."""
# DUPLICATE REMOVED:     try:
# DUPLICATE REMOVED:         with get_db_connection() as conn:
# DUPLICATE REMOVED:             conn.row_factory = sqlite3.Row
# DUPLICATE REMOVED:             sprint = conn.execute("SELECT * FROM sprints WHERE id = ?", (sprint_id,)).fetchone()
# DUPLICATE REMOVED:             if not sprint:
# DUPLICATE REMOVED:                 return jsonify({'error': 'Sprint not found'}), 404
# DUPLICATE REMOVED:             if sprint['status'] != 'planning':
# DUPLICATE REMOVED:                 return jsonify({'error': f"Cannot
# start sprint with status '{sprint['status']}'"}), 400

# Check for active sprints in same project
# DUPLICATE REMOVED:             if sprint['project_id']:
# DUPLICATE REMOVED:                 active = conn.execute("SELECT id, name FROM sprints WHERE project_id = ? AND status = 'active' AND id != ?",
# DUPLICATE REMOVED:                                      (sprint['project_id'], sprint_id)).fetchone()
# DUPLICATE REMOVED:                 if active:
# DUPLICATE REMOVED:                     return jsonify({'error':
# f"Project already has active sprint: {active['name']}"}), 400

# Calculate planned velocity
# DUPLICATE REMOVED:             result = conn.execute("""
# DUPLICATE REMOVED:                 SELECT SUM(st.story_points) as points FROM sprint_tasks st WHERE st.sprint_id = ?
# DUPLICATE REMOVED:             """, (sprint_id,)).fetchone()
# DUPLICATE REMOVED:             planned_velocity = result['points'] or 0

# DUPLICATE REMOVED:             conn.execute("""
# DUPLICATE REMOVED:                 UPDATE sprints SET status = 'active', velocity_planned = ?, updated_at = CURRENT_TIMESTAMP WHERE id = ?
# DUPLICATE REMOVED:             """, (planned_velocity, sprint_id))
# DUPLICATE REMOVED:             conn.commit()
# DUPLICATE REMOVED:             log_activity('start', 'sprint', sprint_id)
# DUPLICATE REMOVED:             return jsonify({'success': True, 'sprint_id': sprint_id, 'velocity_planned': planned_velocity})
# DUPLICATE REMOVED:     except Exception as e:
# DUPLICATE REMOVED:         logger.error(f"Failed to start sprint: {e}")
# DUPLICATE REMOVED:         return jsonify({'error': str(e)}), 500


# DUPLICATE REMOVED: @app.route('/api/sprints/<int:sprint_id>/complete', methods=['POST'])
# DUPLICATE REMOVED: @require_auth
# DUPLICATE REMOVED: def complete_sprint(sprint_id):
# DUPLICATE REMOVED:     """Complete a sprint.

# DUPLICATE REMOVED:     Request body:
# DUPLICATE REMOVED:         retrospective: Sprint retrospective notes
# DUPLICATE REMOVED:         carry_over: Whether to carry incomplete items to next sprint (default false)
# DUPLICATE REMOVED:         next_sprint_id: Sprint ID to carry items to
# DUPLICATE REMOVED:     """
# DUPLICATE REMOVED:     data = request.get_json() or {}
# DUPLICATE REMOVED:     try:
# DUPLICATE REMOVED:         with get_db_connection() as conn:
# DUPLICATE REMOVED:             conn.row_factory = sqlite3.Row
# DUPLICATE REMOVED:             sprint = conn.execute("SELECT * FROM sprints WHERE id = ?", (sprint_id,)).fetchone()
# DUPLICATE REMOVED:             if not sprint:
# DUPLICATE REMOVED:                 return jsonify({'error': 'Sprint not found'}), 404
# DUPLICATE REMOVED:             if sprint['status'] != 'active':
# DUPLICATE REMOVED:                 return jsonify({'error': f"Cannot
# complete sprint with status '{sprint['status']}'"}), 400

# Calculate actual velocity
# DUPLICATE REMOVED:             result = conn.execute("""
# DUPLICATE REMOVED:                 SELECT SUM(CASE WHEN t.status = 'completed' THEN st.story_points ELSE 0 END) as completed,
# DUPLICATE REMOVED:                        SUM(st.story_points) as total
# DUPLICATE REMOVED:                 FROM sprint_tasks st
# DUPLICATE REMOVED:                 JOIN task_queue t ON st.task_id = t.id
# DUPLICATE REMOVED:                 WHERE st.sprint_id = ?
# DUPLICATE REMOVED:             """, (sprint_id,)).fetchone()
# DUPLICATE REMOVED:             velocity_actual = result['completed'] or 0

# DUPLICATE REMOVED:             conn.execute("""
# DUPLICATE REMOVED:                 UPDATE sprints SET status = 'completed', velocity_actual = ?,
# DUPLICATE REMOVED:                     retrospective = COALESCE(?, retrospective),
# DUPLICATE REMOVED:                     completed_at = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP
# DUPLICATE REMOVED:                 WHERE id = ?
# DUPLICATE REMOVED:             """, (velocity_actual,
# data.get('retrospective'), sprint_id))

# Handle carry over
# DUPLICATE REMOVED:             carried_over = []
# DUPLICATE REMOVED:             if data.get('carry_over') and data.get('next_sprint_id'):
# DUPLICATE REMOVED:                 next_sprint_id = data['next_sprint_id']
# DUPLICATE REMOVED:                 incomplete_tasks = conn.execute("""
# DUPLICATE REMOVED:                     SELECT st.task_id, st.story_points FROM sprint_tasks st
# DUPLICATE REMOVED:                     JOIN task_queue t ON st.task_id = t.id
# DUPLICATE REMOVED:                     WHERE st.sprint_id = ? AND t.status != 'completed'
# DUPLICATE REMOVED:                 """, (sprint_id,)).fetchall()
# DUPLICATE REMOVED:                 for task in incomplete_tasks:
# DUPLICATE REMOVED:                     conn.execute("""
# DUPLICATE REMOVED:                         INSERT OR IGNORE INTO sprint_tasks (sprint_id, task_id, story_points, added_by)
# DUPLICATE REMOVED:                         VALUES (?, ?, ?, ?)
# DUPLICATE REMOVED:                     """, (next_sprint_id, task['task_id'], task['story_points'], session.get('user', 'unknown')))
# DUPLICATE REMOVED:                     carried_over.append(task['task_id'])

# DUPLICATE REMOVED:             conn.commit()
# DUPLICATE REMOVED:             log_activity('complete', 'sprint', sprint_id, f"velocity={velocity_actual}")
# DUPLICATE REMOVED:             return jsonify({
# DUPLICATE REMOVED:                 'success': True, 'sprint_id': sprint_id,
# DUPLICATE REMOVED:                 'velocity_actual': velocity_actual,
# DUPLICATE REMOVED:                 'velocity_planned': sprint['velocity_planned'],
# DUPLICATE REMOVED:                 'carried_over': carried_over
# DUPLICATE REMOVED:             })
# DUPLICATE REMOVED:     except Exception as e:
# DUPLICATE REMOVED:         logger.error(f"Failed to complete sprint: {e}")
# DUPLICATE REMOVED:         return jsonify({'error': str(e)}), 500


@app.route("/api/sprints/<int:sprint_id>/tasks", methods=["GET"])
@require_auth
def get_sprint_tasks(sprint_id):
    """Get all tasks in a sprint."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            tasks = conn.execute(
                """
                SELECT t.*, st.story_points, st.added_at, st.added_by
                FROM sprint_tasks st
                JOIN task_queue t ON st.task_id = t.id
                WHERE st.sprint_id = ? AND st.removed_at IS NULL
                ORDER BY t.priority DESC, t.created_at
            """,
                (sprint_id,),
            ).fetchall()
            return jsonify(
                {"tasks": [dict(t) for t in tasks], "count": len(tasks)}
            )
    except Exception as e:
        logger.error(f"Failed to get sprint tasks: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/sprints/<int:sprint_id>/tasks", methods=["POST"])
@require_auth
def add_sprint_task(sprint_id):
    """Add a task to a sprint.

    Request body:
        task_id: Task ID to add
        story_points: Story points for the task
    """
    data = request.get_json() or {}
    task_id = data.get("task_id")
    if not task_id:
        return jsonify({"error": "task_id is required"}), 400
    try:
        with get_db_connection() as conn:
            # Verify sprint exists
            sprint = conn.execute(
                "SELECT status FROM sprints WHERE id = ?", (sprint_id,)
            ).fetchone()
            if not sprint:
                return jsonify({"error": "Sprint not found"}), 404

            # Verify task exists
            task = conn.execute(
                "SELECT id FROM task_queue WHERE id = ?", (task_id,)
            ).fetchone()
            if not task:
                return jsonify({"error": "Task not found"}), 404

            conn.execute(
                """
                INSERT INTO sprint_tasks (sprint_id, task_id, story_points, added_by)
                VALUES (?, ?, ?, ?)
                ON CONFLICT(sprint_id, task_id) DO UPDATE SET
                    story_points = excluded.story_points, removed_at = NULL
            """,
                (
                    sprint_id,
                    task_id,
                    data.get("story_points", 0),
                    session.get("user", "unknown"),
                ),
            )
            conn.commit()
            log_activity("add_task", "sprint", sprint_id, f"task_id={task_id}")
            return jsonify(
                {"success": True, "sprint_id": sprint_id, "task_id": task_id}
            )
    except Exception as e:
        logger.error(f"Failed to add sprint task: {e}")
        return jsonify({"error": str(e)}), 500


@app.route(
    "/api/sprints/<int:sprint_id>/tasks/<int:task_id>", methods=["DELETE"]
)
@require_auth
def remove_sprint_task(sprint_id, task_id):
    """Remove a task from a sprint."""
    try:
        with get_db_connection() as conn:
            conn.execute(
                """
                UPDATE sprint_tasks SET removed_at = CURRENT_TIMESTAMP, removed_by = ?
                WHERE sprint_id = ? AND task_id = ?
            """,
                (session.get("user", "unknown"), sprint_id, task_id),
            )
            conn.commit()
            log_activity(
                "remove_task", "sprint", sprint_id, f"task_id={task_id}"
            )
            return jsonify({"success": True})
    except Exception as e:
        logger.error(f"Failed to remove sprint task: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/sprints/<int:sprint_id>/features", methods=["POST"])
@require_auth
def add_sprint_feature(sprint_id):
    """Add a feature to a sprint."""
    data = request.get_json() or {}
    feature_id = data.get("feature_id")
    if not feature_id:
        return jsonify({"error": "feature_id is required"}), 400
    try:
        with get_db_connection() as conn:
            conn.execute(
                """
                INSERT INTO sprint_features (sprint_id, feature_id, story_points, added_by)
                VALUES (?, ?, ?, ?)
                ON CONFLICT(sprint_id, feature_id) DO UPDATE SET story_points = excluded.story_points
            """,
                (
                    sprint_id,
                    feature_id,
                    data.get("story_points", 0),
                    session.get("user", "unknown"),
                ),
            )
            conn.commit()
            return jsonify({"success": True})
    except Exception as e:
        logger.error(f"Failed to add sprint feature: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/sprints/<int:sprint_id>/bugs", methods=["POST"])
@require_auth
def add_sprint_bug(sprint_id):
    """Add a bug to a sprint."""
    data = request.get_json() or {}
    bug_id = data.get("bug_id")
    if not bug_id:
        return jsonify({"error": "bug_id is required"}), 400
    try:
        with get_db_connection() as conn:
            conn.execute(
                """
                INSERT INTO sprint_bugs (sprint_id, bug_id, story_points, added_by)
                VALUES (?, ?, ?, ?)
                ON CONFLICT(sprint_id, bug_id) DO UPDATE SET story_points = excluded.story_points
            """,
                (
                    sprint_id,
                    bug_id,
                    data.get("story_points", 0),
                    session.get("user", "unknown"),
                ),
            )
            conn.commit()
            return jsonify({"success": True})
    except Exception as e:
        logger.error(f"Failed to add sprint bug: {e}")
        return jsonify({"error": str(e)}), 500


# DUPLICATE REMOVED: @app.route('/api/sprints/<int:sprint_id>/burndown', methods=['GET'])
# DUPLICATE REMOVED: @require_auth
# DUPLICATE REMOVED: def get_sprint_burndown(sprint_id):
# DUPLICATE REMOVED:     """Get burndown chart data for a sprint."""
# DUPLICATE REMOVED:     try:
# DUPLICATE REMOVED:         with get_db_connection() as conn:
# DUPLICATE REMOVED:             conn.row_factory = sqlite3.Row
# DUPLICATE REMOVED:             sprint = conn.execute("SELECT * FROM sprints WHERE id = ?", (sprint_id,)).fetchone()
# DUPLICATE REMOVED:             if not sprint:
# DUPLICATE REMOVED:                 return jsonify({'error': 'Sprint not
# found'}), 404

# Get daily snapshots
# DUPLICATE REMOVED:             snapshots = conn.execute("""
# DUPLICATE REMOVED:                 SELECT * FROM sprint_daily_snapshots WHERE sprint_id = ? ORDER BY snapshot_date
# DUPLICATE REMOVED:             """, (sprint_id,)).fetchall()

# Calculate ideal burndown
# DUPLICATE REMOVED:             start_date = datetime.strptime(sprint['start_date'], '%Y-%m-%d')
# DUPLICATE REMOVED:             end_date = datetime.strptime(sprint['end_date'], '%Y-%m-%d')
# DUPLICATE REMOVED:             total_days = (end_date - start_date).days
# DUPLICATE REMOVED:             total_points = sprint['velocity_planned'] or 0

# DUPLICATE REMOVED:             ideal_line = []
# DUPLICATE REMOVED:             for i in range(total_days + 1):
# DUPLICATE REMOVED:                 day = start_date + timedelta(days=i)
# DUPLICATE REMOVED:                 remaining = total_points * (1 - i / total_days)
# DUPLICATE REMOVED:                 ideal_line.append({'date':
# day.strftime('%Y-%m-%d'), 'remaining': round(remaining, 1)})

# DUPLICATE REMOVED:             return jsonify({
# DUPLICATE REMOVED:                 'sprint': dict(sprint),
# DUPLICATE REMOVED:                 'ideal_burndown': ideal_line,
# DUPLICATE REMOVED:                 'actual_burndown': [dict(s) for s in snapshots],
# DUPLICATE REMOVED:                 'total_points': total_points,
# DUPLICATE REMOVED:                 'days_total': total_days,
# DUPLICATE REMOVED:                 'days_elapsed': (datetime.now() - start_date).days
# DUPLICATE REMOVED:             })
# DUPLICATE REMOVED:     except Exception as e:
# DUPLICATE REMOVED:         logger.error(f"Failed to get sprint burndown: {e}")
# DUPLICATE REMOVED:         return jsonify({'error': str(e)}), 500


@app.route("/api/sprints/<int:sprint_id>/snapshot", methods=["POST"])
@require_auth
def create_sprint_snapshot(sprint_id):
    """Create a daily snapshot for burndown tracking."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            sprint = conn.execute(
                "SELECT * FROM sprints WHERE id = ?", (sprint_id,)
            ).fetchone()
            if not sprint:
                return jsonify({"error": "Sprint not found"}), 404

            # Calculate current state
            stats = conn.execute(
                """
                SELECT SUM(st.story_points) as total_points,
                    SUM(CASE WHEN t.status = 'completed' THEN st.story_points ELSE 0 END) as completed_points,
                    COUNT(*) as tasks_total,
                    SUM(CASE WHEN t.status = 'completed' THEN 1 ELSE 0 END) as tasks_completed,
                    SUM(CASE WHEN t.status = 'in_progress' THEN 1 ELSE 0 END) as tasks_in_progress
                FROM sprint_tasks st
                JOIN task_queue t ON st.task_id = t.id
                WHERE st.sprint_id = ? AND st.removed_at IS NULL
            """,
                (sprint_id,),
            ).fetchone()

            total_points = stats["total_points"] or 0
            completed_points = stats["completed_points"] or 0
            remaining_points = total_points - completed_points

            conn.execute(
                """
                INSERT INTO sprint_daily_snapshots
                    (sprint_id, snapshot_date, total_points, completed_points, remaining_points,
                     tasks_total, tasks_completed, tasks_in_progress)
                VALUES (?, DATE('now'), ?, ?, ?, ?, ?, ?)
                ON CONFLICT(sprint_id, snapshot_date) DO UPDATE SET
                    total_points = excluded.total_points, completed_points = excluded.completed_points,
                    remaining_points = excluded.remaining_points, tasks_total = excluded.tasks_total,
                    tasks_completed = excluded.tasks_completed, tasks_in_progress = excluded.tasks_in_progress
            """,
                (
                    sprint_id,
                    total_points,
                    completed_points,
                    remaining_points,
                    stats["tasks_total"] or 0,
                    stats["tasks_completed"] or 0,
                    stats["tasks_in_progress"] or 0,
                ),
            )
            conn.commit()
            return jsonify(
                {"success": True, "remaining_points": remaining_points}
            )
    except Exception as e:
        logger.error(f"Failed to create sprint snapshot: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/sprints/active", methods=["GET"])
@require_auth
def get_active_sprints():
    """Get all currently active sprints."""
    project_id = request.args.get("project_id", type=int)
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            query = """
                SELECT s.*, p.name as project_name
                FROM sprints s
                LEFT JOIN projects p ON s.project_id = p.id
                WHERE s.status = 'active'
            """
            params = []
            if project_id:
                query += " AND s.project_id = ?"
                params.append(project_id)
            query += " ORDER BY s.start_date"
            sprints = conn.execute(query, params).fetchall()
            return jsonify(
                {"sprints": [dict(s) for s in sprints], "count": len(sprints)}
            )
    except Exception as e:
        logger.error(f"Failed to get active sprints: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/sprints/velocity", methods=["GET"])
@require_auth
def get_sprint_velocity():
    """Get velocity trends across sprints.

    Query params:
        project_id: Filter by project
        sprints: Number of sprints to analyze (default 6)
    """
    project_id = request.args.get("project_id", type=int)
    num_sprints = min(int(request.args.get("sprints", 6)), 20)
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            query = """
                SELECT s.id, s.name, s.start_date, s.end_date, s.velocity_planned, s.velocity_actual, s.status
                FROM sprints s
                WHERE s.status = 'completed'
            """
            params = []
            if project_id:
                query += " AND s.project_id = ?"
                params.append(project_id)
            query += " ORDER BY s.end_date DESC LIMIT ?"
            params.append(num_sprints)

            sprints = conn.execute(query, params).fetchall()
            sprint_list = [dict(s) for s in sprints]
            sprint_list.reverse()

            if sprint_list:
                velocities = [s["velocity_actual"] or 0 for s in sprint_list]
                avg_velocity = round(sum(velocities) / len(velocities), 1)
                trend = "stable"
                if len(velocities) >= 3:
                    recent = sum(velocities[-2:]) / 2
                    older = sum(velocities[:2]) / 2
                    if recent > older * 1.1:
                        trend = "improving"
                    elif recent < older * 0.9:
                        trend = "declining"
            else:
                avg_velocity = 0
                trend = "insufficient_data"

            return jsonify(
                {
                    "sprints": sprint_list,
                    "average_velocity": avg_velocity,
                    "trend": trend,
                    "count": len(sprint_list),
                }
            )
    except Exception as e:
        logger.error(f"Failed to get sprint velocity: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/sprints/settings", methods=["GET"])
@require_auth
def get_sprint_settings():
    """Get sprint configuration settings."""
    project_id = request.args.get("project_id", type=int)
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            # Get global settings
            settings = conn.execute(
                "SELECT setting_key, setting_value FROM sprint_settings WHERE project_id IS NULL"
            ).fetchall()
            result = {s["setting_key"]: s["setting_value"] for s in settings}
            # Override with project settings if project_id provided
            if project_id:
                proj_settings = conn.execute(
                    "SELECT setting_key, setting_value FROM sprint_settings WHERE project_id = ?",
                    (project_id,),
                ).fetchall()
                for s in proj_settings:
                    result[s["setting_key"]] = s["setting_value"]
            return jsonify({"settings": result, "project_id": project_id})
    except Exception as e:
        logger.error(f"Failed to get sprint settings: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/sprints/settings", methods=["PUT"])
@require_auth
def update_sprint_settings():
    """Update sprint configuration settings."""
    data = request.get_json() or {}
    project_id = data.get("project_id")
    settings = data.get("settings", {})
    if not settings:
        return jsonify({"error": "No settings provided"}), 400
    try:
        with get_db_connection() as conn:
            for key, value in settings.items():
                conn.execute(
                    """
                    INSERT INTO sprint_settings (project_id, setting_key, setting_value)
                    VALUES (?, ?, ?)
                    ON CONFLICT(project_id, setting_key) DO UPDATE SET setting_value = excluded.setting_value
                """,
                    (project_id, key, str(value)),
                )
            conn.commit()
            return jsonify({"success": True, "updated": list(settings.keys())})
    except Exception as e:
        logger.error(f"Failed to update sprint settings: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/compare/milestones", methods=["GET"])
@require_auth
def compare_milestones():
    """Compare two or more milestones side by side.

    Query parameters:
        ids: Comma-separated milestone IDs to compare (required, 2-5 milestones)
        include_items: Include feature/bug lists (default: false)

    Returns comparison data for each milestone including:
        - Basic info (name, dates, status)
        - Progress metrics (features, bugs, completion rates)
        - Time tracking (estimated vs actual hours)
        - Health indicators
    """
    ids_param = request.args.get("ids", "")
    include_items = request.args.get("include_items", "").lower() == "true"

    if not ids_param:
        return (
            jsonify(
                {
                    "error": "ids parameter required (comma-separated milestone IDs)"
                }
            ),
            400,
        )

    try:
        milestone_ids = [
            int(id.strip()) for id in ids_param.split(",") if id.strip()
        ]
    except ValueError:
        return jsonify({"error": "Invalid milestone ID format"}), 400

    if len(milestone_ids) < 2:
        return (
            jsonify(
                {"error": "At least 2 milestone IDs required for comparison"}
            ),
            400,
        )

    if len(milestone_ids) > 5:
        return (
            jsonify({"error": "Maximum 5 milestones can be compared at once"}),
            400,
        )

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        placeholders = ",".join(["?" for _ in milestone_ids])

        # Get milestone data with progress metrics
        milestones = conn.execute(
            """
            SELECT m.*, p.name as project_name,
                   COALESCE(fc.total, 0) as total_features,
                   COALESCE(fc.completed, 0) as completed_features,
                   COALESCE(fc.in_progress, 0) as in_progress_features,
                   COALESCE(fc.draft, 0) as draft_features,
                   COALESCE(bc.total, 0) as total_bugs,
                   COALESCE(bc.resolved, 0) as resolved_bugs,
                   COALESCE(bc.critical, 0) as critical_bugs,
                   COALESCE(bc.high, 0) as high_bugs,
                   COALESCE(fc.estimated_hours, 0) as estimated_hours,
                   COALESCE(fc.actual_hours, 0) as actual_hours
            FROM milestones m
            LEFT JOIN projects p ON m.project_id = p.id
            LEFT JOIN (
                SELECT milestone_id,
                       COUNT(*) as total,
                       SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completed,
                       SUM(CASE WHEN status = 'in_progress' THEN 1 ELSE 0 END) as in_progress,
                       SUM(CASE WHEN status = 'draft' THEN 1 ELSE 0 END) as draft,
                       SUM(COALESCE(estimated_hours, 0)) as estimated_hours,
                       SUM(COALESCE(actual_hours, 0)) as actual_hours
                FROM features GROUP BY milestone_id
            ) fc ON m.id = fc.milestone_id
            LEFT JOIN (
                SELECT milestone_id,
                       COUNT(*) as total,
                       SUM(CASE WHEN status = 'resolved' THEN 1 ELSE 0 END) as resolved,
                       SUM(CASE WHEN severity = 'critical' THEN 1 ELSE 0 END) as critical,
                       SUM(CASE WHEN severity = 'high' THEN 1 ELSE 0 END) as high
                FROM bugs GROUP BY milestone_id
            ) bc ON m.id = bc.milestone_id
            WHERE m.id IN ({placeholders})
        """,
            milestone_ids,
        ).fetchall()

        if len(milestones) == 0:
            return (
                jsonify({"error": "No milestones found with provided IDs"}),
                404,
            )

        found_ids = [m["id"] for m in milestones]
        missing_ids = [id for id in milestone_ids if id not in found_ids]

        comparison = []
        today = datetime.now().date()

        for m in milestones:
            m_dict = dict(m)

            # Calculate progress percentages
            total_items = m_dict["total_features"] + m_dict["total_bugs"]
            completed_items = (
                m_dict["completed_features"] + m_dict["resolved_bugs"]
            )

            m_dict["progress"] = {
                "total_items": total_items,
                "completed_items": completed_items,
                "completion_percent": round(
                    (
                        (completed_items / total_items * 100)
                        if total_items > 0
                        else 0
                    ),
                    1,
                ),
                "feature_completion_percent": round(
                    (
                        (
                            m_dict["completed_features"]
                            / m_dict["total_features"]
                            * 100
                        )
                        if m_dict["total_features"] > 0
                        else 0
                    ),
                    1,
                ),
                "bug_resolution_percent": round(
                    (
                        (m_dict["resolved_bugs"] / m_dict["total_bugs"] * 100)
                        if m_dict["total_bugs"] > 0
                        else 0
                    ),
                    1,
                ),
            }

            # Calculate time metrics
            m_dict["time"] = {
                "estimated_hours": round(m_dict["estimated_hours"], 1),
                "actual_hours": round(m_dict["actual_hours"], 1),
                "variance_hours": round(
                    m_dict["actual_hours"] - m_dict["estimated_hours"], 1
                ),
                "variance_percent": round(
                    (
                        (
                            (
                                m_dict["actual_hours"]
                                - m_dict["estimated_hours"]
                            )
                            / m_dict["estimated_hours"]
                            * 100
                        )
                        if m_dict["estimated_hours"] > 0
                        else 0
                    ),
                    1,
                ),
            }

            # Calculate schedule status
            if m_dict["target_date"]:
                target = datetime.strptime(
                    m_dict["target_date"], "%Y-%m-%d"
                ).date()
                days_diff = (target - today).days
                m_dict["schedule"] = {
                    "target_date": m_dict["target_date"],
                    "days_remaining": days_diff if days_diff > 0 else 0,
                    "days_overdue": abs(days_diff) if days_diff < 0 else 0,
                    "is_overdue": days_diff < 0
                    and m_dict["status"] not in ("completed", "cancelled"),
                    "status": (
                        "overdue"
                        if days_diff < 0
                        and m_dict["status"] not in ("completed", "cancelled")
                        else "on_track" if days_diff >= 0 else "completed"
                    ),
                }
            else:
                m_dict["schedule"] = {
                    "target_date": None,
                    "days_remaining": None,
                    "is_overdue": False,
                    "status": "no_date",
                }

            # Health score (0-100)
            health_score = 100
            if m_dict["schedule"].get("is_overdue"):
                health_score -= min(30, m_dict["schedule"]["days_overdue"] * 2)
            if m_dict["critical_bugs"] > 0:
                health_score -= m_dict["critical_bugs"] * 10
            if m_dict["high_bugs"] > 0:
                health_score -= m_dict["high_bugs"] * 5
            if m_dict["time"]["variance_percent"] > 20:
                health_score -= 10
            health_score = max(0, health_score)

            m_dict["health"] = {
                "score": health_score,
                "status": (
                    "healthy"
                    if health_score >= 80
                    else "warning" if health_score >= 50 else "critical"
                ),
                "issues": [],
            }
            if m_dict["schedule"].get("is_overdue"):
                m_dict["health"]["issues"].append(
                    f"Overdue by {m_dict['schedule']['days_overdue']} days"
                )
            if m_dict["critical_bugs"] > 0:
                m_dict["health"]["issues"].append(
                    f"{m_dict['critical_bugs']} critical bugs"
                )
            if m_dict["time"]["variance_percent"] > 20:
                m_dict["health"]["issues"].append(
                    f"Time variance: +{m_dict['time']['variance_percent']}%"
                )

            # Include feature/bug lists if requested
            if include_items:
                features = conn.execute(
                    """
                    SELECT id, title, status, priority, estimated_hours, actual_hours
                    FROM features WHERE milestone_id = ? ORDER BY priority DESC, status
                """,
                    (m_dict["id"],),
                ).fetchall()
                bugs = conn.execute(
                    """
                    SELECT id, title, status, severity, priority
                    FROM bugs WHERE milestone_id = ? ORDER BY severity DESC, status
                """,
                    (m_dict["id"],),
                ).fetchall()
                m_dict["features"] = [dict(f) for f in features]
                m_dict["bugs"] = [dict(b) for b in bugs]

            comparison.append(m_dict)

        # Generate comparison summary
        if len(comparison) >= 2:
            summary = {
                "metrics": [
                    "completion_percent",
                    "total_features",
                    "total_bugs",
                    "estimated_hours",
                    "health_score",
                ],
                "best": {},
                "worst": {},
            }
            for metric in summary["metrics"]:
                if metric == "completion_percent":
                    values = [
                        (m["id"], m["progress"]["completion_percent"])
                        for m in comparison
                    ]
                elif metric == "health_score":
                    values = [
                        (m["id"], m["health"]["score"]) for m in comparison
                    ]
                elif metric in ["total_features", "total_bugs"]:
                    values = [(m["id"], m[metric]) for m in comparison]
                else:
                    values = [
                        (m["id"], m["time"].get(metric, 0)) for m in comparison
                    ]

                sorted_values = sorted(
                    values, key=lambda x: x[1], reverse=True
                )
                summary["best"][metric] = {
                    "milestone_id": sorted_values[0][0],
                    "value": sorted_values[0][1],
                }
                summary["worst"][metric] = {
                    "milestone_id": sorted_values[-1][0],
                    "value": sorted_values[-1][1],
                }
        else:
            summary = None

        return jsonify(
            {
                "milestones": comparison,
                "comparison_summary": summary,
                "missing_ids": missing_ids,
                "count": len(comparison),
            }
        )


# ============================================================================
# FEATURE API ENDPOINTS
# ============================================================================

# Feature status workflow: draft  spec  in_progress  review  completed
# Special statuses: blocked (can transition from/to any status), cancelled
FEATURE_STATUS_WORKFLOW = {
    "draft": ["spec", "cancelled", "blocked"],
    "spec": ["draft", "in_progress", "cancelled", "blocked"],
    "in_progress": ["spec", "review", "blocked", "cancelled"],
    "review": ["in_progress", "completed", "blocked", "cancelled"],
    "completed": ["review"],  # Can reopen to review
    "blocked": [
        "draft",
        "spec",
        "in_progress",
        "review",
        "cancelled",
    ],  # Can go back to previous
    "cancelled": ["draft"],  # Can restart from draft
}

FEATURE_STATUS_LABELS = {
    "draft": "Draft",
    "spec": "Specification",
    "in_progress": "In Progress",
    "review": "In Review",
    "completed": "Completed",
    "blocked": "Blocked",
    "cancelled": "Cancelled",
}


def validate_feature_status_transition(
    from_status: str, to_status: str
) -> tuple[bool, str]:
    """Validate if a status transition is allowed.

    Returns: (is_valid, error_message)
    """
    if from_status == to_status:
        return True, ""

    if from_status not in FEATURE_STATUS_WORKFLOW:
        return False, f"Unknown current status: {from_status}"

    if to_status not in FEATURE_STATUS_WORKFLOW:
        return False, f"Unknown target status: {to_status}"

    allowed = FEATURE_STATUS_WORKFLOW.get(from_status, [])
    if to_status not in allowed:
        return (
            False,
            f"Cannot transition from '{from_status}' to '{to_status}'. Allowed: {
                ', '.join(allowed)}",
        )

    return True, ""


def record_feature_status_change(
    conn,
    feature_id: int,
    from_status: str,
    to_status: str,
    changed_by: str = None,
    comment: str = None,
):
    """Record a status change in the history table."""
    conn.execute(
        """
        INSERT INTO feature_status_history (feature_id, from_status, to_status, changed_by, comment)
        VALUES (?, ?, ?, ?, ?)
    """,
        (feature_id, from_status, to_status, changed_by, comment),
    )


@app.route("/api/features", methods=["GET"])
@require_auth
def get_features():
    """Get features with filtering and pagination.

    Returns a list of features with their associated project and milestone info.

    Query Parameters:
        project_id (int): Filter by project ID
        milestone_id (int): Filter by milestone ID
        status (str): Filter by status (pending, in_progress, completed, blocked)
        page (int): Page number (default: 1)
        per_page (int): Items per page, max 100 (default: 50)
        paginate (str): Set to 'true' for pagination response format
        limit (int): Legacy - max results to return
        offset (int): Legacy - skip first N results

    Returns:
        200: List of features or paginated result

    Example Request:
        GET /api/features
        GET /api/features?project_id=1&status=in_progress
        GET /api/features?paginate=true&page=2&per_page=10

    Example Response:
        [
            {
                "id": 1,
                "name": "User Authentication",
                "description": "Implement login and signup flow",
                "project_id": 1,
                "project_name": "My App",
                "milestone_id": 2,
                "milestone_name": "MVP Release",
                "status": "in_progress",
                "priority": 2,
                "created_at": "2024-01-15T10:30:00"
            }
        ]

    cURL Example:
        curl -X GET "http://localhost:8080/api/features?project_id=1" \\
             -H "Cookie: session=<session_cookie>"
    """
    project_id = request.args.get("project_id")
    milestone_id = request.args.get("milestone_id")
    status = request.args.get("status")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Check if we should include soft-deleted features
        include_deleted = (
            request.args.get("include_deleted", "").lower() == "true"
        )

        query = """
            SELECT f.*, p.name as project_name, m.name as milestone_name
            FROM features f
            LEFT JOIN projects p ON f.project_id = p.id
            LEFT JOIN milestones m ON f.milestone_id = m.id
            WHERE 1=1
        """
        params = []

        # Filter out soft-deleted records by default
        if not include_deleted:
            query += " AND f.deleted_at IS NULL"

        if project_id:
            query += " AND f.project_id = ?"
            params.append(project_id)
        if milestone_id:
            query += " AND f.milestone_id = ?"
            params.append(milestone_id)
        if status:
            query += " AND f.status = ?"
            params.append(status)

        query += " ORDER BY f.priority DESC, f.created_at DESC"

        features = conn.execute(query, params).fetchall()
        feature_list = [dict(f) for f in features]

        # Support pagination if requested
        if request.args.get("paginate", "").lower() == "true":
            page, per_page = get_pagination_params()
            result = paginate_query(feature_list, page, per_page)
            return jsonify(result)

        # Legacy limit/offset support
        limit = request.args.get("limit", type=int)
        offset = request.args.get("offset", 0, type=int)
        if limit:
            return jsonify(feature_list[offset: offset + limit])

        return jsonify(feature_list)


@app.route("/api/features/stats", methods=["GET"])
@require_auth
def get_feature_stats():
    """Get feature statistics and metrics.

    Returns aggregated statistics about features including:
    - Total count and counts by status
    - Counts by priority level
    - Counts by project
    - Completion metrics (avg time to complete, completion rate)
    - Recent activity summary

    Query params:
        project_id: Filter stats by project
        days: Number of days for recent activity (default 30)
    """
    project_id = request.args.get("project_id", type=int)
    days = request.args.get("days", 30, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Build WHERE clause for project filter
        where_clause = ""
        params = []
        if project_id:
            where_clause = "WHERE project_id = ?"
            params = [project_id]

        # Total count
        total = conn.execute(
            f"SELECT COUNT(*) as count FROM features {where_clause}", params
        ).fetchone()["count"]

        # Count by status
        status_query = """
            SELECT status, COUNT(*) as count
            FROM features {where_clause}
            GROUP BY status
        """
        status_rows = conn.execute(status_query, params).fetchall()
        by_status = {row["status"]: row["count"] for row in status_rows}

        # Count by priority
        priority_query = """
            SELECT priority, COUNT(*) as count
            FROM features {where_clause}
            GROUP BY priority
            ORDER BY priority DESC
        """
        priority_rows = conn.execute(priority_query, params).fetchall()
        by_priority = {row["priority"]: row["count"] for row in priority_rows}

        # Count by project (only if not filtering by project)
        by_project = {}
        if not project_id:
            project_query = """
                SELECT p.id, p.name, COUNT(f.id) as count
                FROM projects p
                LEFT JOIN features f ON p.id = f.project_id
                GROUP BY p.id, p.name
                HAVING count > 0
                ORDER BY count DESC
                LIMIT 10
            """
            project_rows = conn.execute(project_query).fetchall()
            by_project = {row["name"]: row["count"] for row in project_rows}

        # Completion metrics
        completed_query = """
            SELECT COUNT(*) as count,
                   AVG(JULIANDAY(updated_at) - JULIANDAY(created_at)) as avg_days
            FROM features
            {where_clause + ' AND ' if where_clause else 'WHERE '}
            status = 'completed'
        """
        completed_row = conn.execute(completed_query, params).fetchone()
        completed_count = completed_row["count"] or 0
        avg_completion_days = round(completed_row["avg_days"] or 0, 1)

        # Completion rate
        completion_rate = (
            round((completed_count / total * 100), 1) if total > 0 else 0
        )

        # Recent activity (created/updated in last N days)
        recent_query = """
            SELECT
                COUNT(CASE WHEN created_at > datetime('now', '-' || ? || ' days') THEN 1 END) as created,
                COUNT(CASE WHEN updated_at > datetime('now', '-' || ? || ' days') THEN 1 END) as updated,
                COUNT(CASE WHEN status = 'completed' AND updated_at > datetime('now', '-' || ? || ' days') THEN 1 END) as completed
            FROM features {where_clause}
        """
        recent_params = [days, days, days] + params
        recent = conn.execute(recent_query, recent_params).fetchone()

        # In progress breakdown
        in_progress_query = """
            SELECT COUNT(*) as count
            FROM features
            {where_clause + ' AND ' if where_clause else 'WHERE '}
            status = 'in_progress'
        """
        in_progress_count = conn.execute(in_progress_query, params).fetchone()[
            "count"
        ]

        # Blocked/on-hold count
        blocked_query = """
            SELECT COUNT(*) as count
            FROM features
            {where_clause + ' AND ' if where_clause else 'WHERE '}
            status IN ('blocked', 'on_hold')
        """
        blocked_count = conn.execute(blocked_query, params).fetchone()["count"]

        return jsonify(
            {
                "total": total,
                "by_status": by_status,
                "by_priority": by_priority,
                "by_project": by_project,
                "completion": {
                    "completed_count": completed_count,
                    "completion_rate": completion_rate,
                    "avg_days_to_complete": avg_completion_days,
                },
                "current": {
                    "in_progress": in_progress_count,
                    "blocked_or_on_hold": blocked_count,
                },
                "recent_activity": {
                    "period_days": days,
                    "created": recent["created"],
                    "updated": recent["updated"],
                    "completed": recent["completed"],
                },
                "project_filter": project_id,
            }
        )


@app.route("/api/features", methods=["POST"])
@require_auth
@rate_limit(requests_per_minute=30)
def create_feature():
    """Create a new feature.

    Creates a new feature associated with a project.

    Request Body:
        name (str, required): Feature name
        project_id (int, required): Associated project ID
        milestone_id (int, optional): Associated milestone ID
        description (str, optional): Feature description
        spec (str, optional): Technical specification
        priority (int, optional): Priority 0-5 (default: 0)
        estimated_hours (float, optional): Estimated work hours

    Returns:
        200: Feature created with ID
        400: Validation error
        404: Project not found
        500: Database error

    Example Request:
        POST /api/features
        Content-Type: application/json

        {
            "name": "User Authentication",
            "project_id": 1,
            "description": "Implement OAuth2 login flow",
            "priority": 2,
            "estimated_hours": 8
        }

    Example Response:
        {
            "id": 42,
            "success": true
        }

    cURL Example:
        curl -X POST "http://localhost:8080/api/features" \\
             -H "Content-Type: application/json" \\
             -H "Cookie: session=<session_cookie>" \\
             -d '{"name": "New Feature", "project_id": 1}'
    """
    data = request.get_json()

    if not data:
        return api_error("Request body is required", 400, "validation_error")

    name = data.get("name")
    if not name or not name.strip():
        return api_error("Feature name is required", 400, "validation_error")

    project_id = data.get("project_id")
    if not project_id:
        return api_error("Project ID is required", 400, "validation_error")

    try:
        with get_db_connection() as conn:
            # Verify project exists
            project = conn.execute(
                "SELECT id FROM projects WHERE id = ?", (project_id,)
            ).fetchone()
            if not project:
                return api_error(
                    f"Project with ID {project_id} not found", 404, "not_found"
                )

            cursor = conn.execute(
                """
                INSERT INTO features (project_id, milestone_id, name, description, spec, priority, estimated_hours)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    project_id,
                    data.get("milestone_id"),
                    name.strip(),
                    data.get("description"),
                    data.get("spec"),
                    data.get("priority", 0),
                    data.get("estimated_hours"),
                ),
            )
            feature_id = cursor.lastrowid

            log_activity("create_feature", "feature", feature_id, name)

            return jsonify({"id": feature_id, "success": True})
    except sqlite3.Error as e:
        logger.error(f"Database error creating feature: {e}")
        return api_error("Failed to create feature", 500, "database_error")


@app.route("/api/features/<int:feature_id>", methods=["PUT"])
@require_auth
def update_feature(feature_id):
    """Update a feature with status workflow validation."""
    data = request.get_json() or {}
    new_status = data.get("status")

    with get_db_connection() as conn:
        # Get current status before updating
        current_row = conn.execute(
            "SELECT status FROM features WHERE id = ?", (feature_id,)
        ).fetchone()
        current_status = current_row["status"] if current_row else None

        # If completing, set completed_at
        completed_at = None
        if new_status == "completed":
            completed_at = datetime.now().isoformat()

        conn.execute(
            """
            UPDATE features SET
                name = COALESCE(?, name),
                description = COALESCE(?, description),
                spec = COALESCE(?, spec),
                status = COALESCE(?, status),
                priority = COALESCE(?, priority),
                milestone_id = COALESCE(?, milestone_id),
                assigned_to = COALESCE(?, assigned_to),
                assigned_node = COALESCE(?, assigned_node),
                tmux_session = COALESCE(?, tmux_session),
                actual_hours = COALESCE(?, actual_hours),
                completed_at = COALESCE(?, completed_at),
                updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (
                data.get("name"),
                data.get("description"),
                data.get("spec"),
                new_status,
                data.get("priority"),
                data.get("milestone_id"),
                data.get("assigned_to"),
                data.get("assigned_node"),
                data.get("tmux_session"),
                data.get("actual_hours"),
                completed_at,
                feature_id,
            ),
        )

        log_activity("update_feature", "feature", feature_id)

    # Broadcast stats update via WebSocket
    broadcast_stats()
    return jsonify(
        {
            "success": True,
            "status": new_status or current_status,
            "status_label": FEATURE_STATUS_LABELS.get(
                new_status or current_status, "Unknown"
            ),
        }
    )


@app.route("/api/features/<int:feature_id>/history", methods=["GET"])
@require_auth
def get_feature_history(feature_id):
    """Get status change history for a feature."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        history = conn.execute(
            """
            SELECT id, from_status, to_status, changed_by, comment, created_at
            FROM feature_status_history
            WHERE feature_id = ?
            ORDER BY created_at DESC
        """,
            (feature_id,),
        ).fetchall()

        return jsonify([dict(h) for h in history])


@app.route("/api/features/<int:feature_id>", methods=["DELETE"])
@require_auth
def delete_feature(feature_id):
    """Delete a feature (soft delete).

    Query Parameters:
        hard (bool): If 'true', permanently delete instead of soft delete
    """
    hard_delete = request.args.get("hard", "").lower() == "true"
    username = session.get("username")

    with get_db_connection() as conn:
        # Check if feature exists
        existing = conn.execute(
            "SELECT id FROM features WHERE id = ?", (feature_id,)
        ).fetchone()
        if not existing:
            return jsonify({"error": "Feature not found"}), 404

        if hard_delete:
            conn.execute("DELETE FROM features WHERE id = ?", (feature_id,))
            log_activity("hard_delete_feature", "feature", feature_id)
        else:
            conn.execute(
                """
                UPDATE features SET
                    deleted_at = CURRENT_TIMESTAMP,
                    deleted_by = ?
                WHERE id = ? AND deleted_at IS NULL
            """,
                (username, feature_id),
            )
            log_activity("soft_delete_feature", "feature", feature_id)

        return jsonify({"success": True, "soft_delete": not hard_delete})


@app.route("/api/features/<int:feature_id>/restore", methods=["POST"])
@require_auth
def restore_feature(feature_id):
    """Restore a soft-deleted feature."""
    with get_db_connection() as conn:
        result = conn.execute(
            """
            UPDATE features SET
                deleted_at = NULL,
                deleted_by = NULL
            WHERE id = ? AND deleted_at IS NOT NULL
        """,
            (feature_id,),
        )

        if result.rowcount == 0:
            return jsonify({"error": "Feature not found or not deleted"}), 404

        log_activity("restore_feature", "feature", feature_id)

    return jsonify({"success": True})


@app.route("/api/features/workflow", methods=["GET"])
@require_auth
def get_feature_workflow():
    """Get the feature status workflow definition."""
    return jsonify(
        {
            "statuses": FEATURE_STATUS_LABELS,
            "transitions": FEATURE_STATUS_WORKFLOW,
            "initial_status": "draft",
        }
    )


@app.route("/api/features/<int:feature_id>/links", methods=["GET"])
@require_auth
def get_feature_links(feature_id):
    """Get all links associated with a feature (TODOs, commits, bugs, errors)."""
    link_type = request.args.get("type")  # Optional filter

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        query = """
            SELECT * FROM feature_links
            WHERE feature_id = ?
        """
        params = [feature_id]

        if link_type:
            query += " AND link_type = ?"
            params.append(link_type)

        query += " ORDER BY created_at DESC"

        links = conn.execute(query, params).fetchall()

        # Enrich with linked item details
        result = []
        for link in links:
            link_dict = dict(link)

            # Get linked item details based on type
            if link_dict["link_type"] == "todo" and link_dict["linked_id"]:
                todo = conn.execute(
                    "SELECT name, status, priority FROM devops_tasks WHERE id = ?",
                    (link_dict["linked_id"],),
                ).fetchone()
                if todo:
                    link_dict["linked_item"] = dict(todo)

            elif link_dict["link_type"] == "bug" and link_dict["linked_id"]:
                bug = conn.execute(
                    "SELECT title, status, severity FROM bugs WHERE id = ?",
                    (link_dict["linked_id"],),
                ).fetchone()
                if bug:
                    link_dict["linked_item"] = dict(bug)

            elif link_dict["link_type"] == "error" and link_dict["linked_id"]:
                error = conn.execute(
                    "SELECT error_type, message, status FROM errors WHERE id = ?",
                    (link_dict["linked_id"],),
                ).fetchone()
                if error:
                    link_dict["linked_item"] = dict(error)

            result.append(link_dict)

        return jsonify(result)


@app.route("/api/features/<int:feature_id>/links", methods=["POST"])
@require_auth
def create_feature_link(feature_id):
    """Link a TODO, commit, bug, or error to a feature.

    For commits:
    - Provide commit_hash and optionally commit_message, commit_url

    For TODOs, bugs, errors:
    - Provide link_type and linked_id
    """
    data = request.get_json()
    link_type = data.get("link_type")

    if link_type not in ["todo", "commit", "bug", "error"]:
        return (
            jsonify(
                {
                    "error": "Invalid link_type. Must be: todo, commit, bug, error"
                }
            ),
            400,
        )

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Verify feature exists
        feature = conn.execute(
            "SELECT id, project_id FROM features WHERE id = ?", (feature_id,)
        ).fetchone()
        if not feature:
            return jsonify({"error": "Feature not found"}), 404

        # For commits, try to get URL from project's git info
        commit_url = data.get("commit_url")
        if (
            link_type == "commit"
            and not commit_url
            and data.get("commit_hash")
        ):
            project = conn.execute(
                "SELECT source_path FROM projects WHERE id = ?",
                (feature["project_id"],),
            ).fetchone()
            if project and project["source_path"]:
                try:
                    import subprocess

                    remote = subprocess.run(
                        [
                            "git",
                            "-C",
                            project["source_path"],
                            "remote",
                            "get-url",
                            "origin",
                        ],
                        capture_output=True,
                        text=True,
                        timeout=5,
                    ).stdout.strip()
                    if remote.startswith("git@"):
                        web_url = (
                            remote.replace(":", "/")
                            .replace("git@", "https://")
                            .replace(".git", "")
                        )
                    elif remote.endswith(".git"):
                        web_url = remote[:-4]
                    else:
                        web_url = remote
                    commit_url = f"{web_url}/commit/{data.get('commit_hash')}"
                except Exception:
                    pass

        cursor = conn.execute(
            """
            INSERT INTO feature_links
                (feature_id, link_type, linked_id, commit_hash, commit_message, commit_url, description, created_by)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                feature_id,
                link_type,
                data.get("linked_id"),
                data.get("commit_hash"),
                data.get("commit_message"),
                commit_url,
                data.get("description"),
                session.get("username", "system"),
            ),
        )

        link_id = cursor.lastrowid
        log_activity(
            "create_feature_link", "feature", feature_id, f"{link_type} link"
        )

        return jsonify({"id": link_id, "success": True})


@app.route(
    "/api/features/<int:feature_id>/links/<int:link_id>", methods=["DELETE"]
)
@require_auth
def delete_feature_link(feature_id, link_id):
    """Remove a link from a feature."""
    with get_db_connection() as conn:
        result = conn.execute(
            "DELETE FROM feature_links WHERE id = ? AND feature_id = ?",
            (link_id, feature_id),
        )
        if result.rowcount == 0:
            return jsonify({"error": "Link not found"}), 404

        log_activity("delete_feature_link", "feature", feature_id)
        return jsonify({"success": True})


@app.route("/api/features/<int:feature_id>/link-commit", methods=["POST"])
@require_auth
def link_feature_to_current_commit(feature_id):
    """Link feature to the current/latest commit in the project's repo."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Get feature and project
        feature = conn.execute(
            """
            SELECT f.id, f.name, p.source_path
            FROM features f
            JOIN projects p ON f.project_id = p.id
            WHERE f.id = ?
        """,
            (feature_id,),
        ).fetchone()

        if not feature:
            return jsonify({"error": "Feature not found"}), 404

        if not feature["source_path"]:
            return jsonify({"error": "Project has no source path"}), 400

        try:
            import subprocess

            source_path = feature["source_path"]

            # Get current commit
            result = subprocess.run(
                ["git", "-C", source_path, "log", "-1", "--format=%H|%s|%an"],
                capture_output=True,
                text=True,
                timeout=5,
            )
            if result.returncode != 0:
                return jsonify({"error": "Failed to get git commit info"}), 500

            parts = result.stdout.strip().split("|", 2)
            if len(parts) < 3:
                return jsonify({"error": "Invalid git output"}), 500

            commit_hash, message, author = parts

            # Get remote URL for commit link
            remote = subprocess.run(
                ["git", "-C", source_path, "remote", "get-url", "origin"],
                capture_output=True,
                text=True,
                timeout=5,
            ).stdout.strip()

            if remote.startswith("git@"):
                web_url = (
                    remote.replace(":", "/")
                    .replace("git@", "https://")
                    .replace(".git", "")
                )
            elif remote.endswith(".git"):
                web_url = remote[:-4]
            else:
                web_url = remote

            commit_url = f"{web_url}/commit/{commit_hash}"

            # Create the link
            cursor = conn.execute(
                """
                INSERT INTO feature_links
                    (feature_id, link_type, commit_hash, commit_message, commit_url, description, created_by)
                VALUES (?, 'commit', ?, ?, ?, ?, ?)
            """,
                (
                    feature_id,
                    commit_hash,
                    message,
                    commit_url,
                    f"Commit by {author}",
                    session.get("username", "system"),
                ),
            )

            return jsonify(
                {
                    "success": True,
                    "link_id": cursor.lastrowid,
                    "commit_hash": commit_hash[:8],
                    "commit_message": message,
                    "commit_url": commit_url,
                }
            )

        except subprocess.TimeoutExpired:
            return jsonify({"error": "Git command timed out"}), 500
        except Exception as e:
            return jsonify({"error": str(e)}), 500


# ============================================================================
# API DOCUMENTATION GENERATION
# ============================================================================


def parse_docstring(docstring: str) -> dict:
    """Parse a docstring into structured documentation."""
    if not docstring:
        return {"summary": "", "description": "", "params": [], "returns": ""}

    lines = docstring.strip().split("\n")
    summary = lines[0].strip() if lines else ""

    description_lines = []
    params = []
    returns = ""
    current_section = "description"

    for line in lines[1:]:
        line = line.strip()
        if line.startswith("Query parameters:") or line.startswith(
            "Parameters:"
        ):
            current_section = "params"
            continue
        elif line.startswith("Returns:"):
            current_section = "returns"
            continue

        if (
            current_section == "description"
            and line
            and not line.startswith("-")
        ):
            description_lines.append(line)
        elif current_section == "params" and line.startswith("-"):
            params.append(line[1:].strip())
        elif current_section == "returns":
            if line.startswith("-"):
                returns += line[1:].strip() + "; "
            elif line:
                returns += line + " "

    return {
        "summary": summary,
        "description": " ".join(description_lines).strip(),
        "params": params,
        "returns": returns.strip(),
    }


def generate_api_docs() -> list:
    """Generate API documentation from Flask routes."""
    docs = []
    seen = set()

    for rule in app.url_map.iter_rules():
        # Skip static and internal routes
        if rule.endpoint in ("static", "index"):
            continue
        if rule.rule.startswith("/static"):
            continue

        # Get view function
        view_func = app.view_functions.get(rule.endpoint)
        if not view_func:
            continue

        # Create unique key
        key = (rule.rule, tuple(sorted(rule.methods - {"OPTIONS", "HEAD"})))
        if key in seen:
            continue
        seen.add(key)

        # Parse docstring
        docstring = view_func.__doc__ or ""
        parsed = parse_docstring(docstring)

        # Determine authentication requirement
        requires_auth = getattr(
            view_func, "__wrapped__", None
        ) is not None or "require_auth" in str(view_func)

        # Categorize by URL prefix
        path = rule.rule
        if path.startswith("/api/projects"):
            category = "Projects"
        elif path.startswith("/api/features"):
            category = "Features"
        elif path.startswith("/api/bugs"):
            category = "Bugs"
        elif path.startswith("/api/errors"):
            category = "Errors"
        elif path.startswith("/api/nodes"):
            category = "Nodes"
        elif path.startswith("/api/tmux"):
            category = "tmux Sessions"
        elif path.startswith("/api/tasks") or path.startswith("/api/workers"):
            category = "Task Queue"
        elif path.startswith("/api/devops"):
            category = "DevOps Tasks"
        elif path.startswith("/api/secrets"):
            category = "Secrets Vault"
        elif path.startswith("/api/milestones"):
            category = "Milestones"
        elif path.startswith("/api/assigner"):
            category = "Assigner"
        elif path.startswith("/api/autopilot"):
            category = "Autopilot"
        elif path.startswith("/api/docs"):
            category = "Documentation"
        else:
            category = "Other"

        methods = sorted(rule.methods - {"OPTIONS", "HEAD"})

        docs.append(
            {
                "path": path,
                "methods": methods,
                "endpoint": rule.endpoint,
                "category": category,
                "summary": parsed["summary"],
                "description": parsed["description"],
                "parameters": parsed["params"],
                "returns": parsed["returns"],
                "requires_auth": requires_auth,
                "url_params": list(rule.arguments) if rule.arguments else [],
            }
        )

    # Sort by category then path
    docs.sort(key=lambda x: (x["category"], x["path"]))
    return docs


@app.route("/api/docs", methods=["GET"])
@require_auth
def get_api_documentation():
    """Get auto-generated API documentation.

    Query parameters:
    - category: Filter by API category
    - search: Search in path, summary, or description
    - format: Output format (json, markdown) - default json
    """
    category = request.args.get("category")
    search = request.args.get("search")
    output_format = request.args.get("format", "json")

    docs = generate_api_docs()

    # Apply filters
    if category:
        docs = [d for d in docs if d["category"].lower() == category.lower()]

    if search:
        search_lower = search.lower()
        docs = [
            d
            for d in docs
            if search_lower in d["path"].lower()
            or search_lower in d["summary"].lower()
            or search_lower in d["description"].lower()
        ]

    if output_format == "markdown":
        # Generate markdown documentation
        md_lines = ["# Architect Dashboard API Documentation\n"]
        md_lines.append(f"*Generated at {datetime.now().isoformat()}*\n")

        current_category = None
        for doc in docs:
            if doc["category"] != current_category:
                current_category = doc["category"]
                md_lines.append(f"\n## {current_category}\n")

            methods_str = ", ".join(doc["methods"])
            md_lines.append(f"\n### `{methods_str}` {doc['path']}\n")

            if doc["summary"]:
                md_lines.append(f"{doc['summary']}\n")

            if doc["description"]:
                md_lines.append(f"\n{doc['description']}\n")

            if doc["url_params"]:
                md_lines.append(
                    f"\n**URL Parameters:** {', '.join(doc['url_params'])}\n"
                )

            if doc["parameters"]:
                md_lines.append("\n**Query Parameters:**\n")
                for param in doc["parameters"]:
                    md_lines.append(f"- {param}\n")

            if doc["returns"]:
                md_lines.append(f"\n**Returns:** {doc['returns']}\n")

            auth_str = (
                " Requires authentication"
                if doc["requires_auth"]
                else " Public"
            )
            md_lines.append(f"\n*{auth_str}*\n")

        return "\n".join(md_lines), 200, {"Content-Type": "text/markdown"}

    # Get unique categories for sidebar
    categories = sorted(set(d["category"] for d in docs))

    return jsonify(
        {
            "endpoints": docs,
            "categories": categories,
            "total": len(docs),
            "generated_at": datetime.now().isoformat(),
        }
    )


@app.route("/api/docs/categories", methods=["GET"])
@require_auth
def get_doc_categories():
    """Get list of API documentation categories."""
    docs = generate_api_docs()
    categories = {}

    for doc in docs:
        cat = doc["category"]
        if cat not in categories:
            categories[cat] = {"count": 0, "endpoints": []}
        categories[cat]["count"] += 1
        categories[cat]["endpoints"].append(
            {
                "path": doc["path"],
                "methods": doc["methods"],
                "summary": doc["summary"],
            }
        )

    return jsonify(categories)


@app.route("/api/sop", methods=["GET"])
@require_auth
def get_sop_documentation():
    """Serve the Standard Operating Procedures documentation.

    Query params:
        format: 'markdown', 'html', 'json' (default 'markdown')
        section: Specific section to return (optional)
        toc_only: Return only table of contents (default false)
    """
    import re

    output_format = request.args.get("format", "markdown")
    section = request.args.get("section")
    toc_only = request.args.get("toc_only", "false").lower() == "true"

    # Find SOP.md file
    sop_path = Path(__file__).parent / "SOP.md"
    if not sop_path.exists():
        return api_error("SOP.md not found", 404, "not_found")

    try:
        content = sop_path.read_text()
    except Exception as e:
        return api_error(f"Failed to read SOP.md: {str(e)}", 500, "read_error")

    # Parse sections
    sections = []
    current_section = None
    current_content = []

    for line in content.split("\n"):
        # Match ## headers (main sections)
        header_match = re.match(r"^## (.+)$", line)
        if header_match:
            if current_section:
                sections.append(
                    {
                        "title": current_section,
                        "slug": re.sub(
                            r"[^a-z0-9]+", "-", current_section.lower()
                        ).strip("-"),
                        "content": "\n".join(current_content).strip(),
                    }
                )
            current_section = header_match.group(1)
            current_content = []
        elif current_section:
            current_content.append(line)

    # Don't forget the last section
    if current_section:
        sections.append(
            {
                "title": current_section,
                "slug": re.sub(
                    r"[^a-z0-9]+", "-", current_section.lower()
                ).strip("-"),
                "content": "\n".join(current_content).strip(),
            }
        )

    # Extract table of contents
    toc = [{"title": s["title"], "slug": s["slug"]} for s in sections]

    # Return TOC only if requested
    if toc_only:
        return jsonify({"table_of_contents": toc, "section_count": len(toc)})

    # Filter to specific section if requested
    if section:
        section_slug = re.sub(r"[^a-z0-9]+", "-", section.lower()).strip("-")
        matching = [
            s
            for s in sections
            if s["slug"] == section_slug
            or section.lower() in s["title"].lower()
        ]
        if not matching:
            return api_error(
                f"Section '{section}' not found", 404, "section_not_found"
            )
        sections = matching

    # Format output
    if output_format == "json":
        return jsonify(
            {
                "title": "Standard Operating Procedures",
                "sections": sections,
                "table_of_contents": toc,
                "last_modified": datetime.fromtimestamp(
                    sop_path.stat().st_mtime
                ).isoformat(),
            }
        )

    elif output_format == "html":
        # Convert markdown to basic HTML
        html_content = content
        if section:
            html_content = "\n\n".join(s["content"] for s in sections)

        # Basic markdown to HTML conversion
        # Headers
        html_content = re.sub(
            r"^### (.+)$", r"<h3>\1</h3>", html_content, flags=re.MULTILINE
        )
        html_content = re.sub(
            r"^## (.+)$", r"<h2>\1</h2>", html_content, flags=re.MULTILINE
        )
        html_content = re.sub(
            r"^# (.+)$", r"<h1>\1</h1>", html_content, flags=re.MULTILINE
        )

        # Code blocks
        html_content = re.sub(
            r"```(\w*)\n(.*?)```",
            r'<pre><code class="\1">\2</code></pre>',
            html_content,
            flags=re.DOTALL,
        )

        # Inline code
        html_content = re.sub(r"`([^`]+)`", r"<code>\1</code>", html_content)

        # Bold and italic
        html_content = re.sub(
            r"\*\*(.+?)\*\*", r"<strong>\1</strong>", html_content
        )
        html_content = re.sub(r"\*(.+?)\*", r"<em>\1</em>", html_content)

        # Links
        html_content = re.sub(
            r"\[([^\]]+)\]\(([^)]+)\)", r'<a href="\2">\1</a>', html_content
        )

        # Lists
        html_content = re.sub(
            r"^- (.+)$", r"<li>\1</li>", html_content, flags=re.MULTILINE
        )
        html_content = re.sub(
            r"(<li>.*</li>\n)+", r"<ul>\g<0></ul>", html_content
        )

        # Paragraphs (simple approach)
        html_content = re.sub(r"\n\n", "</p><p>", html_content)

        html = """<!DOCTYPE html>
<html>
<head>
    <title>Standard Operating Procedures</title>
    <style>
        body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; line-height: 1.6; }}
        h1, h2, h3 {{ color: #333; border-bottom: 1px solid #eee; padding-bottom: 0.3em; }}
        code {{ background: #f4f4f4; padding: 2px 6px; border-radius: 3px; font-family: monospace; }}
        pre {{ background: #f4f4f4; padding: 16px; border-radius: 6px; overflow-x: auto; }}
        pre code {{ background: none; padding: 0; }}
        table {{ border-collapse: collapse; width: 100%; margin: 1em 0; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background: #f4f4f4; }}
        ul {{ padding-left: 20px; }}
        a {{ color: #0066cc; }}
    </style>
</head>
<body>
<p>{html_content}</p>
</body>
</html>"""
        return html, 200, {"Content-Type": "text/html"}

    else:  # markdown (default)
        if section:
            # Return just the section content
            md_content = "\n\n".join(
                f"## {s['title']}\n\n{s['content']}" for s in sections
            )
        else:
            md_content = content

        return md_content, 200, {"Content-Type": "text/markdown"}


@app.route("/api/sop/search", methods=["GET"])
@require_auth
def search_sop():
    """Search within the SOP documentation.

    Query params:
        q: Search query (required)
        case_sensitive: Case sensitive search (default false)
    """
    import re

    query = request.args.get("q")
    if not query:
        return api_error("Search query 'q' is required", 400, "missing_param")

    case_sensitive = (
        request.args.get("case_sensitive", "false").lower() == "true"
    )

    sop_path = Path(__file__).parent / "SOP.md"
    if not sop_path.exists():
        return api_error("SOP.md not found", 404, "not_found")

    content = sop_path.read_text()
    lines = content.split("\n")

    # Search flags
    flags = 0 if case_sensitive else re.IGNORECASE

    results = []
    current_section = "Introduction"

    for i, line in enumerate(lines):
        # Track current section
        header_match = re.match(r"^## (.+)$", line)
        if header_match:
            current_section = header_match.group(1)

        # Search for query
        if re.search(re.escape(query), line, flags):
            # Get context (2 lines before and after)
            start = max(0, i - 2)
            end = min(len(lines), i + 3)
            context = "\n".join(lines[start:end])

            results.append(
                {
                    "line_number": i + 1,
                    "section": current_section,
                    "line": line.strip(),
                    "context": context,
                }
            )

    return jsonify(
        {
            "query": query,
            "results": results,
            "match_count": len(results),
            "case_sensitive": case_sensitive,
        }
    )


# ============================================================================
# BUG API ENDPOINTS
# ============================================================================


@app.route("/api/bugs", methods=["GET"])
@require_auth
def get_bugs():
    """Get bugs with filtering and pagination.

    Returns bugs sorted by severity (critical first) then by creation date.

    Query Parameters:
        project_id (int): Filter by project ID
        status (str): Filter by status (open, in_progress, resolved)
        severity (str): Filter by severity (critical, high, medium, low)
        page (int): Page number (default: 1)
        per_page (int): Items per page, max 100 (default: 50)
        paginate (str): Set to 'true' for pagination response format
        limit (int): Legacy - max results
        offset (int): Legacy - skip first N results

    Returns:
        200: List of bugs or paginated result

    Example Request:
        GET /api/bugs
        GET /api/bugs?status=open&severity=critical
        GET /api/bugs?project_id=1&paginate=true

    Example Response:
        [
            {
                "id": 1,
                "title": "Login button not working",
                "description": "Users cannot click the login button on mobile",
                "project_id": 1,
                "project_name": "My App",
                "status": "open",
                "severity": "high",
                "milestone_id": null,
                "milestone_name": null,
                "created_at": "2024-01-15T10:30:00",
                "resolved_at": null
            }
        ]

    cURL Example:
        curl -X GET "http://localhost:8080/api/bugs?status=open" \\
             -H "Cookie: session=<session_cookie>"
    """
    project_id = request.args.get("project_id")
    status = request.args.get("status")
    severity = request.args.get("severity")

    # Check if we should include soft-deleted bugs
    include_deleted = request.args.get("include_deleted", "").lower() == "true"

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        query = """
            SELECT b.*, p.name as project_name, m.name as milestone_name
            FROM bugs b
            LEFT JOIN projects p ON b.project_id = p.id
            LEFT JOIN milestones m ON b.milestone_id = m.id
            WHERE 1=1
        """
        params = []

        # Filter out soft-deleted records by default
        if not include_deleted:
            query += " AND b.deleted_at IS NULL"

        if project_id:
            query += " AND b.project_id = ?"
            params.append(project_id)
        if status:
            query += " AND b.status = ?"
            params.append(status)
        if severity:
            query += " AND b.severity = ?"
            params.append(severity)

        query += " ORDER BY CASE b.severity WHEN 'critical' THEN 0 WHEN 'high' THEN 1 WHEN 'medium' THEN 2 ELSE 3 END, b.created_at DESC"

        bugs = conn.execute(query, params).fetchall()
        bug_list = [dict(b) for b in bugs]

        # Support pagination if requested
        if request.args.get("paginate", "").lower() == "true":
            page, per_page = get_pagination_params()
            result = paginate_query(bug_list, page, per_page)
            return jsonify(result)

        # Legacy limit/offset support
        limit = request.args.get("limit", type=int)
        offset = request.args.get("offset", 0, type=int)
        if limit:
            return jsonify(bug_list[offset: offset + limit])

        return jsonify(bug_list)


@app.route("/api/bugs/severity-stats", methods=["GET"])
@require_auth
def get_bugs_severity_stats():
    """
    Get bug statistics grouped by severity.

    Query params:
        project_id: Filter by project (optional)
        include_resolved: Include resolved bugs in stats (default: false)

    Returns severity counts, trends, and breakdown by status.
    """
    project_id = request.args.get("project_id", type=int)
    include_resolved = (
        request.args.get("include_resolved", "false").lower() == "true"
    )

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Base conditions
        conditions = []
        params = []

        if project_id:
            conditions.append("project_id = ?")
            params.append(project_id)

        if not include_resolved:
            conditions.append("status NOT IN ('resolved', 'closed')")

        where_clause = (
            " WHERE " + " AND ".join(conditions) if conditions else ""
        )

        # Get severity counts
        severity_stats = conn.execute(
            """
            SELECT
                severity,
                COUNT(*) as count,
                SUM(CASE WHEN status = 'open' THEN 1 ELSE 0 END) as open,
                SUM(CASE WHEN status = 'in_progress' THEN 1 ELSE 0 END) as in_progress,
                SUM(CASE WHEN status = 'resolved' THEN 1 ELSE 0 END) as resolved,
                SUM(CASE WHEN status = 'closed' THEN 1 ELSE 0 END) as closed
            FROM bugs
            {where_clause}
            GROUP BY severity
            ORDER BY
                CASE severity
                    WHEN 'critical' THEN 1
                    WHEN 'high' THEN 2
                    WHEN 'medium' THEN 3
                    WHEN 'low' THEN 4
                    ELSE 5
                END
        """,
            params,
        ).fetchall()

        # Get trends (last 7 days vs previous 7 days)
        trend_conditions = conditions.copy()
        trend_params = params.copy()

        recent_bugs = conn.execute(
            """
            SELECT severity, COUNT(*) as count
            FROM bugs
            {" WHERE " + " AND ".join(trend_conditions) if trend_conditions else "WHERE 1=1"}
            {"AND" if trend_conditions else ""} created_at >= datetime('now', '-7 days')
            GROUP BY severity
        """,
            trend_params,
        ).fetchall()

        previous_bugs = conn.execute(
            """
            SELECT severity, COUNT(*) as count
            FROM bugs
            {" WHERE " + " AND ".join(trend_conditions) if trend_conditions else "WHERE 1=1"}
            {"AND" if trend_conditions else ""} created_at >= datetime('now', '-14 days')
            AND created_at < datetime('now', '-7 days')
            GROUP BY severity
        """,
            trend_params,
        ).fetchall()

        recent_dict = {r["severity"]: r["count"] for r in recent_bugs}
        previous_dict = {r["severity"]: r["count"] for r in previous_bugs}

        # Calculate totals
        total_count = sum(s["count"] for s in severity_stats)
        total_open = sum(s["open"] or 0 for s in severity_stats)

        # Build response
        by_severity = {}
        for s in severity_stats:
            sev = s["severity"] or "unknown"
            recent = recent_dict.get(sev, 0)
            previous = previous_dict.get(sev, 0)
            trend = (
                "up"
                if recent > previous
                else ("down" if recent < previous else "stable")
            )
            trend_change = recent - previous

            by_severity[sev] = {
                "count": s["count"],
                "open": s["open"] or 0,
                "in_progress": s["in_progress"] or 0,
                "resolved": s["resolved"] or 0,
                "closed": s["closed"] or 0,
                "percentage": (
                    round(s["count"] / total_count * 100, 1)
                    if total_count > 0
                    else 0
                ),
                "trend": trend,
                "trend_change": trend_change,
                "last_7_days": recent,
                "previous_7_days": previous,
            }

        # Get age distribution for open bugs
        age_distribution = conn.execute(
            """
            SELECT
                CASE
                    WHEN julianday('now') - julianday(created_at) < 1 THEN 'today'
                    WHEN julianday('now') - julianday(created_at) < 7 THEN 'this_week'
                    WHEN julianday('now') - julianday(created_at) < 30 THEN 'this_month'
                    ELSE 'older'
                END as age_bucket,
                severity,
                COUNT(*) as count
            FROM bugs
            WHERE status = 'open'
            {" AND " + " AND ".join(conditions) if conditions else ""}
            GROUP BY age_bucket, severity
        """,
            params,
        ).fetchall()

        age_by_severity = {}
        for row in age_distribution:
            sev = row["severity"] or "unknown"
            if sev not in age_by_severity:
                age_by_severity[sev] = {
                    "today": 0,
                    "this_week": 0,
                    "this_month": 0,
                    "older": 0,
                }
            age_by_severity[sev][row["age_bucket"]] = row["count"]

        return jsonify(
            {
                "by_severity": by_severity,
                "age_distribution": age_by_severity,
                "summary": {
                    "total": total_count,
                    "open": total_open,
                    "critical_open": by_severity.get("critical", {}).get(
                        "open", 0
                    ),
                    "high_open": by_severity.get("high", {}).get("open", 0),
                    "needs_attention": by_severity.get("critical", {}).get(
                        "open", 0
                    )
                    + by_severity.get("high", {}).get("open", 0),
                },
                "filters": {
                    "project_id": project_id,
                    "include_resolved": include_resolved,
                },
            }
        )


@app.route("/api/bugs", methods=["POST"])
@require_auth
@rate_limit(requests_per_minute=30)
def create_bug():
    """Create a new bug report.

    Creates a new bug associated with a project. Broadcasts stats update.

    Request Body:
        title (str, required): Bug title/summary
        project_id (int, optional): Associated project ID
        milestone_id (int, optional): Associated milestone ID
        description (str, optional): Detailed description
        severity (str, optional): critical, high, medium, low (default: medium)
        source_node (str, optional): Node where bug was discovered
        stack_trace (str, optional): Error stack trace
        screenshot (str, optional): Screenshot URL or base64
        context (dict/str, optional): Additional context data
        category (str, optional): Bug category

    Returns:
        200: Bug created with ID
        429: Rate limit exceeded

    Example Request:
        POST /api/bugs
        Content-Type: application/json

        {
            "title": "Login fails on Safari",
            "project_id": 1,
            "description": "OAuth redirect not working",
            "severity": "high",
            "category": "authentication"
        }

    Example Response:
        {
            "id": 42,
            "success": true
        }

    cURL Example:
        curl -X POST "http://localhost:8080/api/bugs" \\
             -H "Content-Type: application/json" \\
             -H "Cookie: session=<session_cookie>" \\
             -d '{"title": "Bug title", "severity": "high"}'
    """
    data = request.get_json()

    # Handle context as JSON string
    context = data.get("context")
    if isinstance(context, dict):
        context = json.dumps(context)

    with get_db_connection() as conn:
        cursor = conn.execute(
            """
            INSERT INTO bugs (project_id, milestone_id, title, description, severity, source_node, stack_trace, screenshot, context, category)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                data.get("project_id"),
                data.get("milestone_id"),
                data.get("title"),
                data.get("description"),
                data.get("severity", "medium"),
                data.get("source_node"),
                data.get("stack_trace"),
                data.get("screenshot"),
                context,
                data.get("category"),
            ),
        )
        bug_id = cursor.lastrowid

        log_activity("create_bug", "bug", bug_id, data.get("title"))

    # Broadcast stats update via WebSocket
    broadcast_stats()
    return jsonify({"id": bug_id, "success": True})


@app.route("/api/bugs/<int:bug_id>", methods=["PUT"])
@require_auth
def update_bug(bug_id):
    """Update a bug."""
    data = request.get_json()

    with get_db_connection() as conn:
        resolved_at = None
        if data.get("status") == "resolved":
            resolved_at = datetime.now().isoformat()

        conn.execute(
            """
            UPDATE bugs SET
                title = COALESCE(?, title),
                description = COALESCE(?, description),
                severity = COALESCE(?, severity),
                status = COALESCE(?, status),
                milestone_id = COALESCE(?, milestone_id),
                assigned_to = COALESCE(?, assigned_to),
                assigned_node = COALESCE(?, assigned_node),
                tmux_session = COALESCE(?, tmux_session),
                resolved_at = COALESCE(?, resolved_at),
                updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (
                data.get("title"),
                data.get("description"),
                data.get("severity"),
                data.get("status"),
                data.get("milestone_id"),
                data.get("assigned_to"),
                data.get("assigned_node"),
                data.get("tmux_session"),
                resolved_at,
                bug_id,
            ),
        )

        log_activity("update_bug", "bug", bug_id)

    # Broadcast stats update via WebSocket
    broadcast_stats()
    return jsonify({"success": True})


@app.route("/api/bugs/<int:bug_id>", methods=["GET"])
@require_auth
def get_bug(bug_id):
    """Get a single bug by ID."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        bug = conn.execute(
            """
            SELECT b.*, p.name as project_name, m.name as milestone_name
            FROM bugs b
            LEFT JOIN projects p ON b.project_id = p.id
            LEFT JOIN milestones m ON b.milestone_id = m.id
            WHERE b.id = ?
        """,
            (bug_id,),
        ).fetchone()

        if not bug:
            return jsonify({"error": "Bug not found"}), 404

        return jsonify(dict(bug))


@app.route("/api/bugs/<int:bug_id>", methods=["DELETE"])
@require_auth
def delete_bug(bug_id):
    """Delete a bug (soft delete).

    Query Parameters:
        hard (bool): If 'true', permanently delete instead of soft delete
    """
    hard_delete = request.args.get("hard", "").lower() == "true"
    username = session.get("username")

    with get_db_connection() as conn:
        # Check if bug exists
        existing = conn.execute(
            "SELECT id FROM bugs WHERE id = ?", (bug_id,)
        ).fetchone()
        if not existing:
            return jsonify({"error": "Bug not found"}), 404

        if hard_delete:
            conn.execute("DELETE FROM bugs WHERE id = ?", (bug_id,))
            log_activity("hard_delete_bug", "bug", bug_id)
        else:
            conn.execute(
                """
                UPDATE bugs SET
                    deleted_at = CURRENT_TIMESTAMP,
                    deleted_by = ?
                WHERE id = ? AND deleted_at IS NULL
            """,
                (username, bug_id),
            )
            log_activity("soft_delete_bug", "bug", bug_id)

        return jsonify({"success": True, "soft_delete": not hard_delete})


@app.route("/api/bugs/<int:bug_id>/restore", methods=["POST"])
@require_auth
def restore_bug(bug_id):
    """Restore a soft-deleted bug."""
    with get_db_connection() as conn:
        result = conn.execute(
            """
            UPDATE bugs SET
                deleted_at = NULL,
                deleted_by = NULL
            WHERE id = ? AND deleted_at IS NOT NULL
        """,
            (bug_id,),
        )

        if result.rowcount == 0:
            return jsonify({"error": "Bug not found or not deleted"}), 404

        log_activity("restore_bug", "bug", bug_id)

    return jsonify({"success": True})


# ============================================================================
# BUG SOLUTIONS & KNOWLEDGE BASE API
# ============================================================================


@app.route("/api/bugs/<int:bug_id>/solution", methods=["POST"])
@require_auth
def add_bug_solution(bug_id):
    """Add a solution to a bug for knowledge base."""
    data = request.get_json(silent=True) or {}

    with get_db_connection() as conn:
        # Verify bug exists
        bug = conn.execute(
            "SELECT id FROM bugs WHERE id = ?", (bug_id,)
        ).fetchone()
        if not bug:
            return jsonify({"error": "Bug not found"}), 404

        # Update bug with solution details
        conn.execute(
            """
            UPDATE bugs SET
                solution = ?,
                root_cause = ?,
                fix_commit = ?,
                fix_files = ?,
                lessons_learned = ?,
                updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (
                data.get("solution"),
                data.get("root_cause"),
                data.get("fix_commit"),
                data.get("fix_files"),
                data.get("lessons_learned"),
                bug_id,
            ),
        )

        # Add to bug_solutions for searchable knowledge base
        conn.execute(
            """
            INSERT INTO bug_solutions
            (bug_id, error_pattern, solution_summary, detailed_steps, code_changes, verification_steps)
            VALUES (?, ?, ?, ?, ?, ?)
        """,
            (
                bug_id,
                data.get("error_pattern"),
                data.get("solution"),
                data.get("detailed_steps"),
                data.get("code_changes"),
                data.get("verification_steps"),
            ),
        )

        # Log history
        conn.execute(
            """
            INSERT INTO bug_history (bug_id, action, notes, actor)
            VALUES (?, 'solution_added', ?, ?)
        """,
            (bug_id, data.get("solution", "")[:200], "system"),
        )

        log_activity(
            "add_solution",
            "bug",
            bug_id,
            f"Added solution: {data.get('solution', '')[:100]}",
        )

        return jsonify(
            {"success": True, "message": "Solution added to knowledge base"}
        )


@app.route("/api/bugs/<int:bug_id>/history", methods=["GET"])
@require_auth
def get_bug_history(bug_id):
    """Get history of changes for a bug."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        history = conn.execute(
            """
            SELECT * FROM bug_history
            WHERE bug_id = ?
            ORDER BY created_at DESC
        """,
            (bug_id,),
        ).fetchall()
        return jsonify([dict(h) for h in history])


@app.route("/api/bugs/similar", methods=["GET"])
@require_auth
def find_similar_bugs():
    """Find similar bugs based on error pattern or keywords."""
    query = request.args.get("q", "")
    category = request.args.get("category", "")

    if not query and not category:
        return jsonify({"error": "Query or category required"}), 400

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Search in bugs and solutions
        sql = """
            SELECT DISTINCT b.*, p.name as project_name,
                   bs.solution_summary, bs.error_pattern
            FROM bugs b
            LEFT JOIN projects p ON b.project_id = p.id
            LEFT JOIN bug_solutions bs ON b.id = bs.bug_id
            WHERE 1=1
        """
        params = []

        if query:
            sql += """ AND (
                b.title LIKE ? OR
                b.description LIKE ? OR
                b.solution LIKE ? OR
                b.root_cause LIKE ? OR
                bs.error_pattern LIKE ? OR
                bs.solution_summary LIKE ?
            )"""
            like_query = f"%{query}%"
            params.extend([like_query] * 6)

        if category:
            sql += " AND b.category = ?"
            params.append(category)

        sql += " ORDER BY b.resolved_at DESC NULLS LAST, b.created_at DESC LIMIT 20"

        bugs = conn.execute(sql, params).fetchall()

        # Increment reuse count for solutions that match
        if bugs and query:
            for bug in bugs:
                conn.execute(
                    """
                    UPDATE bug_solutions SET reuse_count = reuse_count + 1
                    WHERE bug_id = ?
                """,
                    (bug["id"],),
                )

        return jsonify([dict(b) for b in bugs])


@app.route("/api/bugs/knowledge-base", methods=["GET"])
@require_auth
def get_knowledge_base():
    """Get all resolved bugs with solutions for knowledge base."""
    category = request.args.get("category", "")
    limit = request.args.get("limit", 50, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        sql = """
            SELECT b.*, p.name as project_name,
                   bs.solution_summary, bs.error_pattern, bs.detailed_steps,
                   bs.effectiveness_rating, bs.reuse_count
            FROM bugs b
            LEFT JOIN projects p ON b.project_id = p.id
            LEFT JOIN bug_solutions bs ON b.id = bs.bug_id
            WHERE b.status = 'resolved' AND b.solution IS NOT NULL
        """
        params = []

        if category:
            sql += " AND b.category = ?"
            params.append(category)

        sql += " ORDER BY bs.reuse_count DESC, b.resolved_at DESC LIMIT ?"
        params.append(limit)

        bugs = conn.execute(sql, params).fetchall()
        return jsonify([dict(b) for b in bugs])


# ============================================================================
# DEVOPS TASKS API
# ============================================================================


def get_priority_label(priority: int) -> str:
    """Convert numeric priority to label."""
    labels = {0: "low", 1: "medium", 2: "high", 3: "urgent"}
    return labels.get(priority, "low")


def get_priority_value(label: str) -> int:
    """Convert priority label to numeric value."""
    values = {"low": 0, "medium": 1, "high": 2, "urgent": 3}
    return values.get(label.lower(), 0)


@app.route("/api/devops", methods=["GET"])
@require_auth
def get_devops_tasks():
    """Get DevOps tasks."""
    # Get filter parameters from request
    project_id = request.args.get("project_id", type=int)
    status = request.args.get("status")
    priority = request.args.get("priority")
    task_type = request.args.get("task_type")
    search = request.args.get("search")
    sort_by = request.args.get("sort_by", "created_at")
    sort_order = request.args.get("sort_order", "DESC")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        query = """
            SELECT d.*, p.name as project_name
            FROM devops_tasks d
            LEFT JOIN projects p ON d.project_id = p.id
            WHERE 1=1
        """
        params = []

        if project_id:
            query += " AND d.project_id = ?"
            params.append(project_id)
        if status and status != "all":
            query += " AND d.status = ?"
            params.append(status)
        if priority:
            # Accept both label (high) and number (2)
            if priority.isdigit():
                query += " AND d.priority = ?"
                params.append(int(priority))
            else:
                query += " AND d.priority = ?"
                params.append(get_priority_value(priority))
        if task_type:
            query += " AND d.task_type = ?"
            params.append(task_type)
        if search:
            search_pattern = f"%{search}%"
            query += " AND (d.name LIKE ? OR d.description LIKE ?)"
            params.extend([search_pattern, search_pattern])

        query += f" ORDER BY d.{sort_by} {sort_order}, d.created_at DESC"

        tasks = conn.execute(query, params).fetchall()

        # Add priority_label to each task
        result = []
        for t in tasks:
            task_dict = dict(t)
            task_dict["priority_label"] = get_priority_label(
                task_dict.get("priority", 0)
            )
            result.append(task_dict)

        return jsonify(result)


@app.route("/api/devops", methods=["POST"])
@require_auth
def create_devops_task():
    """Create a DevOps task.

    Accepts priority as number (0-3) or label (low, medium, high, urgent).
    """
    data = request.get_json()

    # Convert priority label to number if needed
    priority_input = data.get("priority", "medium")
    if isinstance(priority_input, str) and not priority_input.isdigit():
        priority = get_priority_value(priority_input)
    else:
        priority = int(priority_input) if priority_input else 1

    with get_db_connection() as conn:
        cursor = conn.execute(
            """
            INSERT INTO devops_tasks (project_id, name, description, task_type, priority, schedule)
            VALUES (?, ?, ?, ?, ?, ?)
        """,
            (
                data.get("project_id"),
                data.get("name"),
                data.get("description"),
                data.get("task_type", "maintenance"),
                priority,
                data.get("schedule"),
            ),
        )
        task_id = cursor.lastrowid

        log_activity("create_devops_task", "devops", task_id, data.get("name"))

        return jsonify(
            {
                "id": task_id,
                "success": True,
                "priority_label": get_priority_label(priority),
            }
        )


# ============================================================================
# ERROR AGGREGATION API
# ============================================================================


@app.route("/api/errors", methods=["GET"])
@require_auth
def get_errors():
    """Get aggregated errors with filtering and pagination.

    Returns deduplicated errors aggregated by type/message/source.

    Query Parameters:
        project_id (int): Filter by project
        node_id (int): Filter by originating node
        status (str): Filter by status - open, resolved, ignored, all (default: all)
        error_type (str): Filter by type - error, warning, info
        search (str): Search in message, source, or type (supports LIKE patterns)
        from_date (str): ISO date - errors after this date
        to_date (str): ISO date - errors before this date
        sort_by (str): Sort field (id, error_type, message, source, count, first_seen, last_seen, status)
        sort_order (str): ASC or DESC (default: DESC)
        limit (int): Max results (default: 100)
        offset (int): Skip first N results

    Returns:
        200: List of aggregated errors with metadata

    Example Request:
        GET /api/errors
        GET /api/errors?status=open&error_type=error
        GET /api/errors?search=connection&sort_by=count&sort_order=DESC

    Example Response:
        {
            "errors": [
                {
                    "id": 1,
                    "error_type": "error",
                    "message": "Connection timeout",
                    "source": "api/client.py:42",
                    "stack_trace": "Traceback...",
                    "project_id": 1,
                    "project_name": "My App",
                    "node_id": 2,
                    "node_hostname": "worker-01",
                    "occurrence_count": 15,
                    "status": "open",
                    "first_seen": "2024-01-10T08:00:00",
                    "last_seen": "2024-01-15T14:30:00"
                }
            ],
            "total": 42,
            "limit": 100,
            "offset": 0
        }

    cURL Example:
        curl -X GET "http://localhost:8080/api/errors?status=open" \\
             -H "Cookie: session=<session_cookie>"
    """
    project_id = request.args.get("project_id")
    node_id = request.args.get("node_id")
    status = request.args.get("status", "all")  # Default to showing all errors
    error_type = request.args.get("error_type")
    search = request.args.get("search")
    from_date = request.args.get("from_date")
    to_date = request.args.get("to_date")
    sort_by = request.args.get("sort_by", "last_seen")
    sort_order = request.args.get("sort_order", "DESC").upper()
    # Validate sort_order to prevent SQL injection
    if sort_order not in ("ASC", "DESC"):
        sort_order = "DESC"
    # Validate sort_by to prevent SQL injection
    valid_sort_columns = {
        "id",
        "error_type",
        "message",
        "source",
        "count",
        "first_seen",
        "last_seen",
        "status",
    }
    if sort_by not in valid_sort_columns:
        sort_by = "last_seen"
    limit = request.args.get(
        "limit", 100, type=int
    )  # Default limit for pagination
    offset = request.args.get("offset", 0, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        query = """
            SELECT e.*, p.name as project_name, n.hostname as node_hostname
            FROM errors e
            LEFT JOIN projects p ON e.project_id = p.id
            LEFT JOIN nodes n ON e.node_id = n.id
            WHERE 1=1
        """
        params = []

        if project_id:
            query += " AND e.project_id = ?"
            params.append(project_id)
        if node_id:
            query += " AND e.node_id = ?"
            params.append(node_id)
        # Only filter by status if not 'all'
        if status and status != "all":
            query += " AND e.status = ?"
            params.append(status)
        if error_type:
            query += " AND e.error_type = ?"
            params.append(error_type)
        if search:
            # Support both simple text search and SQL LIKE patterns
            search_pattern = search if "%" in search else f"%{search}%"
            query += " AND (e.message LIKE ? OR e.source LIKE ? OR e.error_type LIKE ?)"
            params.extend([search_pattern, search_pattern, search_pattern])
        if from_date:
            query += " AND e.last_seen >= ?"
            params.append(from_date)
        if to_date:
            query += " AND e.last_seen <= ?"
            params.append(to_date)

        query += f" ORDER BY e.{sort_by} {sort_order}, e.last_seen DESC"
        query += " LIMIT ? OFFSET ?"
        params.extend([limit, offset])

        errors = conn.execute(query, params).fetchall()

        # Get total count for pagination
        count_query = """
            SELECT COUNT(*) as total FROM errors e WHERE 1=1
        """
        count_params = []
        if project_id:
            count_query += " AND e.project_id = ?"
            count_params.append(project_id)
        if node_id:
            count_query += " AND e.node_id = ?"
            count_params.append(node_id)
        if status and status != "all":
            count_query += " AND e.status = ?"
            count_params.append(status)
        if error_type:
            count_query += " AND e.error_type = ?"
            count_params.append(error_type)
        if search:
            search_pattern = search if "%" in search else f"%{search}%"
            count_query += " AND (e.message LIKE ? OR e.source LIKE ? OR e.error_type LIKE ?)"
            count_params.extend(
                [search_pattern, search_pattern, search_pattern]
            )
        if from_date:
            count_query += " AND e.last_seen >= ?"
            count_params.append(from_date)
        if to_date:
            count_query += " AND e.last_seen <= ?"
            count_params.append(to_date)

        total = conn.execute(count_query, count_params).fetchone()["total"]

        return jsonify(
            {
                "errors": [dict(e) for e in errors],
                "total": total,
                "limit": limit,
                "offset": offset,
                "has_more": offset + len(errors) < total,
            }
        )


@app.route("/api/errors/stats", methods=["GET"])
@require_auth
def get_error_stats():
    """Get error statistics and trending data.

    Returns:
    - by_type: Count of errors grouped by error_type
    - by_status: Count of errors grouped by status
    - by_severity: Count of errors grouped by inferred severity
    - trending: Error counts by day for the last 7 days
    - total_open: Total open errors
    - total_resolved: Total resolved errors
    """
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Errors by type
        by_type = conn.execute(
            """
            SELECT error_type, COUNT(*) as count, SUM(occurrence_count) as total_occurrences
            FROM errors
            WHERE status = 'open'
            GROUP BY error_type
            ORDER BY count DESC
        """
        ).fetchall()

        # Errors by status
        by_status = conn.execute(
            """
            SELECT status, COUNT(*) as count
            FROM errors
            GROUP BY status
        """
        ).fetchall()

        # Infer severity from error_type and calculate counts
        # critical: contains 'critical', 'fatal', 'panic'
        # error: contains 'error', 'exception', 'fail'
        # warning: contains 'warning', 'warn'
        # info: everything else
        severity_query = """
            SELECT
                CASE
                    WHEN LOWER(error_type) LIKE '%critical%' OR LOWER(error_type) LIKE '%fatal%' OR LOWER(error_type) LIKE '%panic%' THEN 'critical'
                    WHEN LOWER(error_type) LIKE '%error%' OR LOWER(error_type) LIKE '%exception%' OR LOWER(error_type) LIKE '%fail%' THEN 'error'
                    WHEN LOWER(error_type) LIKE '%warning%' OR LOWER(error_type) LIKE '%warn%' THEN 'warning'
                    ELSE 'info'
                END as severity,
                COUNT(*) as count,
                SUM(occurrence_count) as total_occurrences
            FROM errors
            WHERE status = 'open'
            GROUP BY severity
            ORDER BY CASE severity WHEN 'critical' THEN 0 WHEN 'error' THEN 1 WHEN 'warning' THEN 2 ELSE 3 END
        """
        by_severity = conn.execute(severity_query).fetchall()

        # Trending: errors per day for last 7 days
        trending = conn.execute(
            """
            SELECT DATE(last_seen) as date, COUNT(*) as count, SUM(occurrence_count) as total_occurrences
            FROM errors
            WHERE last_seen >= DATE('now', '-7 days')
            GROUP BY DATE(last_seen)
            ORDER BY date ASC
        """
        ).fetchall()

        # Totals
        totals = conn.execute(
            """
            SELECT
                SUM(CASE WHEN status = 'open' THEN 1 ELSE 0 END) as total_open,
                SUM(CASE WHEN status = 'resolved' THEN 1 ELSE 0 END) as total_resolved,
                SUM(CASE WHEN status = 'queued' THEN 1 ELSE 0 END) as total_queued,
                SUM(occurrence_count) as total_occurrences
            FROM errors
        """
        ).fetchone()

        return jsonify(
            {
                "by_type": [dict(r) for r in by_type],
                "by_status": [dict(r) for r in by_status],
                "by_severity": [dict(r) for r in by_severity],
                "trending": [dict(r) for r in trending],
                "total_open": totals["total_open"] or 0,
                "total_resolved": totals["total_resolved"] or 0,
                "total_queued": totals["total_queued"] or 0,
                "total_occurrences": totals["total_occurrences"] or 0,
            }
        )


@app.route("/api/errors/summary", methods=["GET"])
@require_auth
def get_errors_summary():
    """Get a quick summary of error statistics.

    Returns a lightweight summary optimized for dashboard widgets:
    - Counts by status (open, queued, in_progress, resolved, ignored)
    - Top error sources
    - Recent error activity
    - Error rate metrics
    """
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        summary = {
            "timestamp": datetime.now().isoformat(),
            "counts": {},
            "status_breakdown": {},
            "top_sources": [],
            "top_types": [],
            "recent": [],
            "metrics": {},
        }

        # Counts by status
        status_rows = conn.execute(
            """
            SELECT status, COUNT(*) as count, SUM(occurrence_count) as occurrences
            FROM errors
            GROUP BY status
        """
        ).fetchall()
        status_counts = {}
        status_occurrences = {}
        for row in status_rows:
            status_key = row["status"] or "unknown"
            status_counts[status_key] = row["count"] or 0
            status_occurrences[status_key] = row["occurrences"] or 0

        total_unique = sum(status_counts.values())
        total_occurrences = sum(status_occurrences.values())

        default_statuses = [
            "open",
            "queued",
            "in_progress",
            "resolved",
            "ignored",
        ]
        summary["counts"] = {
            "total": total_unique,
            "total_occurrences": total_occurrences,
            "active": (
                status_counts.get("open", 0)
                + status_counts.get("queued", 0)
                + status_counts.get("in_progress", 0)
            ),
        }
        for status in default_statuses:
            summary["counts"][status] = status_counts.get(status, 0)

        summary["status_breakdown"] = {
            status: {
                "count": status_counts[status],
                "occurrences": status_occurrences[status],
            }
            for status in status_counts
        }

        # Top error sources (most frequent)
        top_sources = conn.execute(
            """
            SELECT source, COUNT(*) as count, SUM(occurrence_count) as occurrences
            FROM errors
            WHERE status != 'resolved'
            GROUP BY source
            ORDER BY occurrences DESC
            LIMIT 5
        """
        ).fetchall()
        summary["top_sources"] = [
            {
                "source": r["source"],
                "count": r["count"],
                "occurrences": r["occurrences"],
            }
            for r in top_sources
        ]

        # Top error types
        top_types = conn.execute(
            """
            SELECT error_type, COUNT(*) as count, SUM(occurrence_count) as occurrences
            FROM errors
            WHERE status != 'resolved'
            GROUP BY error_type
            ORDER BY occurrences DESC
            LIMIT 5
        """
        ).fetchall()
        summary["top_types"] = [
            {
                "type": r["error_type"],
                "count": r["count"],
                "occurrences": r["occurrences"],
            }
            for r in top_types
        ]

        # Recent errors (last 5)
        recent = conn.execute(
            """
            SELECT id, error_type, message, source, status, occurrence_count, last_seen
            FROM errors
            ORDER BY last_seen DESC
            LIMIT 5
        """
        ).fetchall()
        summary["recent"] = [dict(r) for r in recent]

        # Error rate metrics
        # Errors in last hour
        last_hour = conn.execute(
            """
            SELECT COUNT(*) as count, SUM(occurrence_count) as occurrences
            FROM errors
            WHERE last_seen >= datetime('now', '-1 hour')
        """
        ).fetchone()

        # Errors in last 24 hours
        last_day = conn.execute(
            """
            SELECT COUNT(*) as count, SUM(occurrence_count) as occurrences
            FROM errors
            WHERE last_seen >= datetime('now', '-24 hours')
        """
        ).fetchone()

        # Errors in last 7 days
        last_week = conn.execute(
            """
            SELECT COUNT(*) as count, SUM(occurrence_count) as occurrences
            FROM errors
            WHERE last_seen >= datetime('now', '-7 days')
        """
        ).fetchone()

        summary["metrics"] = {
            "last_hour": {
                "unique": last_hour["count"] or 0,
                "occurrences": last_hour["occurrences"] or 0,
            },
            "last_24h": {
                "unique": last_day["count"] or 0,
                "occurrences": last_day["occurrences"] or 0,
            },
            "last_7d": {
                "unique": last_week["count"] or 0,
                "occurrences": last_week["occurrences"] or 0,
            },
        }

        return jsonify(summary)


@app.route("/api/errors", methods=["POST"])
@rate_limit(requests_per_minute=120)  # Higher limit for error logging
def log_error():
    """Log an error from any node (no auth required).

    Creates or updates an aggregated error entry. Errors are deduplicated
    by type+message+source combination. Updates occurrence count for duplicates.

    Request Body:
        error_type (str): Error type - error, warning, info
        message (str): Error message
        source (str): Source file/location (e.g., "api/client.py:42")
        project_id (int, optional): Associated project ID
        node_id (int, optional): Reporting node ID
        line (int, optional): Line number
        column_num (int, optional): Column number
        stack_trace (str, optional): Full stack trace
        url (str, optional): Request URL if applicable
        user_agent (str, optional): Client user agent
        http_status (int, optional): HTTP status code
        context (str, optional): Additional context JSON

    Returns:
        200: Error logged with ID, deduplicated flag indicates if merged
        429: Rate limit exceeded

    Example Request:
        POST /api/errors
        Content-Type: application/json

        {
            "error_type": "error",
            "message": "Connection refused",
            "source": "services/db.py:128",
            "stack_trace": "Traceback (most recent call last)...",
            "node_id": 1
        }

    Example Response (new error):
        {
            "id": 42,
            "deduplicated": false
        }

    Example Response (duplicate merged):
        {
            "id": 15,
            "deduplicated": true
        }

    cURL Example:
        curl -X POST "http://localhost:8080/api/errors" \\
             -H "Content-Type: application/json" \\
             -d '{"error_type": "error", "message": "Test error"}'
    """
    data = request.get_json()

    # Generate error hash for deduplication
    error_key = f"{
        data.get(
            'error_type',
            '')}:{
            data.get(
                'message',
                '')}:{
                    data.get(
                        'source',
                        '')}"
    error_hash = hashlib.md5(error_key.encode()).hexdigest()

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Check for existing error
        existing = conn.execute(
            """
            SELECT id, occurrence_count FROM errors
            WHERE error_type = ? AND message = ? AND source = ? AND status = 'open'
            LIMIT 1
        """,
            (data.get("error_type"), data.get("message"), data.get("source")),
        ).fetchone()

        if existing:
            # Update existing error
            conn.execute(
                """
                UPDATE errors SET
                    occurrence_count = occurrence_count + 1,
                    last_seen = CURRENT_TIMESTAMP,
                    context = COALESCE(?, context)
                WHERE id = ?
            """,
                (data.get("context"), existing["id"]),
            )
            # Broadcast updates via WebSocket
            broadcast_errors()
            broadcast_stats()
            return jsonify({"id": existing["id"], "deduplicated": True})
        else:
            # Create new error with status 'queued'
            cursor = conn.execute(
                """
                INSERT INTO errors (
                    project_id, node_id, error_type, message, source,
                    line, column_num, stack_trace, url, user_agent,
                    http_status, context, status
                )
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 'queued')
            """,
                (
                    data.get("project_id"),
                    data.get("node_id"),
                    data.get("error_type", "unknown"),
                    data.get("message"),
                    data.get("source"),
                    data.get("line"),
                    data.get("column"),
                    data.get("stack_trace"),
                    data.get("url"),
                    data.get("user_agent"),
                    data.get("http_status"),
                    data.get("context"),
                ),
            )
            error_id = cursor.lastrowid

            # Auto-queue error for Claude processing
            import json as json_module

            dashboard_url = request.host_url.rstrip("/")
            message = """Fix this error and complete the full workflow:

ERROR ID: {error_id}
Type: {data.get('error_type', 'unknown')}
Message: {data.get('message', 'N/A')}
Source: {data.get('source', 'N/A')}
Stack trace: {data.get('stack_trace') or 'N/A'}

WORKFLOW - Complete ALL steps:
1. Analyze and fix the error in the codebase
2. Run tests to verify the fix
3. Commit with a descriptive message
4. Restart/deploy the server
5. Mark error as resolved:
   curl -X POST "{dashboard_url}/api/errors/{error_id}/resolve" -H "Cookie: $(cat /tmp/arch_cookies.txt | grep session | cut -f7)"

Start by analyzing the error."""

            task_data = json_module.dumps(
                {
                    "entity_type": "error",
                    "entity_id": error_id,
                    "session": "arch_env3",
                    "message": message,
                    "dashboard_url": dashboard_url,
                }
            )

            conn.execute(
                """
                INSERT INTO task_queue (task_type, task_data, priority, max_retries)
                VALUES ('claude_task', ?, 2, 3)
            """,
                (task_data,),
            )

            # Broadcast updates via WebSocket
            broadcast_errors()
            broadcast_stats()
            return jsonify(
                {"id": error_id, "deduplicated": False, "queued": True}
            )


@app.route("/api/errors/<int:error_id>/resolve", methods=["POST"])
@require_auth
def resolve_error(error_id):
    """Mark an error as resolved."""
    with get_db_connection() as conn:
        conn.execute(
            """
            UPDATE errors SET status = 'resolved', resolved_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (error_id,),
        )

        log_activity("resolve_error", "error", error_id)

    # Broadcast updates via WebSocket
    broadcast_errors()
    broadcast_stats()
    return jsonify({"success": True})


@app.route("/api/errors/<int:error_id>/create-bug", methods=["POST"])
@require_auth
def create_bug_from_error(error_id):
    """Create a bug from an error."""
    data = request.get_json(silent=True) or {}

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        error = conn.execute(
            "SELECT * FROM errors WHERE id = ?", (error_id,)
        ).fetchone()
        if not error:
            return jsonify({"error": "Error not found"}), 404

        # Get project_id from request body (required since errors don't have
        # project_id)
        project_id = data.get("project_id")
        if not project_id:
            # Try to get a default project if none specified
            default_project = conn.execute(
                "SELECT id FROM projects WHERE status = 'active' ORDER BY id LIMIT 1"
            ).fetchone()
            if default_project:
                project_id = default_project["id"]
            else:
                # Create or get an "Unassigned" project for orphan bugs
                unassigned = conn.execute(
                    "SELECT id FROM projects WHERE name = 'Unassigned'"
                ).fetchone()
                if unassigned:
                    project_id = unassigned["id"]
                else:
                    cursor = conn.execute(
                        "INSERT INTO projects (name, description, status) VALUES (?, ?, ?)",
                        (
                            "Unassigned",
                            "Default project for bugs created from errors without a project",
                            "active",
                        ),
                    )
                    project_id = cursor.lastrowid
                    log_activity(
                        "create_project",
                        "project",
                        project_id,
                        "Auto-created for orphan bugs",
                    )

        # Create bug
        cursor = conn.execute(
            """
            INSERT INTO bugs (project_id, title, description, severity, source_node, source_error_id, stack_trace)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """,
            (
                project_id,
                f"[Error] {error['error_type']}: {error['message'][:100]}",
                f"Error logged {
                    error['occurrence_count']} times.\n\nMessage: {
                    error['message']}\n\nSource: {
                    error['source']}",
                "high" if error["occurrence_count"] > 10 else "medium",
                error["node_id"],
                error_id,
                error["stack_trace"],
            ),
        )
        bug_id = cursor.lastrowid

        # Link error to bug
        conn.execute(
            "UPDATE errors SET assigned_bug_id = ? WHERE id = ?",
            (bug_id, error_id),
        )

        log_activity(
            "create_bug_from_error", "bug", bug_id, f"From error {error_id}"
        )

        return jsonify({"bug_id": bug_id, "success": True})


# ============================================================================
# NODE MANAGEMENT API
# ============================================================================


@app.route("/api/nodes", methods=["GET"])
@require_auth
def get_nodes():
    """Get all cluster nodes with metrics.

    Returns nodes with session and worker counts. Nodes are sorted by role then hostname.

    Query Parameters:
        status (str): Filter by status (online, offline)
        role (str): Filter by role (primary, worker, standby)
        page (int): Page number (default: 1)
        per_page (int): Items per page, max 100 (default: 50)
        paginate (str): Set to 'true' for pagination response

    Returns:
        200: List of nodes or paginated result

    Example Request:
        GET /api/nodes
        GET /api/nodes?status=online
        GET /api/nodes?role=worker&paginate=true

    Example Response:
        [
            {
                "id": "node-001",
                "hostname": "worker-01",
                "ip_address": "192.168.1.101",
                "status": "online",
                "role": "worker",
                "cpu_usage": 45.2,
                "memory_usage": 62.8,
                "disk_usage": 55.0,
                "last_heartbeat": "2024-01-15T14:30:00",
                "session_count": 3,
                "worker_count": 2,
                "services": "task_worker,error_daemon"
            }
        ]

    cURL Example:
        curl -X GET "http://localhost:8080/api/nodes?status=online" \\
             -H "Cookie: session=<session_cookie>"
    """
    status_filter = request.args.get("status")
    role_filter = request.args.get("role")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        query = """
            SELECT n.*,
                   COUNT(DISTINCT ts.id) as session_count,
                   COUNT(DISTINCT w.id) as worker_count
            FROM nodes n
            LEFT JOIN tmux_sessions ts ON n.id = ts.node_id
            LEFT JOIN workers w ON n.id = w.node_id
        """
        params = []
        conditions = []

        if status_filter:
            conditions.append("n.status = ?")
            params.append(status_filter)
        if role_filter:
            conditions.append("n.role = ?")
            params.append(role_filter)

        if conditions:
            query += " WHERE " + " AND ".join(conditions)

        query += " GROUP BY n.id ORDER BY n.role, n.hostname"

        nodes = conn.execute(query, params).fetchall()
        node_list = [dict(n) for n in nodes]

        # Support pagination if requested
        if request.args.get("paginate", "").lower() == "true":
            page, per_page = get_pagination_params()
            result = paginate_query(node_list, page, per_page)
            return jsonify(result)

        return jsonify(node_list)


@app.route("/api/nodes", methods=["POST"])
@require_auth
def add_node():
    """Add or update a node in the cluster.

    Creates a new node or updates existing one (upsert by ID).

    Request Body:
        id (str, optional): Node ID (auto-generated if not provided)
        hostname (str, required): Node hostname
        ip_address (str, optional): IP address
        ssh_port (int, optional): SSH port (default: 22)
        ssh_user (str, optional): SSH username
        role (str, optional): Node role (primary, worker, standby)
        services (str, optional): Comma-separated list of services

    Returns:
        200: Node added/updated with ID

    Example Request:
        POST /api/nodes
        Content-Type: application/json

        {
            "hostname": "worker-02",
            "ip_address": "192.168.1.102",
            "role": "worker",
            "services": "task_worker"
        }

    Example Response:
        {
            "id": "abc12345",
            "success": true
        }

    cURL Example:
        curl -X POST "http://localhost:8080/api/nodes" \\
             -H "Content-Type: application/json" \\
             -H "Cookie: session=<session_cookie>" \\
             -d '{"hostname": "new-worker", "role": "worker"}'
    """
    data = request.get_json()
    node_id = data.get("id") or str(uuid.uuid4())[:8]

    with get_db_connection() as conn:
        conn.execute(
            """
            INSERT OR REPLACE INTO nodes (id, hostname, ip_address, ssh_port, ssh_user, role, services)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """,
            (
                node_id,
                data.get("hostname"),
                data.get("ip_address"),
                data.get("ssh_port", 22),
                data.get("ssh_user"),
                data.get("role", "worker"),
                json.dumps(data.get("services", [])),
            ),
        )

        log_activity("add_node", "node", None, data.get("hostname"))

        return jsonify({"id": node_id, "success": True})


@app.route("/api/nodes/<node_id>/heartbeat", methods=["POST"])
def node_heartbeat(node_id):
    """Receive heartbeat from a node."""
    data = request.get_json(silent=True) or {}

    with get_db_connection() as conn:
        conn.execute(
            """
            UPDATE nodes SET
                status = 'online',
                cpu_usage = ?,
                memory_usage = ?,
                disk_usage = ?,
                last_heartbeat = CURRENT_TIMESTAMP,
                updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (
                data.get("cpu_usage", 0),
                data.get("memory_usage", 0),
                data.get("disk_usage", 0),
                node_id,
            ),
        )

    # Broadcast node update via WebSocket
    broadcast_nodes()
    return jsonify({"success": True})


@app.route("/api/nodes/<node_id>", methods=["DELETE"])
@require_auth
def remove_node(node_id):
    """Remove a node from the cluster."""
    with get_db_connection() as conn:
        conn.execute("DELETE FROM nodes WHERE id = ?", (node_id,))
        log_activity("remove_node", "node", None, node_id)
        return jsonify({"success": True})


# Health monitoring thresholds
NODE_HEALTH_THRESHOLDS = {
    "cpu_usage": {"warning": 80, "critical": 95},
    "memory_usage": {"warning": 85, "critical": 95},
    "disk_usage": {"warning": 80, "critical": 90},
    "load_average": {"warning": 4.0, "critical": 8.0},
    "heartbeat_timeout_seconds": 120,  # Mark offline after 2 minutes
}

# Worker health monitoring thresholds
WORKER_HEALTH_THRESHOLDS = {
    "heartbeat_timeout_warning": 60,  # Warning after 1 minute
    "heartbeat_timeout_critical": 120,  # Critical after 2 minutes
    "heartbeat_timeout_offline": 300,  # Mark offline after 5 minutes
    "task_stuck_warning": 300,  # Task running > 5 minutes
    "task_stuck_critical": 900,  # Task running > 15 minutes
    "consecutive_failures_warning": 3,  # Alert after 3 consecutive failures
    "consecutive_failures_critical": 5,  # Critical after 5 consecutive failures
}

# tmux session cleanup configuration
TMUX_SESSION_CLEANUP_CONFIG = {
    "stale_threshold_hours": 24,  # Sessions inactive > 24 hours are stale
    # Auto-remove DB entries for dead sessions on refresh
    "auto_cleanup_on_refresh": True,
    # Kill unattached sessions after 72 hours (0 = disabled)
    "kill_unattached_after_hours": 72,
    "preserve_worker_sessions": True,  # Never auto-kill worker sessions
    # Never auto-kill sessions with assigned tasks
    "preserve_assigned_sessions": True,
    # Remove DB entries for sessions that don't exist in tmux
    "cleanup_orphaned_db_entries": True,
    "max_sessions_per_node": 50,  # Alert if node has more than this many sessions
    "protected_session_patterns": [  # Patterns for sessions that should never be auto-killed
        "prod",
        "production",
        "main",
        "master",
    ],
}


def check_node_health(node: dict, thresholds: dict = None) -> dict:
    """Check node health against thresholds and return health status."""
    if thresholds is None:
        thresholds = NODE_HEALTH_THRESHOLDS

    issues = []
    status = "healthy"

    # Check heartbeat freshness
    if node.get("last_heartbeat"):
        try:
            last_hb = datetime.fromisoformat(
                node["last_heartbeat"].replace("Z", "+00:00")
            )
            age_seconds = (
                datetime.now() - last_hb.replace(tzinfo=None)
            ).total_seconds()
            if age_seconds > thresholds.get("heartbeat_timeout_seconds", 120):
                issues.append(
                    {
                        "type": "offline",
                        "severity": "critical",
                        "message": f"No heartbeat for {int(age_seconds)}s",
                        "metric": "heartbeat_age",
                        "value": age_seconds,
                    }
                )
                status = "critical"
        except (ValueError, TypeError):
            pass

    # Check resource metrics
    for metric, limits in thresholds.items():
        if metric == "heartbeat_timeout_seconds":
            continue
        if not isinstance(limits, dict):
            continue

        value = node.get(metric, 0)
        if value >= limits.get("critical", 100):
            issues.append(
                {
                    "type": "resource_critical",
                    "severity": "critical",
                    "message": f"{metric} at {
                        value:.1f}% (threshold: {
                        limits['critical']}%)",
                    "metric": metric,
                    "value": value,
                    "threshold": limits["critical"],
                }
            )
            status = "critical"
        elif value >= limits.get("warning", 80):
            issues.append(
                {
                    "type": "resource_warning",
                    "severity": "warning",
                    "message": f"{metric} at {
                        value:.1f}% (threshold: {
                        limits['warning']}%)",
                    "metric": metric,
                    "value": value,
                    "threshold": limits["warning"],
                }
            )
            if status != "critical":
                status = "warning"

    return {"status": status, "issues": issues}


def create_node_alert(
    conn,
    node_id: str,
    alert_type: str,
    severity: str,
    message: str,
    metric_name: str = None,
    metric_value: float = None,
    threshold_value: float = None,
):
    """Create a new alert for a node if one doesn't already exist."""
    # Check for existing active alert of same type
    existing = conn.execute(
        """
        SELECT id FROM node_alerts
        WHERE node_id = ? AND alert_type = ? AND metric_name = ? AND status = 'active'
    """,
        (node_id, alert_type, metric_name),
    ).fetchone()

    if existing:
        # Update existing alert
        conn.execute(
            """
            UPDATE node_alerts SET
                metric_value = ?,
                message = ?,
                created_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (metric_value, message, existing[0]),
        )
        return existing[0]

    # Create new alert
    cursor = conn.execute(
        """
        INSERT INTO node_alerts (node_id, alert_type, severity, message, metric_name, metric_value, threshold_value)
        VALUES (?, ?, ?, ?, ?, ?, ?)
    """,
        (
            node_id,
            alert_type,
            severity,
            message,
            metric_name,
            metric_value,
            threshold_value,
        ),
    )
    return cursor.lastrowid


@app.route("/api/nodes/health", methods=["GET"])
@require_auth
def get_cluster_health():
    """Get health status for all nodes with alerts.

    Returns:
    - nodes: List of nodes with health status
    - summary: Cluster health summary
    - active_alerts: Current active alerts
    """
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        nodes = conn.execute(
            """
            SELECT * FROM nodes ORDER BY role, hostname
        """
        ).fetchall()

        # Check health for each node
        node_health = []
        summary = {
            "total": 0,
            "healthy": 0,
            "warning": 0,
            "critical": 0,
            "offline": 0,
        }

        for node in nodes:
            node_dict = dict(node)
            health = check_node_health(node_dict)
            node_dict["health"] = health

            summary["total"] += 1
            if (
                node_dict["status"] == "offline"
                or health["status"] == "critical"
            ):
                summary["critical" if health["issues"] else "offline"] += 1
            elif health["status"] == "warning":
                summary["warning"] += 1
            else:
                summary["healthy"] += 1

            # Create/update alerts for issues
            for issue in health["issues"]:
                create_node_alert(
                    conn,
                    node_dict["id"],
                    issue["type"],
                    issue["severity"],
                    issue["message"],
                    issue.get("metric"),
                    issue.get("value"),
                    issue.get("threshold"),
                )

            node_health.append(node_dict)

        # Get active alerts
        alerts = conn.execute(
            """
            SELECT a.*, n.hostname as node_hostname
            FROM node_alerts a
            JOIN nodes n ON a.node_id = n.id
            WHERE a.status = 'active'
            ORDER BY
                CASE a.severity WHEN 'critical' THEN 0 WHEN 'warning' THEN 1 ELSE 2 END,
                a.created_at DESC
        """
        ).fetchall()

        # Calculate cluster-wide metrics
        if nodes:
            avg_cpu = sum(n["cpu_usage"] or 0 for n in nodes) / len(nodes)
            avg_memory = sum(n["memory_usage"] or 0 for n in nodes) / len(
                nodes
            )
            avg_disk = sum(n["disk_usage"] or 0 for n in nodes) / len(nodes)
        else:
            avg_cpu = avg_memory = avg_disk = 0

        return jsonify(
            {
                "nodes": node_health,
                "summary": summary,
                "cluster_metrics": {
                    "avg_cpu": round(avg_cpu, 1),
                    "avg_memory": round(avg_memory, 1),
                    "avg_disk": round(avg_disk, 1),
                },
                "active_alerts": [dict(a) for a in alerts],
                "thresholds": NODE_HEALTH_THRESHOLDS,
            }
        )


@app.route("/api/nodes/<node_id>/health", methods=["GET"])
@require_auth
def get_node_health(node_id):
    """Get detailed health status for a specific node."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        node = conn.execute(
            "SELECT * FROM nodes WHERE id = ?", (node_id,)
        ).fetchone()
        if not node:
            return jsonify({"error": "Node not found"}), 404

        node_dict = dict(node)
        health = check_node_health(node_dict)

        # Get alert history for this node
        alerts = conn.execute(
            """
            SELECT * FROM node_alerts
            WHERE node_id = ?
            ORDER BY created_at DESC
            LIMIT 50
        """,
            (node_id,),
        ).fetchall()

        return jsonify(
            {
                "node": node_dict,
                "health": health,
                "alerts": [dict(a) for a in alerts],
                "thresholds": NODE_HEALTH_THRESHOLDS,
            }
        )


@app.route("/api/nodes/alerts", methods=["GET"])
@require_auth
def get_node_alerts():
    """Get all node alerts.

    Query parameters:
    - status: Filter by status (active, acknowledged, resolved)
    - severity: Filter by severity (critical, warning)
    - node_id: Filter by node
    """
    status = request.args.get("status", "active")
    severity = request.args.get("severity")
    node_id = request.args.get("node_id")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        query = """
            SELECT a.*, n.hostname as node_hostname
            FROM node_alerts a
            JOIN nodes n ON a.node_id = n.id
            WHERE 1=1
        """
        params = []

        if status:
            query += " AND a.status = ?"
            params.append(status)
        if severity:
            query += " AND a.severity = ?"
            params.append(severity)
        if node_id:
            query += " AND a.node_id = ?"
            params.append(node_id)

        query += """
            ORDER BY
                CASE a.severity WHEN 'critical' THEN 0 WHEN 'warning' THEN 1 ELSE 2 END,
                a.created_at DESC
        """

        alerts = conn.execute(query, params).fetchall()
        return jsonify([dict(a) for a in alerts])


@app.route("/api/nodes/alerts/<int:alert_id>/acknowledge", methods=["POST"])
@require_auth
def acknowledge_alert(alert_id):
    """Acknowledge an alert."""
    with get_db_connection() as conn:
        conn.execute(
            """
            UPDATE node_alerts SET
                status = 'acknowledged',
                acknowledged_by = ?,
                acknowledged_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (session.get("username", "system"), alert_id),
        )

        return jsonify({"success": True})


@app.route("/api/nodes/alerts/<int:alert_id>/resolve", methods=["POST"])
@require_auth
def resolve_alert(alert_id):
    """Resolve an alert."""
    with get_db_connection() as conn:
        conn.execute(
            """
            UPDATE node_alerts SET
                status = 'resolved',
                resolved_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (alert_id,),
        )

        return jsonify({"success": True})


@app.route("/api/nodes/thresholds", methods=["GET"])
@require_auth
def get_health_thresholds():
    """Get current health monitoring thresholds."""
    return jsonify(NODE_HEALTH_THRESHOLDS)


@app.route("/api/nodes/thresholds", methods=["PUT"])
@require_auth
def update_health_thresholds():
    """Update health monitoring thresholds (runtime only, not persisted)."""
    data = request.get_json()

    for key in ["cpu_usage", "memory_usage", "disk_usage", "load_average"]:
        if key in data:
            NODE_HEALTH_THRESHOLDS[key] = data[key]

    if "heartbeat_timeout_seconds" in data:
        NODE_HEALTH_THRESHOLDS["heartbeat_timeout_seconds"] = data[
            "heartbeat_timeout_seconds"
        ]

    log_activity("update_thresholds", "config", None, "health thresholds")
    return jsonify({"success": True, "thresholds": NODE_HEALTH_THRESHOLDS})


# ============================================================================
# LOAD BALANCER / TASK DISTRIBUTION
# ============================================================================


def calculate_node_score(node: dict) -> float:
    """Calculate a load score for a node (lower is better for task assignment).

    Scoring factors:
    - CPU usage (40% weight)
    - Memory usage (30% weight)
    - Current task count (20% weight)
    - Health status penalty (10% weight)
    """
    cpu_weight = 0.4
    memory_weight = 0.3
    task_weight = 0.2
    health_weight = 0.1

    cpu_score = (node.get("cpu_usage") or 0) * cpu_weight
    memory_score = (node.get("memory_usage") or 0) * memory_weight

    # Task count normalized to 0-100 scale (assuming max 10 tasks per node)
    task_count = node.get("active_tasks", 0)
    task_score = min(task_count * 10, 100) * task_weight

    # Health penalty
    health_status = node.get("health_status", "unknown")
    health_penalties = {
        "healthy": 0,
        "warning": 25,
        "critical": 100,
        "unknown": 50,
    }
    health_score = health_penalties.get(health_status, 50) * health_weight

    return cpu_score + memory_score + task_score + health_score


def get_available_nodes(
    conn, role: str = None, min_health: str = "warning"
) -> list:
    """Get nodes available for task assignment, sorted by load score."""
    query = """
        SELECT n.*,
               COUNT(DISTINCT tq.id) as active_tasks
        FROM nodes n
        LEFT JOIN task_queue tq ON n.id = tq.assigned_node AND tq.status IN ('pending', 'running')
        WHERE n.status = 'online'
    """
    params = []

    if role:
        query += " AND n.role = ?"
        params.append(role)

    # Exclude unhealthy nodes based on min_health
    if min_health == "healthy":
        query += " AND n.health_status = 'healthy'"
    elif min_health == "warning":
        query += " AND n.health_status IN ('healthy', 'warning', 'unknown')"
    # 'any' includes all online nodes

    query += " GROUP BY n.id"

    nodes = conn.execute(query, params).fetchall()

    # Calculate scores and sort
    node_list = []
    for node in nodes:
        node_dict = dict(node)
        node_dict["load_score"] = calculate_node_score(node_dict)
        node_list.append(node_dict)

    # Sort by load score (lower is better)
    node_list.sort(key=lambda x: x["load_score"])
    return node_list


@app.route("/api/nodes/recommend", methods=["GET"])
@require_auth
def recommend_node():
    """Recommend the best node for a new task based on current load.

    Query parameters:
    - role: Filter by node role (worker, primary, etc.)
    - count: Number of recommendations to return (default 1)
    - min_health: Minimum health status (healthy, warning, any) - default warning
    """
    role = request.args.get("role")
    count = request.args.get("count", 1, type=int)
    min_health = request.args.get("min_health", "warning")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        nodes = get_available_nodes(conn, role, min_health)

        if not nodes:
            return (
                jsonify(
                    {"error": "No available nodes", "recommendations": []}
                ),
                404,
            )

        recommendations = nodes[:count]

        return jsonify(
            {"recommendations": recommendations, "total_available": len(nodes)}
        )


@app.route("/api/nodes/distribution", methods=["GET"])
@require_auth
def get_task_distribution():
    """Get current task distribution across nodes."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        distribution = conn.execute(
            """
            SELECT
                n.id as node_id,
                n.hostname,
                n.role,
                n.status,
                n.cpu_usage,
                n.memory_usage,
                n.health_status,
                COUNT(DISTINCT tq.id) as pending_tasks,
                COUNT(DISTINCT CASE WHEN tq.status = 'running' THEN tq.id END) as running_tasks,
                COUNT(DISTINCT w.id) as active_workers
            FROM nodes n
            LEFT JOIN task_queue tq ON n.id = tq.assigned_node AND tq.status IN ('pending', 'running')
            LEFT JOIN workers w ON n.id = w.node_id AND w.status = 'busy'
            GROUP BY n.id
            ORDER BY n.role, n.hostname
        """
        ).fetchall()

        # Calculate distribution metrics
        total_pending = sum(d["pending_tasks"] or 0 for d in distribution)
        total_running = sum(d["running_tasks"] or 0 for d in distribution)
        online_nodes = sum(1 for d in distribution if d["status"] == "online")

        # Check for imbalance
        if online_nodes > 0:
            avg_tasks = (total_pending + total_running) / online_nodes
            max_tasks = max(
                (d["pending_tasks"] or 0) + (d["running_tasks"] or 0)
                for d in distribution
            )
            imbalance_ratio = max_tasks / avg_tasks if avg_tasks > 0 else 0
        else:
            avg_tasks = 0
            imbalance_ratio = 0

        return jsonify(
            {
                "distribution": [dict(d) for d in distribution],
                "summary": {
                    "total_pending": total_pending,
                    "total_running": total_running,
                    "online_nodes": online_nodes,
                    "avg_tasks_per_node": round(avg_tasks, 2),
                    "imbalance_ratio": round(imbalance_ratio, 2),
                    "is_balanced": imbalance_ratio
                    < 2.0,  # Less than 2x difference is "balanced"
                },
            }
        )


@app.route("/api/nodes/rebalance", methods=["POST"])
@require_auth
def rebalance_tasks():
    """Rebalance pending tasks across nodes based on current load.

    Only moves 'pending' tasks, not running ones.
    """
    data = request.get_json() or {}
    dry_run = data.get("dry_run", False)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Get available nodes with their scores
        available = get_available_nodes(conn, min_health="warning")
        if not available:
            return (
                jsonify({"error": "No available nodes for rebalancing"}),
                400,
            )

        # Get unassigned or poorly-assigned pending tasks
        tasks = conn.execute(
            """
            SELECT id, task_type, priority, assigned_node
            FROM task_queue
            WHERE status = 'pending'
            ORDER BY priority DESC, created_at ASC
        """
        ).fetchall()

        if not tasks:
            return jsonify(
                {"message": "No pending tasks to rebalance", "moved": 0}
            )

        # Distribute tasks across nodes
        moves = []
        node_assignments = {n["id"]: 0 for n in available}

        for task in tasks:
            # Find node with lowest current load + assignments
            best_node = min(
                available,
                key=lambda n: n["load_score"] + node_assignments[n["id"]] * 10,
            )
            old_node = task["assigned_node"]
            new_node = best_node["id"]

            if old_node != new_node:
                moves.append(
                    {
                        "task_id": task["id"],
                        "from_node": old_node,
                        "to_node": new_node,
                        "to_hostname": best_node["hostname"],
                    }
                )

            node_assignments[new_node] += 1

        # Apply moves if not dry run
        if not dry_run and moves:
            for move in moves:
                conn.execute(
                    "UPDATE task_queue SET assigned_node = ? WHERE id = ?",
                    (move["to_node"], move["task_id"]),
                )
            log_activity(
                "rebalance_tasks", "cluster", None, f"Moved {len(moves)} tasks"
            )

        return jsonify(
            {
                "dry_run": dry_run,
                "moved": len(moves),
                "moves": moves,
                "message": f"{
                    'Would move' if dry_run else 'Moved'} {
                    len(moves)} tasks",
            }
        )


@app.route("/api/nodes/topology", methods=["GET"])
@require_auth
def get_cluster_topology():
    """Get cluster topology for visualization.

    Returns a graph structure with nodes and edges representing:
    - Cluster nodes grouped by role
    - Connections based on task assignments and worker registrations
    - Health and resource metrics for each node

    Query params:
        include_offline: Include offline nodes (default false)
        include_metrics: Include detailed metrics (default true)
    """
    include_offline = request.args.get("include_offline", "").lower() == "true"
    include_metrics = (
        request.args.get("include_metrics", "true").lower() != "false"
    )

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Get all nodes with their stats
        query = """
            SELECT n.*,
                   COUNT(DISTINCT ts.id) as session_count,
                   COUNT(DISTINCT w.id) as worker_count,
                   COUNT(DISTINCT CASE WHEN tq.status = 'pending' THEN tq.id END) as pending_tasks,
                   COUNT(DISTINCT CASE WHEN tq.status = 'running' THEN tq.id END) as running_tasks
            FROM nodes n
            LEFT JOIN tmux_sessions ts ON n.id = ts.node_id
            LEFT JOIN workers w ON n.id = w.node_id
            LEFT JOIN task_queue tq ON n.id = tq.assigned_node
        """
        if not include_offline:
            query += " WHERE n.status = 'online'"
        query += " GROUP BY n.id ORDER BY n.role, n.hostname"

        nodes_data = conn.execute(query).fetchall()

        # Build topology nodes
        topology_nodes = []
        nodes_by_role = {}
        node_ids = set()

        for node in nodes_data:
            node_dict = dict(node)
            node_id = node_dict["id"]
            node_ids.add(node_id)
            role = node_dict.get("role", "worker")

            # Parse services JSON
            try:
                services = json.loads(node_dict.get("services", "[]"))
            except (json.JSONDecodeError, TypeError):
                services = []

            topology_node = {
                "id": node_id,
                "label": node_dict.get("hostname", node_id),
                "role": role,
                "status": node_dict.get("status", "unknown"),
                "health_status": node_dict.get("health_status", "unknown"),
                "ip_address": node_dict.get("ip_address"),
                "services": services,
                "session_count": node_dict.get("session_count", 0),
                "worker_count": node_dict.get("worker_count", 0),
                "pending_tasks": node_dict.get("pending_tasks", 0),
                "running_tasks": node_dict.get("running_tasks", 0),
            }

            if include_metrics:
                topology_node["metrics"] = {
                    "cpu_usage": node_dict.get("cpu_usage", 0),
                    "memory_usage": node_dict.get("memory_usage", 0),
                    "disk_usage": node_dict.get("disk_usage", 0),
                    "load_average": node_dict.get("load_average", 0),
                    "process_count": node_dict.get("process_count", 0),
                    "uptime_seconds": node_dict.get("uptime_seconds", 0),
                }

            topology_nodes.append(topology_node)

            # Group by role
            if role not in nodes_by_role:
                nodes_by_role[role] = []
            nodes_by_role[role].append(topology_node)

        # Build edges based on relationships
        edges = []

        # Edge type 1: Coordinator to worker connections (if coordinator
        # exists)
        coordinators = nodes_by_role.get("coordinator", [])
        workers = nodes_by_role.get("worker", [])

        for coord in coordinators:
            for worker in workers:
                edges.append(
                    {
                        "source": coord["id"],
                        "target": worker["id"],
                        "type": "manages",
                        "label": "manages",
                    }
                )

        # Edge type 2: Task flow between nodes
        task_flows = conn.execute(
            """
            SELECT assigned_node as source,
                   (SELECT assigned_node FROM task_queue WHERE status = 'completed'
                    AND task_type = tq.task_type LIMIT 1) as target,
                   COUNT(*) as weight
            FROM task_queue tq
            WHERE assigned_node IS NOT NULL
            GROUP BY assigned_node
            HAVING target IS NOT NULL AND source != target
        """
        ).fetchall()

        for flow in task_flows:
            if flow["source"] in node_ids and flow["target"] in node_ids:
                edges.append(
                    {
                        "source": flow["source"],
                        "target": flow["target"],
                        "type": "task_flow",
                        "weight": flow["weight"],
                    }
                )

        # Cluster summary stats
        total_nodes = len(topology_nodes)
        online_nodes = sum(
            1 for n in topology_nodes if n["status"] == "online"
        )
        healthy_nodes = sum(
            1 for n in topology_nodes if n["health_status"] == "healthy"
        )

        # Get active alerts count
        alerts_count = conn.execute(
            "SELECT COUNT(*) as cnt FROM node_alerts WHERE status = 'active'"
        ).fetchone()["cnt"]

        summary = {
            "total_nodes": total_nodes,
            "online_nodes": online_nodes,
            "offline_nodes": total_nodes - online_nodes,
            "healthy_nodes": healthy_nodes,
            "warning_nodes": sum(
                1 for n in topology_nodes if n["health_status"] == "warning"
            ),
            "critical_nodes": sum(
                1 for n in topology_nodes if n["health_status"] == "critical"
            ),
            "active_alerts": alerts_count,
            "roles": {
                role: len(nodes) for role, nodes in nodes_by_role.items()
            },
            "total_sessions": sum(n["session_count"] for n in topology_nodes),
            "total_workers": sum(n["worker_count"] for n in topology_nodes),
            "total_pending_tasks": sum(
                n["pending_tasks"] for n in topology_nodes
            ),
            "total_running_tasks": sum(
                n["running_tasks"] for n in topology_nodes
            ),
        }

        # Calculate avg metrics if requested
        if include_metrics and topology_nodes:
            online_with_metrics = [
                n for n in topology_nodes if n["status"] == "online"
            ]
            if online_with_metrics:
                summary["avg_metrics"] = {
                    "cpu_usage": round(
                        sum(
                            n["metrics"]["cpu_usage"]
                            for n in online_with_metrics
                        )
                        / len(online_with_metrics),
                        2,
                    ),
                    "memory_usage": round(
                        sum(
                            n["metrics"]["memory_usage"]
                            for n in online_with_metrics
                        )
                        / len(online_with_metrics),
                        2,
                    ),
                    "disk_usage": round(
                        sum(
                            n["metrics"]["disk_usage"]
                            for n in online_with_metrics
                        )
                        / len(online_with_metrics),
                        2,
                    ),
                }

        return jsonify(
            {
                "nodes": topology_nodes,
                "edges": edges,
                "groups": nodes_by_role,
                "summary": summary,
            }
        )


# ============================================================================
# MULTI-REGION MANAGEMENT API
# ============================================================================


@app.route("/api/regions", methods=["GET"])
@require_auth
def get_regions():
    """Get all regions with their node counts and health status.

    Returns a list of all deployment regions with metrics.

    Query Parameters:
        status (str): Filter by status (active, inactive, maintenance, draining)
        include_metrics (str): Include detailed metrics (default: true)

    Returns:
        200: List of regions with node counts and health

    Example Request:
        GET /api/regions
        GET /api/regions?status=active

    cURL Example:
        curl -X GET "http://localhost:8080/api/regions" \\
             -H "Cookie: session=<session_cookie>"
    """
    status_filter = request.args.get("status")
    include_metrics = (
        request.args.get("include_metrics", "true").lower() != "false"
    )

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Check if regions table exists
        cursor = conn.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='regions'"
        )
        if not cursor.fetchone():
            return jsonify([])

        query = """
            SELECT r.*,
                   COUNT(DISTINCT n.id) as node_count,
                   COUNT(DISTINCT CASE WHEN n.status = 'online' THEN n.id END) as online_nodes,
                   COUNT(DISTINCT CASE WHEN n.health_status = 'healthy' THEN n.id END) as healthy_nodes,
                   AVG(CASE WHEN n.status = 'online' THEN n.cpu_usage END) as avg_cpu,
                   AVG(CASE WHEN n.status = 'online' THEN n.memory_usage END) as avg_memory,
                   AVG(CASE WHEN n.status = 'online' THEN n.disk_usage END) as avg_disk,
                   COUNT(DISTINCT w.id) as worker_count,
                   COUNT(DISTINCT ts.id) as session_count
            FROM regions r
            LEFT JOIN nodes n ON n.region_id = r.id AND n.deleted_at IS NULL
            LEFT JOIN workers w ON w.node_id = n.id
            LEFT JOIN tmux_sessions ts ON ts.node_id = n.id
            WHERE r.deleted_at IS NULL
        """
        params = []

        if status_filter:
            query += " AND r.status = ?"
            params.append(status_filter)

        query += (
            " GROUP BY r.id ORDER BY r.is_primary DESC, r.priority ASC, r.name"
        )

        regions = conn.execute(query, params).fetchall()

        result = []
        for region in regions:
            region_dict = dict(region)

            try:
                region_dict["replication_targets"] = json.loads(
                    region_dict.get("replication_targets", "[]")
                )
            except (json.JSONDecodeError, TypeError):
                region_dict["replication_targets"] = []

            try:
                region_dict["config"] = json.loads(
                    region_dict.get("config", "{}")
                )
            except (json.JSONDecodeError, TypeError):
                region_dict["config"] = {}

            if include_metrics:
                region_dict["avg_cpu"] = round(
                    region_dict.get("avg_cpu") or 0, 1
                )
                region_dict["avg_memory"] = round(
                    region_dict.get("avg_memory") or 0, 1
                )
                region_dict["avg_disk"] = round(
                    region_dict.get("avg_disk") or 0, 1
                )
            else:
                for key in ["avg_cpu", "avg_memory", "avg_disk"]:
                    region_dict.pop(key, None)

            if region_dict["node_count"] == 0:
                region_dict["computed_health"] = "unknown"
            elif region_dict["online_nodes"] == 0:
                region_dict["computed_health"] = "offline"
            elif region_dict["healthy_nodes"] == region_dict["online_nodes"]:
                region_dict["computed_health"] = "healthy"
            elif (
                region_dict["healthy_nodes"]
                >= region_dict["online_nodes"] * 0.5
            ):
                region_dict["computed_health"] = "warning"
            else:
                region_dict["computed_health"] = "critical"

            result.append(region_dict)

        return jsonify(result)


@app.route("/api/regions", methods=["POST"])
@require_auth
def create_region():
    """Create a new deployment region.

    Request Body:
        name (str): Region display name (required)
        code (str): Unique region code (required, e.g., "us-east-1")
        description (str): Region description
        provider (str): Cloud provider (aws, gcp, azure, on-premise)
        location (str): Physical location description
        latitude (float): Geographic latitude
        longitude (float): Geographic longitude
        timezone (str): Region timezone
        is_primary (bool): Whether this is the primary region
        priority (int): Failover priority (lower = higher priority)
        max_nodes (int): Maximum nodes allowed in region
        config (dict): Additional region configuration

    Returns:
        201: Region created successfully
        400: Invalid request data
        409: Region code already exists

    cURL Example:
        curl -X POST "http://localhost:8080/api/regions" \\
             -H "Content-Type: application/json" \\
             -H "Cookie: session=<session_cookie>" \\
             -d '{"name": "US East", "code": "us-east-1"}'
    """
    data = request.get_json()

    if not data:
        return jsonify({"error": "Request body required"}), 400

    name = data.get("name", "").strip()
    code = data.get("code", "").strip().lower()

    if not name:
        return jsonify({"error": "Region name is required"}), 400
    if not code:
        return jsonify({"error": "Region code is required"}), 400

    import re

    if not re.match(r"^[a-z0-9][a-z0-9\-_]*[a-z0-9]$|^[a-z0-9]$", code):
        return (
            jsonify(
                {
                    "error": "Region code must be lowercase alphanumeric with optional hyphens/underscores"
                }
            ),
            400,
        )

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS regions (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                code TEXT UNIQUE NOT NULL,
                description TEXT,
                provider TEXT DEFAULT 'on-premise',
                location TEXT,
                latitude REAL,
                longitude REAL,
                timezone TEXT,
                status TEXT DEFAULT 'active',
                is_primary INTEGER DEFAULT 0,
                priority INTEGER DEFAULT 100,
                max_nodes INTEGER DEFAULT 100,
                current_nodes INTEGER DEFAULT 0,
                replication_targets TEXT DEFAULT '[]',
                config TEXT DEFAULT '{}',
                health_status TEXT DEFAULT 'unknown',
                last_health_check TIMESTAMP,
                avg_latency_ms REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                created_by TEXT,
                deleted_at TIMESTAMP,
                deleted_by TEXT
            )
        """
        )

        existing = conn.execute(
            "SELECT id FROM regions WHERE code = ? AND deleted_at IS NULL",
            (code,),
        ).fetchone()
        if existing:
            return (
                jsonify(
                    {"error": f'Region with code "{code}" already exists'}
                ),
                409,
            )

        region_id = code

        if data.get("is_primary"):
            conn.execute(
                "UPDATE regions SET is_primary = 0 WHERE is_primary = 1"
            )

        config = data.get("config", {})
        if not isinstance(config, dict):
            config = {}

        replication_targets = data.get("replication_targets", [])
        if not isinstance(replication_targets, list):
            replication_targets = []

        username = session.get("username", "system")

        conn.execute(
            """
            INSERT INTO regions (
                id, name, code, description, provider, location,
                latitude, longitude, timezone, status, is_primary,
                priority, max_nodes, replication_targets, config, created_by
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                region_id,
                name,
                code,
                data.get("description", ""),
                data.get("provider", "on-premise"),
                data.get("location", ""),
                data.get("latitude"),
                data.get("longitude"),
                data.get("timezone", ""),
                data.get("status", "active"),
                1 if data.get("is_primary") else 0,
                data.get("priority", 100),
                data.get("max_nodes", 100),
                json.dumps(replication_targets),
                json.dumps(config),
                username,
            ),
        )
        conn.commit()

        details = json.dumps(
            {
                "message": f"Created region: {name} ({code})",
                "region_id": region_id,
                "region_code": code,
            }
        )
        log_activity("region_created", "region", region_id, details)

        region = conn.execute(
            "SELECT * FROM regions WHERE id = ?", (region_id,)
        ).fetchone()

        return jsonify(dict(region)), 201


@app.route("/api/regions/<region_id>", methods=["GET"])
@require_auth
def get_region(region_id):
    """Get a specific region with detailed information."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        region = conn.execute(
            """
            SELECT r.*,
                   COUNT(DISTINCT n.id) as node_count,
                   COUNT(DISTINCT CASE WHEN n.status = 'online' THEN n.id END) as online_nodes
            FROM regions r
            LEFT JOIN nodes n ON n.region_id = r.id AND n.deleted_at IS NULL
            WHERE r.id = ? AND r.deleted_at IS NULL
            GROUP BY r.id
        """,
            (region_id,),
        ).fetchone()

        if not region:
            return jsonify({"error": "Region not found"}), 404

        region_dict = dict(region)

        try:
            region_dict["replication_targets"] = json.loads(
                region_dict.get("replication_targets", "[]")
            )
        except (json.JSONDecodeError, TypeError):
            region_dict["replication_targets"] = []

        try:
            region_dict["config"] = json.loads(region_dict.get("config", "{}"))
        except (json.JSONDecodeError, TypeError):
            region_dict["config"] = {}

        nodes = conn.execute(
            """
            SELECT id, hostname, ip_address, role, status, health_status,
                   cpu_usage, memory_usage, disk_usage, last_heartbeat
            FROM nodes
            WHERE region_id = ? AND deleted_at IS NULL
            ORDER BY role, hostname
        """,
            (region_id,),
        ).fetchall()

        region_dict["nodes"] = [dict(n) for n in nodes]

        try:
            connections = conn.execute(
                """
                SELECT * FROM region_connections
                WHERE source_region = ? OR target_region = ?
            """,
                (region_id, region_id),
            ).fetchall()
            region_dict["connections"] = [dict(c) for c in connections]
        except Exception:
            region_dict["connections"] = []

        return jsonify(region_dict)


@app.route("/api/regions/<region_id>", methods=["PUT"])
@require_auth
def update_region(region_id):
    """Update a deployment region."""
    data = request.get_json()

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        region = conn.execute(
            "SELECT * FROM regions WHERE id = ? AND deleted_at IS NULL",
            (region_id,),
        ).fetchone()

        if not region:
            return jsonify({"error": "Region not found"}), 404

        updates = []
        params = []

        updateable_fields = [
            "name",
            "description",
            "provider",
            "location",
            "latitude",
            "longitude",
            "timezone",
            "status",
            "priority",
            "max_nodes",
        ]

        for field in updateable_fields:
            if field in data:
                updates.append(f"{field} = ?")
                params.append(data[field])

        if "is_primary" in data:
            if data["is_primary"]:
                conn.execute(
                    "UPDATE regions SET is_primary = 0 WHERE is_primary = 1"
                )
            updates.append("is_primary = ?")
            params.append(1 if data["is_primary"] else 0)

        if "config" in data:
            updates.append("config = ?")
            params.append(json.dumps(data["config"]))

        if "replication_targets" in data:
            updates.append("replication_targets = ?")
            params.append(json.dumps(data["replication_targets"]))

        if updates:
            updates.append("updated_at = CURRENT_TIMESTAMP")
            params.append(region_id)

            conn.execute(
                f"UPDATE regions SET {', '.join(updates)} WHERE id = ?", params
            )
            conn.commit()

            log_activity(
                "region_updated",
                f"Updated region: {region_id}",
                {"region_id": region_id, "changes": list(data.keys())},
            )

        updated = conn.execute(
            "SELECT * FROM regions WHERE id = ?", (region_id,)
        ).fetchone()

        return jsonify(dict(updated))


@app.route("/api/regions/<region_id>", methods=["DELETE"])
@require_auth
def delete_region(region_id):
    """Delete (soft-delete) a deployment region."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        region = conn.execute(
            "SELECT * FROM regions WHERE id = ? AND deleted_at IS NULL",
            (region_id,),
        ).fetchone()

        if not region:
            return jsonify({"error": "Region not found"}), 404

        if region["is_primary"]:
            node_count = conn.execute(
                "SELECT COUNT(*) as cnt FROM nodes WHERE region_id = ? AND status = 'online'",
                (region_id,),
            ).fetchone()["cnt"]

            if node_count > 0:
                return (
                    jsonify(
                        {
                            "error": "Cannot delete primary region with active nodes"
                        }
                    ),
                    400,
                )

        username = session.get("username", "system")

        conn.execute(
            "UPDATE nodes SET region_id = NULL WHERE region_id = ?",
            (region_id,),
        )

        conn.execute(
            """
            UPDATE regions
            SET deleted_at = CURRENT_TIMESTAMP, deleted_by = ?, status = 'inactive'
            WHERE id = ?
        """,
            (username, region_id),
        )

        conn.commit()

        log_activity(
            "region_deleted",
            f"Deleted region: {region_id}",
            {"region_id": region_id},
        )

        return jsonify({"status": "deleted", "region_id": region_id})


@app.route("/api/regions/<region_id>/nodes", methods=["GET"])
@require_auth
def get_region_nodes(region_id):
    """Get all nodes in a specific region."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        nodes = conn.execute(
            """
            SELECT n.*,
                   COUNT(DISTINCT ts.id) as session_count,
                   COUNT(DISTINCT w.id) as worker_count
            FROM nodes n
            LEFT JOIN tmux_sessions ts ON n.id = ts.node_id
            LEFT JOIN workers w ON n.id = w.node_id
            WHERE n.region_id = ? AND n.deleted_at IS NULL
            GROUP BY n.id
            ORDER BY n.role, n.hostname
        """,
            (region_id,),
        ).fetchall()

        return jsonify([dict(n) for n in nodes])


@app.route("/api/regions/<region_id>/assign-node", methods=["POST"])
@require_auth
def assign_node_to_region(region_id):
    """Assign a node to a region."""
    data = request.get_json()
    node_id = data.get("node_id")

    if not node_id:
        return jsonify({"error": "node_id is required"}), 400

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        region = conn.execute(
            "SELECT id, name, max_nodes FROM regions WHERE id = ? AND deleted_at IS NULL",
            (region_id,),
        ).fetchone()

        if not region:
            return jsonify({"error": "Region not found"}), 404

        node = conn.execute(
            "SELECT id, hostname FROM nodes WHERE id = ? AND deleted_at IS NULL",
            (node_id,),
        ).fetchone()

        if not node:
            return jsonify({"error": "Node not found"}), 404

        current_count = conn.execute(
            "SELECT COUNT(*) as cnt FROM nodes WHERE region_id = ?",
            (region_id,),
        ).fetchone()["cnt"]

        if current_count >= region["max_nodes"]:
            return (
                jsonify(
                    {
                        "error": f'Region has reached maximum node limit ({region["max_nodes"]})'
                    }
                ),
                400,
            )

        conn.execute(
            "UPDATE nodes SET region_id = ?, updated_at = CURRENT_TIMESTAMP WHERE id = ?",
            (region_id, node_id),
        )

        conn.execute(
            """
            UPDATE regions SET
                current_nodes = (SELECT COUNT(*) FROM nodes WHERE region_id = regions.id),
                updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (region_id,),
        )

        conn.commit()

        log_activity(
            "node_assigned_to_region",
            f"Assigned node {node_id} to region {region_id}",
            {"node_id": node_id, "region_id": region_id},
        )

        return jsonify(
            {
                "status": "assigned",
                "node_id": node_id,
                "region_id": region_id,
                "region_name": region["name"],
            }
        )


@app.route("/api/regions/health", methods=["GET"])
@require_auth
def get_regions_health():
    """Get health status of all regions with cross-region connectivity."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        cursor = conn.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='regions'"
        )
        if not cursor.fetchone():
            return jsonify(
                {
                    "regions": [],
                    "connections": [],
                    "summary": {
                        "total_regions": 0,
                        "healthy": 0,
                        "degraded": 0,
                        "offline": 0,
                    },
                }
            )

        regions = conn.execute(
            """
            SELECT r.id, r.name, r.code, r.status, r.is_primary, r.health_status,
                   r.avg_latency_ms, r.last_health_check,
                   COUNT(DISTINCT n.id) as node_count,
                   COUNT(DISTINCT CASE WHEN n.status = 'online' THEN n.id END) as online_nodes,
                   COUNT(DISTINCT CASE WHEN n.health_status = 'healthy' THEN n.id END) as healthy_nodes,
                   AVG(CASE WHEN n.status = 'online' THEN n.cpu_usage END) as avg_cpu,
                   AVG(CASE WHEN n.status = 'online' THEN n.memory_usage END) as avg_memory
            FROM regions r
            LEFT JOIN nodes n ON n.region_id = r.id AND n.deleted_at IS NULL
            WHERE r.deleted_at IS NULL
            GROUP BY r.id
            ORDER BY r.is_primary DESC, r.priority ASC
        """
        ).fetchall()

        region_health = []
        summary = {
            "total_regions": 0,
            "healthy": 0,
            "warning": 0,
            "critical": 0,
            "offline": 0,
        }

        for region in regions:
            r = dict(region)
            summary["total_regions"] += 1

            if r["status"] != "active":
                health = "offline"
            elif r["node_count"] == 0:
                health = "unknown"
            elif r["online_nodes"] == 0:
                health = "offline"
            elif r["healthy_nodes"] >= r["online_nodes"] * 0.8:
                health = "healthy"
            elif r["healthy_nodes"] >= r["online_nodes"] * 0.5:
                health = "warning"
            else:
                health = "critical"

            r["computed_health"] = health
            r["avg_cpu"] = round(r["avg_cpu"] or 0, 1)
            r["avg_memory"] = round(r["avg_memory"] or 0, 1)

            if health in summary:
                summary[health] += 1

            region_health.append(r)

        try:
            connections = conn.execute(
                "SELECT * FROM region_connections ORDER BY source_region, target_region"
            ).fetchall()
            connections_list = [dict(c) for c in connections]
        except Exception:
            connections_list = []

        return jsonify(
            {
                "regions": region_health,
                "connections": connections_list,
                "summary": summary,
            }
        )


@app.route("/api/regions/topology", methods=["GET"])
@require_auth
def get_regions_topology():
    """Get region topology for visualization."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        cursor = conn.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='regions'"
        )
        if not cursor.fetchone():
            return jsonify({"nodes": [], "edges": [], "summary": {}})

        regions = conn.execute(
            """
            SELECT r.*,
                   COUNT(DISTINCT n.id) as node_count,
                   COUNT(DISTINCT CASE WHEN n.status = 'online' THEN n.id END) as online_nodes
            FROM regions r
            LEFT JOIN nodes n ON n.region_id = r.id AND n.deleted_at IS NULL
            WHERE r.deleted_at IS NULL
            GROUP BY r.id
        """
        ).fetchall()

        nodes = []
        for region in regions:
            r = dict(region)
            nodes.append(
                {
                    "id": r["id"],
                    "label": r["name"],
                    "code": r["code"],
                    "status": r["status"],
                    "is_primary": bool(r["is_primary"]),
                    "node_count": r["node_count"],
                    "online_nodes": r["online_nodes"],
                    "latitude": r.get("latitude"),
                    "longitude": r.get("longitude"),
                    "location": r.get("location"),
                    "health_status": r.get("health_status", "unknown"),
                }
            )

        try:
            connections = conn.execute(
                "SELECT * FROM region_connections"
            ).fetchall()
            edges = [
                {
                    "source": c["source_region"],
                    "target": c["target_region"],
                    "latency_ms": c.get("latency_ms"),
                    "bandwidth_mbps": c.get("bandwidth_mbps"),
                    "status": c.get("status", "unknown"),
                }
                for c in connections
            ]
        except Exception:
            edges = []

        summary = {
            "total_regions": len(nodes),
            "active_regions": sum(1 for n in nodes if n["status"] == "active"),
            "total_nodes": sum(n["node_count"] for n in nodes),
            "online_nodes": sum(n["online_nodes"] for n in nodes),
            "primary_region": next(
                (n["code"] for n in nodes if n["is_primary"]), None
            ),
        }

        return jsonify({"nodes": nodes, "edges": edges, "summary": summary})


@app.route("/api/regions/failover", methods=["POST"])
@require_auth
def trigger_region_failover():
    """Trigger failover to a different primary region."""
    data = request.get_json()
    target_region = data.get("target_region")
    reason = data.get("reason", "Manual failover")

    if not target_region:
        return jsonify({"error": "target_region is required"}), 400

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        target = conn.execute(
            "SELECT * FROM regions WHERE id = ? AND deleted_at IS NULL",
            (target_region,),
        ).fetchone()

        if not target:
            return jsonify({"error": "Target region not found"}), 404

        if target["status"] != "active":
            return (
                jsonify(
                    {
                        "error": f'Cannot failover to region with status: {target["status"]}'
                    }
                ),
                400,
            )

        current_primary = conn.execute(
            "SELECT id, name FROM regions WHERE is_primary = 1 AND deleted_at IS NULL"
        ).fetchone()

        old_primary_id = current_primary["id"] if current_primary else None

        conn.execute("UPDATE regions SET is_primary = 0 WHERE is_primary = 1")
        conn.execute(
            "UPDATE regions SET is_primary = 1, updated_at = CURRENT_TIMESTAMP WHERE id = ?",
            (target_region,),
        )

        conn.commit()

        log_activity(
            "region_failover",
            f"Failover from {old_primary_id} to {target_region}",
            {
                "old_primary": old_primary_id,
                "new_primary": target_region,
                "reason": reason,
            },
        )

        try:
            socketio.emit(
                "region_failover",
                {
                    "old_primary": old_primary_id,
                    "new_primary": target_region,
                    "timestamp": datetime.now().isoformat(),
                },
            )
        except Exception:
            pass

        return jsonify(
            {
                "status": "failover_complete",
                "old_primary": old_primary_id,
                "new_primary": target_region,
                "reason": reason,
            }
        )


@app.route("/api/tasks/assign-optimal", methods=["POST"])
@require_auth
def assign_task_optimal():
    """Create a task and assign it to the optimal node.

    Uses load balancing to pick the best available node.
    """
    data = request.get_json()

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Get recommended node
        available = get_available_nodes(
            conn, role=data.get("preferred_role"), min_health="warning"
        )

        if not available:
            return (
                jsonify({"error": "No available nodes for task assignment"}),
                503,
            )

        best_node = available[0]

        # Create task with assignment
        cursor = conn.execute(
            """
            INSERT INTO task_queue (task_type, task_data, priority, assigned_node, max_retries)
            VALUES (?, ?, ?, ?, ?)
        """,
            (
                data.get("task_type", "shell"),
                json.dumps(data.get("task_data", {})),
                data.get("priority", 0),
                best_node["id"],
                data.get("max_retries", 3),
            ),
        )

        task_id = cursor.lastrowid
        log_activity(
            "create_task_optimal",
            "task",
            task_id,
            f"Assigned to {best_node['hostname']}",
        )

        broadcast_queue()

        # Send task assignment alert
        try:
            from services.task_alerts import notify_task_assignment

            notify_task_assignment(
                task_id=task_id,
                task_type=data.get("task_type", "shell"),
                task_title=data.get("title", f"Task #{task_id}"),
                assigned_to=best_node["id"],
                assigned_by=session.get("user"),
                priority="high" if data.get("priority", 0) > 5 else "normal",
                db_path=str(DB_PATH),
            )
        except Exception as e:
            logger.debug(f"Could not send task alert: {e}")

        return jsonify(
            {
                "task_id": task_id,
                "assigned_node": best_node["id"],
                "assigned_hostname": best_node["hostname"],
                "node_load_score": best_node["load_score"],
                "success": True,
            }
        )


# ============================================================================
# CLUSTER VISUALIZATION API
# ============================================================================


@app.route("/api/cluster/topology", methods=["GET"])
@require_auth
def get_cluster_topology_graph():
    """Get cluster topology data for visualization.

    Returns nodes with their connections, status, and metrics formatted
    for rendering as a network graph.
    """
    import math

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Get all nodes with their stats
        nodes = conn.execute(
            """
            SELECT n.*,
                   COUNT(DISTINCT ts.id) as session_count,
                   COUNT(DISTINCT w.id) as worker_count,
                   COUNT(DISTINCT tq.id) as task_count
            FROM nodes n
            LEFT JOIN tmux_sessions ts ON n.id = ts.node_id
            LEFT JOIN workers w ON n.id = w.node_id
            LEFT JOIN task_queue tq ON n.id = tq.assigned_node AND tq.status IN ('pending', 'running')
            GROUP BY n.id
        """
        ).fetchall()

        # Build topology structure
        topology = {"nodes": [], "edges": [], "clusters": {}}

        # Primary/controller node is the center
        primary_nodes = [n for n in nodes if n["role"] == "primary"]
        worker_nodes = [n for n in nodes if n["role"] != "primary"]

        # Add primary nodes
        for i, node in enumerate(primary_nodes):
            node_dict = dict(node)
            health = check_node_health(node_dict)
            topology["nodes"].append(
                {
                    "id": node["id"],
                    "label": node["hostname"],
                    "type": "primary",
                    "status": node["status"],
                    "health": health["status"],
                    "metrics": {
                        "cpu": node["cpu_usage"] or 0,
                        "memory": node["memory_usage"] or 0,
                        "disk": node["disk_usage"] or 0,
                        "sessions": node["session_count"],
                        "workers": node["worker_count"],
                        "tasks": node["task_count"],
                    },
                    "position": {"x": 0, "y": 0},  # Center
                }
            )

        # Add worker nodes in a circle around primary
        for i, node in enumerate(worker_nodes):
            node_dict = dict(node)
            health = check_node_health(node_dict)
            angle = (2 * math.pi * i) / max(len(worker_nodes), 1)
            radius = 200

            topology["nodes"].append(
                {
                    "id": node["id"],
                    "label": node["hostname"],
                    "type": node["role"],
                    "status": node["status"],
                    "health": health["status"],
                    "metrics": {
                        "cpu": node["cpu_usage"] or 0,
                        "memory": node["memory_usage"] or 0,
                        "disk": node["disk_usage"] or 0,
                        "sessions": node["session_count"],
                        "workers": node["worker_count"],
                        "tasks": node["task_count"],
                    },
                    "position": {
                        "x": int(radius * math.cos(angle)),
                        "y": int(radius * math.sin(angle)),
                    },
                }
            )

            # Create edge from worker to primary
            for primary in primary_nodes:
                topology["edges"].append(
                    {
                        "from": primary["id"],
                        "to": node["id"],
                        "type": "management",
                        "status": (
                            "active"
                            if node["status"] == "online"
                            else "inactive"
                        ),
                    }
                )

        # Group nodes by role
        for node in topology["nodes"]:
            role = node["type"]
            if role not in topology["clusters"]:
                topology["clusters"][role] = []
            topology["clusters"][role].append(node["id"])

        return jsonify(topology)


@app.route("/api/cluster/flow", methods=["GET"])
@require_auth
def get_task_flow():
    """Get task flow data showing tasks moving between nodes."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Get recent task activities
        flows = conn.execute(
            """
            SELECT
                tq.id as task_id,
                tq.task_type,
                tq.status,
                tq.assigned_node,
                tq.created_at,
                tq.started_at,
                tq.completed_at,
                n.hostname as node_hostname
            FROM task_queue tq
            LEFT JOIN nodes n ON tq.assigned_node = n.id
            WHERE tq.created_at >= datetime('now', '-1 hour')
            ORDER BY tq.created_at DESC
            LIMIT 50
        """
        ).fetchall()

        # Get task counts by status per node
        task_counts = conn.execute(
            """
            SELECT assigned_node, status, COUNT(*) as count
            FROM task_queue
            WHERE assigned_node IS NOT NULL
            GROUP BY assigned_node, status
        """
        ).fetchall()

        node_tasks = {}
        for tc in task_counts:
            node_id = tc["assigned_node"]
            if node_id not in node_tasks:
                node_tasks[node_id] = {
                    "pending": 0,
                    "running": 0,
                    "completed": 0,
                    "failed": 0,
                }
            node_tasks[node_id][tc["status"]] = tc["count"]

        return jsonify(
            {
                "recent_tasks": [dict(f) for f in flows],
                "node_task_counts": node_tasks,
                "timestamp": datetime.now().isoformat(),
            }
        )


@app.route("/api/cluster/stats", methods=["GET"])
@require_auth
def get_cluster_stats():
    """Get comprehensive cluster statistics."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Node counts by status
        node_stats = conn.execute(
            """
            SELECT status, role, COUNT(*) as count,
                   AVG(cpu_usage) as avg_cpu, AVG(memory_usage) as avg_memory
            FROM nodes GROUP BY status, role
        """
        ).fetchall()

        # Task stats
        task_stats = conn.execute(
            """
            SELECT status, COUNT(*) as count
            FROM task_queue
            WHERE created_at >= datetime('now', '-24 hours')
            GROUP BY status
        """
        ).fetchall()

        # Session and worker counts
        session_count = conn.execute(
            "SELECT COUNT(*) as c FROM tmux_sessions"
        ).fetchone()["c"]
        worker_count = conn.execute(
            "SELECT COUNT(*) as c FROM workers WHERE status = 'busy'"
        ).fetchone()["c"]

        total_nodes = sum(n["count"] for n in node_stats)
        online_nodes = sum(
            n["count"] for n in node_stats if n["status"] == "online"
        )

        return jsonify(
            {
                "nodes": {
                    "total": total_nodes,
                    "online": online_nodes,
                    "by_status": [dict(n) for n in node_stats],
                },
                "tasks": {"by_status": [dict(t) for t in task_stats]},
                "sessions": session_count,
                "active_workers": worker_count,
                "timestamp": datetime.now().isoformat(),
            }
        )


# ============================================================================
# SSH CONNECTION POOLING
# ============================================================================


class SSHConnectionPool:
    """Manage a pool of SSH connections to cluster nodes."""

    def __init__(
        self,
        max_connections_per_host: int = 3,
        connection_timeout: int = 30,
        idle_timeout: int = 300,
    ):
        self.max_per_host = max_connections_per_host
        self.connection_timeout = connection_timeout
        self.idle_timeout = idle_timeout
        self.pools = {}  # host -> list of (connection, last_used)
        self.stats = {
            "total_created": 0,
            "total_reused": 0,
            "total_closed": 0,
            "active_connections": 0,
        }
        self._lock = threading.Lock()

    def get_connection(self, node: dict) -> dict:
        """Get an SSH connection for a node (from pool or new).

        Returns dict with connection info for use with subprocess.
        """
        host = node.get("ip_address") or node.get("hostname")
        port = node.get("ssh_port", 22)
        user = node.get("ssh_user")
        key = f"{user}@{host}:{port}" if user else f"{host}:{port}"

        with self._lock:
            # Check for available connection in pool
            if key in self.pools:
                now = datetime.now()
                for i, (conn_info, last_used) in enumerate(self.pools[key]):
                    age = (now - last_used).total_seconds()
                    if age < self.idle_timeout:
                        # Reuse this connection
                        self.pools[key][i] = (conn_info, now)
                        self.stats["total_reused"] += 1
                        return conn_info

                # All connections expired, clean up
                self.pools[key] = []

            # Create new connection info
            conn_info = {
                "host": host,
                "port": port,
                "user": user,
                "key": key,
                "ssh_args": self._build_ssh_args(host, port, user),
                "created_at": datetime.now().isoformat(),
            }

            # Add to pool
            if key not in self.pools:
                self.pools[key] = []

            if len(self.pools[key]) < self.max_per_host:
                self.pools[key].append((conn_info, datetime.now()))
                self.stats["total_created"] += 1
                self.stats["active_connections"] += 1

            return conn_info

    def _build_ssh_args(self, host: str, port: int, user: str = None) -> list:
        """Build SSH command arguments for subprocess."""
        args = [
            "ssh",
            "-o",
            "StrictHostKeyChecking=no",
            "-o",
            "UserKnownHostsFile=/dev/null",
            "-o",
            "ConnectTimeout=10",
            "-o",
            "ControlMaster=auto",
            "-o",
            f"ControlPath=/tmp/ssh-{host}-{port}-%r@%h:%p",
            "-o",
            "ControlPersist=300",
            "-p",
            str(port),
        ]
        if user:
            args.extend(["-l", user])
        args.append(host)
        return args

    def release_connection(self, key: str):
        """Release a connection back to the pool."""
        with self._lock:
            if key in self.pools:
                # Update last_used for all connections with this key
                now = datetime.now()
                self.pools[key] = [(c, now) for c, _ in self.pools[key]]

    def close_connection(self, key: str):
        """Close and remove a connection from the pool."""
        with self._lock:
            if key in self.pools:
                self.stats["total_closed"] += len(self.pools[key])
                self.stats["active_connections"] -= len(self.pools[key])
                del self.pools[key]

    def cleanup_idle(self):
        """Remove idle connections that have exceeded timeout."""
        now = datetime.now()
        with self._lock:
            for key in list(self.pools.keys()):
                active = []
                for conn_info, last_used in self.pools[key]:
                    if (now - last_used).total_seconds() < self.idle_timeout:
                        active.append((conn_info, last_used))
                    else:
                        self.stats["total_closed"] += 1
                        self.stats["active_connections"] -= 1
                if active:
                    self.pools[key] = active
                else:
                    del self.pools[key]

    def get_stats(self) -> dict:
        """Get connection pool statistics."""
        with self._lock:
            return {
                **self.stats,
                "pools": {k: len(v) for k, v in self.pools.items()},
                "total_pools": len(self.pools),
            }

    def execute_remote(
        self, node: dict, command: str, timeout: int = 30
    ) -> dict:
        """Execute a command on a remote node using pooled connection."""
        import subprocess

        conn = self.get_connection(node)

        try:
            result = subprocess.run(
                conn["ssh_args"] + [command],
                capture_output=True,
                text=True,
                timeout=timeout,
            )

            self.release_connection(conn["key"])

            return {
                "success": result.returncode == 0,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "return_code": result.returncode,
                "node": node.get("hostname"),
                "connection": conn["key"],
            }

        except subprocess.TimeoutExpired:
            return {
                "success": False,
                "error": "Command timed out",
                "node": node.get("hostname"),
                "connection": conn["key"],
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "node": node.get("hostname"),
                "connection": conn["key"],
            }


# Global SSH connection pool
ssh_pool = SSHConnectionPool()


@app.route("/api/ssh/pool/stats", methods=["GET"])
@require_auth
def get_ssh_pool_stats():
    """Get SSH connection pool statistics."""
    return jsonify(ssh_pool.get_stats())


@app.route("/api/ssh/pool/cleanup", methods=["POST"])
@require_auth
def cleanup_ssh_pool():
    """Cleanup idle SSH connections."""
    ssh_pool.cleanup_idle()
    return jsonify({"success": True, "stats": ssh_pool.get_stats()})


@app.route("/api/ssh/execute", methods=["POST"])
@require_auth
def execute_ssh_command():
    """Execute a command on a remote node using the connection pool.

    Request body:
    - node_id: ID of the target node
    - command: Command to execute
    - timeout: Timeout in seconds (default 30)
    """
    data = request.get_json()
    node_id = data.get("node_id")
    command = data.get("command")
    timeout = data.get("timeout", 30)

    if not node_id or not command:
        return jsonify({"error": "node_id and command required"}), 400

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        node = conn.execute(
            "SELECT * FROM nodes WHERE id = ?", (node_id,)
        ).fetchone()

        if not node:
            return jsonify({"error": "Node not found"}), 404

        if node["status"] != "online":
            return jsonify({"error": "Node is offline"}), 503

        result = ssh_pool.execute_remote(dict(node), command, timeout)
        log_activity(
            "ssh_execute", "node", None, f"{node['hostname']}: {command[:50]}"
        )

        return jsonify(result)


@app.route("/api/ssh/broadcast", methods=["POST"])
@require_auth
def broadcast_ssh_command():
    """Execute a command on all online nodes.

    Request body:
    - command: Command to execute
    - role: Optional filter by node role
    - timeout: Timeout per node in seconds (default 30)
    """
    data = request.get_json()
    command = data.get("command")
    role = data.get("role")
    timeout = data.get("timeout", 30)

    if not command:
        return jsonify({"error": "command required"}), 400

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        query = "SELECT * FROM nodes WHERE status = 'online'"
        params = []
        if role:
            query += " AND role = ?"
            params.append(role)

        nodes = conn.execute(query, params).fetchall()

        results = []
        for node in nodes:
            result = ssh_pool.execute_remote(dict(node), command, timeout)
            results.append(result)

        log_activity(
            "ssh_broadcast", "cluster", None, f"Command to {len(nodes)} nodes"
        )

        return jsonify(
            {
                "results": results,
                "total_nodes": len(nodes),
                "successful": sum(1 for r in results if r.get("success")),
                "failed": sum(1 for r in results if not r.get("success")),
            }
        )


# ============================================================================
# TMUX SESSION MANAGEMENT API
# ============================================================================


@app.route("/api/tmux/sessions", methods=["GET"])
@require_auth
def get_tmux_sessions():
    """Get all tmux sessions from all nodes.

    Returns tmux sessions with associated entity tracking (milestones, autopilot runs).
    Deduplicates by name, preferring local node when multiple nodes report same session.

    Query Parameters:
        node_id (int): Filter by specific node
        dedupe (str): Deduplicate by name, 'true' or 'false' (default: true)
        page (int): Page number (default: 1)
        per_page (int): Items per page, max 100 (default: 50)
        paginate (str): Set to 'true' for pagination response

    Returns:
        200: List of sessions or paginated result

    Example Request:
        GET /api/tmux/sessions
        GET /api/tmux/sessions?node_id=1
        GET /api/tmux/sessions?paginate=true&page=1

    Example Response:
        [
            {
                "id": 1,
                "session_name": "dev-main",
                "node_id": "local",
                "node_hostname": "localhost",
                "window_count": 3,
                "created_at": "2024-01-15T10:00:00",
                "last_activity": "2024-01-15T14:30:00",
                "milestone_id": 5,
                "milestone_name": "Sprint 3",
                "milestone_status": "active",
                "autopilot_run_id": null,
                "project_name": "My App"
            }
        ]

    cURL Example:
        curl -X GET "http://localhost:8080/api/tmux/sessions" \\
             -H "Cookie: session=<session_cookie>"
    """
    node_id = request.args.get("node_id")
    dedupe = request.args.get("dedupe", "true").lower() == "true"

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        if node_id:
            # Single node - no deduplication needed
            query = """
                SELECT ts.*, n.hostname as node_hostname,
                       m.name as milestone_name, m.status as milestone_status,
                       r.run_number, r.status as run_status,
                       p.name as project_name
                FROM tmux_sessions ts
                LEFT JOIN nodes n ON ts.node_id = n.id
                LEFT JOIN autopilot_milestones m ON ts.milestone_id = m.id
                LEFT JOIN autopilot_runs r ON ts.autopilot_run_id = r.id
                LEFT JOIN projects p ON m.project_id = p.id
                WHERE ts.node_id = ?
                ORDER BY ts.session_name
            """
            sessions = conn.execute(query, (node_id,)).fetchall()
        elif dedupe:
            # Deduplicate by session_name, prefer 'local' node
            query = """
                SELECT ts.*, n.hostname as node_hostname,
                       m.name as milestone_name, m.status as milestone_status,
                       r.run_number, r.status as run_status,
                       p.name as project_name
                FROM tmux_sessions ts
                LEFT JOIN nodes n ON ts.node_id = n.id
                LEFT JOIN autopilot_milestones m ON ts.milestone_id = m.id
                LEFT JOIN autopilot_runs r ON ts.autopilot_run_id = r.id
                LEFT JOIN projects p ON m.project_id = p.id
                WHERE ts.id IN (
                    SELECT id FROM (
                        SELECT id, session_name,
                               ROW_NUMBER() OVER (
                                   PARTITION BY session_name
                                   ORDER BY CASE WHEN node_id = 'local' THEN 0 ELSE 1 END,
                                            last_activity DESC
                               ) as rn
                        FROM tmux_sessions
                    ) WHERE rn = 1
                )
                ORDER BY ts.session_name
            """
            sessions = conn.execute(query).fetchall()
        else:
            # No deduplication - show all from all nodes
            query = """
                SELECT ts.*, n.hostname as node_hostname,
                       m.name as milestone_name, m.status as milestone_status,
                       r.run_number, r.status as run_status,
                       p.name as project_name
                FROM tmux_sessions ts
                LEFT JOIN nodes n ON ts.node_id = n.id
                LEFT JOIN autopilot_milestones m ON ts.milestone_id = m.id
                LEFT JOIN autopilot_runs r ON ts.autopilot_run_id = r.id
                LEFT JOIN projects p ON m.project_id = p.id
                ORDER BY n.hostname, ts.session_name
            """
            sessions = conn.execute(query).fetchall()

        session_list = [dict(s) for s in sessions]

        # Support pagination if requested
        if request.args.get("paginate", "").lower() == "true":
            page, per_page = get_pagination_params()
            result = paginate_query(session_list, page, per_page)
            return jsonify(result)

        return jsonify(session_list)


@app.route("/api/sessions/active", methods=["GET"])
@require_auth
def get_active_sessions():
    """Get list of active tmux sessions directly from tmux server.

    Returns real-time session list from the local tmux server,
    including session name, windows, attached status, and activity time.

    Query params:
        format: Response format - 'simple' (names only) or 'detailed' (default)
    """
    try:
        # Get sessions directly from tmux
        result = subprocess.run(
            [
                "tmux",
                "list-sessions",
                "-F",
                "#{session_name}|#{session_windows}|#{session_attached}|#{session_activity}|#{session_created}",
            ],
            capture_output=True,
            text=True,
            timeout=5,
        )

        if result.returncode != 0:
            # No tmux server running or no sessions
            if (
                "no server running" in result.stderr
                or "no sessions" in result.stderr.lower()
            ):
                return jsonify(
                    {
                        "sessions": [],
                        "count": 0,
                        "message": "No active tmux sessions",
                    }
                )
            return api_error(f"tmux error: {result.stderr}", 500, "tmux_error")

        sessions = []
        format_type = request.args.get("format", "detailed")

        for line in result.stdout.strip().split("\n"):
            if not line:
                continue

            parts = line.split("|")
            if len(parts) >= 3:
                session_name = parts[0]

                if format_type == "simple":
                    sessions.append(session_name)
                else:
                    # Parse activity timestamp (Unix timestamp)
                    activity_ts = (
                        int(parts[3])
                        if len(parts) > 3 and parts[3].isdigit()
                        else None
                    )
                    created_ts = (
                        int(parts[4])
                        if len(parts) > 4 and parts[4].isdigit()
                        else None
                    )

                    sessions.append(
                        {
                            "name": session_name,
                            "windows": (
                                int(parts[1]) if parts[1].isdigit() else 0
                            ),
                            "attached": parts[2] == "1",
                            "activity": (
                                datetime.fromtimestamp(activity_ts).isoformat()
                                if activity_ts
                                else None
                            ),
                            "created": (
                                datetime.fromtimestamp(created_ts).isoformat()
                                if created_ts
                                else None
                            ),
                        }
                    )

        return jsonify(
            {
                "sessions": sessions,
                "count": len(sessions),
                "format": format_type,
            }
        )

    except subprocess.TimeoutExpired:
        return api_error("tmux command timed out", 500, "timeout_error")
    except FileNotFoundError:
        return api_error("tmux not installed", 500, "not_found_error")
    except Exception as e:
        logger.error(f"Error getting active sessions: {e}")
        return api_error(str(e), 500, "session_error")


@app.route("/api/sessions/live", methods=["GET"])
def get_live_sessions():
    """Get live activity from all tmux sessions.

    Reads from SessionStateManager JSON files (real-time, non-intrusive) rather than
    parsing tmux output. This allows sessions to report their own status without
    affecting performance or requiring keyword matching.

    Returns list of sessions with accurate status from state files.

    Returns:
        {
            "sessions": [
                {
                    "session_name": "codex",
                    "status": "working",  # From SessionStateManager
                    "is_active": true,    # Computed from status
                    "current_work": "Processing task #42",
                    "last_activity": "2024-02-04T10:30:00",
                    "windows": 1,
                    "attached": false
                }
            ],
            "count": 6,
            "timestamp": "2024-02-04T10:30:05"
        }
    """
    try:
        from workers.session_state_manager import SessionStateManager

        # Get sessions list from tmux (for metadata like windows, attached)
        sessions_result = subprocess.run(
            [
                "tmux",
                "list-sessions",
                "-F",
                "#{session_name}|#{session_windows}|#{session_attached}|#{session_activity}",
            ],
            capture_output=True,
            text=True,
            timeout=5,
        )

        if sessions_result.returncode != 0:
            return jsonify(
                {
                    "sessions": [],
                    "count": 0,
                    "message": "No active tmux sessions",
                    "timestamp": datetime.now().isoformat(),
                }
            )

        # Load all session states from JSON files (real-time, non-intrusive)
        all_session_states = SessionStateManager.get_all_sessions()

        sessions = []
        for line in sessions_result.stdout.strip().split("\n"):
            if not line:
                continue

            parts = line.split("|")
            if len(parts) < 3:
                continue

            session_name = parts[0]

            # Skip coordination/monitoring sessions (not worker sessions)
            EXCLUDED_SESSIONS = {
                "architect",
                "claude_orchestrator",
                "arch_dev",
                "autoconfirm",
            }
            if session_name in EXCLUDED_SESSIONS:
                continue

            windows = int(parts[1]) if parts[1].isdigit() else 0
            attached = parts[2] == "1"
            activity_ts = (
                int(parts[3])
                if len(parts) > 3 and parts[3].isdigit()
                else None
            )
            last_activity = (
                datetime.fromtimestamp(activity_ts).isoformat()
                if activity_ts
                else None
            )

            # Get session state from SessionStateManager (preferred source)
            session_state = all_session_states.get(session_name, {})

            # Use state from SessionStateManager if available, otherwise default values
            status = session_state.get("status", "idle")
            current_task = session_state.get("current_task", None)
            last_state_activity = session_state.get("last_activity", None)

            # Compute is_active from status
            is_active = status == "working" and current_task is not None

            # Format activity timestamp if available
            if last_state_activity:
                last_state_activity = datetime.fromtimestamp(last_state_activity).isoformat()

            sessions.append(
                {
                    "session_name": session_name,
                    "status": status,  # idle, working, waiting, error, stopped
                    "is_active": is_active,  # True if working with a task
                    "current_work": current_task if is_active else status.title(),
                    "last_activity": last_state_activity or last_activity,
                    "windows": windows,
                    "attached": attached,
                    "prompts_handled": session_state.get("prompts_handled", 0),
                    "errors": session_state.get("errors", 0),
                }
            )

        return jsonify(
            {
                "success": True,
                "sessions": sessions,
                "count": len(sessions),
                "timestamp": datetime.now().isoformat(),
            }
        )

    except subprocess.TimeoutExpired:
        return (
            jsonify(
                {
                    "error": "tmux command timed out",
                    "sessions": [],
                    "count": 0,
                    "timestamp": datetime.now().isoformat(),
                }
            ),
            500,
        )
    except FileNotFoundError:
        return (
            jsonify(
                {
                    "error": "tmux not installed",
                    "sessions": [],
                    "count": 0,
                    "timestamp": datetime.now().isoformat(),
                }
            ),
            500,
        )
    except Exception as e:
        logger.error(f"Error getting live sessions: {e}")
        return (
            jsonify(
                {
                    "error": str(e),
                    "sessions": [],
                    "count": 0,
                    "timestamp": datetime.now().isoformat(),
                }
            ),
            500,
        )


@app.route("/api/tmux/sessions/refresh", methods=["POST"])
@require_auth
def refresh_tmux_sessions():
    """Refresh tmux sessions from local machine."""
    try:
        result = subprocess.run(
            [
                "tmux",
                "list-sessions",
                "-F",
                "#{session_name}:#{session_windows}:#{session_attached}",
            ],
            capture_output=True,
            text=True,
            timeout=5,
        )

        sessions = []
        if result.returncode == 0:
            for line in result.stdout.strip().split("\n"):
                if line:
                    parts = line.split(":")
                    if len(parts) >= 3:
                        session_name = parts[0]
                        # Get working directory for this session
                        working_dir = None
                        try:
                            cwd_result = subprocess.run(
                                [
                                    "tmux",
                                    "display-message",
                                    "-t",
                                    session_name,
                                    "-p",
                                    "#{pane_current_path}",
                                ],
                                capture_output=True,
                                text=True,
                                timeout=2,
                            )
                            if (
                                cwd_result.returncode == 0
                                and cwd_result.stdout.strip()
                            ):
                                working_dir = cwd_result.stdout.strip()
                        except Exception:
                            pass
                        sessions.append(
                            {
                                "name": session_name,
                                "windows": int(parts[1]),
                                "attached": parts[2] == "1",
                                "working_directory": working_dir,
                            }
                        )

        # Update database
        node_id = "local"
        with get_db_connection() as conn:
            # Ensure local node exists
            conn.execute(
                """
                INSERT OR IGNORE INTO nodes (id, hostname, ip_address, role, status)
                VALUES (?, ?, ?, ?, ?)
            """,
                (node_id, "localhost", "127.0.0.1", "primary", "online"),
            )

            # Get existing sessions with their associations
            existing = {
                row[0]: row
                for row in conn.execute(
                    """
                SELECT session_name, environment, milestone_id, is_worker,
                       autopilot_run_id, assigned_task_type, assigned_task_id, purpose
                FROM tmux_sessions WHERE node_id = ?
            """,
                    (node_id,),
                ).fetchall()
            }

            # Delete sessions that no longer exist
            current_names = {s["name"] for s in sessions}
            if current_names:
                conn.execute(
                    """
                    DELETE FROM tmux_sessions
                    WHERE node_id = ? AND session_name NOT IN ({})
                """.format(
                        ",".join("?" * len(current_names))
                    ),
                    (node_id, *current_names),
                )
            else:
                # No sessions exist - delete all for this node
                conn.execute(
                    "DELETE FROM tmux_sessions WHERE node_id = ?", (node_id,)
                )

            # Upsert sessions while preserving associations
            for s in sessions:
                old = existing.get(s["name"])
                working_dir = s.get("working_directory")
                if old:
                    # Update existing, preserve associations
                    conn.execute(
                        """
                        UPDATE tmux_sessions SET
                            window_count = ?,
                            attached = ?,
                            working_directory = COALESCE(?, working_directory),
                            last_activity = CURRENT_TIMESTAMP
                        WHERE node_id = ? AND session_name = ?
                    """,
                        (
                            s["windows"],
                            1 if s["attached"] else 0,
                            working_dir,
                            node_id,
                            s["name"],
                        ),
                    )
                else:
                    # Insert new session with inferred environment
                    env = None
                    is_worker = 0
                    if "dev" in s["name"].lower():
                        env = "dev"
                    elif "qa" in s["name"].lower():
                        env = "qa"
                    elif "prod" in s["name"].lower():
                        env = "prod"
                    elif "env3" in s["name"].lower():
                        env = "env3"
                    if s["name"] in (
                        "autoconfirm",
                        "deploy_worker",
                        "task_worker",
                        "nodes_manager",
                        "audit_manager",
                        "command_runner",
                    ):
                        is_worker = 1

                    conn.execute(
                        """
                        INSERT INTO tmux_sessions (node_id, session_name, window_count, attached, environment, is_worker, working_directory)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """,
                        (
                            node_id,
                            s["name"],
                            s["windows"],
                            1 if s["attached"] else 0,
                            env,
                            is_worker,
                            working_dir,
                        ),
                    )

        # Broadcast tmux update via WebSocket
        broadcast_tmux()
        broadcast_stats()
        return jsonify({"sessions": sessions, "success": True})

    except subprocess.TimeoutExpired:
        return jsonify({"error": "tmux command timed out"}), 500
    except FileNotFoundError:
        return jsonify({"error": "tmux not found"}), 500
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/tmux/sessions/<session_name>/health", methods=["GET"])
@require_auth
def get_session_health(session_name):
    """Check health of a tmux session - detect loops, stuck state, errors."""
    try:
        result = subprocess.run(
            ["tmux", "capture-pane", "-t", session_name, "-p"],
            capture_output=True,
            text=True,
            timeout=5,
        )
        if result.returncode != 0:
            return (
                jsonify({"status": "error", "message": "Session not found"}),
                404,
            )

        output = result.stdout
        output_tail = output[-2000:] if len(output) > 2000 else output

        # Detect issues
        issues = []
        if "error" in output.lower() and "rate limit" in output.lower():
            issues.append(
                {
                    "type": "rate_limited",
                    "severity": "high",
                    "message": "Rate limit detected",
                }
            )
        if "context" in output.lower() and "exceeded" in output.lower():
            issues.append(
                {
                    "type": "context_exceeded",
                    "severity": "high",
                    "message": "Context limit exceeded",
                }
            )
        if "permission denied" in output.lower():
            issues.append(
                {
                    "type": "permission_error",
                    "severity": "medium",
                    "message": "Permission denied",
                }
            )
        if output.count("thinking") > 5:
            issues.append(
                {
                    "type": "possible_loop",
                    "severity": "medium",
                    "message": "Multiple thinking states detected",
                }
            )
        if "SIGTERM" in output or "killed" in output.lower():
            issues.append(
                {
                    "type": "process_killed",
                    "severity": "high",
                    "message": "Process was killed",
                }
            )

        # Check for common Claude Code patterns and activity
        is_active = (
            ">" in output_tail[-200:]
            or "thinking" in output_tail[-500:].lower()
            or "working" in output_tail[-500:].lower()
            or "analyzing" in output_tail[-500:].lower()
            or "processing" in output_tail[-500:].lower()
            or "running" in output_tail[-500:].lower()
            or "executing" in output_tail[-500:].lower()
            or "building" in output_tail[-500:].lower()
            or "testing" in output_tail[-500:].lower()
            or "writing" in output_tail[-500:].lower()
            or "reading" in output_tail[-500:].lower()
            or "searching" in output_tail[-500:].lower()
            or len(output_tail.strip()) > 50  # Has recent output
        )
        is_waiting = "?" in output_tail[-100:]

        # Analyze for Claude interactions (confirmations, questions, errors)
        interactions = []
        run_id = None
        project_id = None
        try:
            import re

            with get_db_connection() as conn:
                conn.row_factory = sqlite3.Row

                # Get session info
                session_info = conn.execute(
                    """
                    SELECT t.autopilot_run_id, r.project_id
                    FROM tmux_sessions t
                    LEFT JOIN autopilot_runs r ON t.autopilot_run_id = r.id
                    WHERE t.session_name = ?
                """,
                    (session_name,),
                ).fetchone()
                if session_info:
                    run_id = session_info[0]
                    project_id = session_info[1]

                # Get enabled patterns
                patterns = conn.execute(
                    """
                    SELECT * FROM claude_patterns WHERE enabled = 1 ORDER BY priority DESC
                """
                ).fetchall()

                # Analyze output lines
                lines = output.split("\n")
                for line in lines[-50:]:
                    line = line.strip()
                    if not line or len(line) < 10:
                        continue

                    for pattern in patterns:
                        try:
                            if re.search(
                                pattern["pattern_regex"], line, re.IGNORECASE
                            ):
                                interaction = {
                                    "type": pattern["pattern_type"],
                                    "pattern": pattern["pattern_regex"],
                                    "content": line[:500],
                                    "auto_response": pattern["auto_response"],
                                    "action": pattern["action"],
                                }
                                interactions.append(interaction)

                                # Log to database
                                conn.execute(
                                    """
                                    INSERT INTO claude_interactions
                                    (session_name, interaction_type, pattern, content, context,
                                     run_id, project_id, handled, handler_action, requires_review)
                                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                                """,
                                    (
                                        session_name,
                                        pattern["pattern_type"],
                                        pattern["pattern_regex"],
                                        line[:500],
                                        output_tail[-1000:],
                                        run_id,
                                        project_id,
                                        1 if pattern["auto_response"] else 0,
                                        pattern["action"],
                                        0 if pattern["auto_response"] else 1,
                                    ),
                                )
                                break
                        except (re.error, sqlite3.Error, KeyError):
                            continue
        except Exception as e:
            pass  # Don't fail health check if interaction analysis fails

        return jsonify(
            {
                "status": "issues" if issues else "healthy",
                "session_name": session_name,
                "is_active": is_active,
                "is_waiting_input": is_waiting,
                "issues": issues,
                "output_preview": output_tail[-500:],
                "interactions": interactions,
            }
        )

    except subprocess.TimeoutExpired:
        return (
            jsonify({"status": "error", "message": "Health check timed out"}),
            500,
        )
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


@app.route("/api/autopilot/health", methods=["GET"])
@require_auth
def get_autopilot_health():
    """Get overall autopilot system health including all active sessions."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            # Get active runs
            active_runs = conn.execute(
                """
                SELECT r.id, r.tmux_session, r.status, r.started_at,
                       m.name as milestone_name, p.name as project_name,
                       (julianday('now') - julianday(r.started_at)) * 24 * 60 as minutes_active
                FROM autopilot_runs r
                JOIN autopilot_milestones m ON r.milestone_id = m.id
                JOIN projects p ON r.project_id = p.id
                WHERE r.status IN ('planning', 'executing', 'testing', 'fixing')
            """
            ).fetchall()

            # Get pending tasks
            pending_tasks = conn.execute(
                """
                SELECT COUNT(*) as count FROM task_queue WHERE status = 'pending'
            """
            ).fetchone()["count"]

            # Get recent errors
            recent_errors = conn.execute(
                """
                SELECT COUNT(*) as count FROM errors
                WHERE status IN ('open', 'queued') AND first_seen > datetime('now', '-1 hour')
            """
            ).fetchone()["count"]

            # Get alerts
            open_alerts = conn.execute(
                """
                SELECT COUNT(*) as count FROM autopilot_alerts WHERE status = 'open'
            """
            ).fetchone()["count"]

            # Check each active session
            session_health = []
            for run in active_runs:
                if run["tmux_session"]:
                    try:
                        result = subprocess.run(
                            [
                                "tmux",
                                "capture-pane",
                                "-t",
                                run["tmux_session"],
                                "-p",
                            ],
                            capture_output=True,
                            text=True,
                            timeout=3,
                        )
                        output = (
                            result.stdout[-500:]
                            if result.returncode == 0
                            else ""
                        )
                        is_responsive = result.returncode == 0
                        has_errors = "error" in output.lower() and (
                            "rate" in output.lower()
                            or "context" in output.lower()
                        )
                    except (subprocess.SubprocessError, OSError):
                        is_responsive = False
                        has_errors = False

                    session_health.append(
                        {
                            "run_id": run["id"],
                            "session": run["tmux_session"],
                            "project": run["project_name"],
                            "status": run["status"],
                            "minutes_active": round(
                                run["minutes_active"] or 0, 1
                            ),
                            "is_responsive": is_responsive,
                            "has_errors": has_errors,
                        }
                    )

            overall_status = "healthy"
            if open_alerts > 0 or any(
                not s["is_responsive"] for s in session_health
            ):
                overall_status = "issues"
            if any(s["has_errors"] for s in session_health):
                overall_status = "critical"

            return jsonify(
                {
                    "status": overall_status,
                    "active_runs": len(active_runs),
                    "pending_tasks": pending_tasks,
                    "recent_errors": recent_errors,
                    "open_alerts": open_alerts,
                    "sessions": session_health,
                }
            )

    except Exception as e:
        log_error_to_db(
            "autopilot_health_check", str(e), "/api/autopilot/health"
        )
        return jsonify({"status": "error", "message": str(e)}), 500


# ============ Claude Interactions API ============


@app.route("/api/claude/interactions", methods=["GET"])
@require_auth
def get_claude_interactions():
    """Get Claude interactions with optional filters."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            # Filter parameters
            interaction_type = request.args.get("type")
            requires_review = request.args.get("requires_review")
            session_name = request.args.get("session")
            handled = request.args.get("handled")
            limit = request.args.get("limit", 50, type=int)

            query = "SELECT * FROM claude_interactions WHERE 1=1"
            params = []

            if interaction_type:
                query += " AND interaction_type = ?"
                params.append(interaction_type)
            if requires_review:
                query += " AND requires_review = ?"
                params.append(1 if requires_review.lower() == "true" else 0)
            if session_name:
                query += " AND session_name = ?"
                params.append(session_name)
            if handled:
                query += " AND handled = ?"
                params.append(1 if handled.lower() == "true" else 0)

            query += " ORDER BY created_at DESC LIMIT ?"
            params.append(limit)

            interactions = [
                dict(row) for row in conn.execute(query, params).fetchall()
            ]

            # Get summary counts - optimized: single query instead of 3
            # separate queries
            counts = conn.execute(
                """
                SELECT
                    COUNT(*) as total,
                    SUM(CASE WHEN handled = 0 THEN 1 ELSE 0 END) as unhandled,
                    SUM(CASE WHEN requires_review = 1 AND handled = 0 THEN 1 ELSE 0 END) as requires_review
                FROM claude_interactions
            """
            ).fetchone()
            summary = {
                "total": counts["total"] or 0,
                "unhandled": counts["unhandled"] or 0,
                "requires_review": counts["requires_review"] or 0,
                "by_type": {},
            }
            for row in conn.execute(
                "SELECT interaction_type, COUNT(*) as count FROM claude_interactions GROUP BY interaction_type"
            ).fetchall():
                summary["by_type"][row["interaction_type"]] = row["count"]

            return jsonify({"interactions": interactions, "summary": summary})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/claude/interactions/<int:interaction_id>", methods=["GET"])
@require_auth
def get_claude_interaction(interaction_id):
    """Get a single Claude interaction with full context."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            interaction = conn.execute(
                "SELECT * FROM claude_interactions WHERE id = ?",
                (interaction_id,),
            ).fetchone()

            if not interaction:
                return jsonify({"error": "Interaction not found"}), 404

            return jsonify({"interaction": dict(interaction)})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route(
    "/api/claude/interactions/<int:interaction_id>/handle", methods=["POST"]
)
@require_auth
def handle_claude_interaction(interaction_id):
    """Mark an interaction as handled with optional action."""
    try:
        data = request.get_json(silent=True) or {}
        action = data.get("action", "manual")

        with get_db_connection() as conn:
            conn.execute(
                """
                UPDATE claude_interactions
                SET handled = 1, handler_action = ?, requires_review = 0
                WHERE id = ?
            """,
                (action, interaction_id),
            )

            log_activity(
                "claude_interaction",
                f"Handled interaction {interaction_id} with action: {action}",
            )

            return jsonify(
                {
                    "success": True,
                    "message": f"Interaction marked as handled with action: {action}",
                }
            )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route(
    "/api/claude/interactions/<int:interaction_id>/create-pattern",
    methods=["POST"],
)
@require_auth
def create_pattern_from_interaction(interaction_id):
    """Create a new pattern from an interaction."""
    try:
        data = request.get_json(silent=True) or {}

        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            interaction = conn.execute(
                "SELECT * FROM claude_interactions WHERE id = ?",
                (interaction_id,),
            ).fetchone()

            if not interaction:
                return jsonify({"error": "Interaction not found"}), 404

            pattern_regex = data.get(
                "pattern_regex",
                interaction["pattern"] or interaction["content"][:100],
            )
            auto_response = data.get("auto_response")
            action = data.get("action", "approve")
            priority = data.get("priority", 5)

            cursor = conn.execute(
                """
                INSERT INTO claude_patterns (pattern_type, pattern_regex, auto_response, action, priority)
                VALUES (?, ?, ?, ?, ?)
            """,
                (
                    interaction["interaction_type"],
                    pattern_regex,
                    auto_response,
                    action,
                    priority,
                ),
            )

            # Mark interaction as handled
            conn.execute(
                """
                UPDATE claude_interactions
                SET handled = 1, handler_action = 'created_pattern', requires_review = 0
                WHERE id = ?
            """,
                (interaction_id,),
            )

            log_activity(
                "claude_pattern",
                f"Created pattern from interaction {interaction_id}",
            )

            return jsonify(
                {
                    "success": True,
                    "pattern_id": cursor.lastrowid,
                    "message": "Pattern created successfully",
                }
            )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/claude/patterns", methods=["GET"])
@require_auth
def get_claude_patterns():
    """Get all Claude patterns."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            patterns = [
                dict(row)
                for row in conn.execute(
                    "SELECT * FROM claude_patterns ORDER BY priority DESC, created_at DESC"
                ).fetchall()
            ]

            return jsonify({"patterns": patterns})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/claude/patterns", methods=["POST"])
@require_auth
def create_claude_pattern():
    """Create a new Claude pattern."""
    try:
        data = request.get_json(silent=True)
        if not data:
            return jsonify({"error": "Invalid JSON"}), 400

        pattern_type = data.get("pattern_type", "confirmation")
        pattern_regex = data.get("pattern_regex")
        auto_response = data.get("auto_response")
        action = data.get("action", "escalate")
        priority = data.get("priority", 5)

        if not pattern_regex:
            return jsonify({"error": "pattern_regex is required"}), 400

        with get_db_connection() as conn:
            cursor = conn.execute(
                """
                INSERT INTO claude_patterns (pattern_type, pattern_regex, auto_response, action, priority)
                VALUES (?, ?, ?, ?, ?)
            """,
                (pattern_type, pattern_regex, auto_response, action, priority),
            )

            log_activity(
                "claude_pattern", f"Created new pattern: {pattern_regex[:50]}"
            )

            return jsonify({"success": True, "pattern_id": cursor.lastrowid})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/claude/patterns/<int:pattern_id>", methods=["PUT"])
@require_auth
def update_claude_pattern(pattern_id):
    """Update a Claude pattern."""
    try:
        data = request.get_json(silent=True)
        if not data:
            return jsonify({"error": "Invalid JSON"}), 400

        with get_db_connection() as conn:
            updates = []
            params = []

            for field in [
                "pattern_type",
                "pattern_regex",
                "auto_response",
                "action",
                "priority",
                "enabled",
            ]:
                if field in data:
                    updates.append(f"{field} = ?")
                    params.append(data[field])

            if not updates:
                return jsonify({"error": "No fields to update"}), 400

            updates.append("updated_at = CURRENT_TIMESTAMP")
            params.append(pattern_id)

            conn.execute(
                """
                UPDATE claude_patterns SET {', '.join(updates)} WHERE id = ?
            """,
                params,
            )

            log_activity("claude_pattern", f"Updated pattern {pattern_id}")

            return jsonify({"success": True})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/claude/patterns/<int:pattern_id>", methods=["DELETE"])
@require_auth
def delete_claude_pattern(pattern_id):
    """Delete a Claude pattern."""
    try:
        with get_db_connection() as conn:
            conn.execute(
                "DELETE FROM claude_patterns WHERE id = ?", (pattern_id,)
            )
            log_activity("claude_pattern", f"Deleted pattern {pattern_id}")
            return jsonify({"success": True})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/tmux/send", methods=["POST"])
@require_auth
def send_to_tmux():
    """Send a command to a tmux session."""
    data = request.get_json(silent=True)
    if not data:
        return jsonify({"error": "Invalid JSON in request body"}), 400
    session_name = data.get("session")
    message = data.get("message")
    send_enter = data.get("send_enter", True)  # Default to sending Enter

    if not session_name or not message:
        return jsonify({"error": "Session and message required"}), 400

    try:
        # First send Ctrl-C to interrupt any current work (optional)
        if data.get("interrupt_first", False):
            subprocess.run(
                ["tmux", "send-keys", "-t", session_name, "C-c"],
                capture_output=True,
                timeout=5,
            )
            time.sleep(0.5)

        # Send text to tmux session
        subprocess.run(
            ["tmux", "send-keys", "-t", session_name, message],
            check=True,
            timeout=10,
        )

        # Send Enter as separate command (works reliably with Claude Code)
        if send_enter:
            time.sleep(0.3)  # Small delay before Enter
            subprocess.run(
                ["tmux", "send-keys", "-t", session_name, "Enter"],
                check=True,
                timeout=5,
            )

        log_activity(
            "tmux_send", "tmux", None, f"{session_name}: {message[:50]}"
        )

        return jsonify({"success": True})

    except subprocess.CalledProcessError as e:
        return jsonify({"error": f"tmux error: {e}"}), 500
    except subprocess.TimeoutExpired:
        return jsonify({"error": "tmux command timed out"}), 500
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/tmux/send-key", methods=["POST"])
@require_auth
def send_key_to_tmux():
    """Send a special key to a tmux session (Escape, Tab, Ctrl+C, etc)."""
    data = request.get_json()
    session_name = data.get("session")
    key = data.get("key")

    if not session_name or not key:
        return jsonify({"error": "Session and key required"}), 400

    try:
        # Send special key to tmux session (no Enter)
        subprocess.run(
            ["tmux", "send-keys", "-t", session_name, key],
            check=True,
            timeout=5,
        )

        return jsonify({"success": True})

    except subprocess.CalledProcessError as e:
        return jsonify({"error": f"tmux error: {e}"}), 500
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/tmux/capture", methods=["POST"])
@require_auth
def capture_tmux():
    """Capture output from a tmux session with scrollback support."""
    data = request.get_json()
    session_name = data.get("session")
    scrollback_lines = data.get("scrollback", 0)  # 0 = current pane only

    if not session_name:
        return jsonify({"error": "Session required"}), 400

    try:
        # Build capture command
        cmd = ["tmux", "capture-pane", "-t", session_name, "-p"]

        if scrollback_lines > 0:
            # Capture scrollback history
            # -S specifies start line (negative = scrollback), -E specifies end line
            cmd.extend(["-S", f"-{scrollback_lines}", "-E", "-1"])

        result = subprocess.run(
            cmd, capture_output=True, text=True, timeout=10
        )

        # Get scrollback buffer size
        history_size = 0
        try:
            info_result = subprocess.run(
                [
                    "tmux",
                    "display-message",
                    "-t",
                    session_name,
                    "-p",
                    "#{history_size}",
                ],
                capture_output=True,
                text=True,
                timeout=5,
            )
            if (
                info_result.returncode == 0
                and info_result.stdout.strip().isdigit()
            ):
                history_size = int(info_result.stdout.strip())
        except Exception:
            pass  # Ignore errors getting history size

        return jsonify(
            {
                "output": result.stdout,
                "success": True,
                "history_size": history_size,
                "lines_captured": (
                    scrollback_lines if scrollback_lines > 0 else "current"
                ),
            }
        )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/tmux/create", methods=["POST"])
@require_auth
def create_tmux_session():
    """Create a new tmux session."""
    data = request.get_json()
    session_name = data.get("name")
    command = data.get("command")

    if not session_name:
        return jsonify({"error": "Session name required"}), 400

    try:
        cmd = ["tmux", "new-session", "-d", "-s", session_name]
        if command:
            cmd.extend([command])

        subprocess.run(cmd, check=True, timeout=10)

        log_activity("tmux_create", "tmux", None, session_name)

        # Refresh sessions
        refresh_tmux_sessions()

        return jsonify({"success": True})

    except subprocess.CalledProcessError as e:
        return jsonify({"error": f"Failed to create session: {e}"}), 500


@app.route("/api/tmux/kill", methods=["POST"])
@require_auth
def kill_tmux_session():
    """Kill a tmux session."""
    data = request.get_json()
    session_name = data.get("session")

    if not session_name:
        return jsonify({"error": "Session required"}), 400

    try:
        subprocess.run(
            ["tmux", "kill-session", "-t", session_name], check=True, timeout=5
        )

        log_activity("tmux_kill", "tmux", None, session_name)

        return jsonify({"success": True})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ============================================================================
# TASK DELEGATOR API (ENVIRONMENT & SESSION TRACKING)
# ============================================================================

DELEGATOR_DB = os.path.join(
    os.path.dirname(__file__), "data", "task_delegator.db"
)
COMMAND_LOG_DB = os.path.join(
    os.path.dirname(__file__), "data", "command_log.db"
)
RESPONSE_LOG_DB = os.path.join(
    os.path.dirname(__file__), "data", "response_log.db"
)
ENVS_PATH = "/Users/jgirmay/Desktop/gitrepo/pyWork/basic_edu_apps_final/environments/feature_environments"


@app.route("/api/delegator/checkout", methods=["GET"])
@require_auth
def get_delegator_checkout():
    """Get environment and session checkout status from task delegator."""
    try:
        if not os.path.exists(DELEGATOR_DB):
            return jsonify({"error": "Task delegator database not found"}), 404

        conn = sqlite3.connect(DELEGATOR_DB)
        conn.row_factory = sqlite3.Row

        # Get environment status
        environments = []
        for env_id in range(1, 6):
            env = conn.execute(
                "SELECT status, current_task_id, session_name FROM environments WHERE env_id=?",
                (env_id,),
            ).fetchone()

            env_path = os.path.join(ENVS_PATH, f"env_{env_id}")
            has_code = (
                os.path.exists(env_path) and len(os.listdir(env_path)) > 2
            )

            # Get branch name
            branch = None
            if has_code:
                try:
                    result = subprocess.run(
                        ["git", "-C", env_path, "branch", "--show-current"],
                        capture_output=True,
                        text=True,
                        timeout=2,
                    )
                    branch = result.stdout.strip() or None
                except Exception:
                    pass

            # Get task description if assigned
            task_desc = None
            if env and env["current_task_id"]:
                task = conn.execute(
                    "SELECT description FROM tasks WHERE id=?",
                    (env["current_task_id"],),
                ).fetchone()
                task_desc = task["description"][:60] if task else None

            environments.append(
                {
                    "env_id": env_id,
                    "status": env["status"] if env else "unknown",
                    "task_id": env["current_task_id"] if env else None,
                    "task_desc": task_desc,
                    "session": env["session_name"] if env else None,
                    "has_code": has_code,
                    "branch": branch,
                    "path": env_path,
                }
            )

        # Get session status - which are assigned
        assigned_sessions = []
        envs_with_sessions = conn.execute(
            "SELECT session_name, env_id FROM environments WHERE status='busy' AND session_name IS NOT NULL"
        ).fetchall()
        for row in envs_with_sessions:
            assigned_sessions.append(
                {"session": row["session_name"], "env_id": row["env_id"]}
            )

        # Get pending and running tasks
        pending_tasks = conn.execute(
            "SELECT id, description, priority FROM tasks WHERE status='pending' ORDER BY id"
        ).fetchall()
        running_tasks = conn.execute(
            "SELECT id, description, assigned_env, assigned_session FROM tasks WHERE status='running' ORDER BY id"
        ).fetchall()

        conn.close()

        return jsonify(
            {
                "environments": environments,
                "assigned_sessions": assigned_sessions,
                "pending_tasks": [dict(t) for t in pending_tasks],
                "running_tasks": [dict(t) for t in running_tasks],
            }
        )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/delegator/logs", methods=["GET"])
@require_auth
def get_delegator_logs():
    """Get command and response logs from task delegator."""
    log_type = request.args.get("type", "commands")
    limit = min(int(request.args.get("limit", 50)), 200)

    try:
        if log_type == "commands":
            if not os.path.exists(COMMAND_LOG_DB):
                return jsonify({"logs": [], "total": 0})

            conn = sqlite3.connect(COMMAND_LOG_DB)
            conn.row_factory = sqlite3.Row
            logs = conn.execute(
                """
                SELECT id, timestamp, session_name, env_id, app_name, command_type, command_summary, task_id, success
                FROM command_log ORDER BY id DESC LIMIT ?
            """,
                (limit,),
            ).fetchall()
            total = conn.execute(
                "SELECT COUNT(*) FROM command_log"
            ).fetchone()[0]
            conn.close()

            return jsonify(
                {"logs": [dict(log) for log in logs], "total": total}
            )

        elif log_type == "responses":
            if not os.path.exists(RESPONSE_LOG_DB):
                return jsonify({"logs": [], "total": 0})

            conn = sqlite3.connect(RESPONSE_LOG_DB)
            conn.row_factory = sqlite3.Row
            logs = conn.execute(
                """
                SELECT id, timestamp, session_name, env_id, app_name, response_type, status_indicator, is_idle, is_busy, is_error
                FROM response_types ORDER BY id DESC LIMIT ?
            """,
                (limit,),
            ).fetchall()
            total = conn.execute(
                "SELECT COUNT(*) FROM response_types"
            ).fetchone()[0]
            conn.close()

            return jsonify(
                {"logs": [dict(log) for log in logs], "total": total}
            )

        else:
            return (
                jsonify(
                    {
                        "error": 'Invalid log type. Use "commands" or "responses"'
                    }
                ),
                400,
            )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ============================================================================
# TASK QUEUE API (FOR OFFLINE WORKERS)
# ============================================================================


@app.route("/api/tasks", methods=["GET"])
@require_auth
def get_tasks():
    """Get tasks from the queue with filtering and pagination.

    Returns tasks with effective priority (includes aging bonus) and age info.

    Query Parameters:
        status (str): Filter by status (pending, running, completed, failed)
        type (str): Filter by task type (shell, python, git, deploy, test, build)
        page (int): Page number (default: 1)
        per_page (int): Items per page, max 100 (default: 50)
        paginate (str): Set to 'true' for pagination response
        include_deps (str): Set to 'true' to include dependency counts
        blocked_only (str): Set to 'true' for only blocked tasks
        unblocked_only (str): Set to 'true' for only unblocked tasks
        hours (int): Filter tasks from last N hours
        created_after (str): ISO date - tasks created after
        created_before (str): ISO date - tasks created before
        started_after (str): ISO date - tasks started after
        started_before (str): ISO date - tasks started before
        completed_after (str): ISO date - tasks completed after
        completed_before (str): ISO date - tasks completed before
        risk_level (str): Filter by risk (low, medium, high, critical)
        min_risk_score (int): Minimum risk score filter

    Returns:
        200: List of tasks or paginated result

    Example Request:
        GET /api/tasks
        GET /api/tasks?status=pending&type=shell
        GET /api/tasks?hours=24&paginate=true

    Example Response:
        [
            {
                "id": 1,
                "task_type": "shell",
                "title": "Deploy to staging",
                "task_data": "{\\"command\\": \\"./deploy.sh\\"}",
                "status": "pending",
                "priority": 5,
                "effective_priority": 7.5,
                "age_minutes": 125.3,
                "created_at": "2024-01-15T10:30:00"
            }
        ]

    cURL Example:
        curl -X GET "http://localhost:8080/api/tasks?status=pending" \\
             -H "Cookie: session=<session_cookie>"
    """
    status = request.args.get("status")
    task_type = request.args.get("type")
    include_deps = request.args.get("include_deps", "").lower() == "true"
    blocked_only = request.args.get("blocked_only", "").lower() == "true"
    unblocked_only = request.args.get("unblocked_only", "").lower() == "true"
    include_deleted = request.args.get("include_deleted", "").lower() == "true"

    # Date range filters
    created_after = request.args.get("created_after")
    created_before = request.args.get("created_before")
    started_after = request.args.get("started_after")
    started_before = request.args.get("started_before")
    completed_after = request.args.get("completed_after")
    completed_before = request.args.get("completed_before")
    hours = request.args.get("hours", type=int)

    # Risk filters
    risk_level = request.args.get("risk_level")
    min_risk_score = request.args.get("min_risk_score", type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Include effective_priority calculation for task aging
        # effective_priority = priority + min(max_bonus, age_in_minutes * aging_factor)
        query = """SELECT *,
            (priority + MIN({TASK_PRIORITY_MAX_AGE_BONUS},
             (strftime('%s', 'now') - strftime('%s', created_at)) / 60.0 * {TASK_PRIORITY_AGING_FACTOR})) as effective_priority,
            ROUND((strftime('%s', 'now') - strftime('%s', created_at)) / 60.0, 1) as age_minutes
            FROM task_queue WHERE 1=1"""
        params = []

        # Filter out soft-deleted records by default
        if not include_deleted:
            query += " AND deleted_at IS NULL"

        if status:
            query += " AND status = ?"
            params.append(status)
        if task_type:
            query += " AND task_type = ?"
            params.append(task_type)

        # Date range filtering
        if hours:
            query += " AND created_at > datetime('now', '-' || ? || ' hours')"
            params.append(hours)
        if created_after:
            query += " AND created_at >= ?"
            params.append(created_after)
        if created_before:
            query += " AND created_at <= ?"
            params.append(created_before)
        if started_after:
            query += " AND started_at >= ?"
            params.append(started_after)
        if started_before:
            query += " AND started_at <= ?"
            params.append(started_before)
        if completed_after:
            query += " AND completed_at >= ?"
            params.append(completed_after)
        if completed_before:
            query += " AND completed_at <= ?"
            params.append(completed_before)

        # Risk filtering
        if risk_level:
            query += " AND risk_level = ?"
            params.append(risk_level)
        if min_risk_score is not None:
            query += " AND risk_score >= ?"
            params.append(min_risk_score)

        # Filter by blocked status
        if blocked_only:
            query += """ AND id IN (
                SELECT td.task_id FROM task_dependencies td
                JOIN task_queue tq ON td.depends_on_id = tq.id
                WHERE tq.status NOT IN ('completed', 'failed')
            )"""
        elif unblocked_only:
            query += """ AND id NOT IN (
                SELECT td.task_id FROM task_dependencies td
                JOIN task_queue tq ON td.depends_on_id = tq.id
                WHERE tq.status NOT IN ('completed', 'failed')
            )"""

        query += " ORDER BY effective_priority DESC, created_at ASC"

        tasks = conn.execute(query, params).fetchall()
        task_list = [dict(t) for t in tasks]

        # Add dependency info if requested
        if include_deps and task_list:
            task_ids = [t["id"] for t in task_list]
            placeholders = ",".join("?" * len(task_ids))

            # Get blocked_by counts (tasks this depends on)
            blocked_by_counts = {}
            blocked_by_rows = conn.execute(
                """
                SELECT td.task_id, COUNT(*) as cnt,
                       SUM(CASE WHEN tq.status NOT IN ('completed', 'failed') THEN 1 ELSE 0 END) as incomplete
                FROM task_dependencies td
                JOIN task_queue tq ON td.depends_on_id = tq.id
                WHERE td.task_id IN ({placeholders})
                GROUP BY td.task_id
            """,
                task_ids,
            ).fetchall()
            for row in blocked_by_rows:
                blocked_by_counts[row["task_id"]] = {
                    "total": row["cnt"],
                    "incomplete": row["incomplete"],
                }

            # Get blocks counts (tasks that depend on this)
            blocks_counts = {}
            blocks_rows = conn.execute(
                """
                SELECT td.depends_on_id as task_id, COUNT(*) as cnt
                FROM task_dependencies td
                WHERE td.depends_on_id IN ({placeholders})
                GROUP BY td.depends_on_id
            """,
                task_ids,
            ).fetchall()
            for row in blocks_rows:
                blocks_counts[row["task_id"]] = row["cnt"]

            # Add to task list
            for t in task_list:
                tid = t["id"]
                dep_info = blocked_by_counts.get(
                    tid, {"total": 0, "incomplete": 0}
                )
                t["blocked_by_count"] = dep_info["total"]
                t["incomplete_blockers"] = dep_info["incomplete"]
                t["is_blocked"] = dep_info["incomplete"] > 0
                t["blocks_count"] = blocks_counts.get(tid, 0)

        # Support pagination if requested
        if request.args.get("paginate", "").lower() == "true":
            page, per_page = get_pagination_params()
            result = paginate_query(task_list, page, per_page)
            return jsonify(result)

        # Legacy limit/offset support
        limit = request.args.get("limit", type=int)
        offset = request.args.get("offset", 0, type=int)
        if limit:
            return jsonify(task_list[offset: offset + limit])

        return jsonify(task_list)


@app.route("/api/tasks", methods=["POST"])
@require_auth
@rate_limit(requests_per_minute=30)
def create_task():
    """Create a new task in the queue.

    Creates a background task for workers to process.

    Request Body:
        task_type (str, required): Type - shell, python, git, deploy, test, build, tmux
        task_data (dict, optional): Task-specific data (command, script, etc.)
        priority (int, optional): Priority level 0-10 (default: 0)
        max_retries (int, optional): Max retry attempts (default: 3)
        timeout_seconds (int, optional): Task timeout (has defaults per type)
        story_points (int, optional): Fibonacci estimate (1,2,3,5,8,13,21,34)
        estimated_hours (float, optional): Time estimate in hours
        risk_level (str, optional): low, medium, high, critical (default: low)
        risk_score (int, optional): Numeric risk score
        risk_factors (list, optional): List of risk factor strings
        parent_id (int, optional): Parent task ID for hierarchy
        auto_assign (bool, optional): Auto-assign task to worker based on workload
        auto_assign_strategy (str, optional): Assignment strategy (default: least_loaded)

    Returns:
        200: Task created with ID
        400: Validation error
        404: Parent task not found
        429: Rate limit exceeded

    Example Request:
        POST /api/tasks
        Content-Type: application/json

        {
            "task_type": "shell",
            "task_data": {
                "command": "npm run build",
                "cwd": "/app"
            },
            "priority": 5,
            "story_points": 3
        }

    Example Response:
        {
            "id": 42,
            "success": true
        }

    cURL Example:
        curl -X POST "http://localhost:8080/api/tasks" \\
             -H "Content-Type: application/json" \\
             -H "Cookie: session=<session_cookie>" \\
             -d '{"task_type": "shell", "task_data": {"command": "ls -la"}}'
    """
    data = request.get_json() or {}

    # Validate required field
    task_type = data.get("task_type")
    if not task_type:
        return jsonify({"error": "task_type is required"}), 400

    # Get timeout - use provided value, or default for task type
    timeout = data.get("timeout_seconds") or get_task_timeout(task_type)

    # Auto-assignment options
    auto_assign = data.get("auto_assign", False)
    if isinstance(auto_assign, str):
        auto_assign = auto_assign.lower() in ("1", "true", "yes", "y")
    auto_assign_strategy = data.get("auto_assign_strategy")

    # Validate story points if provided
    story_points = data.get("story_points")
    if story_points is not None and story_points not in [
        1,
        2,
        3,
        5,
        8,
        13,
        21,
        34,
    ]:
        return (
            jsonify(
                {
                    "error": "Invalid story_points. Use Fibonacci: 1,2,3,5,8,13,21,34"
                }
            ),
            400,
        )

    # Risk assessment fields
    risk_level = data.get("risk_level", "low")
    if risk_level not in ["low", "medium", "high", "critical"]:
        risk_level = "low"
    risk_score = data.get("risk_score", 0)
    risk_factors = json.dumps(data.get("risk_factors", []))

    # Hierarchy fields for parent/child relationships
    parent_id = data.get("parent_id")
    hierarchy_level = 0
    hierarchy_path = "/"

    with get_db_connection() as conn:
        # If parent_id provided, validate and calculate hierarchy
        if parent_id:
            parent = conn.execute(
                "SELECT id, hierarchy_level, hierarchy_path FROM task_queue WHERE id = ?",
                (parent_id,),
            ).fetchone()
            if not parent:
                return (
                    jsonify({"error": f"Parent task {parent_id} not found"}),
                    404,
                )
            hierarchy_level = (parent["hierarchy_level"] or 0) + 1
            hierarchy_path = f"{parent['hierarchy_path'] or '/'}{parent_id}/"

        cursor = conn.execute(
            """
            INSERT INTO task_queue (task_type, task_data, priority, max_retries, timeout_seconds, story_points, estimated_hours, risk_level, risk_score, risk_factors, risk_assessed_at, parent_id, hierarchy_level, hierarchy_path)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                task_type,
                json.dumps(data.get("task_data", {})),
                data.get("priority", 0),
                data.get("max_retries", 3),
                timeout,
                story_points,
                data.get("estimated_hours"),
                risk_level,
                risk_score,
                risk_factors,
                (
                    datetime.now().isoformat()
                    if risk_level != "low" or risk_score > 0
                    else None
                ),
                parent_id,
                hierarchy_level,
                hierarchy_path,
            ),
        )
        task_id = cursor.lastrowid

        # Update parent's child_count
        if parent_id:
            conn.execute(
                "UPDATE task_queue SET child_count = child_count + 1 WHERE id = ?",
                (parent_id,),
            )

        log_activity("create_task", "task", task_id, task_type)

    assignment_result = None
    if auto_assign:
        try:
            from services.auto_assign import STRATEGY_LEAST_LOADED, get_auto_assign_service

            strategy = auto_assign_strategy or STRATEGY_LEAST_LOADED
            service = get_auto_assign_service(str(DB_PATH))
            assignment_result = service.assign_task(task_id, strategy=strategy)
        except Exception as e:
            logger.error(f"Auto-assign failed for task {task_id}: {e}")
            assignment_result = {"success": False, "error": str(e)}

    # Broadcast queue update via WebSocket
    broadcast_queue()

    # Trigger webhook notification for new task
    trigger_task_webhook(
        task_id=task_id,
        task_type=task_type,
        new_status="pending",
        old_status=None,
        task_data=data.get("task_data", {}),
    )

    response = {"id": task_id, "timeout_seconds": timeout, "success": True}
    if auto_assign:
        response["auto_assign"] = assignment_result or {
            "success": False,
            "error": "Auto-assign failed",
        }
        if assignment_result and assignment_result.get("success"):
            response["assigned_worker"] = assignment_result.get(
                "assigned_worker"
            )
            response["assignment_strategy"] = assignment_result.get("strategy")

    return jsonify(response)


@app.route("/api/tasks/ai-breakdown", methods=["POST"])
@require_auth
@rate_limit(requests_per_minute=10)
def ai_task_breakdown():
    """Break down high-level instruction into actionable tasks using AI.

    Takes a natural language instruction and uses LLM to decompose it into
    specific, executable subtasks. Optionally queues tasks for execution.

    Request Body:
        instruction (str, required): High-level instruction to break down
        auto_execute (bool, optional): Automatically queue tasks (default: false)
        target_provider (str, optional): Route to specific provider (codex, claude, comet)
        priority (int, optional): Base priority for tasks (default: 5)

    Returns:
        200: Breakdown successful with task list
        400: Validation error
        500: LLM error

    Example Request:
        POST /api/tasks/ai-breakdown
        {
            "instruction": "Fix the login bug and add tests",
            "auto_execute": true,
            "target_provider": "claude"
        }

    Example Response:
        {
            "success": true,
            "instruction": "Fix the login bug and add tests",
            "breakdown": [
                {
                    "step": 1,
                    "description": "Investigate login bug in auth.py",
                    "type": "investigation",
                    "estimated_time": "15 minutes"
                },
                {
                    "step": 2,
                    "description": "Fix authentication logic",
                    "type": "code",
                    "estimated_time": "30 minutes"
                },
                {
                    "step": 3,
                    "description": "Add unit tests for login",
                    "type": "test",
                    "estimated_time": "20 minutes"
                }
            ],
            "tasks_queued": [1, 2, 3],
            "estimated_total_time": "65 minutes"
        }
    """
    data = request.get_json() or {}

    # Validate required field
    instruction = data.get("instruction", "").strip()
    if not instruction:
        return jsonify({"error": "instruction is required"}), 400

    auto_execute = data.get("auto_execute", False)
    if isinstance(auto_execute, str):
        auto_execute = auto_execute.lower() in ("1", "true", "yes", "y")

    target_provider = data.get("target_provider", "auto")
    base_priority = data.get("priority", 5)

    try:
        # Import LLM provider
        from services.llm_provider import UnifiedLLMClient

        # Create LLM client
        client = UnifiedLLMClient()

        # Prompt for task breakdown
        prompt = """Break down this high-level instruction into specific, actionable tasks:

INSTRUCTION: {instruction}

Analyze the instruction and create a structured task breakdown. For each task, provide:
1. Step number
2. Clear, specific description
3. Task type (investigation, code, test, deploy, documentation, review)
4. Estimated time (in minutes)
5. Any dependencies on previous steps

Return the response as a JSON array with this structure:
[
  {{
    "step": 1,
    "description": "Clear task description",
    "type": "code",
    "estimated_time": "30 minutes",
    "dependencies": []
  }},
  ...
]

Be specific and actionable. Each task should be something a developer can execute directly.
Only return the JSON array, no additional text."""

        # Call LLM
        response = client.messages.create(
            model="claude-sonnet-4-5-20250929",
            max_tokens=2000,
            messages=[{"role": "user", "content": prompt}],
        )

        # Extract response text
        response_text = response.content[0].text.strip()

        # Parse JSON from response
        # Handle markdown code blocks if present
        if "```json" in response_text:
            response_text = (
                response_text.split("```json")[1].split("```")[0].strip()
            )
        elif "```" in response_text:
            response_text = (
                response_text.split("```")[1].split("```")[0].strip()
            )

        breakdown = json.loads(response_text)

        if not isinstance(breakdown, list):
            breakdown = [breakdown]

        # Calculate total estimated time
        total_minutes = 0
        for task in breakdown:
            est_time = task.get("estimated_time", "0 minutes")
            # Extract number from "30 minutes" or "1 hour"
            if "hour" in est_time.lower():
                hours = float(est_time.split()[0])
                total_minutes += hours * 60
            elif "minute" in est_time.lower():
                minutes = float(est_time.split()[0])
                total_minutes += minutes

        result = {
            "success": True,
            "instruction": instruction,
            "breakdown": breakdown,
            "estimated_total_time": f"{int(total_minutes)} minutes",
        }

        # Optionally queue tasks for execution
        if auto_execute:
            tasks_queued = []

            # Check if assigner database exists
            assigner_db = Path("data/assigner/assigner.db")

            if assigner_db.exists():
                # Queue in assigner for Claude sessions
                import sqlite3

                with sqlite3.connect(str(assigner_db)) as conn:
                    conn.row_factory = sqlite3.Row

                    for task in breakdown:
                        cursor = conn.execute(
                            """
                            INSERT INTO prompts (content, priority, target_session, timeout_minutes, status)
                            VALUES (?, ?, ?, 30, 'pending')
                        """,
                            (
                                task["description"],
                                base_priority,
                                (
                                    target_provider
                                    if target_provider != "auto"
                                    else None
                                ),
                            ),
                        )
                        tasks_queued.append(cursor.lastrowid)

                    conn.commit()

                result["tasks_queued"] = tasks_queued
                result["queue_type"] = "assigner"
            else:
                # Queue in main task queue
                with get_db_connection() as conn:
                    for task in breakdown:
                        # Determine task type
                        task_type = "shell"
                        if task["type"] == "code":
                            task_type = "python"
                        elif task["type"] == "test":
                            task_type = "test"
                        elif task["type"] == "deploy":
                            task_type = "deploy"

                        cursor = conn.execute(
                            """
                            INSERT INTO task_queue (task_type, task_data, priority, status)
                            VALUES (?, ?, ?, 'pending')
                        """,
                            (
                                task_type,
                                json.dumps(
                                    {"description": task["description"]}
                                ),
                                base_priority,
                            ),
                        )
                        tasks_queued.append(cursor.lastrowid)

                    conn.commit()

                result["tasks_queued"] = tasks_queued
                result["queue_type"] = "task_queue"

        return jsonify(result)

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse LLM response as JSON: {e}")
        logger.error(f"Response text: {response_text}")
        return (
            jsonify(
                {
                    "error": "Failed to parse task breakdown",
                    "details": str(e),
                    "raw_response": response_text,
                }
            ),
            500,
        )

    except Exception as e:
        logger.error(f"AI task breakdown failed: {e}")
        return (
            jsonify({"error": "Failed to break down task", "details": str(e)}),
            500,
        )


@app.route("/api/tasks/import", methods=["POST"])
@require_auth
def import_tasks_batch():
    """
    Batch import tasks from CSV or JSON.

    JSON format: {"tasks": [{"task_type": "...", "task_data": {...}, ...}, ...]}
    CSV format: task_type,priority,max_retries,timeout_seconds,task_data_json
                shell,0,3,300,"{""command"": ""ls""}"

    Query params:
      - format: 'json' (default) or 'csv'
      - validate_only: 'true' to validate without importing
      - skip_errors: 'true' to continue on individual task errors
    """
    import csv
    import io

    format_type = request.args.get("format", "json").lower()
    validate_only = (
        request.args.get("validate_only", "false").lower() == "true"
    )
    skip_errors = request.args.get("skip_errors", "false").lower() == "true"

    tasks_to_import = []
    errors = []

    try:
        if format_type == "json":
            # JSON import
            data = request.get_json()
            if not data:
                return jsonify({"error": "No JSON data provided"}), 400

            tasks_list = data.get("tasks", [])
            if not isinstance(tasks_list, list):
                return jsonify({"error": "tasks must be an array"}), 400

            for idx, task in enumerate(tasks_list):
                if not isinstance(task, dict):
                    errors.append(
                        {"index": idx, "error": "Task must be an object"}
                    )
                    continue
                if not task.get("task_type"):
                    errors.append(
                        {"index": idx, "error": "task_type is required"}
                    )
                    continue
                tasks_to_import.append(
                    {
                        "task_type": task.get("task_type"),
                        "task_data": task.get("task_data", {}),
                        "priority": task.get("priority", 0),
                        "max_retries": task.get("max_retries", 3),
                        "timeout_seconds": task.get("timeout_seconds"),
                    }
                )

        elif format_type == "csv":
            # CSV import
            content = request.get_data(as_text=True)
            if not content:
                return jsonify({"error": "No CSV data provided"}), 400

            reader = csv.DictReader(io.StringIO(content))
            required_fields = {"task_type"}

            for idx, row in enumerate(reader):
                # Validate required fields
                if not row.get("task_type"):
                    errors.append(
                        {"index": idx, "error": "task_type is required"}
                    )
                    continue

                # Parse task_data if provided as JSON string
                task_data = {}
                if row.get("task_data") or row.get("task_data_json"):
                    raw_data = row.get("task_data") or row.get(
                        "task_data_json"
                    )
                    try:
                        task_data = json.loads(raw_data) if raw_data else {}
                    except json.JSONDecodeError as e:
                        errors.append(
                            {
                                "index": idx,
                                "error": f"Invalid task_data JSON: {e}",
                            }
                        )
                        continue

                # Parse numeric fields
                try:
                    priority = (
                        int(row.get("priority", 0))
                        if row.get("priority")
                        else 0
                    )
                except ValueError:
                    priority = 0

                try:
                    max_retries = (
                        int(row.get("max_retries", 3))
                        if row.get("max_retries")
                        else 3
                    )
                except ValueError:
                    max_retries = 3

                timeout = None
                if row.get("timeout_seconds"):
                    try:
                        timeout = int(row["timeout_seconds"])
                    except ValueError:
                        pass

                tasks_to_import.append(
                    {
                        "task_type": row["task_type"].strip(),
                        "task_data": task_data,
                        "priority": priority,
                        "max_retries": max_retries,
                        "timeout_seconds": timeout,
                    }
                )
        else:
            return (
                jsonify(
                    {
                        "error": f"Unsupported format: {format_type}. Use json or csv"
                    }
                ),
                400,
            )

        # Check if we have tasks to import
        if not tasks_to_import and not errors:
            return jsonify({"error": "No valid tasks found in input"}), 400

        # If there are errors and not skipping, fail early
        if errors and not skip_errors and not validate_only:
            return (
                jsonify(
                    {
                        "error": "Validation failed",
                        "errors": errors,
                        "valid_count": len(tasks_to_import),
                    }
                ),
                400,
            )

        # Validation only - return summary without importing
        if validate_only:
            return jsonify(
                {
                    "valid": True,
                    "tasks_count": len(tasks_to_import),
                    "errors": errors,
                    "tasks_preview": tasks_to_import[:10],  # Preview first 10
                }
            )

        # Import tasks
        imported = []
        import_errors = []

        with get_db_connection() as conn:
            for idx, task in enumerate(tasks_to_import):
                try:
                    timeout = task["timeout_seconds"] or get_task_timeout(
                        task["task_type"]
                    )
                    cursor = conn.execute(
                        """
                        INSERT INTO task_queue (task_type, task_data, priority, max_retries, timeout_seconds)
                        VALUES (?, ?, ?, ?, ?)
                    """,
                        (
                            task["task_type"],
                            json.dumps(task["task_data"]),
                            task["priority"],
                            task["max_retries"],
                            timeout,
                        ),
                    )
                    imported.append(
                        {
                            "id": cursor.lastrowid,
                            "task_type": task["task_type"],
                            "priority": task["priority"],
                        }
                    )
                except Exception as e:
                    import_errors.append(
                        {
                            "index": idx,
                            "task_type": task["task_type"],
                            "error": str(e),
                        }
                    )
                    if not skip_errors:
                        raise

            log_activity(
                "batch_import_tasks",
                "task",
                None,
                f"Imported {len(imported)} tasks ({format_type})",
            )

        # Broadcast queue update
        broadcast_queue()

        return jsonify(
            {
                "success": True,
                "imported_count": len(imported),
                "imported": imported,
                "validation_errors": errors,
                "import_errors": import_errors,
            }
        )

    except Exception as e:
        return jsonify({"error": f"Import failed: {str(e)}"}), 500


@app.route("/api/tasks/import/template", methods=["GET"])
@require_auth
def get_import_template():
    """Get CSV/JSON template for batch task import."""
    format_type = request.args.get("format", "json").lower()

    if format_type == "json":
        template = {
            "tasks": [
                {
                    "task_type": "shell",
                    "task_data": {
                        "command": 'echo "Hello World"',
                        "cwd": "/tmp",
                    },
                    "priority": 0,
                    "max_retries": 3,
                    "timeout_seconds": 300,
                },
                {
                    "task_type": "python",
                    "task_data": {"script": 'print("Hello")', "args": []},
                    "priority": 1,
                    "max_retries": 2,
                    "timeout_seconds": 600,
                },
                {
                    "task_type": "git",
                    "task_data": {
                        "operation": "pull",
                        "repo_path": "/path/to/repo",
                    },
                    "priority": 0,
                    "max_retries": 3,
                },
            ],
            "_task_types": [
                "shell",
                "python",
                "git",
                "deploy",
                "test",
                "build",
                "tmux",
            ],
            "_notes": "task_type is required. task_data structure depends on task_type.",
        }
        return jsonify(template)

    elif format_type == "csv":
        csv_template = """task_type,priority,max_retries,timeout_seconds,task_data_json
shell,0,3,300,"{""command"": ""echo Hello"", ""cwd"": ""/tmp""}"
python,1,2,600,"{""script"": ""print('Hello')"", ""args"": []}"
git,0,3,,"{""operation"": ""pull"", ""repo_path"": ""/path/to/repo""}"
test,2,1,1800,"{""test_path"": ""tests/"", ""framework"": ""pytest""}"
"""
        from flask import Response

        return Response(
            csv_template,
            mimetype="text/csv",
            headers={
                "Content-Disposition": "attachment; filename=task_import_template.csv"
            },
        )

    return jsonify({"error": "Unsupported format. Use json or csv"}), 400


# RECURRING TASKS API
@app.route("/api/tasks/recurring", methods=["GET"])
@require_auth
def get_recurring_tasks():
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        q = (
            "SELECT * FROM recurring_tasks"
            + (
                " WHERE enabled=1"
                if request.args.get("enabled_only") == "true"
                else ""
            )
            + " ORDER BY name"
        )
        return jsonify(
            {"recurring_tasks": [dict(t) for t in conn.execute(q).fetchall()]}
        )


@app.route("/api/tasks/recurring", methods=["POST"])
@require_auth
def create_recurring_task():
    d = request.get_json() or {}
    n, tt, rt = d.get("name"), d.get("task_type"), d.get("recurrence_type")
    if not all([n, tt, rt]) or rt not in {
        "hourly",
        "daily",
        "weekly",
        "monthly",
        "cron",
    }:
        return api_error("name, task_type, recurrence_type required", 400)
    nr = calculate_next_run(
        rt,
        d.get("recurrence_interval", 1),
        d.get("recurrence_days"),
        d.get("recurrence_time", "00:00"),
    )
    with get_db_connection() as conn:
        c = conn.execute(
            "INSERT INTO recurring_tasks (name,description,task_type,task_data,priority,max_retries,timeout_seconds,recurrence_type,recurrence_interval,recurrence_days,recurrence_time,cron_expression,enabled,next_run_at,max_instances,catch_up_missed,created_by) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)",
            (
                n,
                d.get("description"),
                tt,
                json.dumps(d.get("task_data", {})),
                d.get("priority", 0),
                d.get("max_retries", 3),
                d.get("timeout_seconds"),
                rt,
                d.get("recurrence_interval", 1),
                d.get("recurrence_days"),
                d.get("recurrence_time", "00:00"),
                d.get("cron_expression"),
                d.get("enabled", True),
                nr.isoformat(),
                d.get("max_instances", 1),
                d.get("catch_up_missed", False),
                session.get("user", "unknown"),
            ),
        )
        log_activity("create_recurring_task", "recurring_task", c.lastrowid, n)
        return jsonify(
            {"id": c.lastrowid, "next_run_at": nr.isoformat(), "success": True}
        )


@app.route("/api/tasks/recurring/<int:tid>", methods=["GET"])
@require_auth
def get_recurring_task(tid):
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        t = conn.execute(
            "SELECT * FROM recurring_tasks WHERE id=?", (tid,)
        ).fetchone()
        return jsonify(dict(t)) if t else api_error("Not found", 404)


@app.route("/api/tasks/recurring/<int:tid>", methods=["PUT"])
@require_auth
def update_recurring_task(tid):
    d = request.get_json() or {}
    with get_db_connection() as conn:
        if not conn.execute(
            "SELECT 1 FROM recurring_tasks WHERE id=?", (tid,)
        ).fetchone():
            return api_error("Not found", 404)
        u, p = [], []
        for f in [
            "name",
            "description",
            "task_type",
            "priority",
            "max_retries",
            "timeout_seconds",
            "recurrence_type",
            "recurrence_interval",
            "recurrence_days",
            "recurrence_time",
            "cron_expression",
            "enabled",
            "max_instances",
            "catch_up_missed",
        ]:
            if f in d:
                u.append(f"{f}=?")
                p.append(d[f])
        if "task_data" in d:
            u.append("task_data=?")
            p.append(json.dumps(d["task_data"]))
        if u:
            p.append(tid)
            conn.execute(
                f"UPDATE recurring_tasks SET {
                    ','.join(u)},updated_at=CURRENT_TIMESTAMP WHERE id=?",
                p,
            )
        return jsonify({"success": True})


@app.route("/api/tasks/recurring/<int:tid>", methods=["DELETE"])
@require_auth
def delete_recurring_task(tid):
    with get_db_connection() as conn:
        if (
            conn.execute(
                "DELETE FROM recurring_tasks WHERE id=?", (tid,)
            ).rowcount
            == 0
        ):
            return api_error("Not found", 404)
        log_activity("delete_recurring_task", "recurring_task", tid)
        return jsonify({"success": True})


@app.route("/api/tasks/recurring/<int:tid>/run", methods=["POST"])
@require_auth
def run_recurring_task_now(tid):
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        t = conn.execute(
            "SELECT * FROM recurring_tasks WHERE id=?", (tid,)
        ).fetchone()
        if not t:
            return api_error("Not found", 404)
        sid = spawn_recurring_task(dict(t), conn, log_activity)
        if sid:
            broadcast_queue()
            return jsonify({"success": True, "task_id": sid})
        return api_error("Max instances reached", 409)


@app.route("/api/tasks/recurring/<int:tid>/toggle", methods=["POST"])
@require_auth
def toggle_recurring_task(tid):
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        t = conn.execute(
            "SELECT enabled FROM recurring_tasks WHERE id=?", (tid,)
        ).fetchone()
        if not t:
            return api_error("Not found", 404)
        nw = not t["enabled"]
        conn.execute(
            "UPDATE recurring_tasks SET enabled=?,updated_at=CURRENT_TIMESTAMP WHERE id=?",
            (nw, tid),
        )
        return jsonify({"success": True, "enabled": nw})


@app.route("/api/tasks/recurring/check", methods=["POST"])
@require_auth
def check_recurring_tasks_api():
    with get_db_connection() as conn:
        r = check_due_recurring_tasks(conn, spawn_recurring_task, log_activity)
        if r["spawned"]:
            broadcast_queue()
        return jsonify(r)


@app.route("/api/tasks/recurring/stats", methods=["GET"])
@require_auth
def get_recurring_tasks_stats():
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        s = conn.execute(
            "SELECT COUNT(*) as total,SUM(CASE WHEN enabled=1 THEN 1 ELSE 0 END) as enabled,SUM(run_count) as runs,SUM(failure_count) as failures FROM recurring_tasks"
        ).fetchone()
        u = conn.execute(
            "SELECT id,name,task_type,next_run_at FROM recurring_tasks WHERE enabled=1 ORDER BY next_run_at LIMIT 10"
        ).fetchall()
        return jsonify({"stats": dict(s), "upcoming": [dict(t) for t in u]})


@app.route("/api/config/task-timeouts", methods=["GET"])
@require_auth
def get_task_timeouts():
    """Get task timeout configuration for all task types.

    Returns the configured timeout (in seconds) for each task type.
    Tasks running longer than their timeout are considered stuck.
    """
    return jsonify(
        {
            "timeouts": TASK_TYPE_TIMEOUTS,
            "default": TASK_TYPE_TIMEOUTS.get("default", 600),
        }
    )


@app.route("/api/config/task-timeouts", methods=["POST"])
@require_auth
def update_task_timeouts():
    """Update task timeout configuration for specific task types.

    Request body:
        {
            "timeouts": {
                "shell": 300,
                "deploy": 3600
            }
        }

    Only updates the specified task types, others remain unchanged.
    Changes are applied in-memory and affect new tasks immediately.
    """
    data = request.get_json() or {}
    timeouts = data.get("timeouts", {})

    if not timeouts:
        return jsonify({"error": "No timeouts provided"}), 400

    updated = []
    for task_type, timeout in timeouts.items():
        if not isinstance(timeout, int) or timeout < 0:
            return (
                jsonify(
                    {
                        "error": f"Invalid timeout for {task_type}: must be a positive integer"
                    }
                ),
                400,
            )

        TASK_TYPE_TIMEOUTS[task_type] = timeout
        updated.append(task_type)

    log_activity(
        "update_task_timeouts",
        "config",
        None,
        f'Updated: {", ".join(updated)}',
    )

    return jsonify(
        {"success": True, "updated": updated, "timeouts": TASK_TYPE_TIMEOUTS}
    )


@app.route("/api/config/task-timeouts/<task_type>", methods=["GET"])
@require_auth
def get_task_type_timeout(task_type):
    """Get timeout configuration for a specific task type."""
    timeout = get_task_timeout(task_type)
    is_default = task_type not in TASK_TYPE_TIMEOUTS

    return jsonify(
        {
            "task_type": task_type,
            "timeout_seconds": timeout,
            "is_default": is_default,
        }
    )


@app.route("/api/config/task-timeouts/<task_type>", methods=["PUT"])
@require_auth
def set_task_type_timeout(task_type):
    """Set timeout configuration for a specific task type.

    Request body:
        {
            "timeout_seconds": 600
        }
    """
    data = request.get_json() or {}
    timeout = data.get("timeout_seconds")

    if timeout is None:
        return jsonify({"error": "timeout_seconds is required"}), 400

    if not isinstance(timeout, int) or timeout < 0:
        return (
            jsonify({"error": "timeout_seconds must be a positive integer"}),
            400,
        )

    old_timeout = TASK_TYPE_TIMEOUTS.get(task_type)
    TASK_TYPE_TIMEOUTS[task_type] = timeout

    log_activity(
        "set_task_timeout",
        "config",
        None,
        f"{task_type}: {old_timeout} -> {timeout}",
    )

    return jsonify(
        {
            "success": True,
            "task_type": task_type,
            "timeout_seconds": timeout,
            "previous_timeout": old_timeout,
        }
    )


# ============================================================================
# TASK ARCHIVE API
# ============================================================================


def archive_old_tasks(
    days_old: int = None,
    statuses: list = None,
    batch_size: int = None,
    keep_recent: int = None,
) -> dict:
    """Archive old completed/failed tasks to the archive table.

    Args:
        days_old: Archive tasks older than this many days (default from config)
        statuses: List of statuses to archive (default from config)
        batch_size: Max tasks to archive per call (default from config)
        keep_recent: Always keep at least this many recent tasks (default from config)

    Returns:
        Dict with archive results
    """
    # Use config defaults if not specified
    days_old = days_old or TASK_ARCHIVE_CONFIG["archive_after_days"]
    statuses = statuses or TASK_ARCHIVE_CONFIG["archive_statuses"]
    batch_size = batch_size or TASK_ARCHIVE_CONFIG["batch_size"]
    keep_recent = keep_recent or TASK_ARCHIVE_CONFIG["keep_recent_count"]

    archived_count = 0
    archived_ids = []

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Get count of tasks that would remain after archiving
        status_placeholders = ",".join(["?" for _ in statuses])
        remaining_query = """
            SELECT COUNT(*) as count FROM task_queue
            WHERE status NOT IN ({status_placeholders})
               OR completed_at > datetime('now', '-' || ? || ' days')
        """
        remaining = conn.execute(
            remaining_query, statuses + [days_old]
        ).fetchone()["count"]

        # Calculate how many we can safely archive
        total_query = "SELECT COUNT(*) as count FROM task_queue"
        total = conn.execute(total_query).fetchone()["count"]

        # Don't archive if we'd go below keep_recent threshold
        if total <= keep_recent:
            return {
                "archived": 0,
                "archived_ids": [],
                "reason": f"Total tasks ({total}) is at or below keep_recent threshold ({keep_recent})",
                "total_remaining": total,
            }

        # Find tasks to archive
        archive_query = """
            SELECT * FROM task_queue
            WHERE status IN ({status_placeholders})
              AND completed_at < datetime('now', '-' || ? || ' days')
            ORDER BY completed_at ASC
            LIMIT ?
        """
        tasks_to_archive = conn.execute(
            archive_query, statuses + [days_old, batch_size]
        ).fetchall()

        # Archive each task
        for task in tasks_to_archive:
            task_dict = dict(task)

            # Insert into archive
            conn.execute(
                """
                INSERT INTO task_archive (
                    original_id, task_type, task_data, priority, status,
                    assigned_node, assigned_worker, retries, max_retries,
                    timeout_seconds, error_message, created_at, started_at, completed_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    task_dict["id"],
                    task_dict["task_type"],
                    task_dict["task_data"],
                    task_dict["priority"],
                    task_dict["status"],
                    task_dict.get("assigned_node"),
                    task_dict.get("assigned_worker"),
                    task_dict["retries"],
                    task_dict["max_retries"],
                    task_dict.get("timeout_seconds"),
                    task_dict.get("error_message"),
                    task_dict["created_at"],
                    task_dict.get("started_at"),
                    task_dict.get("completed_at"),
                ),
            )

            # Delete from main queue
            conn.execute(
                "DELETE FROM task_queue WHERE id = ?", (task_dict["id"],)
            )

            archived_ids.append(task_dict["id"])
            archived_count += 1

        conn.commit()

        # Get new total
        new_total = conn.execute(total_query).fetchone()["count"]

    return {
        "archived": archived_count,
        "archived_ids": archived_ids,
        "days_old_threshold": days_old,
        "statuses": statuses,
        "total_remaining": new_total,
    }


def maybe_auto_archive():
    """Run auto-archive if enabled and interval has passed."""
    global _last_auto_archive

    if not TASK_ARCHIVE_CONFIG["enabled"]:
        return None

    now = datetime.now()
    interval_hours = TASK_ARCHIVE_CONFIG["auto_run_interval_hours"]

    if _last_auto_archive is not None:
        elapsed = (now - _last_auto_archive).total_seconds() / 3600
        if elapsed < interval_hours:
            return None

    # Run archive
    result = archive_old_tasks()
    _last_auto_archive = now

    if result["archived"] > 0:
        logger.info(f"Auto-archived {result['archived']} old tasks")

    return result


@app.route("/api/tasks/archive", methods=["POST"])
@require_auth
def run_archive_tasks():
    """Archive old completed/failed tasks.

    Request body (all optional):
        {
            "days_old": 7,
            "statuses": ["completed", "failed"],
            "batch_size": 100,
            "keep_recent": 1000
        }
    """
    data = request.get_json() or {}

    result = archive_old_tasks(
        days_old=data.get("days_old"),
        statuses=data.get("statuses"),
        batch_size=data.get("batch_size"),
        keep_recent=data.get("keep_recent"),
    )

    if result["archived"] > 0:
        log_activity(
            "archive_tasks",
            "task",
            None,
            f"Archived {result['archived']} tasks",
        )
        broadcast_queue()

    return jsonify(result)


@app.route("/api/tasks/archive/config", methods=["GET"])
@require_auth
def get_archive_config():
    """Get current task archive configuration."""
    return jsonify(TASK_ARCHIVE_CONFIG)


@app.route("/api/tasks/archive/config", methods=["POST"])
@require_auth
def update_archive_config():
    """Update task archive configuration.

    Request body (all optional):
        {
            "enabled": true,
            "archive_after_days": 7,
            "batch_size": 100,
            "keep_recent_count": 1000,
            "auto_run_interval_hours": 24
        }
    """
    data = request.get_json() or {}

    updated = []
    for key in [
        "enabled",
        "archive_after_days",
        "archive_statuses",
        "batch_size",
        "keep_recent_count",
        "auto_run_interval_hours",
    ]:
        if key in data:
            TASK_ARCHIVE_CONFIG[key] = data[key]
            updated.append(key)

    if updated:
        log_activity(
            "update_archive_config",
            "config",
            None,
            f"Updated: {', '.join(updated)}",
        )

    return jsonify(
        {"success": True, "updated": updated, "config": TASK_ARCHIVE_CONFIG}
    )


@app.route("/api/tasks/archive/stats", methods=["GET"])
@require_auth
def get_archive_stats():
    """Get statistics about archived tasks."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Archive counts
        archive_total = conn.execute(
            "SELECT COUNT(*) as count FROM task_archive"
        ).fetchone()["count"]

        # By status
        status_query = """
            SELECT status, COUNT(*) as count
            FROM task_archive
            GROUP BY status
        """
        by_status = {
            row["status"]: row["count"]
            for row in conn.execute(status_query).fetchall()
        }

        # By task type
        type_query = """
            SELECT task_type, COUNT(*) as count
            FROM task_archive
            GROUP BY task_type
            ORDER BY count DESC
            LIMIT 10
        """
        by_type = {
            row["task_type"]: row["count"]
            for row in conn.execute(type_query).fetchall()
        }

        # Archive date range
        date_range = conn.execute(
            """
            SELECT MIN(archived_at) as oldest, MAX(archived_at) as newest
            FROM task_archive
        """
        ).fetchone()

        # Current queue stats
        queue_total = conn.execute(
            "SELECT COUNT(*) as count FROM task_queue"
        ).fetchone()["count"]

        archivable = conn.execute(
            """
            SELECT COUNT(*) as count FROM task_queue
            WHERE status IN ('completed', 'failed')
              AND completed_at < datetime('now', '-' || ? || ' days')
        """,
            [TASK_ARCHIVE_CONFIG["archive_after_days"]],
        ).fetchone()["count"]

        return jsonify(
            {
                "archive": {
                    "total": archive_total,
                    "by_status": by_status,
                    "by_type": by_type,
                    "oldest": date_range["oldest"],
                    "newest": date_range["newest"],
                },
                "queue": {"total": queue_total, "archivable": archivable},
                "config": TASK_ARCHIVE_CONFIG,
            }
        )


@app.route("/api/tasks/archive/search", methods=["GET"])
@require_auth
def search_archived_tasks():
    """Search archived tasks.

    Query params:
        task_type: Filter by task type
        status: Filter by status
        from_date: Filter by archived_at >= date
        to_date: Filter by archived_at <= date
        limit: Max results (default 50, max 200)
        offset: Pagination offset
    """
    task_type = request.args.get("task_type")
    status = request.args.get("status")
    from_date = request.args.get("from_date")
    to_date = request.args.get("to_date")
    limit = min(request.args.get("limit", 50, type=int), 200)
    offset = request.args.get("offset", 0, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        query = "SELECT * FROM task_archive WHERE 1=1"
        params = []

        if task_type:
            query += " AND task_type = ?"
            params.append(task_type)
        if status:
            query += " AND status = ?"
            params.append(status)
        if from_date:
            query += " AND archived_at >= ?"
            params.append(from_date)
        if to_date:
            query += " AND archived_at <= ?"
            params.append(to_date)

        query += " ORDER BY archived_at DESC LIMIT ? OFFSET ?"
        params.extend([limit, offset])

        tasks = conn.execute(query, params).fetchall()

        return jsonify(
            {
                "tasks": [dict(t) for t in tasks],
                "count": len(tasks),
                "limit": limit,
                "offset": offset,
            }
        )


@app.route("/api/tasks/claim", methods=["POST"])
def claim_task():
    """Claim a pending task for a worker.

    Only claims tasks that:
    - Are pending
    - Have not exceeded retry limit
    - Are not blocked by incomplete dependencies

    Tasks are ordered by effective priority which includes aging:
    effective_priority = priority + min(max_age_bonus, age_in_minutes * aging_factor)
    This prevents task starvation by gradually increasing priority of waiting tasks.
    """
    data = request.get_json()
    worker_id = data.get("worker_id")
    task_types = data.get("task_types", [])

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Find a pending task that is not blocked by incomplete dependencies
        # Order by effective priority (base priority + aging bonus)
        query = """
            SELECT *,
                (priority + MIN(?, (strftime('%s', 'now') - strftime('%s', created_at)) / 60.0 * ?)) as effective_priority
            FROM task_queue
            WHERE status = 'pending' AND retries < max_retries
            AND id NOT IN (
                SELECT td.task_id FROM task_dependencies td
                JOIN task_queue tq ON td.depends_on_id = tq.id
                WHERE tq.status NOT IN ('completed', 'failed')
            )
        """
        # Start with aging parameters
        params = [TASK_PRIORITY_MAX_AGE_BONUS, TASK_PRIORITY_AGING_FACTOR]

        if task_types:
            placeholders = ",".join("?" * len(task_types))
            query += f" AND task_type IN ({placeholders})"
            params.extend(task_types)

        query += " ORDER BY effective_priority DESC, created_at ASC LIMIT 1"

        task = conn.execute(query, params).fetchone()

        if task:
            # Claim the task
            conn.execute(
                """
                UPDATE task_queue SET
                    status = 'running',
                    assigned_worker = ?,
                    started_at = CURRENT_TIMESTAMP
                WHERE id = ?
            """,
                (worker_id, task["id"]),
            )

            # Trigger webhook notification for task claimed
            trigger_task_webhook(
                task_id=task["id"],
                task_type=task["task_type"],
                new_status="running",
                old_status="pending",
                worker_id=worker_id,
                task_data=dict(task),
            )

            return jsonify({"task": dict(task), "success": True})

        return jsonify({"task": None, "success": True})


@app.route("/api/tasks/claim-balanced", methods=["POST"])
def claim_task_balanced():
    """Claim a pending task with automatic worker selection via load balancing.

    Uses the load balancer to select the best available worker based on
    current load, capacity, and optionally skill matching.

    Request body (all optional):
        - task_types: List of task types to claim (optional filter)
        - strategy: Load balancing strategy (default: least_loaded)
        - worker_type: Filter by worker type (optional)

    Returns:
        - task: The claimed task (or null if none available)
        - worker: Selected worker info
        - strategy_used: Load balancing strategy used
    """
    from services.load_balancer import LoadBalancingStrategy, get_load_balancer

    data = request.get_json() or {}
    task_types = data.get("task_types", [])
    strategy_name = data.get("strategy", "least_loaded")
    worker_type = data.get("worker_type")

    try:
        strategy = LoadBalancingStrategy(strategy_name)
    except ValueError:
        return (
            jsonify(
                {
                    "success": False,
                    "error": f"Invalid strategy: {strategy_name}",
                }
            ),
            400,
        )

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # First find an available task
        query = """
            SELECT *,
                (priority + MIN(?, (strftime('%s', 'now') - strftime('%s', created_at)) / 60.0 * ?)) as effective_priority
            FROM task_queue
            WHERE status = 'pending' AND retries < max_retries
            AND id NOT IN (
                SELECT td.task_id FROM task_dependencies td
                JOIN task_queue tq ON td.depends_on_id = tq.id
                WHERE tq.status NOT IN ('completed', 'failed')
            )
        """
        params = [TASK_PRIORITY_MAX_AGE_BONUS, TASK_PRIORITY_AGING_FACTOR]

        if task_types:
            placeholders = ",".join("?" * len(task_types))
            query += f" AND task_type IN ({placeholders})"
            params.extend(task_types)

        query += " ORDER BY effective_priority DESC, created_at ASC LIMIT 1"
        task = conn.execute(query, params).fetchone()

        if not task:
            return jsonify(
                {
                    "task": None,
                    "worker": None,
                    "success": True,
                    "message": "No pending tasks available",
                }
            )

        # Use load balancer to select best worker
        try:
            lb = get_load_balancer(DB_PATH)
            selection = lb.select_worker(
                task_type=task["task_type"],
                strategy=strategy,
                worker_type=worker_type,
            )

            if not selection.worker_id:
                return (
                    jsonify(
                        {
                            "task": dict(task),
                            "worker": None,
                            "success": False,
                            "error": "No workers available",
                            "strategy_used": selection.strategy_used,
                        }
                    ),
                    503,
                )

            worker_id = selection.worker_id
        except Exception as e:
            logger.error(f"Load balancer error: {e}")
            return (
                jsonify(
                    {
                        "success": False,
                        "error": f"Load balancer error: {str(e)}",
                    }
                ),
                500,
            )

        # Claim the task with the selected worker
        conn.execute(
            """
            UPDATE task_queue SET
                status = 'running',
                assigned_worker = ?,
                started_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (worker_id, task["id"]),
        )

        # Trigger webhook notification
        trigger_task_webhook(
            task_id=task["id"],
            task_type=task["task_type"],
            new_status="running",
            old_status="pending",
            worker_id=worker_id,
            task_data=dict(task),
        )

        return jsonify(
            {
                "task": dict(task),
                "worker": {
                    "worker_id": selection.worker_id,
                    "worker_type": selection.worker_type,
                    "node_id": selection.node_id,
                    "load_before": round(selection.load_before, 1),
                    "load_after": round(selection.estimated_load_after, 1),
                },
                "strategy_used": selection.strategy_used,
                "success": True,
            }
        )


# ============================================================================
# BULK TASK ACTIONS API
# ============================================================================


@app.route("/api/tasks/bulk/select", methods=["POST"])
@require_auth
def bulk_select_tasks():
    """Get tasks matching selection criteria for bulk operations."""
    data = request.get_json() or {}
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        query = "SELECT id, task_type, status, priority, created_at, started_at, completed_at FROM task_queue WHERE 1=1"
        params = []
        if data.get("status"):
            query += " AND status = ?"
            params.append(data["status"])
        if data.get("task_type"):
            query += " AND task_type = ?"
            params.append(data["task_type"])
        if data.get("priority_min") is not None:
            query += " AND priority >= ?"
            params.append(data["priority_min"])
        if data.get("priority_max") is not None:
            query += " AND priority <= ?"
            params.append(data["priority_max"])
        if data.get("created_after"):
            query += " AND created_at >= ?"
            params.append(data["created_after"])
        if data.get("created_before"):
            query += " AND created_at <= ?"
            params.append(data["created_before"])
        if data.get("search"):
            query += " AND task_data LIKE ?"
            params.append(f"%{data['search']}%")
        limit = min(int(data.get("limit", 100)), 500)
        query += " ORDER BY created_at DESC LIMIT ?"
        params.append(limit)
        tasks = conn.execute(query, params).fetchall()
    return jsonify({"tasks": [dict(t) for t in tasks], "count": len(tasks)})


@app.route("/api/tasks/bulk/status", methods=["PUT"])
@require_auth
def bulk_update_status():
    """Update status for multiple tasks."""
    data = request.get_json() or {}
    task_ids = data.get("task_ids", [])
    new_status = data.get("status")
    if not task_ids:
        return jsonify({"error": "task_ids is required"}), 400
    if not new_status or new_status not in [
        "pending",
        "completed",
        "failed",
        "cancelled",
    ]:
        return (
            jsonify(
                {
                    "error": "Invalid status. Use: pending, completed, failed, cancelled"
                }
            ),
            400,
        )
    if len(task_ids) > 500:
        return jsonify({"error": "Maximum 500 tasks per request"}), 400
    with get_db_connection() as conn:
        placeholders = ",".join("?" * len(task_ids))
        if new_status == "completed":
            conn.execute(
                f"UPDATE task_queue SET status=?, completed_at=CURRENT_TIMESTAMP WHERE id IN ({placeholders})",
                [new_status] + task_ids,
            )
        elif new_status == "pending":
            conn.execute(
                f"UPDATE task_queue SET status=?, started_at=NULL, completed_at=NULL, assigned_worker=NULL WHERE id IN ({placeholders})",
                [new_status] + task_ids,
            )
        else:
            conn.execute(
                f"UPDATE task_queue SET status=? WHERE id IN ({placeholders})",
                [new_status] + task_ids,
            )
        affected = conn.execute(
            f"SELECT COUNT(*) FROM task_queue WHERE id IN ({placeholders})",
            task_ids,
        ).fetchone()[0]
    log_activity(
        "bulk_update_status",
        "task",
        None,
        f"status={new_status}, count={len(task_ids)}",
    )
    broadcast_queue()
    return jsonify(
        {"success": True, "updated": affected, "status": new_status}
    )


@app.route("/api/tasks/bulk/priority", methods=["PUT"])
@require_auth
def bulk_update_priority():
    """Update priority for multiple tasks."""
    data = request.get_json() or {}
    task_ids = data.get("task_ids", [])
    priority = data.get("priority")
    adjust = data.get("adjust", False)
    if not task_ids:
        return jsonify({"error": "task_ids is required"}), 400
    if priority is None:
        return jsonify({"error": "priority is required"}), 400
    if len(task_ids) > 500:
        return jsonify({"error": "Maximum 500 tasks per request"}), 400
    with get_db_connection() as conn:
        placeholders = ",".join("?" * len(task_ids))
        if adjust:
            conn.execute(
                f"UPDATE task_queue SET priority = priority + ? WHERE id IN ({placeholders})",
                [priority] + task_ids,
            )
        else:
            conn.execute(
                f"UPDATE task_queue SET priority = ? WHERE id IN ({placeholders})",
                [priority] + task_ids,
            )
        affected = conn.execute(
            f"SELECT COUNT(*) FROM task_queue WHERE id IN ({placeholders})",
            task_ids,
        ).fetchone()[0]
    log_activity(
        "bulk_update_priority",
        "task",
        None,
        f"priority={priority}, adjust={adjust}, count={len(task_ids)}",
    )
    broadcast_queue()
    return jsonify({"success": True, "updated": affected})


@app.route("/api/tasks/bulk/delete", methods=["POST"])
@require_auth
def bulk_delete_tasks():
    """Delete multiple tasks.

    Body:
        task_ids: List of task IDs to delete
        archive: If True, archive before deleting
        cascade: If True, also delete all child tasks
    """
    data = request.get_json() or {}
    task_ids = data.get("task_ids", [])
    archive = data.get("archive", False)
    cascade = data.get("cascade", False)
    if not task_ids:
        return jsonify({"error": "task_ids is required"}), 400
    if len(task_ids) > 500:
        return jsonify({"error": "Maximum 500 tasks per request"}), 400
    with get_db_connection() as conn:
        # If cascade, find all descendant tasks
        all_ids = list(task_ids)
        if cascade:
            for tid in task_ids:
                # Find all descendants using hierarchy_path
                descendants = conn.execute(
                    "SELECT id FROM task_queue WHERE hierarchy_path LIKE ?",
                    (f"%/{tid}/%",),
                ).fetchall()
                all_ids.extend([d["id"] for d in descendants])
            all_ids = list(set(all_ids))  # Remove duplicates

        placeholders = ",".join("?" * len(all_ids))
        if archive:
            conn.execute(
                """
                INSERT INTO task_archive (original_id, task_type, task_data, priority, status, assigned_node,
                    assigned_worker, retries, max_retries, timeout_seconds, error_message,
                    parent_id, hierarchy_level, hierarchy_path, child_count,
                    created_at, started_at, completed_at, archived_at, archive_reason)
                SELECT id, task_type, task_data, priority, status, assigned_node, assigned_worker, retries,
                    max_retries, timeout_seconds, error_message,
                    parent_id, hierarchy_level, hierarchy_path, child_count,
                    created_at, started_at, completed_at, CURRENT_TIMESTAMP, 'bulk_archive'
                FROM task_queue WHERE id IN ({placeholders})
            """,
                all_ids,
            )

        # Update parent child_count for tasks being deleted
        for tid in all_ids:
            task = conn.execute(
                "SELECT parent_id FROM task_queue WHERE id = ?", (tid,)
            ).fetchone()
            if task and task["parent_id"] and task["parent_id"] not in all_ids:
                conn.execute(
                    "UPDATE task_queue SET child_count = child_count - 1 WHERE id = ? AND child_count > 0",
                    (task["parent_id"],),
                )

        conn.execute(
            f"DELETE FROM task_queue WHERE id IN ({placeholders})", all_ids
        )
    log_activity(
        "bulk_delete",
        "task",
        None,
        f"archive={archive}, cascade={cascade}, count={len(all_ids)}",
    )
    broadcast_queue()
    return jsonify(
        {
            "success": True,
            "deleted": len(all_ids),
            "archived": archive,
            "cascaded": cascade,
        }
    )


@app.route("/api/tasks/bulk/assign", methods=["PUT"])
@require_auth
def bulk_assign_tasks():
    """Assign multiple tasks to a worker or node."""
    data = request.get_json() or {}
    task_ids = data.get("task_ids", [])
    worker_id = data.get("worker_id")
    node_id = data.get("node_id")
    if not task_ids:
        return jsonify({"error": "task_ids is required"}), 400
    if not worker_id and not node_id:
        return jsonify({"error": "worker_id or node_id is required"}), 400
    if len(task_ids) > 500:
        return jsonify({"error": "Maximum 500 tasks per request"}), 400
    with get_db_connection() as conn:
        placeholders = ",".join("?" * len(task_ids))
        updates = []
        params = []
        if worker_id:
            updates.append("assigned_worker = ?")
            params.append(worker_id)
        if node_id:
            updates.append("assigned_node = ?")
            params.append(node_id)
        params.extend(task_ids)
        conn.execute(
            f"UPDATE task_queue SET {
                ', '.join(updates)} WHERE id IN ({placeholders})",
            params,
        )
    log_activity(
        "bulk_assign",
        "task",
        None,
        f"worker={worker_id}, node={node_id}, count={len(task_ids)}",
    )
    broadcast_queue()
    return jsonify({"success": True, "assigned": len(task_ids)})


@app.route("/api/tasks/bulk/retry", methods=["POST"])
@require_auth
def bulk_retry_tasks():
    """Reset failed tasks to pending for retry."""
    data = request.get_json() or {}
    task_ids = data.get("task_ids", [])
    reset_retries = data.get("reset_retries", False)
    if not task_ids:
        return jsonify({"error": "task_ids is required"}), 400
    if len(task_ids) > 500:
        return jsonify({"error": "Maximum 500 tasks per request"}), 400
    with get_db_connection() as conn:
        placeholders = ",".join("?" * len(task_ids))
        if reset_retries:
            conn.execute(
                """
                UPDATE task_queue SET status='pending', retries=0, error_message=NULL,
                    started_at=NULL, completed_at=NULL, assigned_worker=NULL
                WHERE id IN ({placeholders}) AND status IN ('failed', 'cancelled')
            """,
                task_ids,
            )
        else:
            conn.execute(
                """
                UPDATE task_queue SET status='pending', error_message=NULL,
                    started_at=NULL, completed_at=NULL, assigned_worker=NULL
                WHERE id IN ({placeholders}) AND status IN ('failed', 'cancelled')
            """,
                task_ids,
            )
        affected = conn.execute(
            f"SELECT COUNT(*) FROM task_queue WHERE id IN ({placeholders}) AND status='pending'",
            task_ids,
        ).fetchone()[0]
    log_activity(
        "bulk_retry",
        "task",
        None,
        f"reset_retries={reset_retries}, count={len(task_ids)}",
    )
    broadcast_queue()
    return jsonify({"success": True, "retried": affected})


@app.route("/api/tasks/bulk/clone", methods=["POST"])
@require_auth
def bulk_clone_tasks():
    """Clone multiple tasks."""
    data = request.get_json() or {}
    task_ids = data.get("task_ids", [])
    reset_status = data.get("reset_status", True)
    if not task_ids:
        return jsonify({"error": "task_ids is required"}), 400
    if len(task_ids) > 100:
        return jsonify({"error": "Maximum 100 tasks per clone request"}), 400
    new_ids = []
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        placeholders = ",".join("?" * len(task_ids))
        tasks = conn.execute(
            f"SELECT * FROM task_queue WHERE id IN ({placeholders})", task_ids
        ).fetchall()
        for task in tasks:
            status = "pending" if reset_status else task["status"]
            cursor = conn.execute(
                """
                INSERT INTO task_queue (task_type, task_data, priority, status, max_retries, timeout_seconds, story_points, estimated_hours)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    task["task_type"],
                    task["task_data"],
                    task["priority"],
                    status,
                    task["max_retries"],
                    task["timeout_seconds"],
                    task["story_points"],
                    task["estimated_hours"],
                ),
            )
            new_ids.append(cursor.lastrowid)
    log_activity("bulk_clone", "task", None, f"count={len(task_ids)}")
    broadcast_queue()
    return jsonify(
        {"success": True, "cloned": len(new_ids), "new_ids": new_ids}
    )


@app.route("/api/tasks/bulk/tag", methods=["PUT"])
@require_auth
def bulk_tag_tasks():
    """Add or remove tags from multiple tasks."""
    data = request.get_json() or {}
    task_ids = data.get("task_ids", [])
    add_tags = data.get("add_tags", [])
    remove_tags = data.get("remove_tags", [])
    if not task_ids:
        return jsonify({"error": "task_ids is required"}), 400
    if not add_tags and not remove_tags:
        return jsonify({"error": "add_tags or remove_tags is required"}), 400
    if len(task_ids) > 500:
        return jsonify({"error": "Maximum 500 tasks per request"}), 400
    updated = 0
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        placeholders = ",".join("?" * len(task_ids))
        tasks = conn.execute(
            f"SELECT id, task_data FROM task_queue WHERE id IN ({placeholders})",
            task_ids,
        ).fetchall()
        for task in tasks:
            try:
                task_data = (
                    json.loads(task["task_data"]) if task["task_data"] else {}
                )
            except json.JSONDecodeError:
                task_data = {}
            tags = set(task_data.get("tags", []))
            if add_tags:
                tags.update(add_tags)
            if remove_tags:
                tags -= set(remove_tags)
            task_data["tags"] = list(tags)
            conn.execute(
                "UPDATE task_queue SET task_data = ? WHERE id = ?",
                (json.dumps(task_data), task["id"]),
            )
            updated += 1
    log_activity(
        "bulk_tag",
        "task",
        None,
        f"add={add_tags}, remove={remove_tags}, count={updated}",
    )
    return jsonify({"success": True, "updated": updated})


@app.route("/api/tasks/batch/move", methods=["POST"])
@require_auth
def batch_move_tasks():
    """Move multiple tasks to a different project, milestone, category, or position.

    Request body:
        task_ids: List of task IDs to move (required)
        target_project_id: Move tasks to this project (optional)
        target_milestone_id: Move tasks to this milestone (optional)
        target_category: Move tasks to this category (optional)
        target_parent_id: Set parent task for all tasks (optional)
        position: Move tasks to this queue position (optional)
            - 'top': Move to top of queue
            - 'bottom': Move to bottom of queue
            - integer: Move to specific priority value
        preserve_order: Keep relative order when moving (default: true)
        update_status: Optionally update status when moving (optional)

    Returns:
        success: True if operation succeeded
        moved: Number of tasks moved
        details: Operation details
    """
    data = request.get_json()
    if not data:
        return api_error("Request body is required", 400, "validation_error")

    task_ids = data.get("task_ids", [])
    if not task_ids:
        return api_error("task_ids is required", 400, "validation_error")

    if len(task_ids) > 500:
        return api_error(
            "Maximum 500 tasks per request", 400, "validation_error"
        )

    target_project_id = data.get("target_project_id")
    target_milestone_id = data.get("target_milestone_id")
    target_category = data.get("target_category")
    target_parent_id = data.get("target_parent_id")
    position = data.get("position")
    preserve_order = data.get("preserve_order", True)
    update_status = data.get("update_status")

    # At least one target must be specified
    if not any(
        [
            target_project_id,
            target_milestone_id,
            target_category,
            target_parent_id is not None,
            position,
        ]
    ):
        return api_error(
            "At least one target field is required", 400, "validation_error"
        )

    moved = 0
    details = {"operations": []}

    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            placeholders = ",".join("?" * len(task_ids))

            # Get current tasks
            tasks = conn.execute(
                f"SELECT id, task_data, priority FROM task_queue WHERE id IN ({placeholders})",
                task_ids,
            ).fetchall()

            if not tasks:
                return api_error(
                    "No tasks found with provided IDs", 404, "not_found"
                )

            # Validate targets exist
            if target_project_id:
                project = conn.execute(
                    "SELECT id, name FROM projects WHERE id = ?",
                    (target_project_id,),
                ).fetchone()
                if not project:
                    return api_error(
                        f"Project {target_project_id} not found",
                        404,
                        "not_found",
                    )
                details["target_project"] = project["name"]

            if target_milestone_id:
                milestone = conn.execute(
                    "SELECT id, name FROM milestones WHERE id = ?",
                    (target_milestone_id,),
                ).fetchone()
                if not milestone:
                    return api_error(
                        f"Milestone {target_milestone_id} not found",
                        404,
                        "not_found",
                    )
                details["target_milestone"] = milestone["name"]

            if target_parent_id:
                parent = conn.execute(
                    "SELECT id FROM task_queue WHERE id = ?",
                    (target_parent_id,),
                ).fetchone()
                if not parent:
                    return api_error(
                        f"Parent task {target_parent_id} not found",
                        404,
                        "not_found",
                    )
                # Ensure we're not creating circular reference
                if target_parent_id in task_ids:
                    return api_error(
                        "Cannot set a task as its own parent",
                        400,
                        "validation_error",
                    )

            # Calculate new priorities if repositioning
            new_priority = None
            if position:
                if position == "top":
                    max_priority = (
                        conn.execute(
                            "SELECT MAX(priority) FROM task_queue"
                        ).fetchone()[0]
                        or 0
                    )
                    new_priority = max_priority + 1
                    details["operations"].append("moved_to_top")
                elif position == "bottom":
                    min_priority = (
                        conn.execute(
                            "SELECT MIN(priority) FROM task_queue"
                        ).fetchone()[0]
                        or 0
                    )
                    new_priority = min_priority - 1
                    details["operations"].append("moved_to_bottom")
                elif isinstance(position, int):
                    new_priority = position
                    details["operations"].append(f"priority_set_to_{position}")

            # Process each task
            for i, task in enumerate(tasks):
                task_data = (
                    json.loads(task["task_data"]) if task["task_data"] else {}
                )
                updates = []
                params = []

                # Update task_data fields
                data_changed = False

                if target_project_id:
                    task_data["project_id"] = target_project_id
                    data_changed = True
                    if "project_id" not in [
                        op for op in details["operations"]
                    ]:
                        details["operations"].append("project_updated")

                if target_milestone_id:
                    task_data["milestone_id"] = target_milestone_id
                    data_changed = True
                    if "milestone_id" not in [
                        op for op in details["operations"]
                    ]:
                        details["operations"].append("milestone_updated")

                if target_category:
                    task_data["category"] = target_category
                    data_changed = True
                    if "category" not in [op for op in details["operations"]]:
                        details["operations"].append("category_updated")

                if target_parent_id is not None:
                    task_data["parent_id"] = (
                        target_parent_id if target_parent_id else None
                    )
                    data_changed = True
                    if "parent" not in [op for op in details["operations"]]:
                        details["operations"].append("parent_updated")

                if data_changed:
                    updates.append("task_data = ?")
                    params.append(json.dumps(task_data))

                # Update priority
                if new_priority is not None:
                    if preserve_order:
                        # Preserve relative order by incrementing
                        updates.append("priority = ?")
                        params.append(
                            new_priority - i
                            if position == "top"
                            else new_priority + i
                        )
                    else:
                        updates.append("priority = ?")
                        params.append(new_priority)

                # Update status if requested
                if update_status:
                    updates.append("status = ?")
                    params.append(update_status)
                    if "status" not in [op for op in details["operations"]]:
                        details["operations"].append(
                            f"status_set_to_{update_status}"
                        )

                if updates:
                    params.append(task["id"])
                    conn.execute(
                        f"UPDATE task_queue SET {
                            ', '.join(updates)} WHERE id = ?",
                        params,
                    )
                    moved += 1

            # Update task dependencies if moving to new parent
            if target_parent_id:
                for task_id in task_ids:
                    # Check if dependency already exists
                    existing = conn.execute(
                        "SELECT id FROM task_dependencies WHERE task_id = ? AND depends_on_id = ?",
                        (task_id, target_parent_id),
                    ).fetchone()
                    if not existing:
                        try:
                            conn.execute(
                                "INSERT INTO task_dependencies (task_id, depends_on_id) VALUES (?, ?)",
                                (task_id, target_parent_id),
                            )
                        except sqlite3.IntegrityError:
                            pass  # Ignore if already exists

        log_activity(
            "batch_move",
            "task",
            None,
            f"moved={moved}, project={target_project_id}, milestone={target_milestone_id}, "
            + f"category={target_category}, position={position}",
        )

        broadcast_queue()

        return jsonify(
            {
                "success": True,
                "moved": moved,
                "total_requested": len(task_ids),
                "details": details,
            }
        )

    except sqlite3.Error as e:
        logger.error(f"Database error in batch move: {e}")
        return api_error(
            "Database error during batch move", 500, "database_error"
        )
    except json.JSONDecodeError as e:
        logger.error(f"JSON error in batch move: {e}")
        return api_error("Invalid task data format", 500, "data_error")


@app.route("/api/tasks/batch/reorder", methods=["POST"])
@require_auth
def batch_reorder_tasks():
    """Reorder tasks by setting explicit priorities.

    Request body:
        task_order: List of {task_id, priority} or just [task_id, task_id, ...]
            If just IDs, priorities are assigned in descending order
        base_priority: Starting priority when using ID list (default: 1000)

    Returns:
        success: True if operation succeeded
        reordered: Number of tasks reordered
    """
    data = request.get_json()
    if not data:
        return api_error("Request body is required", 400, "validation_error")

    task_order = data.get("task_order", [])
    if not task_order:
        return api_error("task_order is required", 400, "validation_error")

    if len(task_order) > 500:
        return api_error(
            "Maximum 500 tasks per request", 400, "validation_error"
        )

    base_priority = data.get("base_priority", 1000)
    reordered = 0

    try:
        with get_db_connection() as conn:
            # Determine format: list of IDs or list of {task_id, priority}
            if isinstance(task_order[0], dict):
                # Explicit priorities
                for item in task_order:
                    task_id = item.get("task_id")
                    priority = item.get("priority")
                    if task_id and priority is not None:
                        conn.execute(
                            "UPDATE task_queue SET priority = ? WHERE id = ?",
                            (priority, task_id),
                        )
                        reordered += 1
            else:
                # List of IDs - assign descending priorities
                for i, task_id in enumerate(task_order):
                    priority = base_priority - i
                    conn.execute(
                        "UPDATE task_queue SET priority = ? WHERE id = ?",
                        (priority, task_id),
                    )
                    reordered += 1

        log_activity("batch_reorder", "task", None, f"reordered={reordered}")
        broadcast_queue()

        return jsonify({"success": True, "reordered": reordered})

    except sqlite3.Error as e:
        logger.error(f"Database error in batch reorder: {e}")
        return api_error(
            "Database error during reorder", 500, "database_error"
        )


@app.route("/api/tasks/bulk/actions", methods=["GET"])
@require_auth
def get_bulk_actions():
    """Get available bulk actions and their descriptions."""
    return jsonify(
        {
            "actions": [
                {
                    "action": "status",
                    "method": "PUT",
                    "endpoint": "/api/tasks/bulk/status",
                    "description": "Update status for multiple tasks",
                },
                {
                    "action": "priority",
                    "method": "PUT",
                    "endpoint": "/api/tasks/bulk/priority",
                    "description": "Update priority for multiple tasks",
                },
                {
                    "action": "delete",
                    "method": "POST",
                    "endpoint": "/api/tasks/bulk/delete",
                    "description": "Delete multiple tasks",
                },
                {
                    "action": "assign",
                    "method": "PUT",
                    "endpoint": "/api/tasks/bulk/assign",
                    "description": "Assign tasks to worker or node",
                },
                {
                    "action": "retry",
                    "method": "POST",
                    "endpoint": "/api/tasks/bulk/retry",
                    "description": "Reset failed tasks to pending",
                },
                {
                    "action": "clone",
                    "method": "POST",
                    "endpoint": "/api/tasks/bulk/clone",
                    "description": "Clone multiple tasks",
                },
                {
                    "action": "tag",
                    "method": "PUT",
                    "endpoint": "/api/tasks/bulk/tag",
                    "description": "Add or remove tags from tasks",
                },
                {
                    "action": "move",
                    "method": "POST",
                    "endpoint": "/api/tasks/batch/move",
                    "description": "Move tasks to different project, milestone, category, or position",
                },
                {
                    "action": "reorder",
                    "method": "POST",
                    "endpoint": "/api/tasks/batch/reorder",
                    "description": "Reorder tasks by setting explicit priorities",
                },
            ]
        }
    )


# ============================================================================
# TASK LINKING API (Related Tasks, Duplicates)
# ============================================================================

# Link types: related, duplicate, parent, child, blocks, blocked_by,
# cloned_from, cloned_to
TASK_LINK_TYPES = {
    "related": {"description": "Related task", "bidirectional": True},
    "duplicate": {
        "description": "Duplicate of another task",
        "bidirectional": True,
    },
    "parent": {"description": "Parent task", "inverse": "child"},
    "child": {"description": "Child/subtask", "inverse": "parent"},
    "blocks": {"description": "Blocks another task", "inverse": "blocked_by"},
    "blocked_by": {
        "description": "Blocked by another task",
        "inverse": "blocks",
    },
    "cloned_from": {
        "description": "Cloned from another task",
        "inverse": "cloned_to",
    },
    "cloned_to": {
        "description": "Cloned to another task",
        "inverse": "cloned_from",
    },
    "causes": {
        "description": "Causes/triggers another task",
        "inverse": "caused_by",
    },
    "caused_by": {
        "description": "Caused by another task",
        "inverse": "causes",
    },
}


@app.route("/api/tasks/link-types", methods=["GET"])
@require_auth
def get_task_link_types():
    """Get available task link types."""
    return jsonify({"link_types": TASK_LINK_TYPES})


@app.route("/api/tasks/<int:task_id>/links", methods=["GET"])
@require_auth
def get_task_links(task_id):
    """Get all links for a task."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        # Get outgoing links (this task is source)
        outgoing = conn.execute(
            """
            SELECT l.*, t.task_type, t.status, t.priority, t.created_at as task_created
            FROM task_links l
            JOIN task_queue t ON l.target_task_id = t.id
            WHERE l.source_task_id = ?
            ORDER BY l.link_type, l.created_at
        """,
            (task_id,),
        ).fetchall()
        # Get incoming links (this task is target)
        incoming = conn.execute(
            """
            SELECT l.*, t.task_type, t.status, t.priority, t.created_at as task_created
            FROM task_links l
            JOIN task_queue t ON l.source_task_id = t.id
            WHERE l.target_task_id = ?
            ORDER BY l.link_type, l.created_at
        """,
            (task_id,),
        ).fetchall()
        # Process and format results
        links = []
        for link in outgoing:
            link_data = dict(link)
            link_data["direction"] = "outgoing"
            link_data["linked_task_id"] = link["target_task_id"]
            links.append(link_data)
        for link in incoming:
            link_data = dict(link)
            link_data["direction"] = "incoming"
            link_data["linked_task_id"] = link["source_task_id"]
            # Show inverse link type for incoming
            link_type_info = TASK_LINK_TYPES.get(link["link_type"], {})
            if "inverse" in link_type_info:
                link_data["display_type"] = link_type_info["inverse"]
            else:
                link_data["display_type"] = link["link_type"]
            links.append(link_data)
        # Group by type
        by_type = {}
        for link in links:
            lt = link.get("display_type", link["link_type"])
            if lt not in by_type:
                by_type[lt] = []
            by_type[lt].append(link)
    return jsonify(
        {
            "links": links,
            "by_type": by_type,
            "count": len(links),
            "task_id": task_id,
        }
    )


@app.route("/api/tasks/<int:task_id>/links", methods=["POST"])
@require_auth
def create_task_link(task_id):
    """Create a link between two tasks."""
    data = request.get_json() or {}
    target_task_id = data.get("target_task_id")
    link_type = data.get("link_type", "related")
    description = data.get("description")
    if not target_task_id:
        return jsonify({"error": "target_task_id is required"}), 400
    if target_task_id == task_id:
        return jsonify({"error": "Cannot link task to itself"}), 400
    if link_type not in TASK_LINK_TYPES:
        return (
            jsonify(
                {
                    "error": f"Invalid link_type. Valid types: {list(TASK_LINK_TYPES.keys())}"
                }
            ),
            400,
        )
    with get_db_connection() as conn:
        # Verify both tasks exist
        source = conn.execute(
            "SELECT id FROM task_queue WHERE id=?", (task_id,)
        ).fetchone()
        target = conn.execute(
            "SELECT id FROM task_queue WHERE id=?", (target_task_id,)
        ).fetchone()
        if not source:
            return jsonify({"error": "Source task not found"}), 404
        if not target:
            return jsonify({"error": "Target task not found"}), 404
        # Check for existing link
        existing = conn.execute(
            "SELECT id FROM task_links WHERE source_task_id=? AND target_task_id=? AND link_type=?",
            (task_id, target_task_id, link_type),
        ).fetchone()
        if existing:
            return (
                jsonify(
                    {"error": "Link already exists", "link_id": existing[0]}
                ),
                409,
            )
        # Create the link
        cursor = conn.execute(
            """
            INSERT INTO task_links (source_task_id, target_task_id, link_type, description, created_by)
            VALUES (?, ?, ?, ?, ?)
        """,
            (
                task_id,
                target_task_id,
                link_type,
                description,
                session.get("user", "unknown"),
            ),
        )
        link_id = cursor.lastrowid
        # For bidirectional types, create reverse link too
        link_info = TASK_LINK_TYPES.get(link_type, {})
        reverse_link_id = None
        if link_info.get("bidirectional"):
            try:
                cursor2 = conn.execute(
                    """
                    INSERT INTO task_links (source_task_id, target_task_id, link_type, description, created_by)
                    VALUES (?, ?, ?, ?, ?)
                """,
                    (
                        target_task_id,
                        task_id,
                        link_type,
                        description,
                        session.get("user", "unknown"),
                    ),
                )
                reverse_link_id = cursor2.lastrowid
            except sqlite3.IntegrityError:
                pass  # Reverse already exists
        log_activity(
            "create_task_link",
            "task",
            task_id,
            f"target={target_task_id}, type={link_type}",
        )
    return jsonify(
        {
            "success": True,
            "link_id": link_id,
            "reverse_link_id": reverse_link_id,
        }
    )


@app.route("/api/tasks/<int:task_id>/links/<int:link_id>", methods=["DELETE"])
@require_auth
def delete_task_link(task_id, link_id):
    """Delete a task link."""
    with get_db_connection() as conn:
        # Get link info before deleting
        conn.row_factory = sqlite3.Row
        link = conn.execute(
            "SELECT * FROM task_links WHERE id=?", (link_id,)
        ).fetchone()
        if not link:
            return jsonify({"error": "Link not found"}), 404
        if (
            link["source_task_id"] != task_id
            and link["target_task_id"] != task_id
        ):
            return jsonify({"error": "Link does not belong to this task"}), 403
        # Delete the link
        conn.execute("DELETE FROM task_links WHERE id=?", (link_id,))
        # For bidirectional types, also delete reverse
        link_info = TASK_LINK_TYPES.get(link["link_type"], {})
        if link_info.get("bidirectional"):
            conn.execute(
                "DELETE FROM task_links WHERE source_task_id=? AND target_task_id=? AND link_type=?",
                (
                    link["target_task_id"],
                    link["source_task_id"],
                    link["link_type"],
                ),
            )
        log_activity("delete_task_link", "task", task_id, f"link_id={link_id}")
    return jsonify({"success": True})


@app.route("/api/tasks/<int:task_id>/links/<int:link_id>", methods=["PUT"])
@require_auth
def update_task_link(task_id, link_id):
    """Update a task link description."""
    data = request.get_json() or {}
    description = data.get("description")
    with get_db_connection() as conn:
        link = conn.execute(
            "SELECT * FROM task_links WHERE id=?", (link_id,)
        ).fetchone()
        if not link:
            return jsonify({"error": "Link not found"}), 404
        conn.execute(
            "UPDATE task_links SET description=? WHERE id=?",
            (description, link_id),
        )
    return jsonify({"success": True})


@app.route("/api/tasks/<int:task_id>/duplicates", methods=["GET"])
@require_auth
def get_task_duplicates(task_id):
    """Get all duplicate tasks."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        duplicates = conn.execute(
            """
            SELECT t.* FROM task_queue t
            JOIN task_links l ON (l.target_task_id = t.id OR l.source_task_id = t.id)
            WHERE l.link_type = 'duplicate'
            AND (l.source_task_id = ? OR l.target_task_id = ?)
            AND t.id != ?
        """,
            (task_id, task_id, task_id),
        ).fetchall()
    return jsonify(
        {"duplicates": [dict(d) for d in duplicates], "count": len(duplicates)}
    )


@app.route("/api/tasks/<int:task_id>/duplicates", methods=["POST"])
@require_auth
def mark_task_duplicate(task_id):
    """Mark a task as duplicate of another."""
    data = request.get_json() or {}
    duplicate_of = data.get("duplicate_of")
    close_duplicate = data.get("close_duplicate", False)
    if not duplicate_of:
        return jsonify({"error": "duplicate_of is required"}), 400
    with get_db_connection() as conn:
        # Create duplicate link
        try:
            conn.execute(
                """
                INSERT INTO task_links (source_task_id, target_task_id, link_type, description, created_by)
                VALUES (?, ?, 'duplicate', 'Marked as duplicate', ?)
            """,
                (task_id, duplicate_of, session.get("user", "unknown")),
            )
            conn.execute(
                """
                INSERT INTO task_links (source_task_id, target_task_id, link_type, description, created_by)
                VALUES (?, ?, 'duplicate', 'Has duplicate', ?)
            """,
                (duplicate_of, task_id, session.get("user", "unknown")),
            )
        except sqlite3.IntegrityError:
            pass  # Already linked
        # Optionally close the duplicate
        if close_duplicate:
            conn.execute(
                "UPDATE task_queue SET status='cancelled', error_message='Closed as duplicate of #' || ? WHERE id=?",
                (duplicate_of, task_id),
            )
        log_activity(
            "mark_duplicate", "task", task_id, f"duplicate_of={duplicate_of}"
        )
    return jsonify({"success": True, "closed": close_duplicate})


@app.route("/api/tasks/<int:task_id>/related", methods=["GET"])
@require_auth
def get_related_tasks(task_id):
    """Get all related tasks (any link type)."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        related = conn.execute(
            """
            SELECT DISTINCT t.*, l.link_type,
                CASE WHEN l.source_task_id = ? THEN 'outgoing' ELSE 'incoming' END as direction
            FROM task_queue t
            JOIN task_links l ON (l.target_task_id = t.id OR l.source_task_id = t.id)
            WHERE (l.source_task_id = ? OR l.target_task_id = ?)
            AND t.id != ?
            ORDER BY l.link_type, t.created_at DESC
        """,
            (task_id, task_id, task_id, task_id),
        ).fetchall()
    return jsonify(
        {"related": [dict(r) for r in related], "count": len(related)}
    )


@app.route("/api/tasks/<int:task_id>/children", methods=["GET"])
@require_auth
def get_child_tasks(task_id):
    """Get child/subtasks of a task."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        children = conn.execute(
            """
            SELECT t.* FROM task_queue t
            JOIN task_links l ON l.target_task_id = t.id
            WHERE l.source_task_id = ? AND l.link_type = 'parent'
            ORDER BY t.priority DESC, t.created_at
        """,
            (task_id,),
        ).fetchall()
        # Calculate completion stats
        total = len(children)
        completed = len([c for c in children if c["status"] == "completed"])
    return jsonify(
        {
            "children": [dict(c) for c in children],
            "count": total,
            "completed": completed,
            "progress": round(completed / total * 100) if total else 0,
        }
    )


@app.route("/api/tasks/<int:task_id>/children", methods=["POST"])
@require_auth
def create_child_task(task_id):
    """Create a child/subtask."""
    data = request.get_json() or {}
    task_type = data.get("task_type", "subtask")
    task_data = data.get("task_data", {})
    priority = data.get("priority", 0)
    with get_db_connection() as conn:
        # Verify parent exists
        parent = conn.execute(
            "SELECT * FROM task_queue WHERE id=?", (task_id,)
        ).fetchone()
        if not parent:
            return jsonify({"error": "Parent task not found"}), 404
        # Create child task
        task_data["parent_task_id"] = task_id
        cursor = conn.execute(
            """
            INSERT INTO task_queue (task_type, task_data, priority, status)
            VALUES (?, ?, ?, 'pending')
        """,
            (task_type, json.dumps(task_data), priority),
        )
        child_id = cursor.lastrowid
        # Create parent-child link
        conn.execute(
            """
            INSERT INTO task_links (source_task_id, target_task_id, link_type, created_by)
            VALUES (?, ?, 'parent', ?)
        """,
            (task_id, child_id, session.get("user", "unknown")),
        )
        log_activity(
            "create_child_task", "task", task_id, f"child_id={child_id}"
        )
    return jsonify({"success": True, "child_id": child_id})


@app.route("/api/tasks/links/search", methods=["GET"])
@require_auth
def search_task_links():
    """Search task links by type or description."""
    link_type = request.args.get("link_type")
    search = request.args.get("search")
    limit = min(int(request.args.get("limit", 50)), 200)
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        query = """
            SELECT l.*,
                s.task_type as source_type, s.status as source_status,
                t.task_type as target_type, t.status as target_status
            FROM task_links l
            JOIN task_queue s ON l.source_task_id = s.id
            JOIN task_queue t ON l.target_task_id = t.id
            WHERE 1=1
        """
        params = []
        if link_type:
            query += " AND l.link_type = ?"
            params.append(link_type)
        if search:
            query += " AND l.description LIKE ?"
            params.append(f"%{search}%")
        query += " ORDER BY l.created_at DESC LIMIT ?"
        params.append(limit)
        links = conn.execute(query, params).fetchall()


# ============================================================================
# TASK HIERARCHY API (Parent/Child Relationships)
# ============================================================================


@app.route("/api/tasks/hierarchy", methods=["GET"])
@require_auth
def get_task_hierarchy():
    """Get tasks organized in a hierarchical tree structure."""
    root_only = request.args.get("root_only", "false").lower() == "true"
    max_depth = min(int(request.args.get("max_depth", 10)), 20)
    status_filter = request.args.get("status")
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            query = "SELECT id, task_type, task_data, priority, status, parent_id, hierarchy_level, hierarchy_path, child_count, created_at FROM task_queue WHERE 1=1"
            params = []
            if root_only:
                query += " AND (parent_id IS NULL OR parent_id = 0)"
            if status_filter:
                query += " AND status = ?"
                params.append(status_filter)
            query += " ORDER BY COALESCE(hierarchy_level, 0), priority DESC, created_at"
            tasks = conn.execute(query, params).fetchall()
            task_list = [dict(t) for t in tasks]
            task_map = {t["id"]: {**t, "children": []} for t in task_list}
            root_tasks = []
            for task in task_list:
                parent_id = task.get("parent_id")
                if parent_id and parent_id in task_map:
                    task_map[parent_id]["children"].append(
                        task_map[task["id"]]
                    )
                else:
                    root_tasks.append(task_map[task["id"]])

            def limit_depth(node, depth=0):
                if depth >= max_depth:
                    node["children"] = []
                    node["children_truncated"] = True
                else:
                    for child in node.get("children", []):
                        limit_depth(child, depth + 1)
                return node

            root_tasks = [limit_depth(t) for t in root_tasks]
            return jsonify(
                {
                    "hierarchy": root_tasks,
                    "total_tasks": len(task_list),
                    "root_count": len(root_tasks),
                }
            )
    except Exception as e:
        logger.error(f"Failed to get task hierarchy: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/<int:task_id>/hierarchy", methods=["GET"])
@require_auth
def get_task_hierarchy_details(task_id):
    """Get hierarchy details for a specific task with ancestors and descendants."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            task = conn.execute(
                "SELECT * FROM task_queue WHERE id = ?", (task_id,)
            ).fetchone()
            if not task:
                return jsonify({"error": "Task not found"}), 404
            task_dict = dict(task)
            ancestors = []
            current_parent_id = task_dict.get("parent_id")
            while current_parent_id:
                parent = conn.execute(
                    "SELECT * FROM task_queue WHERE id = ?",
                    (current_parent_id,),
                ).fetchone()
                if parent:
                    ancestors.append(dict(parent))
                    current_parent_id = parent["parent_id"]
                else:
                    break
            ancestors.reverse()
            children = conn.execute(
                "SELECT * FROM task_queue WHERE parent_id = ? ORDER BY priority DESC",
                (task_id,),
            ).fetchall()
            child_list = [dict(c) for c in children]
            completed = len(
                [c for c in child_list if c["status"] == "completed"]
            )
            return jsonify(
                {
                    "task": task_dict,
                    "ancestors": ancestors,
                    "children": child_list,
                    "hierarchy": {
                        "depth": task_dict.get("hierarchy_level", 0),
                        "path": task_dict.get("hierarchy_path", "/"),
                        "child_count": len(child_list),
                        "completed_children": completed,
                        "progress": (
                            round(completed / len(child_list) * 100)
                            if child_list
                            else 0
                        ),
                        "is_root": not task_dict.get("parent_id"),
                        "is_leaf": len(child_list) == 0,
                    },
                }
            )
    except Exception as e:
        logger.error(f"Failed to get task hierarchy details: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/<int:task_id>/ancestors", methods=["GET"])
@require_auth
def get_task_ancestors(task_id):
    """Get the ancestor chain for a task."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            task = conn.execute(
                "SELECT * FROM task_queue WHERE id = ?", (task_id,)
            ).fetchone()
            if not task:
                return jsonify({"error": "Task not found"}), 404
            ancestors = []
            current_parent_id = task["parent_id"]
            while current_parent_id:
                parent = conn.execute(
                    "SELECT id, task_type, task_data, status, parent_id FROM task_queue WHERE id = ?",
                    (current_parent_id,),
                ).fetchone()
                if parent:
                    ancestors.append(dict(parent))
                    current_parent_id = parent["parent_id"]
                else:
                    break
            ancestors.reverse()
            return jsonify(
                {
                    "task_id": task_id,
                    "ancestors": ancestors,
                    "depth": len(ancestors),
                }
            )
    except Exception as e:
        logger.error(f"Failed to get task ancestors: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/<int:task_id>/descendants", methods=["GET"])
@require_auth
def get_task_descendants(task_id):
    """Get all descendants of a task."""
    max_depth = min(int(request.args.get("max_depth", 10)), 20)
    flat = request.args.get("flat", "false").lower() == "true"
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            task = conn.execute(
                "SELECT * FROM task_queue WHERE id = ?", (task_id,)
            ).fetchone()
            if not task:
                return jsonify({"error": "Task not found"}), 404
            task_dict = dict(task)
            descendants = []

            def get_children(parent_id, depth=0):
                if depth >= max_depth:
                    return
                children = conn.execute(
                    "SELECT * FROM task_queue WHERE parent_id = ?",
                    (parent_id,),
                ).fetchall()
                for child in children:
                    descendants.append(dict(child))
                    get_children(child["id"], depth + 1)

            get_children(task_id)
            if flat:
                return jsonify(
                    {
                        "task_id": task_id,
                        "descendants": descendants,
                        "count": len(descendants),
                    }
                )
            task_map = {d["id"]: {**d, "children": []} for d in descendants}
            task_map[task_id] = {**task_dict, "children": []}
            for d in descendants:
                parent_id = d.get("parent_id")
                if parent_id in task_map:
                    task_map[parent_id]["children"].append(task_map[d["id"]])
            return jsonify(
                {
                    "task_id": task_id,
                    "tree": task_map[task_id],
                    "count": len(descendants),
                }
            )
    except Exception as e:
        logger.error(f"Failed to get task descendants: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/<int:task_id>/parent", methods=["PUT"])
@require_auth
def update_task_parent(task_id):
    """Move a task to a new parent."""
    data = request.get_json() or {}
    new_parent_id = data.get("parent_id")
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            task = conn.execute(
                "SELECT * FROM task_queue WHERE id = ?", (task_id,)
            ).fetchone()
            if not task:
                return jsonify({"error": "Task not found"}), 404
            old_parent_id = task["parent_id"]
            if new_parent_id:
                new_parent = conn.execute(
                    "SELECT * FROM task_queue WHERE id = ?", (new_parent_id,)
                ).fetchone()
                if not new_parent:
                    return jsonify({"error": "New parent task not found"}), 404
                if new_parent_id == task_id:
                    return (
                        jsonify({"error": "Task cannot be its own parent"}),
                        400,
                    )
                check_id = new_parent["parent_id"]
                while check_id:
                    if check_id == task_id:
                        return (
                            jsonify(
                                {
                                    "error": "Cannot move task under its own descendant"
                                }
                            ),
                            400,
                        )
                    ancestor = conn.execute(
                        "SELECT parent_id FROM task_queue WHERE id = ?",
                        (check_id,),
                    ).fetchone()
                    check_id = ancestor["parent_id"] if ancestor else None
                new_level = (new_parent["hierarchy_level"] or 0) + 1
                new_path = (
                    f"{new_parent.get('hierarchy_path', '/')}{new_parent_id}/"
                )
            else:
                new_level = 0
                new_path = "/"
                new_parent_id = None
            conn.execute(
                "UPDATE task_queue SET parent_id = ?, hierarchy_level = ?, hierarchy_path = ? WHERE id = ?",
                (new_parent_id, new_level, new_path, task_id),
            )
            if old_parent_id:
                conn.execute(
                    "UPDATE task_queue SET child_count = (SELECT COUNT(*) FROM task_queue WHERE parent_id = ?) WHERE id = ?",
                    (old_parent_id, old_parent_id),
                )
            if new_parent_id:
                conn.execute(
                    "UPDATE task_queue SET child_count = (SELECT COUNT(*) FROM task_queue WHERE parent_id = ?) WHERE id = ?",
                    (new_parent_id, new_parent_id),
                )
            conn.commit()
            log_activity(
                "move_task",
                "task",
                task_id,
                f"from={old_parent_id} to={new_parent_id}",
            )
            return jsonify(
                {
                    "success": True,
                    "task_id": task_id,
                    "old_parent_id": old_parent_id,
                    "new_parent_id": new_parent_id,
                }
            )
    except Exception as e:
        logger.error(f"Failed to update task parent: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/<int:task_id>/subtasks/bulk", methods=["POST"])
@require_auth
def create_bulk_subtasks(task_id):
    """Create multiple subtasks at once."""
    data = request.get_json() or {}
    subtasks = data.get("subtasks", [])
    if not subtasks:
        return jsonify({"error": "No subtasks provided"}), 400
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            parent = conn.execute(
                "SELECT * FROM task_queue WHERE id = ?", (task_id,)
            ).fetchone()
            if not parent:
                return jsonify({"error": "Parent task not found"}), 404
            parent_dict = dict(parent)
            child_level = (parent_dict.get("hierarchy_level", 0) or 0) + 1
            child_path = f"{parent_dict.get('hierarchy_path', '/')}{task_id}/"
            created_ids = []
            for subtask in subtasks:
                task_type = subtask.get("task_type", "subtask")
                task_data = subtask.get("task_data", {})
                task_data["parent_task_id"] = task_id
                cursor = conn.execute(
                    "INSERT INTO task_queue (task_type, task_data, priority, status, parent_id, hierarchy_level, hierarchy_path) VALUES (?, ?, ?, 'pending', ?, ?, ?)",
                    (
                        task_type,
                        json.dumps(task_data),
                        subtask.get("priority", 0),
                        task_id,
                        child_level,
                        child_path,
                    ),
                )
                created_ids.append(cursor.lastrowid)
            conn.execute(
                "UPDATE task_queue SET child_count = child_count + ? WHERE id = ?",
                (len(created_ids), task_id),
            )
            conn.commit()
            log_activity(
                "create_bulk_subtasks",
                "task",
                task_id,
                f"count={len(created_ids)}",
            )
            return jsonify(
                {
                    "success": True,
                    "parent_id": task_id,
                    "created_ids": created_ids,
                    "count": len(created_ids),
                }
            )
    except Exception as e:
        logger.error(f"Failed to create bulk subtasks: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/hierarchy/templates", methods=["GET"])
@require_auth
def get_hierarchy_templates():
    """Get available task hierarchy templates."""
    task_type = request.args.get("task_type")
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            query = "SELECT * FROM task_hierarchy_templates WHERE 1=1"
            params = []
            if task_type:
                query += " AND (task_type IS NULL OR task_type = ?)"
                params.append(task_type)
            templates = conn.execute(
                query + " ORDER BY name", params
            ).fetchall()
            return jsonify(
                {
                    "templates": [dict(t) for t in templates],
                    "count": len(templates),
                }
            )
    except Exception as e:
        logger.error(f"Failed to get hierarchy templates: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/hierarchy/templates", methods=["POST"])
@require_auth
def create_hierarchy_template():
    """Create a task hierarchy template."""
    data = request.get_json() or {}
    name = data.get("name")
    if not name:
        return jsonify({"error": "Template name is required"}), 400
    template_data = data.get("template_data")
    if not template_data:
        return jsonify({"error": "Template data is required"}), 400
    try:
        with get_db_connection() as conn:
            cursor = conn.execute(
                "INSERT INTO task_hierarchy_templates (name, description, template_data, task_type, created_by) VALUES (?, ?, ?, ?, ?)",
                (
                    name,
                    data.get("description", ""),
                    json.dumps(template_data),
                    data.get("task_type"),
                    session.get("user", "unknown"),
                ),
            )
            conn.commit()
            return jsonify(
                {
                    "success": True,
                    "template_id": cursor.lastrowid,
                    "name": name,
                }
            )
    except Exception as e:
        logger.error(f"Failed to create hierarchy template: {e}")
        return jsonify({"error": str(e)}), 500


@app.route(
    "/api/tasks/hierarchy/templates/<int:template_id>", methods=["DELETE"]
)
@require_auth
def delete_hierarchy_template(template_id):
    """Delete a task hierarchy template."""
    try:
        with get_db_connection() as conn:
            result = conn.execute(
                "DELETE FROM task_hierarchy_templates WHERE id = ?",
                (template_id,),
            )
            if result.rowcount == 0:
                return jsonify({"error": "Template not found"}), 404
            conn.commit()
            return jsonify({"success": True, "template_id": template_id})
    except Exception as e:
        logger.error(f"Failed to delete hierarchy template: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/<int:task_id>/apply-template", methods=["POST"])
@require_auth
def apply_hierarchy_template(task_id):
    """Apply a hierarchy template to create subtasks."""
    data = request.get_json() or {}
    template_id = data.get("template_id")
    if not template_id:
        return jsonify({"error": "Template ID is required"}), 400
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            parent = conn.execute(
                "SELECT * FROM task_queue WHERE id = ?", (task_id,)
            ).fetchone()
            if not parent:
                return jsonify({"error": "Task not found"}), 404
            template = conn.execute(
                "SELECT * FROM task_hierarchy_templates WHERE id = ?",
                (template_id,),
            ).fetchone()
            if not template:
                return jsonify({"error": "Template not found"}), 404
            template_data = json.loads(template["template_data"])
            parent_dict = dict(parent)
            created_ids = []
            variables = data.get("variables", {})

            def create_from_template(pid, nodes, level, path):
                for node in nodes:
                    task_type = node.get("task_type", "subtask")
                    td = node.get("task_data", {})
                    td_str = json.dumps(td)
                    for k, v in variables.items():
                        td_str = td_str.replace(f"{{{{{k}}}}}", str(v))
                    td = json.loads(td_str)
                    td["parent_task_id"] = pid
                    child_path = f"{path}{pid}/"
                    cursor = conn.execute(
                        "INSERT INTO task_queue (task_type, task_data, priority, status, parent_id, hierarchy_level, hierarchy_path) VALUES (?, ?, ?, 'pending', ?, ?, ?)",
                        (
                            task_type,
                            json.dumps(td),
                            node.get("priority", 0),
                            pid,
                            level,
                            child_path,
                        ),
                    )
                    created_ids.append(cursor.lastrowid)
                    if node.get("children"):
                        create_from_template(
                            cursor.lastrowid,
                            node["children"],
                            level + 1,
                            child_path,
                        )

            create_from_template(
                task_id,
                template_data.get("subtasks", []),
                (parent_dict.get("hierarchy_level", 0) or 0) + 1,
                parent_dict.get("hierarchy_path", "/"),
            )
            conn.commit()
            return jsonify(
                {
                    "success": True,
                    "task_id": task_id,
                    "created_ids": created_ids,
                    "count": len(created_ids),
                }
            )
    except Exception as e:
        logger.error(f"Failed to apply hierarchy template: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/hierarchy/settings", methods=["GET"])
@require_auth
def get_hierarchy_settings():
    """Get task hierarchy configuration settings."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            settings = conn.execute(
                "SELECT * FROM task_hierarchy_settings ORDER BY setting_key"
            ).fetchall()
            return jsonify(
                {
                    "settings": {
                        s["setting_key"]: {
                            "value": s["setting_value"],
                            "description": s["description"],
                        }
                        for s in settings
                    }
                }
            )
    except Exception as e:
        logger.error(f"Failed to get hierarchy settings: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/hierarchy/settings", methods=["PUT"])
@require_auth
def update_hierarchy_settings():
    """Update task hierarchy configuration settings."""
    data = request.get_json() or {}
    settings = data.get("settings", {})
    if not settings:
        return jsonify({"error": "No settings provided"}), 400
    try:
        with get_db_connection() as conn:
            for key, value in settings.items():
                conn.execute(
                    "UPDATE task_hierarchy_settings SET setting_value = ?, updated_at = CURRENT_TIMESTAMP WHERE setting_key = ?",
                    (str(value), key),
                )
            conn.commit()
            return jsonify({"success": True, "updated": list(settings.keys())})
    except Exception as e:
        logger.error(f"Failed to update hierarchy settings: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/<int:task_id>/complete-with-children", methods=["POST"])
@require_auth
def complete_task_with_children(task_id):
    """Complete a task and optionally all its children."""
    data = request.get_json() or {}
    cascade = data.get("cascade", False)
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            task = conn.execute(
                "SELECT * FROM task_queue WHERE id = ?", (task_id,)
            ).fetchone()
            if not task:
                return jsonify({"error": "Task not found"}), 404
            completed_ids = [task_id]
            now = datetime.now().isoformat()
            if cascade:

                def get_all_children(pid):
                    children = conn.execute(
                        "SELECT id FROM task_queue WHERE parent_id = ?", (pid,)
                    ).fetchall()
                    for child in children:
                        completed_ids.append(child["id"])
                        get_all_children(child["id"])

                get_all_children(task_id)
            placeholders = ",".join("?" * len(completed_ids))
            conn.execute(
                f"UPDATE task_queue SET status = 'completed', completed_at = ? WHERE id IN ({placeholders})",
                [now] + completed_ids,
            )
            conn.commit()
            log_activity(
                "complete_with_children",
                "task",
                task_id,
                f"cascade={cascade} count={len(completed_ids)}",
            )
            return jsonify(
                {
                    "success": True,
                    "task_id": task_id,
                    "completed_ids": completed_ids,
                    "count": len(completed_ids),
                }
            )
    except Exception as e:
        logger.error(f"Failed to complete task with children: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/hierarchy/stats", methods=["GET"])
@require_auth
def get_hierarchy_stats():
    """Get statistics about task hierarchy usage."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            root_count = conn.execute(
                "SELECT COUNT(*) as c FROM task_queue WHERE parent_id IS NULL"
            ).fetchone()["c"]
            child_count = conn.execute(
                "SELECT COUNT(*) as c FROM task_queue WHERE parent_id IS NOT NULL"
            ).fetchone()["c"]
            max_depth = (
                conn.execute(
                    "SELECT MAX(hierarchy_level) as m FROM task_queue"
                ).fetchone()["m"]
                or 0
            )
            depth_stats = conn.execute(
                "SELECT COALESCE(hierarchy_level, 0) as depth, COUNT(*) as count FROM task_queue GROUP BY depth ORDER BY depth"
            ).fetchall()
            avg_children = (
                conn.execute(
                    "SELECT AVG(child_count) as avg FROM task_queue WHERE child_count > 0"
                ).fetchone()["avg"]
                or 0
            )
            return jsonify(
                {
                    "total_tasks": root_count + child_count,
                    "root_tasks": root_count,
                    "child_tasks": child_count,
                    "max_depth": max_depth,
                    "avg_children_per_parent": round(avg_children, 1),
                    "tasks_by_depth": [
                        {"depth": d["depth"], "count": d["count"]}
                        for d in depth_stats
                    ],
                }
            )
    except Exception as e:
        logger.error(f"Failed to get hierarchy stats: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/priority-config", methods=["GET"])
@require_auth
def get_priority_config():
    """Get task priority aging configuration.

    Returns:
        aging_factor: Points gained per minute waiting
        max_age_bonus: Maximum bonus from aging
        description: How the aging formula works
    """
    return jsonify(
        {
            "aging_factor": TASK_PRIORITY_AGING_FACTOR,
            "max_age_bonus": TASK_PRIORITY_MAX_AGE_BONUS,
            "description": "effective_priority = priority + min(max_age_bonus, age_in_minutes * aging_factor)",
            "example": f"A task with priority 0 waiting 50 minutes has effective_priority: {
                min(
                    TASK_PRIORITY_MAX_AGE_BONUS,
                    50 *
                    TASK_PRIORITY_AGING_FACTOR)}",
        }
    )


@app.route("/api/tasks/<int:task_id>/effective-priority", methods=["GET"])
@require_auth
def get_task_effective_priority(task_id):
    """Get the effective priority of a specific task including aging bonus.

    Returns:
        base_priority: The original priority value
        age_minutes: How long the task has been waiting
        aging_bonus: Priority bonus from aging
        effective_priority: Total effective priority (base + aging bonus)
    """
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        task = conn.execute(
            """
            SELECT id, priority, status, created_at,
                ROUND((strftime('%s', 'now') - strftime('%s', created_at)) / 60.0, 2) as age_minutes,
                ROUND(MIN({TASK_PRIORITY_MAX_AGE_BONUS},
                    (strftime('%s', 'now') - strftime('%s', created_at)) / 60.0 * {TASK_PRIORITY_AGING_FACTOR}), 2) as aging_bonus,
                ROUND(priority + MIN({TASK_PRIORITY_MAX_AGE_BONUS},
                    (strftime('%s', 'now') - strftime('%s', created_at)) / 60.0 * {TASK_PRIORITY_AGING_FACTOR}), 2) as effective_priority
            FROM task_queue WHERE id = ?
        """,
            (task_id,),
        ).fetchone()

        if not task:
            return jsonify({"error": "Task not found"}), 404

        return jsonify(
            {
                "task_id": task["id"],
                "status": task["status"],
                "base_priority": task["priority"],
                "age_minutes": task["age_minutes"],
                "aging_bonus": task["aging_bonus"],
                "effective_priority": task["effective_priority"],
                "config": {
                    "aging_factor": TASK_PRIORITY_AGING_FACTOR,
                    "max_age_bonus": TASK_PRIORITY_MAX_AGE_BONUS,
                },
            }
        )


def trigger_sheets_update(task_id, status, result=None, session_name=None):
    """Trigger async Google Sheets update for task status change.

    Runs in background thread to avoid slowing down API response.
    """

    def update_sheets():
        try:
            # Import here to avoid circular imports and only when needed
            import sys

            sys.path.insert(0, str(Path(__file__).parent / "workers"))
            from sheets_sync import update_task_status

            update_task_status(task_id, status, result, session_name)
            logger.info(f"[Sheets] Updated task {task_id} to {status}")
        except ImportError:
            logger.debug("[Sheets] sheets_sync not available, skipping update")
        except Exception as e:
            logger.warning(f"[Sheets] Failed to update task {task_id}: {e}")

    # Run in background thread
    thread = threading.Thread(target=update_sheets, daemon=True)
    thread.start()


def trigger_task_webhook(
    task_id,
    task_type,
    new_status,
    old_status=None,
    worker_id=None,
    session_name=None,
    result=None,
    error=None,
    task_data=None,
):
    """Trigger webhook notification for task status change.

    Runs in background thread to avoid slowing down API response.
    Triggers both the general notification service and task-specific webhooks.
    """
    # Trigger general notification service webhooks
    try:
        from services.notifications import trigger_task_webhook as _trigger

        _trigger(
            task_id=task_id,
            task_type=task_type,
            new_status=new_status,
            old_status=old_status,
            worker_id=worker_id,
            session_name=session_name,
            result=result,
            error=error,
            task_data=task_data,
            db_path=str(DB_PATH),
        )
    except ImportError:
        logger.debug("[Webhook] notifications service not available")
    except Exception as e:
        logger.warning(
            f"[Webhook] Failed to trigger notification for task {task_id}: {e}"
        )

    # Trigger task-specific webhooks
    try:
        event = task_webhooks.STATUS_TO_EVENT.get(
            new_status, f"task.{new_status}"
        )
        task_webhooks.trigger_task_event(
            db_path=str(DB_PATH),
            task_id=task_id,
            event=event,
            task_type=task_type,
            task_data=task_data,
            old_status=old_status,
            new_status=new_status,
            worker_id=worker_id,
            result=result,
            error=error,
        )
    except Exception as e:
        logger.warning(
            f"[Webhook] Failed to trigger task webhook for task {task_id}: {e}"
        )


@app.route("/api/tasks/<int:task_id>/risk", methods=["GET", "PUT", "POST"])
@require_auth
def task_risk_assessment(task_id):
    """Get or update task risk assessment.

    GET: Returns current risk assessment
    PUT/POST: Update risk assessment with:
        - risk_level: low, medium, high, critical
        - risk_score: 0-100
        - risk_factors: array of risk factor strings
    """
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Check task exists
        task = conn.execute(
            "SELECT * FROM task_queue WHERE id = ?", (task_id,)
        ).fetchone()
        if not task:
            return jsonify({"error": "Task not found"}), 404

        if request.method == "GET":
            return jsonify(
                {
                    "task_id": task_id,
                    "risk_level": task["risk_level"] or "low",
                    "risk_score": task["risk_score"] or 0,
                    "risk_factors": json.loads(task["risk_factors"] or "[]"),
                    "risk_assessed_at": task["risk_assessed_at"],
                }
            )

        # PUT/POST - Update risk assessment
        data = request.get_json() or {}

        risk_level = data.get("risk_level", task["risk_level"] or "low")
        if risk_level not in ["low", "medium", "high", "critical"]:
            return (
                jsonify(
                    {
                        "error": "Invalid risk_level. Must be: low, medium, high, critical"
                    }
                ),
                400,
            )

        risk_score = data.get("risk_score", task["risk_score"] or 0)
        if (
            not isinstance(risk_score, int)
            or risk_score < 0
            or risk_score > 100
        ):
            return (
                jsonify(
                    {
                        "error": "risk_score must be an integer between 0 and 100"
                    }
                ),
                400,
            )

        risk_factors = data.get(
            "risk_factors", json.loads(task["risk_factors"] or "[]")
        )
        if not isinstance(risk_factors, list):
            return jsonify({"error": "risk_factors must be an array"}), 400

        conn.execute(
            """
            UPDATE task_queue SET
                risk_level = ?,
                risk_score = ?,
                risk_factors = ?,
                risk_assessed_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (risk_level, risk_score, json.dumps(risk_factors), task_id),
        )
        conn.commit()

        log_activity(
            "update_risk",
            "task",
            task_id,
            f"risk_level={risk_level}, score={risk_score}",
        )

        return jsonify(
            {
                "success": True,
                "task_id": task_id,
                "risk_level": risk_level,
                "risk_score": risk_score,
                "risk_factors": risk_factors,
            }
        )


@app.route("/api/tasks/risk/auto-assess", methods=["POST"])
@require_auth
def auto_assess_task_risk():
    """Auto-assess risk for tasks based on task type and data.

    Request body:
        - task_ids: array of task IDs to assess (optional, defaults to all pending)

    Risk factors considered:
        - Task type (deploy, shell = higher risk)
        - Production mentions in data
        - Destructive operations (delete, drop, rm -rf)
        - External dependencies
        - Retry count
    """
    data = request.get_json() or {}
    task_ids = data.get("task_ids")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        if task_ids:
            placeholders = ",".join("?" * len(task_ids))
            tasks = conn.execute(
                f"SELECT * FROM task_queue WHERE id IN ({placeholders})",
                task_ids,
            ).fetchall()
        else:
            tasks = conn.execute(
                "SELECT * FROM task_queue WHERE status = 'pending'"
            ).fetchall()

        assessed = []
        for task in tasks:
            risk_factors = []
            risk_score = 0
            task_data_str = task["task_data"] or "{}"

            # Task type risk
            high_risk_types = ["deploy", "shell", "git"]
            if task["task_type"] in high_risk_types:
                risk_factors.append(
                    f"High-risk task type: {task['task_type']}"
                )
                risk_score += 20

            # Production/critical environment
            if (
                "prod" in task_data_str.lower()
                or "production" in task_data_str.lower()
            ):
                risk_factors.append("Involves production environment")
                risk_score += 30

            # Destructive operations
            destructive_patterns = [
                "delete",
                "drop",
                "rm -r",
                "truncate",
                "destroy",
                "remove",
            ]
            for pattern in destructive_patterns:
                if pattern in task_data_str.lower():
                    risk_factors.append(
                        f"Contains destructive operation: {pattern}"
                    )
                    risk_score += 25
                    break

            # External dependencies
            if (
                "http" in task_data_str.lower()
                or "api" in task_data_str.lower()
            ):
                risk_factors.append("Has external dependencies")
                risk_score += 10

            # Retry risk
            if task["retries"] and task["retries"] > 0:
                risk_factors.append(f"Has {task['retries']} previous retries")
                risk_score += task["retries"] * 10

            # Database operations
            if (
                "database" in task_data_str.lower()
                or "sql" in task_data_str.lower()
                or "migrate" in task_data_str.lower()
            ):
                risk_factors.append("Involves database operations")
                risk_score += 15

            # Cap score at 100
            risk_score = min(100, risk_score)

            # Determine risk level
            if risk_score >= 70:
                risk_level = "critical"
            elif risk_score >= 50:
                risk_level = "high"
            elif risk_score >= 25:
                risk_level = "medium"
            else:
                risk_level = "low"

            # Update task
            conn.execute(
                """
                UPDATE task_queue SET
                    risk_level = ?,
                    risk_score = ?,
                    risk_factors = ?,
                    risk_assessed_at = CURRENT_TIMESTAMP
                WHERE id = ?
            """,
                (risk_level, risk_score, json.dumps(risk_factors), task["id"]),
            )

            assessed.append(
                {
                    "task_id": task["id"],
                    "risk_level": risk_level,
                    "risk_score": risk_score,
                    "risk_factors": risk_factors,
                }
            )

        conn.commit()

    return jsonify(
        {"success": True, "assessed_count": len(assessed), "tasks": assessed}
    )


@app.route("/api/tasks/<int:task_id>/complete", methods=["POST"])
def complete_task(task_id):
    """Mark a task as completed."""
    data = request.get_json() or {}
    cascade_complete = data.get("cascade_complete", False)

    with get_db_connection() as conn:
        # Get task info for hierarchy tracking
        task = conn.execute(
            "SELECT parent_id FROM task_queue WHERE id = ?", (task_id,)
        ).fetchone()

        conn.execute(
            """
            UPDATE task_queue SET
                status = 'completed',
                completed_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (task_id,),
        )

        # Update worker stats
        worker_id = data.get("worker_id")
        if worker_id:
            conn.execute(
                """
                UPDATE workers SET
                    tasks_completed = tasks_completed + 1,
                    current_task_id = NULL,
                    status = 'idle'
                WHERE id = ?
            """,
                (worker_id,),
            )

        # Auto-complete parent if all children are done and cascade_complete is
        # set
        if task and task["parent_id"] and cascade_complete:
            parent_id = task["parent_id"]
            # Check if all children are completed
            incomplete = conn.execute(
                """
                SELECT COUNT(*) as cnt FROM task_queue
                WHERE parent_id = ? AND status != 'completed'
            """,
                (parent_id,),
            ).fetchone()["cnt"]
            if incomplete == 0:
                conn.execute(
                    """
                    UPDATE task_queue SET status = 'completed', completed_at = CURRENT_TIMESTAMP
                    WHERE id = ?
                """,
                    (parent_id,),
                )

    # Broadcast queue update via WebSocket
    broadcast_queue()
    broadcast_stats()

    # Trigger async Google Sheets update
    result = data.get("result")
    session = data.get("session_name") or data.get("worker_id")
    trigger_sheets_update(task_id, "completed", result, session)

    # Trigger webhook notification
    trigger_task_webhook(
        task_id=task_id,
        task_type=data.get("task_type", "unknown"),
        new_status="completed",
        old_status="running",
        worker_id=data.get("worker_id"),
        session_name=session,
        result=result,
    )

    return jsonify({"success": True})


@app.route("/api/tasks/<int:task_id>/clone", methods=["POST"])
@require_auth
def clone_task(task_id):
    """Clone/duplicate an existing task.

    Creates a new task based on an existing one with optional modifications.

    Body (all optional):
        task_type: Override the task type
        task_data: Override task data (replaces original)
        merge_data: Merge with original task_data (partial override)
        priority: Override priority
        reset_status: If true, cloned task starts as 'pending' (default true)
        copy_dependencies: If true, copies task dependencies (default false)
        suffix: Suffix to add to task description (default ' (copy)')
        count: Number of clones to create (default 1, max 50)
    """
    import sqlite3

    data = request.get_json() or {}

    count = min(data.get("count", 1), 50)  # Max 50 clones at once
    suffix = data.get("suffix", " (copy)")
    reset_status = data.get("reset_status", True)
    copy_dependencies = data.get("copy_dependencies", False)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Get original task
        original = conn.execute(
            """
            SELECT * FROM task_queue WHERE id = ?
        """,
            (task_id,),
        ).fetchone()

        if not original:
            return api_error("Task not found", 404, "not_found")

        original = dict(original)

        # Prepare cloned task data
        new_task_type = data.get("task_type", original["task_type"])
        new_priority = data.get("priority", original["priority"])

        # Handle task_data merging
        original_data = json.loads(original["task_data"] or "{}")
        if "task_data" in data:
            new_task_data = data["task_data"]
        elif "merge_data" in data:
            new_task_data = {**original_data, **data["merge_data"]}
        else:
            new_task_data = original_data

        # Add suffix to description if present
        if "description" in new_task_data and suffix:
            new_task_data["description"] = (
                new_task_data["description"] + suffix
            )

        new_status = "pending" if reset_status else original["status"]

        cloned_ids = []
        for i in range(count):
            # For multiple clones, add number suffix
            task_data_copy = dict(new_task_data)
            if count > 1 and "description" in task_data_copy:
                task_data_copy["description"] = (
                    task_data_copy["description"].replace(
                        suffix, f" (copy {i + 1})"
                    )
                    if suffix
                    else task_data_copy["description"] + f" #{i + 1}"
                )

            cursor = conn.execute(
                """
                INSERT INTO task_queue
                    (task_type, task_data, status, priority, created_at,
                     assigned_worker, assigned_node, max_retries, timeout_seconds)
                VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP, NULL, ?, ?, ?)
            """,
                (
                    new_task_type,
                    json.dumps(task_data_copy),
                    new_status,
                    new_priority,
                    data.get("assigned_node", original.get("assigned_node")),
                    original.get("max_retries", 3),
                    original.get("timeout_seconds", 3600),
                ),
            )
            cloned_id = cursor.lastrowid
            cloned_ids.append(cloned_id)

            # Copy dependencies if requested
            if copy_dependencies:
                # Copy tasks this task depends on
                conn.execute(
                    """
                    INSERT INTO task_dependencies (task_id, depends_on_id, dependency_type, created_at)
                    SELECT ?, depends_on_id, dependency_type, CURRENT_TIMESTAMP
                    FROM task_dependencies WHERE task_id = ?
                """,
                    (cloned_id, task_id),
                )

        log_activity(
            conn,
            "clone_task",
            "task",
            task_id,
            f"Cloned task to {len(cloned_ids)} new task(s): {cloned_ids}",
        )

    broadcast_queue()

    return jsonify(
        {
            "success": True,
            "original_task_id": task_id,
            "cloned_task_ids": cloned_ids,
            "count": len(cloned_ids),
        }
    )


@app.route("/api/tasks/bulk-clone", methods=["POST"])
@require_auth
def bulk_clone_tasks_with_options():
    """Clone multiple tasks at once with options.

    Body:
        task_ids: List of task IDs to clone (required)
        options: Dict of clone options applied to all tasks (optional)
            - task_type, priority, reset_status, copy_dependencies, suffix
    """
    import sqlite3

    data = request.get_json() or {}
    task_ids = data.get("task_ids", [])
    options = data.get("options", {})

    if not task_ids:
        return api_error("task_ids is required", 400, "missing_field")

    if len(task_ids) > 100:
        return api_error(
            "Maximum 100 tasks can be cloned at once", 400, "limit_exceeded"
        )

    suffix = options.get("suffix", " (copy)")
    reset_status = options.get("reset_status", True)
    copy_dependencies = options.get("copy_dependencies", False)
    override_priority = options.get("priority")
    override_type = options.get("task_type")

    results = []
    errors = []

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        for task_id in task_ids:
            original = conn.execute(
                """
                SELECT * FROM task_queue WHERE id = ?
            """,
                (task_id,),
            ).fetchone()

            if not original:
                errors.append({"task_id": task_id, "error": "Task not found"})
                continue

            original = dict(original)
            original_data = json.loads(original["task_data"] or "{}")

            # Add suffix to description
            if "description" in original_data and suffix:
                original_data["description"] = (
                    original_data["description"] + suffix
                )

            new_status = "pending" if reset_status else original["status"]

            cursor = conn.execute(
                """
                INSERT INTO task_queue
                    (task_type, task_data, status, priority, created_at,
                     assigned_worker, assigned_node, max_retries, timeout_seconds)
                VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP, NULL, ?, ?, ?)
            """,
                (
                    override_type or original["task_type"],
                    json.dumps(original_data),
                    new_status,
                    (
                        override_priority
                        if override_priority is not None
                        else original["priority"]
                    ),
                    original.get("assigned_node"),
                    original.get("max_retries", 3),
                    original.get("timeout_seconds", 3600),
                ),
            )
            cloned_id = cursor.lastrowid

            if copy_dependencies:
                conn.execute(
                    """
                    INSERT INTO task_dependencies (task_id, depends_on_id, dependency_type, created_at)
                    SELECT ?, depends_on_id, dependency_type, CURRENT_TIMESTAMP
                    FROM task_dependencies WHERE task_id = ?
                """,
                    (cloned_id, task_id),
                )

            results.append(
                {"original_task_id": task_id, "cloned_task_id": cloned_id}
            )

        if results:
            log_activity(
                conn,
                "bulk_clone_tasks",
                "task",
                None,
                f"Bulk cloned {len(results)} tasks",
            )

    broadcast_queue()

    return jsonify(
        {
            "success": True,
            "cloned": results,
            "errors": errors,
            "total_cloned": len(results),
            "total_errors": len(errors),
        }
    )


@app.route("/api/tasks/<int:task_id>/fail", methods=["POST"])
def fail_task(task_id):
    """Mark a task as failed."""
    data = request.get_json()
    error_message = data.get("error")

    with get_db_connection() as conn:
        conn.execute(
            """
            UPDATE task_queue SET
                status = CASE WHEN retries + 1 >= max_retries THEN 'failed' ELSE 'pending' END,
                retries = retries + 1,
                error_message = ?,
                assigned_worker = NULL,
                started_at = NULL
            WHERE id = ?
        """,
            (error_message, task_id),
        )

        # Update worker stats
        worker_id = data.get("worker_id")
        if worker_id:
            conn.execute(
                """
                UPDATE workers SET
                    tasks_failed = tasks_failed + 1,
                    current_task_id = NULL,
                    status = 'idle'
                WHERE id = ?
            """,
                (worker_id,),
            )

    # Broadcast queue update via WebSocket
    broadcast_queue()
    broadcast_stats()

    # Trigger async Google Sheets update
    session = data.get("session_name") or data.get("worker_id")
    trigger_sheets_update(task_id, "failed", error_message, session)

    # Trigger webhook notification
    trigger_task_webhook(
        task_id=task_id,
        task_type=data.get("task_type", "unknown"),
        new_status="failed",
        old_status="running",
        worker_id=data.get("worker_id"),
        session_name=session,
        error=error_message,
    )

    return jsonify({"success": True})


# ============================================================================
# TASK TIME TRACKING API
# ============================================================================


@app.route("/api/tasks/<int:task_id>/time/start", methods=["POST"])
@require_auth
def start_time_tracking(task_id):
    """Start tracking time on a task."""
    user_id = session.get("user", "anonymous")
    data = request.get_json() or {}
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        if not conn.execute(
            "SELECT 1 FROM task_queue WHERE id=?", (task_id,)
        ).fetchone():
            return jsonify({"error": "Task not found"}), 404
        running = conn.execute(
            "SELECT id FROM task_time_entries WHERE task_id=? AND user_id=? AND is_running=1",
            (task_id, user_id),
        ).fetchone()
        if running:
            return (
                jsonify(
                    {
                        "error": "Timer already running",
                        "entry_id": running["id"],
                    }
                ),
                409,
            )
        conn.execute(
            "UPDATE task_time_entries SET is_running=0, end_time=CURRENT_TIMESTAMP, duration_seconds=CAST((julianday(CURRENT_TIMESTAMP)-julianday(start_time))*86400 AS INTEGER) WHERE user_id=? AND is_running=1",
            (user_id,),
        )
        cursor = conn.execute(
            "INSERT INTO task_time_entries (task_id,user_id,start_time,description,is_running) VALUES (?,?,CURRENT_TIMESTAMP,?,1)",
            (task_id, user_id, data.get("description", "")),
        )
    log_activity("start_time", "task", task_id)
    return jsonify(
        {
            "success": True,
            "entry_id": cursor.lastrowid,
            "started_at": datetime.now().isoformat(),
        }
    )


@app.route("/api/tasks/<int:task_id>/time/stop", methods=["POST"])
@require_auth
def stop_time_tracking(task_id):
    """Stop tracking time on a task."""
    user_id = session.get("user", "anonymous")
    data = request.get_json() or {}
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        entry = conn.execute(
            "SELECT id,start_time FROM task_time_entries WHERE task_id=? AND user_id=? AND is_running=1",
            (task_id, user_id),
        ).fetchone()
        if not entry:
            return jsonify({"error": "No running timer"}), 404
        conn.execute(
            "UPDATE task_time_entries SET is_running=0, end_time=CURRENT_TIMESTAMP, duration_seconds=CAST((julianday(CURRENT_TIMESTAMP)-julianday(start_time))*86400 AS INTEGER), description=COALESCE(?,description) WHERE id=?",
            (data.get("description"), entry["id"]),
        )
        duration = (
            conn.execute(
                "SELECT duration_seconds FROM task_time_entries WHERE id=?",
                (entry["id"],),
            ).fetchone()[0]
            or 0
        )
        total = conn.execute(
            "SELECT COALESCE(SUM(duration_seconds),0) FROM task_time_entries WHERE task_id=?",
            (task_id,),
        ).fetchone()[0]
        conn.execute(
            "UPDATE task_queue SET actual_hours=? WHERE id=?",
            (round(total / 3600, 2), task_id),
        )
    log_activity("stop_time", "task", task_id, f"duration={duration}s")
    return jsonify(
        {
            "success": True,
            "entry_id": entry["id"],
            "duration_seconds": duration,
            "formatted": f"{duration//3600}h {(duration % 3600)//60}m",
        }
    )


@app.route("/api/tasks/<int:task_id>/time", methods=["GET"])
@require_auth
def get_task_time_entries(task_id):
    """Get all time entries for a task."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        entries = conn.execute(
            "SELECT * FROM task_time_entries WHERE task_id=? ORDER BY start_time DESC",
            (task_id,),
        ).fetchall()
        total = sum(e["duration_seconds"] or 0 for e in entries)
        running = next((dict(e) for e in entries if e["is_running"]), None)
        return jsonify(
            {
                "task_id": task_id,
                "entries": [dict(e) for e in entries],
                "total_seconds": total,
                "total_formatted": f"{total//3600}h {(total % 3600)//60}m",
                "running": running,
                "count": len(entries),
            }
        )


@app.route("/api/tasks/<int:task_id>/time", methods=["POST"])
@require_auth
def add_manual_time_entry(task_id):
    """Add manual time entry."""
    user_id = session.get("user", "anonymous")
    data = request.get_json() or {}
    duration = data.get("duration_seconds") or (
        data.get("duration_hours", 0) * 3600
        + data.get("duration_minutes", 0) * 60
    )
    if not duration or duration <= 0:
        return jsonify({"error": "Valid duration required"}), 400
    with get_db_connection() as conn:
        if not conn.execute(
            "SELECT 1 FROM task_queue WHERE id=?", (task_id,)
        ).fetchone():
            return jsonify({"error": "Task not found"}), 404
        start = data.get("start_time", datetime.now().isoformat())
        cursor = conn.execute(
            "INSERT INTO task_time_entries (task_id,user_id,start_time,end_time,duration_seconds,description,is_running) VALUES (?,?,?,datetime(?,'+'||?||' seconds'),?,?,0)",
            (
                task_id,
                user_id,
                start,
                start,
                duration,
                duration,
                data.get("description", "Manual entry"),
            ),
        )
        total = conn.execute(
            "SELECT COALESCE(SUM(duration_seconds),0) FROM task_time_entries WHERE task_id=?",
            (task_id,),
        ).fetchone()[0]
        conn.execute(
            "UPDATE task_queue SET actual_hours=? WHERE id=?",
            (round(total / 3600, 2), task_id),
        )
    log_activity("add_time", "task", task_id, f"duration={duration}s")
    return jsonify(
        {
            "success": True,
            "entry_id": cursor.lastrowid,
            "duration_seconds": duration,
        }
    )


@app.route("/api/tasks/time/entries/<int:entry_id>", methods=["PUT"])
@require_auth
def update_time_entry(entry_id):
    """Update a time entry."""
    data = request.get_json() or {}
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        entry = conn.execute(
            "SELECT * FROM task_time_entries WHERE id=?", (entry_id,)
        ).fetchone()
        if not entry:
            return jsonify({"error": "Entry not found"}), 404
        updates, params = [], []
        if "description" in data:
            updates.append("description=?")
            params.append(data["description"])
        if "duration_seconds" in data and not entry["is_running"]:
            updates.append("duration_seconds=?")
            params.append(data["duration_seconds"])
        if updates:
            conn.execute(
                f"UPDATE task_time_entries SET {
                    ','.join(updates)},updated_at=CURRENT_TIMESTAMP WHERE id=?",
                params + [entry_id],
            )
            total = conn.execute(
                "SELECT COALESCE(SUM(duration_seconds),0) FROM task_time_entries WHERE task_id=?",
                (entry["task_id"],),
            ).fetchone()[0]
            conn.execute(
                "UPDATE task_queue SET actual_hours=? WHERE id=?",
                (round(total / 3600, 2), entry["task_id"]),
            )
    return jsonify({"success": True})


@app.route("/api/tasks/time/entries/<int:entry_id>", methods=["DELETE"])
@require_auth
def delete_time_entry(entry_id):
    """Delete a time entry."""
    with get_db_connection() as conn:
        entry = conn.execute(
            "SELECT task_id FROM task_time_entries WHERE id=?", (entry_id,)
        ).fetchone()
        if not entry:
            return jsonify({"error": "Entry not found"}), 404
        task_id = entry[0]
        conn.execute("DELETE FROM task_time_entries WHERE id=?", (entry_id,))
        total = conn.execute(
            "SELECT COALESCE(SUM(duration_seconds),0) FROM task_time_entries WHERE task_id=?",
            (task_id,),
        ).fetchone()[0]
        conn.execute(
            "UPDATE task_queue SET actual_hours=? WHERE id=?",
            (round(total / 3600, 2), task_id),
        )
    log_activity("delete_time_entry", "time_entry", entry_id)
    return jsonify({"success": True})


@app.route("/api/tasks/time/running", methods=["GET"])
@require_auth
def get_running_timers():
    """Get currently running timers."""
    user_id = session.get("user", "anonymous")
    all_users = request.args.get("all", "false").lower() == "true"
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        q = "SELECT te.*,tq.task_type FROM task_time_entries te JOIN task_queue tq ON te.task_id=tq.id WHERE te.is_running=1"
        entries = conn.execute(
            q if all_users else q + " AND te.user_id=?",
            [] if all_users else [user_id],
        ).fetchall()
        result = []
        now = datetime.now()
        for e in entries:
            entry = dict(e)
            start = (
                datetime.fromisoformat(e["start_time"].replace("Z", ""))
                if isinstance(e["start_time"], str)
                else e["start_time"]
            )
            elapsed = int((now - start).total_seconds())
            entry["elapsed_seconds"] = elapsed
            entry["elapsed_formatted"] = (
                f"{elapsed//3600}h {(elapsed % 3600)//60}m"
            )
            result.append(entry)
        return jsonify({"running_timers": result, "count": len(result)})


@app.route("/api/tasks/time/summary", methods=["GET"])
@require_auth
def get_time_summary():
    """Get time tracking summary."""
    days = request.args.get("days", 7, type=int)
    group_by = request.args.get("group_by", "task")
    cutoff = (datetime.now() - timedelta(days=days)).isoformat()
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        entries = conn.execute(
            "SELECT te.*,tq.task_type FROM task_time_entries te JOIN task_queue tq ON te.task_id=tq.id WHERE te.start_time>=? ORDER BY te.start_time DESC",
            (cutoff,),
        ).fetchall()
        total = sum(e["duration_seconds"] or 0 for e in entries)
        grouped = {}
        for e in entries:
            key = (
                e["task_id"]
                if group_by == "task"
                else (
                    e["user_id"]
                    if group_by == "user"
                    else (
                        e["task_type"]
                        if group_by == "task_type"
                        else (
                            e["start_time"][:10]
                            if e["start_time"]
                            else "unknown"
                        )
                    )
                )
            )
            if key not in grouped:
                grouped[key] = {"key": key, "seconds": 0, "entries": 0}
            grouped[key]["seconds"] += e["duration_seconds"] or 0
            grouped[key]["entries"] += 1
        for k in grouped:
            grouped[k]["hours"] = round(grouped[k]["seconds"] / 3600, 2)
        return jsonify(
            {
                "summary": {
                    "total_seconds": total,
                    "total_hours": round(total / 3600, 2),
                    "entries": len(entries),
                },
                "grouped": list(grouped.values()),
                "group_by": group_by,
                "days": days,
            }
        )


# ============================================================================
# TASK MARKDOWN PREVIEW API
# ============================================================================


@app.route("/api/tasks/markdown/preview", methods=["POST"])
@require_auth
def preview_task_markdown():
    """Preview markdown content rendered as HTML.

    Request body:
        content: Markdown text to preview (required)
        context: Optional context type ('description', 'notes', 'comment')

    Returns:
        html: Rendered HTML
        word_count: Number of words
        char_count: Number of characters
        has_code/has_links/has_tables: Content analysis flags
    """
    import re

    data = request.get_json() or {}
    content = data.get("content", "")
    context = data.get("context", "description")

    if not content:
        return jsonify(
            {
                "html": "",
                "word_count": 0,
                "char_count": 0,
                "has_code": False,
                "has_links": False,
                "has_tables": False,
            }
        )

    # Analyze content
    has_code = bool(re.search(r"```[\s\S]*?```|`[^`]+`", content))
    has_links = bool(
        re.search(r"\[([^\]]+)\]\(([^)]+)\)|https?://\S+", content)
    )
    has_tables = bool(re.search(r"^\|.+\|$", content, re.MULTILINE))
    has_lists = bool(
        re.search(r"^[\s]*[-*+]\s|^\s*\d+\.\s", content, re.MULTILINE)
    )
    has_headers = bool(re.search(r"^#{1,6}\s", content, re.MULTILINE))
    has_checkboxes = bool(
        re.search(r"^\s*[-*]\s*\[([ xX])\]", content, re.MULTILINE)
    )

    plain_text = re.sub(r"[#*`\[\]()_~]", "", content)
    words, chars = len(plain_text.split()), len(content)

    # Convert markdown to HTML
    html = (
        content.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
    )

    # Code blocks
    def format_code_block(m):
        lang = m.group(1) or ""
        return (
            f'<pre><code class="language-{lang}">{m.group(2)}</code></pre>'
            if lang
            else f"<pre><code>{m.group(2)}</code></pre>"
        )

    html = re.sub(r"```(\w*)\n([\s\S]*?)```", format_code_block, html)
    html = re.sub(r"`([^`]+)`", r"<code>\1</code>", html)

    # Headers
    for i in range(6, 0, -1):
        html = re.sub(
            f'^{"#" * i} (.*)$', f"<h{i}>\\1</h{i}>", html, flags=re.MULTILINE
        )

    # Formatting
    html = re.sub(r"\*\*\*(.*?)\*\*\*", r"<strong><em>\1</em></strong>", html)
    html = re.sub(r"\*\*(.*?)\*\*", r"<strong>\1</strong>", html)
    html = re.sub(r"\*(.*?)\*", r"<em>\1</em>", html)
    html = re.sub(r"~~(.*?)~~", r"<del>\1</del>", html)

    # Links
    html = re.sub(
        r"\[([^\]]+)\]\(([^)]+)\)",
        r'<a href="\2" target="_blank" rel="noopener">\1</a>',
        html,
    )

    # Checkboxes
    html = re.sub(
        r"^\s*[-*]\s*\[ \](.*)$",
        r'<div class="checkbox"><input type="checkbox" disabled>\1</div>',
        html,
        flags=re.MULTILINE,
    )
    html = re.sub(
        r"^\s*[-*]\s*\[[xX]\](.*)$",
        r'<div class="checkbox"><input type="checkbox" checked disabled>\1</div>',
        html,
        flags=re.MULTILINE,
    )

    # Lists
    html = re.sub(r"^[-*+] (.*)$", r"<li>\1</li>", html, flags=re.MULTILINE)
    html = re.sub(r"^\d+\. (.*)$", r"<li>\1</li>", html, flags=re.MULTILINE)
    html = re.sub(r"((?:<li>.*</li>\s*)+)", r"<ul>\1</ul>", html)

    # Blockquotes
    html = re.sub(
        r"^&gt; (.*)$",
        r"<blockquote>\1</blockquote>",
        html,
        flags=re.MULTILINE,
    )

    # Tables
    def process_table(m):
        rows = m.group(0).strip().split("\n")
        if len(rows) < 2:
            return m.group(0)
        html_rows, is_header = [], True
        for row in rows:
            cells = [c.strip() for c in row.strip("|").split("|")]
            if all(re.match(r"^[-:]+$", c) for c in cells):
                is_header = False
                continue
            tag = "th" if is_header else "td"
            html_rows.append(
                "<tr>"
                + "".join(f"<{tag}>{c}</{tag}>" for c in cells)
                + "</tr>"
            )
            is_header = False
        return (
            '<table class="markdown-table">' + "".join(html_rows) + "</table>"
        )

    html = re.sub(r"(^\|.+\|\s*\n?)+", process_table, html, flags=re.MULTILINE)

    # Horizontal rules
    html = re.sub(r"^---+$", r"<hr>", html, flags=re.MULTILINE)

    # Paragraphs
    html = re.sub(r"\n\n+", r"</p><p>", html)
    html = re.sub(r"\n", r"<br>", html)
    html = "<p>" + html + "</p>"

    # Clean up
    html = re.sub(r"<p>\s*</p>", "", html)
    html = re.sub(r"<p>(<[hupbt])", r"\1", html)
    html = re.sub(r"(</[hupbt][^>]*>)</p>", r"\1", html)

    html = f'<div class="markdown-preview markdown-{context}">{html}</div>'

    return jsonify(
        {
            "html": html,
            "word_count": words,
            "char_count": chars,
            "line_count": len(content.split("\n")),
            "has_code": has_code,
            "has_links": has_links,
            "has_tables": has_tables,
            "has_lists": has_lists,
            "has_headers": has_headers,
            "has_checkboxes": has_checkboxes,
            "context": context,
        }
    )


@app.route("/api/tasks/<int:task_id>/markdown", methods=["GET"])
@require_auth
def get_task_markdown_preview(task_id):
    """Get rendered markdown preview for a task's description and notes."""
    import re

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        task = conn.execute(
            "SELECT id, task_type, description, notes, result FROM task_queue WHERE id = ?",
            (task_id,),
        ).fetchone()
        if not task:
            return jsonify({"error": "Task not found"}), 404

        def render_field(content):
            if not content:
                return {"html": "", "word_count": 0, "has_content": False}
            html = (
                content.replace("&", "&amp;")
                .replace("<", "&lt;")
                .replace(">", "&gt;")
            )
            html = re.sub(r"`([^`]+)`", r"<code>\1</code>", html)
            for i in range(6, 0, -1):
                html = re.sub(
                    f'^{"#" * i} (.*)$',
                    f"<h{i}>\\1</h{i}>",
                    html,
                    flags=re.MULTILINE,
                )
            html = re.sub(r"\*\*(.*?)\*\*", r"<strong>\1</strong>", html)
            html = re.sub(r"\*(.*?)\*", r"<em>\1</em>", html)
            html = re.sub(
                r"\[([^\]]+)\]\(([^)]+)\)",
                r'<a href="\2" target="_blank">\1</a>',
                html,
            )
            html = re.sub(r"\n\n+", r"</p><p>", html)
            html = re.sub(r"\n", r"<br>", html)
            html = "<p>" + html + "</p>"
            plain = re.sub(r"[#*`\[\]()_~]", "", content)
            return {
                "html": html,
                "word_count": len(plain.split()),
                "has_content": True,
            }

        return jsonify(
            {
                "task_id": task_id,
                "description": render_field(task["description"]),
                "notes": render_field(task["notes"]),
                "result": render_field(task["result"]),
            }
        )


# ============================================================================
# TASK CUSTOM FIELDS API
# ============================================================================

CUSTOM_FIELD_TYPES = [
    "text",
    "number",
    "date",
    "datetime",
    "boolean",
    "select",
    "multiselect",
    "url",
    "email",
    "json",
]


@app.route("/api/tasks/custom-fields", methods=["GET"])
@require_auth
def list_custom_fields():
    """List all custom field definitions."""
    include_inactive = (
        request.args.get("include_inactive", "false").lower() == "true"
    )
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        query = "SELECT * FROM task_custom_fields"
        if not include_inactive:
            query += " WHERE is_active = 1"
        query += " ORDER BY display_order, name"
        fields = conn.execute(query).fetchall()
        result = []
        for f in fields:
            field = dict(f)
            if field["options"]:
                try:
                    field["options"] = json.loads(field["options"])
                except Exception:
                    field["options"] = []
            result.append(field)
        return jsonify(
            {
                "fields": result,
                "total": len(result),
                "types": CUSTOM_FIELD_TYPES,
            }
        )


@app.route("/api/tasks/custom-fields", methods=["POST"])
@require_auth
def create_custom_field():
    """Create a new custom field definition."""
    data = request.get_json() or {}
    name, display_name = data.get("name"), data.get("display_name")
    field_type = data.get("field_type", "text")
    if not name or not display_name:
        return jsonify({"error": "name and display_name are required"}), 400
    import re

    if not re.match(r"^[a-zA-Z][a-zA-Z0-9_]*$", name):
        return (
            jsonify(
                {
                    "error": "name must start with letter and contain only alphanumeric/underscore"
                }
            ),
            400,
        )
    if field_type not in CUSTOM_FIELD_TYPES:
        return (
            jsonify(
                {"error": f"field_type must be one of: {CUSTOM_FIELD_TYPES}"}
            ),
            400,
        )
    options = data.get("options")
    if field_type in ["select", "multiselect"] and not options:
        return (
            jsonify(
                {"error": "options required for select/multiselect fields"}
            ),
            400,
        )
    user_id = session.get("user_id")
    with get_db_connection() as conn:
        if conn.execute(
            "SELECT id FROM task_custom_fields WHERE name = ?", (name,)
        ).fetchone():
            return jsonify({"error": f'Field "{name}" already exists'}), 409
        max_order = conn.execute(
            "SELECT MAX(display_order) as m FROM task_custom_fields"
        ).fetchone()
        cursor = conn.execute(
            """
            INSERT INTO task_custom_fields (name, display_name, field_type, description, options, default_value,
                is_required, validation_regex, min_value, max_value, display_order, show_in_list, show_in_filters, created_by)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                name,
                display_name,
                field_type,
                data.get("description"),
                json.dumps(options) if options else None,
                data.get("default_value"),
                data.get("is_required", False),
                data.get("validation_regex"),
                data.get("min_value"),
                data.get("max_value"),
                (max_order[0] or 0) + 1,
                data.get("show_in_list", False),
                data.get("show_in_filters", False),
                user_id,
            ),
        )
        log_activity(
            "create",
            "custom_field",
            cursor.lastrowid,
            f"Created custom field: {name}",
        )
        return (
            jsonify({"success": True, "id": cursor.lastrowid, "name": name}),
            201,
        )


@app.route("/api/tasks/custom-fields/<int:field_id>", methods=["GET"])
@require_auth
def get_custom_field(field_id):
    """Get a custom field definition."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        field = conn.execute(
            "SELECT * FROM task_custom_fields WHERE id = ?", (field_id,)
        ).fetchone()
        if not field:
            return jsonify({"error": "Field not found"}), 404
        result = dict(field)
        if result["options"]:
            try:
                result["options"] = json.loads(result["options"])
            except Exception:
                result["options"] = []
        return jsonify(result)


@app.route("/api/tasks/custom-fields/<int:field_id>", methods=["PUT"])
@require_auth
def update_custom_field(field_id):
    """Update a custom field definition."""
    data = request.get_json() or {}
    with get_db_connection() as conn:
        if not conn.execute(
            "SELECT id FROM task_custom_fields WHERE id = ?", (field_id,)
        ).fetchone():
            return jsonify({"error": "Field not found"}), 404
        updates, params = [], []
        for key in [
            "display_name",
            "description",
            "default_value",
            "is_required",
            "is_active",
            "validation_regex",
            "min_value",
            "max_value",
            "display_order",
            "show_in_list",
            "show_in_filters",
        ]:
            if key in data:
                updates.append(f"{key} = ?")
                params.append(data[key])
        if "options" in data:
            updates.append("options = ?")
            params.append(
                json.dumps(data["options"]) if data["options"] else None
            )
        if not updates:
            return jsonify({"error": "No valid fields to update"}), 400
        updates.append("updated_at = CURRENT_TIMESTAMP")
        params.append(field_id)
        conn.execute(
            f"UPDATE task_custom_fields SET {', '.join(updates)} WHERE id = ?",
            params,
        )
        log_activity(
            "update", "custom_field", field_id, "Updated custom field"
        )
        return jsonify({"success": True})


@app.route("/api/tasks/custom-fields/<int:field_id>", methods=["DELETE"])
@require_auth
def delete_custom_field(field_id):
    """Delete a custom field and all its values."""
    with get_db_connection() as conn:
        field = conn.execute(
            "SELECT name FROM task_custom_fields WHERE id = ?", (field_id,)
        ).fetchone()
        if not field:
            return jsonify({"error": "Field not found"}), 404
        conn.execute(
            "DELETE FROM task_custom_field_values WHERE field_id = ?",
            (field_id,),
        )
        conn.execute(
            "DELETE FROM task_custom_fields WHERE id = ?", (field_id,)
        )
        log_activity(
            "delete",
            "custom_field",
            field_id,
            f"Deleted custom field: {field[0]}",
        )
        return jsonify({"success": True})


@app.route("/api/tasks/custom-fields/reorder", methods=["PUT"])
@require_auth
def reorder_custom_fields():
    """Reorder custom fields. Body: {field_ids: [id1, id2, ...]}"""
    data = request.get_json() or {}
    field_ids = data.get("field_ids", [])
    if not field_ids:
        return jsonify({"error": "field_ids array required"}), 400
    with get_db_connection() as conn:
        for order, fid in enumerate(field_ids):
            conn.execute(
                "UPDATE task_custom_fields SET display_order = ? WHERE id = ?",
                (order, fid),
            )
        return jsonify({"success": True})


@app.route("/api/tasks/<int:task_id>/custom-fields", methods=["GET"])
@require_auth
def get_task_custom_field_values(task_id):
    """Get all custom field values for a task."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        if not conn.execute(
            "SELECT id FROM task_queue WHERE id = ?", (task_id,)
        ).fetchone():
            return jsonify({"error": "Task not found"}), 404
        values = conn.execute(
            """
            SELECT v.*, f.name, f.display_name, f.field_type FROM task_custom_field_values v
            JOIN task_custom_fields f ON v.field_id = f.id WHERE v.task_id = ? AND f.is_active = 1 ORDER BY f.display_order
        """,
            (task_id,),
        ).fetchall()
        result = {}
        for v in values:
            ft = v["field_type"]
            val = (
                v["value_number"]
                if ft == "number"
                else (
                    v["value_date"]
                    if ft in ["date", "datetime"]
                    else (
                        json.loads(v["value_json"])
                        if ft in ["json", "multiselect"] and v["value_json"]
                        else (
                            v["value_text"] == "true"
                            if ft == "boolean" and v["value_text"]
                            else v["value_text"]
                        )
                    )
                )
            )
            result[v["name"]] = {
                "field_id": v["field_id"],
                "display_name": v["display_name"],
                "field_type": ft,
                "value": val,
            }
        return jsonify({"task_id": task_id, "custom_fields": result})


@app.route("/api/tasks/<int:task_id>/custom-fields", methods=["PUT"])
@require_auth
def set_task_custom_field_values(task_id):
    """Set custom field values for a task. Body: {field_name: value, ...}"""
    data = request.get_json() or {}
    if not data:
        return jsonify({"error": "No values provided"}), 400
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        if not conn.execute(
            "SELECT id FROM task_queue WHERE id = ?", (task_id,)
        ).fetchone():
            return jsonify({"error": "Task not found"}), 404
        fields = {
            f["name"]: dict(f)
            for f in conn.execute(
                "SELECT * FROM task_custom_fields WHERE is_active = 1"
            ).fetchall()
        }
        updated, errors = [], []
        for name, value in data.items():
            if name not in fields:
                errors.append({"field": name, "error": "Unknown field"})
                continue
            field = fields[name]
            if field["is_required"] and value in [None, "", []]:
                errors.append({"field": name, "error": "Required field"})
                continue
            v_text, v_num, v_date, v_json = None, None, None, None
            ft = field["field_type"]
            if value is not None:
                if ft == "number":
                    try:
                        v_num = float(value)
                    except Exception:
                        errors.append(
                            {"field": name, "error": "Invalid number"}
                        )
                        continue
                elif ft in ["date", "datetime"]:
                    v_date = value
                elif ft in ["json", "multiselect"]:
                    v_json = json.dumps(value) if value else None
                elif ft == "boolean":
                    v_text = "true" if value else "false"
                else:
                    v_text = str(value)
            conn.execute(
                """
                INSERT INTO task_custom_field_values (task_id, field_id, value_text, value_number, value_date, value_json)
                VALUES (?, ?, ?, ?, ?, ?) ON CONFLICT(task_id, field_id) DO UPDATE SET
                value_text=excluded.value_text, value_number=excluded.value_number, value_date=excluded.value_date, value_json=excluded.value_json, updated_at=CURRENT_TIMESTAMP
            """,
                (task_id, field["id"], v_text, v_num, v_date, v_json),
            )
            updated.append(name)
        return jsonify(
            {"success": len(errors) == 0, "updated": updated, "errors": errors}
        )


@app.route(
    "/api/tasks/<int:task_id>/custom-fields/<field_name>", methods=["DELETE"]
)
@require_auth
def delete_task_custom_field_value(task_id, field_name):
    """Remove a custom field value from a task."""
    with get_db_connection() as conn:
        field = conn.execute(
            "SELECT id FROM task_custom_fields WHERE name = ?", (field_name,)
        ).fetchone()
        if not field:
            return jsonify({"error": "Field not found"}), 404
        conn.execute(
            "DELETE FROM task_custom_field_values WHERE task_id = ? AND field_id = ?",
            (task_id, field[0]),
        )
        return jsonify({"success": True})


@app.route("/api/tasks/custom-fields/search", methods=["GET"])
@require_auth
def search_tasks_by_custom_field():
    """Search tasks by custom field values. Query: field, value, operator (eq|ne|gt|lt|contains)"""
    field_name, value = request.args.get("field"), request.args.get("value")
    operator = request.args.get("operator", "eq")
    if not field_name:
        return jsonify({"error": "field parameter required"}), 400
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        field = conn.execute(
            "SELECT * FROM task_custom_fields WHERE name = ?", (field_name,)
        ).fetchone()
        if not field:
            return jsonify({"error": "Field not found"}), 404
        ft = field["field_type"]
        col = (
            "value_number"
            if ft == "number"
            else "value_date" if ft in ["date", "datetime"] else "value_text"
        )
        ops = {
            "eq": "=",
            "ne": "!=",
            "gt": ">",
            "lt": "<",
            "gte": ">=",
            "lte": "<=",
            "contains": "LIKE",
        }
        if operator not in ops:
            return jsonify({"error": "Invalid operator"}), 400
        if operator == "contains":
            value = f"%{value}%"
        tasks = conn.execute(
            f"SELECT t.* FROM task_queue t JOIN task_custom_field_values v ON t.id=v.task_id WHERE v.field_id=? AND v.{col} {
                ops[operator]} ?",
            (field["id"], value),
        ).fetchall()
        return jsonify(
            {"tasks": [dict(t) for t in tasks], "total": len(tasks)}
        )


# ============================================================================
# TASK ESTIMATION API (Story Points)
# ============================================================================


@app.route("/api/tasks/<int:task_id>/estimate", methods=["GET"])
@require_auth
def get_task_estimate(task_id):
    """Get estimation details for a task."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        task = conn.execute(
            """
            SELECT id, task_type, status, story_points, estimated_hours, actual_hours,
                   created_at, started_at, completed_at
            FROM task_queue WHERE id = ?
        """,
            (task_id,),
        ).fetchone()

        if not task:
            return jsonify({"error": "Task not found"}), 404

        # Calculate actual duration if completed
        actual_duration = None
        if task["started_at"] and task["completed_at"]:
            start = (
                datetime.fromisoformat(
                    task["started_at"].replace("Z", "+00:00")
                )
                if isinstance(task["started_at"], str)
                else task["started_at"]
            )
            end = (
                datetime.fromisoformat(
                    task["completed_at"].replace("Z", "+00:00")
                )
                if isinstance(task["completed_at"], str)
                else task["completed_at"]
            )
            if start and end:
                actual_duration = (end - start).total_seconds() / 3600  # hours

        return jsonify(
            {
                "task_id": task_id,
                "story_points": task["story_points"],
                "estimated_hours": task["estimated_hours"],
                "actual_hours": task["actual_hours"] or actual_duration,
                "status": task["status"],
            }
        )


@app.route("/api/tasks/<int:task_id>/estimate", methods=["PUT"])
@require_auth
def update_task_estimate(task_id):
    """Update estimation for a task.

    Body:
        story_points: Story points (1, 2, 3, 5, 8, 13, 21 recommended)
        estimated_hours: Estimated hours to complete
        actual_hours: Actual hours spent (usually set on completion)
    """
    data = request.get_json() or {}

    with get_db_connection() as conn:
        task = conn.execute(
            "SELECT id FROM task_queue WHERE id = ?", (task_id,)
        ).fetchone()
        if not task:
            return jsonify({"error": "Task not found"}), 404

        updates, params = [], []

        if "story_points" in data:
            sp = data["story_points"]
            if sp is not None and sp not in [1, 2, 3, 5, 8, 13, 21, 34]:
                return (
                    jsonify(
                        {
                            "error": "Invalid story points. Use Fibonacci: 1,2,3,5,8,13,21,34"
                        }
                    ),
                    400,
                )
            updates.append("story_points = ?")
            params.append(sp)

        if "estimated_hours" in data:
            updates.append("estimated_hours = ?")
            params.append(data["estimated_hours"])

        if "actual_hours" in data:
            updates.append("actual_hours = ?")
            params.append(data["actual_hours"])

        if not updates:
            return jsonify({"error": "No fields to update"}), 400

        params.append(task_id)
        conn.execute(
            f"UPDATE task_queue SET {', '.join(updates)} WHERE id = ?", params
        )

        log_activity("update_estimate", "task", task_id, data)

    return jsonify({"success": True, "task_id": task_id})


@app.route("/api/tasks/velocity", methods=["GET"])
@require_auth
def get_velocity():
    """Get velocity metrics (story points completed per period).

    Query params:
        period: 'week', 'sprint' (2 weeks), 'month' (default: 'week')
        periods: Number of periods to analyze (default: 4)
    """
    period = request.args.get("period", "week")
    num_periods = request.args.get("periods", 4, type=int)

    days_per_period = {"week": 7, "sprint": 14, "month": 30}.get(period, 7)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        velocity_data = []
        for i in range(num_periods):
            start_days = (i + 1) * days_per_period
            end_days = i * days_per_period

            result = conn.execute(
                """
                SELECT
                    COUNT(*) as tasks_completed,
                    COALESCE(SUM(story_points), 0) as points_completed,
                    COALESCE(SUM(estimated_hours), 0) as estimated_hours,
                    COALESCE(SUM(actual_hours), 0) as actual_hours,
                    COALESCE(AVG(story_points), 0) as avg_points
                FROM task_queue
                WHERE status = 'completed'
                  AND completed_at >= DATE('now', ?)
                  AND completed_at < DATE('now', ?)
            """,
                (
                    f"-{start_days} days",
                    f"-{end_days} days" if end_days > 0 else "now",
                ),
            ).fetchone()

            velocity_data.append(
                {
                    "period": i + 1,
                    "period_name": (
                        f"{period} -{i+1}" if i > 0 else f"current {period}"
                    ),
                    "tasks_completed": result["tasks_completed"],
                    "points_completed": result["points_completed"],
                    "estimated_hours": round(result["estimated_hours"], 1),
                    "actual_hours": round(result["actual_hours"], 1),
                    "avg_points_per_task": round(result["avg_points"], 1),
                }
            )

        # Calculate averages
        total_points = sum(v["points_completed"] for v in velocity_data)
        total_tasks = sum(v["tasks_completed"] for v in velocity_data)
        avg_velocity = round(total_points / max(num_periods, 1), 1)

        return jsonify(
            {
                "period_type": period,
                "periods_analyzed": num_periods,
                "velocity_by_period": velocity_data,
                "average_velocity": avg_velocity,
                "total_points": total_points,
                "total_tasks": total_tasks,
            }
        )


@app.route("/api/tasks/date-range-stats", methods=["GET"])
@require_auth
def get_task_date_range_stats():
    """Get task statistics for a date range.

    Query params:
        start_date: Start of date range (ISO format or YYYY-MM-DD, required)
        end_date: End of date range (defaults to now)
        group_by: 'day', 'week', or 'month' (default 'day')
        task_type: Filter by task type
    """
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")
    group_by = request.args.get("group_by", "day")
    task_type = request.args.get("task_type")

    if not start_date:
        return api_error("start_date is required", 400, "missing_param")

    # Determine date format for grouping
    if group_by == "week":
        date_format = "%Y-W%W"
    elif group_by == "month":
        date_format = "%Y-%m"
    else:
        date_format = "%Y-%m-%d"

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        params = [start_date]
        type_filter = ""
        if task_type:
            type_filter = " AND task_type = ?"
            params.append(task_type)

        end_clause = ""
        if end_date:
            end_clause = " AND created_at <= ?"
            params.append(end_date)

        # Overall counts for the range
        summary = conn.execute(
            """
            SELECT
                COUNT(*) as total_tasks,
                SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completed,
                SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed,
                SUM(CASE WHEN status = 'pending' THEN 1 ELSE 0 END) as pending,
                SUM(CASE WHEN status = 'running' THEN 1 ELSE 0 END) as running,
                AVG(CASE WHEN status = 'completed' AND started_at IS NOT NULL
                    THEN (julianday(completed_at) - julianday(started_at)) * 24 * 60
                    ELSE NULL END) as avg_duration_minutes
            FROM task_queue
            WHERE created_at >= ? {type_filter} {end_clause}
        """,
            params,
        ).fetchone()

        # Tasks by date group
        params_by_date = [start_date]
        if task_type:
            params_by_date.append(task_type)
        if end_date:
            params_by_date.append(end_date)

        by_date = conn.execute(
            """
            SELECT
                strftime('{date_format}', created_at) as period,
                COUNT(*) as created,
                SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completed,
                SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed
            FROM task_queue
            WHERE created_at >= ? {type_filter} {end_clause}
            GROUP BY period
            ORDER BY period
        """,
            params_by_date,
        ).fetchall()

        # Tasks by type
        params_by_type = [start_date]
        if task_type:
            params_by_type.append(task_type)
        if end_date:
            params_by_type.append(end_date)

        by_type = conn.execute(
            """
            SELECT
                task_type,
                COUNT(*) as count,
                SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completed,
                SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed,
                AVG(CASE WHEN status = 'completed' AND started_at IS NOT NULL
                    THEN (julianday(completed_at) - julianday(started_at)) * 24 * 60
                    ELSE NULL END) as avg_duration_minutes
            FROM task_queue
            WHERE created_at >= ? {type_filter} {end_clause}
            GROUP BY task_type
            ORDER BY count DESC
        """,
            params_by_type,
        ).fetchall()

        # Completion rate
        total = summary["total_tasks"] or 0
        completed = summary["completed"] or 0
        completion_rate = (
            round((completed / total) * 100, 1) if total > 0 else 0
        )

        return jsonify(
            {
                "date_range": {
                    "start": start_date,
                    "end": end_date or "now",
                    "group_by": group_by,
                },
                "summary": {
                    "total_tasks": total,
                    "completed": completed,
                    "failed": summary["failed"] or 0,
                    "pending": summary["pending"] or 0,
                    "running": summary["running"] or 0,
                    "completion_rate": completion_rate,
                    "avg_duration_minutes": round(
                        summary["avg_duration_minutes"] or 0, 1
                    ),
                },
                "by_period": [dict(r) for r in by_date],
                "by_type": [dict(r) for r in by_type],
            }
        )


@app.route("/api/tasks/estimation-accuracy", methods=["GET"])
@require_auth
def get_estimation_accuracy():
    """Get estimation accuracy metrics.

    Returns how accurate story point and hour estimates are compared to actuals.
    """
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Get completed tasks with estimates
        tasks = conn.execute(
            """
            SELECT story_points, estimated_hours, actual_hours,
                   (julianday(completed_at) - julianday(started_at)) * 24 as duration_hours
            FROM task_queue
            WHERE status = 'completed'
              AND (story_points IS NOT NULL OR estimated_hours IS NOT NULL)
              AND started_at IS NOT NULL AND completed_at IS NOT NULL
            ORDER BY completed_at DESC
            LIMIT 100
        """
        ).fetchall()

        if not tasks:
            return jsonify(
                {
                    "message": "No completed tasks with estimates",
                    "accuracy": None,
                }
            )

        # Calculate accuracy metrics
        hour_estimates = [
            (t["estimated_hours"], t["actual_hours"] or t["duration_hours"])
            for t in tasks
            if t["estimated_hours"]
            and (t["actual_hours"] or t["duration_hours"])
        ]

        point_distribution = {}
        for t in tasks:
            if t["story_points"]:
                sp = t["story_points"]
                if sp not in point_distribution:
                    point_distribution[sp] = {"count": 0, "total_hours": 0}
                point_distribution[sp]["count"] += 1
                point_distribution[sp]["total_hours"] += (
                    t["actual_hours"] or t["duration_hours"] or 0
                )

        # Hours per story point
        for sp in point_distribution:
            point_distribution[sp]["avg_hours"] = round(
                point_distribution[sp]["total_hours"]
                / point_distribution[sp]["count"],
                2,
            )

        # Estimation variance
        if hour_estimates:
            variances = [
                (est - act) / max(act, 0.1) * 100
                for est, act in hour_estimates
                if act
            ]
            avg_variance = (
                round(sum(variances) / len(variances), 1) if variances else 0
            )
            overestimated = len([v for v in variances if v > 10])
            underestimated = len([v for v in variances if v < -10])
            accurate = len(variances) - overestimated - underestimated
        else:
            avg_variance, overestimated, underestimated, accurate = 0, 0, 0, 0

        return jsonify(
            {
                "tasks_analyzed": len(tasks),
                "estimation_variance_pct": avg_variance,
                "breakdown": {
                    "accurate": accurate,
                    "overestimated": overestimated,
                    "underestimated": underestimated,
                },
                "hours_per_story_point": point_distribution,
            }
        )


# ============================================================================
# TASK ATTACHMENTS API
# ============================================================================


@app.route("/api/tasks/<int:task_id>/attachments", methods=["GET"])
@require_auth
def get_task_attachments(task_id):
    """Get all attachments for a task."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        attachments = conn.execute(
            """
            SELECT ta.*, u.username as uploader_name
            FROM task_attachments ta
            LEFT JOIN users u ON ta.uploaded_by = u.id
            WHERE ta.task_id = ?
            ORDER BY ta.created_at DESC
        """,
            (task_id,),
        ).fetchall()

        return jsonify(
            {
                "task_id": task_id,
                "attachments": [dict(a) for a in attachments],
                "count": len(attachments),
            }
        )


@app.route("/api/tasks/<int:task_id>/attachments", methods=["POST"])
@require_auth
def upload_task_attachment(task_id):
    """Upload a file attachment to a task."""
    import mimetypes
    import uuid

    # Check if task exists
    with get_db_connection() as conn:
        task = conn.execute(
            "SELECT id FROM task_queue WHERE id = ?", (task_id,)
        ).fetchone()
        if not task:
            return jsonify({"error": "Task not found"}), 404

    # Check if file was uploaded
    if "file" not in request.files:
        return jsonify({"error": "No file provided"}), 400

    file = request.files["file"]
    if file.filename == "":
        return jsonify({"error": "No file selected"}), 400

    if not allowed_file(file.filename):
        return (
            jsonify(
                {
                    "error": "File type not allowed",
                    "allowed_types": list(ALLOWED_EXTENSIONS),
                }
            ),
            400,
        )

    # Generate unique filename
    original_filename = file.filename
    safe_filename = secure_filename_custom(original_filename)
    unique_filename = f"{task_id}_{uuid.uuid4().hex[:8]}_{safe_filename}"

    # Create task-specific upload folder
    task_folder = UPLOAD_FOLDER / str(task_id)
    task_folder.mkdir(exist_ok=True)

    # Save file
    file_path = task_folder / unique_filename
    file.save(str(file_path))

    # Get file info
    file_size = file_path.stat().st_size
    mime_type = (
        mimetypes.guess_type(original_filename)[0]
        or "application/octet-stream"
    )

    # Save to database
    with get_db_connection() as conn:
        cursor = conn.execute(
            """
            INSERT INTO task_attachments (task_id, filename, original_filename, file_path, file_size, mime_type, uploaded_by)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """,
            (
                task_id,
                unique_filename,
                original_filename,
                str(file_path),
                file_size,
                mime_type,
                session.get("user_id"),
            ),
        )

        attachment_id = cursor.lastrowid

    log_activity(
        "upload_attachment",
        "task",
        task_id,
        {"filename": original_filename, "size": file_size},
    )

    return (
        jsonify(
            {
                "success": True,
                "attachment": {
                    "id": attachment_id,
                    "task_id": task_id,
                    "filename": unique_filename,
                    "original_filename": original_filename,
                    "file_size": file_size,
                    "mime_type": mime_type,
                },
            }
        ),
        201,
    )


@app.route(
    "/api/tasks/<int:task_id>/attachments/<int:attachment_id>", methods=["GET"]
)
@require_auth
def download_task_attachment(task_id, attachment_id):
    """Download a task attachment."""
    from flask import send_file

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        attachment = conn.execute(
            """
            SELECT * FROM task_attachments WHERE id = ? AND task_id = ?
        """,
            (attachment_id, task_id),
        ).fetchone()

        if not attachment:
            return jsonify({"error": "Attachment not found"}), 404

        file_path = Path(attachment["file_path"])
        if not file_path.exists():
            return jsonify({"error": "File not found on disk"}), 404

        return send_file(
            str(file_path),
            mimetype=attachment["mime_type"],
            as_attachment=True,
            download_name=attachment["original_filename"],
        )


@app.route(
    "/api/tasks/<int:task_id>/attachments/<int:attachment_id>",
    methods=["DELETE"],
)
@require_auth
def delete_task_attachment(task_id, attachment_id):
    """Delete a task attachment."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        attachment = conn.execute(
            """
            SELECT * FROM task_attachments WHERE id = ? AND task_id = ?
        """,
            (attachment_id, task_id),
        ).fetchone()

        if not attachment:
            return jsonify({"error": "Attachment not found"}), 404

        # Delete file from disk
        file_path = Path(attachment["file_path"])
        if file_path.exists():
            file_path.unlink()

        # Delete from database
        conn.execute(
            "DELETE FROM task_attachments WHERE id = ?", (attachment_id,)
        )

    log_activity(
        "delete_attachment",
        "task",
        task_id,
        {"filename": attachment["original_filename"]},
    )

    return jsonify({"success": True})


@app.route("/api/attachments", methods=["GET"])
@require_auth
def list_all_attachments():
    """List all attachments across all tasks."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        attachments = conn.execute(
            """
            SELECT ta.*, u.username as uploader_name, tq.task_type
            FROM task_attachments ta
            LEFT JOIN users u ON ta.uploaded_by = u.id
            LEFT JOIN task_queue tq ON ta.task_id = tq.id
            ORDER BY ta.created_at DESC
            LIMIT 100
        """
        ).fetchall()

        total_size = sum(a["file_size"] or 0 for a in attachments)

        return jsonify(
            {
                "attachments": [dict(a) for a in attachments],
                "count": len(attachments),
                "total_size_bytes": total_size,
                "total_size_mb": round(total_size / (1024 * 1024), 2),
            }
        )


# ============================================================================
# TASK COMMENTS/NOTES API
# ============================================================================


@app.route("/api/tasks/<int:task_id>/comments", methods=["GET"])
@require_auth
def get_task_comments(task_id):
    """Get all comments/notes for a task."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Verify task exists
        task = conn.execute(
            "SELECT id FROM task_queue WHERE id = ?", (task_id,)
        ).fetchone()
        if not task:
            return jsonify({"error": "Task not found"}), 404

        comments = conn.execute(
            """
            SELECT tc.*, u.username as author_name
            FROM task_comments tc
            LEFT JOIN users u ON tc.author_id = u.id
            WHERE tc.task_id = ?
            ORDER BY tc.created_at DESC
        """,
            (task_id,),
        ).fetchall()

        return jsonify(
            {
                "task_id": task_id,
                "comments": [dict(c) for c in comments],
                "count": len(comments),
            }
        )


@app.route("/api/tasks/<int:task_id>/comments", methods=["POST"])
@require_auth
def add_task_comment(task_id):
    """Add a comment/note to a task.

    Body:
        content: Comment text (required)
        comment_type: Type of comment - 'note', 'update', 'issue', 'resolution' (default: 'note')
        is_internal: Whether comment is internal only (default: false)
    """
    data = request.get_json() or {}
    content = data.get("content", "").strip()

    if not content:
        return jsonify({"error": "content is required"}), 400

    with get_db_connection() as conn:
        # Verify task exists
        task = conn.execute(
            "SELECT id FROM task_queue WHERE id = ?", (task_id,)
        ).fetchone()
        if not task:
            return jsonify({"error": "Task not found"}), 404

        author = session.get("username", "anonymous")
        author_id = session.get("user_id")

        cursor = conn.execute(
            """
            INSERT INTO task_comments (task_id, content, author, author_id, comment_type, is_internal)
            VALUES (?, ?, ?, ?, ?, ?)
        """,
            (
                task_id,
                content,
                author,
                author_id,
                data.get("comment_type", "note"),
                1 if data.get("is_internal") else 0,
            ),
        )

        comment_id = cursor.lastrowid

        # Fetch the created comment
        conn.row_factory = sqlite3.Row
        comment = conn.execute(
            "SELECT * FROM task_comments WHERE id = ?", (comment_id,)
        ).fetchone()

    log_activity("add_comment", "task", task_id, {"comment_id": comment_id})

    return jsonify({"success": True, "comment": dict(comment)}), 201


@app.route(
    "/api/tasks/<int:task_id>/comments/<int:comment_id>", methods=["PUT"]
)
@require_auth
def update_task_comment(task_id, comment_id):
    """Update a task comment.

    Body:
        content: New comment text
        comment_type: New comment type
        is_internal: Update internal flag
    """
    data = request.get_json() or {}

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Verify comment exists and belongs to task
        comment = conn.execute(
            """
            SELECT * FROM task_comments WHERE id = ? AND task_id = ?
        """,
            (comment_id, task_id),
        ).fetchone()

        if not comment:
            return jsonify({"error": "Comment not found"}), 404

        # Build update
        updates = ["edited_at = CURRENT_TIMESTAMP"]
        params = []

        if "content" in data:
            content = data["content"].strip()
            if not content:
                return jsonify({"error": "content cannot be empty"}), 400
            updates.append("content = ?")
            params.append(content)

        if "comment_type" in data:
            updates.append("comment_type = ?")
            params.append(data["comment_type"])

        if "is_internal" in data:
            updates.append("is_internal = ?")
            params.append(1 if data["is_internal"] else 0)

        params.append(comment_id)

        conn.execute(
            f"UPDATE task_comments SET {', '.join(updates)} WHERE id = ?",
            params,
        )

        # Fetch updated comment
        updated = conn.execute(
            "SELECT * FROM task_comments WHERE id = ?", (comment_id,)
        ).fetchone()

    return jsonify({"success": True, "comment": dict(updated)})


@app.route(
    "/api/tasks/<int:task_id>/comments/<int:comment_id>", methods=["DELETE"]
)
@require_auth
def delete_task_comment(task_id, comment_id):
    """Delete a task comment."""
    with get_db_connection() as conn:
        # Verify comment exists and belongs to task
        comment = conn.execute(
            """
            SELECT id FROM task_comments WHERE id = ? AND task_id = ?
        """,
            (comment_id, task_id),
        ).fetchone()

        if not comment:
            return jsonify({"error": "Comment not found"}), 404

        conn.execute("DELETE FROM task_comments WHERE id = ?", (comment_id,))

    log_activity("delete_comment", "task", task_id, {"comment_id": comment_id})

    return jsonify({"success": True})


@app.route("/api/comments/recent", methods=["GET"])
@require_auth
def get_recent_comments():
    """Get recent comments across all tasks.

    Query params:
        limit: Number of comments to return (default: 20, max: 100)
        comment_type: Filter by comment type
    """
    limit = min(request.args.get("limit", 20, type=int), 100)
    comment_type = request.args.get("comment_type")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        query = """
            SELECT tc.*, u.username as author_name, tq.task_type
            FROM task_comments tc
            LEFT JOIN users u ON tc.author_id = u.id
            LEFT JOIN task_queue tq ON tc.task_id = tq.id
            WHERE 1=1
        """
        params = []

        if comment_type:
            query += " AND tc.comment_type = ?"
            params.append(comment_type)

        query += " ORDER BY tc.created_at DESC LIMIT ?"
        params.append(limit)

        comments = conn.execute(query, params).fetchall()

        return jsonify(
            {"comments": [dict(c) for c in comments], "count": len(comments)}
        )


# ============================================================================
# TASK DEPENDENCIES API
# ============================================================================


@app.route("/api/tasks/<int:task_id>/dependencies", methods=["GET"])
@require_auth
def get_task_dependencies(task_id):
    """Get all dependencies for a task.

    Returns:
        blocks: Tasks that this task blocks (cannot start until this completes)
        blocked_by: Tasks that block this task (must complete before this can start)
    """
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Verify task exists
        task = conn.execute(
            "SELECT id, task_type, status FROM task_queue WHERE id = ?",
            (task_id,),
        ).fetchone()
        if not task:
            return jsonify({"error": "Task not found"}), 404

        # Get tasks that this task blocks (depends on this task)
        blocks = conn.execute(
            """
            SELECT td.id as dependency_id, td.dependency_type, td.created_at,
                   tq.id, tq.task_type, tq.status, tq.priority, tq.created_at as task_created
            FROM task_dependencies td
            JOIN task_queue tq ON td.task_id = tq.id
            WHERE td.depends_on_id = ?
            ORDER BY tq.priority DESC, tq.created_at
        """,
            (task_id,),
        ).fetchall()

        # Get tasks that block this task (this task depends on)
        blocked_by = conn.execute(
            """
            SELECT td.id as dependency_id, td.dependency_type, td.created_at,
                   tq.id, tq.task_type, tq.status, tq.priority, tq.created_at as task_created
            FROM task_dependencies td
            JOIN task_queue tq ON td.depends_on_id = tq.id
            WHERE td.task_id = ?
            ORDER BY tq.priority DESC, tq.created_at
        """,
            (task_id,),
        ).fetchall()

        # Check if task is blocked (any incomplete blocking tasks)
        is_blocked = any(
            dep["status"] not in ("completed", "failed") for dep in blocked_by
        )

        return jsonify(
            {
                "task_id": task_id,
                "task_status": task["status"],
                "is_blocked": is_blocked,
                "blocks": [dict(b) for b in blocks],
                "blocked_by": [dict(b) for b in blocked_by],
            }
        )


@app.route("/api/tasks/<int:task_id>/dependencies", methods=["POST"])
@require_auth
def add_task_dependency(task_id):
    """Add a dependency to a task.

    Body:
        depends_on_id: ID of the task that must complete first
        dependency_type: Type of dependency (default: 'blocks')
    """
    data = request.get_json() or {}
    depends_on_id = data.get("depends_on_id")
    dependency_type = data.get("dependency_type", "blocks")

    if not depends_on_id:
        return jsonify({"error": "depends_on_id is required"}), 400

    if task_id == depends_on_id:
        return jsonify({"error": "A task cannot depend on itself"}), 400

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Verify both tasks exist
        task = conn.execute(
            "SELECT id FROM task_queue WHERE id = ?", (task_id,)
        ).fetchone()
        if not task:
            return jsonify({"error": "Task not found"}), 404

        dep_task = conn.execute(
            "SELECT id FROM task_queue WHERE id = ?", (depends_on_id,)
        ).fetchone()
        if not dep_task:
            return jsonify({"error": "Dependency task not found"}), 404

        # Check for circular dependency (simple check - dep cannot already
        # depend on task)
        circular = conn.execute(
            """
            SELECT id FROM task_dependencies
            WHERE task_id = ? AND depends_on_id = ?
        """,
            (depends_on_id, task_id),
        ).fetchone()
        if circular:
            return jsonify({"error": "Circular dependency detected"}), 400

        # Check for existing dependency
        existing = conn.execute(
            """
            SELECT id FROM task_dependencies
            WHERE task_id = ? AND depends_on_id = ?
        """,
            (task_id, depends_on_id),
        ).fetchone()
        if existing:
            return (
                jsonify(
                    {
                        "error": "Dependency already exists",
                        "id": existing["id"],
                    }
                ),
                409,
            )

        # Create dependency
        cursor = conn.execute(
            """
            INSERT INTO task_dependencies (task_id, depends_on_id, dependency_type)
            VALUES (?, ?, ?)
        """,
            (task_id, depends_on_id, dependency_type),
        )

        dependency_id = cursor.lastrowid

    log_activity(
        "add_dependency", "task", task_id, f"depends on task {depends_on_id}"
    )

    return (
        jsonify(
            {
                "success": True,
                "id": dependency_id,
                "task_id": task_id,
                "depends_on_id": depends_on_id,
                "dependency_type": dependency_type,
            }
        ),
        201,
    )


@app.route(
    "/api/tasks/<int:task_id>/dependencies/<int:dependency_id>",
    methods=["DELETE"],
)
@require_auth
def remove_task_dependency(task_id, dependency_id):
    """Remove a dependency from a task."""
    with get_db_connection() as conn:
        # Verify dependency exists and belongs to task
        dep = conn.execute(
            """
            SELECT id, depends_on_id FROM task_dependencies
            WHERE id = ? AND task_id = ?
        """,
            (dependency_id, task_id),
        ).fetchone()

        if not dep:
            return jsonify({"error": "Dependency not found"}), 404

        conn.execute(
            "DELETE FROM task_dependencies WHERE id = ?", (dependency_id,)
        )

    log_activity(
        "remove_dependency",
        "task",
        task_id,
        f"removed dependency {dependency_id}",
    )

    return jsonify({"success": True})


@app.route("/api/tasks/<int:task_id>/blocked", methods=["GET"])
@require_auth
def check_task_blocked(task_id):
    """Check if a task is blocked by incomplete dependencies."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Get incomplete blocking tasks
        blockers = conn.execute(
            """
            SELECT tq.id, tq.task_type, tq.status, tq.priority
            FROM task_dependencies td
            JOIN task_queue tq ON td.depends_on_id = tq.id
            WHERE td.task_id = ? AND tq.status NOT IN ('completed', 'failed')
            ORDER BY tq.priority DESC
        """,
            (task_id,),
        ).fetchall()

        return jsonify(
            {
                "task_id": task_id,
                "is_blocked": len(blockers) > 0,
                "blocker_count": len(blockers),
                "blockers": [dict(b) for b in blockers],
            }
        )


# ============================================================================
# AUTOMATIC TASK PRIORITIZATION
# ============================================================================


def calculate_dependency_priority(conn, task_id):
    """Calculate priority boost based on dependencies.

    Returns:
        dependency_boost: Priority boost from blocking other tasks
        is_on_critical_path: Whether task is on the critical path
        chain_depth: Depth in the dependency chain
    """
    # Count how many tasks depend on this one (directly and indirectly)
    # Direct dependents
    direct_blocks = conn.execute(
        """
        SELECT COUNT(*) as cnt FROM task_dependencies
        WHERE depends_on_id = ?
    """,
        (task_id,),
    ).fetchone()["cnt"]

    # Calculate transitive dependents using recursive CTE
    transitive_result = conn.execute(
        """
        WITH RECURSIVE dependents(id, depth) AS (
            -- Direct dependents
            SELECT task_id, 1 FROM task_dependencies WHERE depends_on_id = ?
            UNION ALL
            -- Transitive dependents
            SELECT td.task_id, d.depth + 1
            FROM task_dependencies td
            JOIN dependents d ON td.depends_on_id = d.id
            WHERE d.depth < 10  -- Limit recursion depth
        )
        SELECT COUNT(DISTINCT id) as total_dependents, MAX(depth) as max_depth
        FROM dependents
    """,
        (task_id,),
    ).fetchone()

    total_dependents = transitive_result["total_dependents"] or 0
    max_depth = transitive_result["max_depth"] or 0

    # Calculate boost: more dependents = higher priority
    dependency_boost = min(
        DEPENDENCY_BOOST_MAX,
        direct_blocks * DEPENDENCY_BOOST_PER_BLOCKED
        + (total_dependents - direct_blocks),
    )

    return {
        "dependency_boost": dependency_boost,
        "direct_dependents": direct_blocks,
        "total_dependents": total_dependents,
        "chain_depth": max_depth,
    }


def find_critical_path(conn):
    """Find the critical path through pending tasks.

    Critical path = longest chain of dependent tasks.
    Returns list of task IDs in order from root to leaf.
    """
    # Find all pending tasks with their dependencies
    tasks = conn.execute(
        """
        SELECT id, task_type, priority, status FROM task_queue
        WHERE status = 'pending'
    """
    ).fetchall()

    if not tasks:
        return []

    task_ids = [t["id"] for t in tasks]

    # Build dependency graph
    deps = conn.execute(
        """
        SELECT task_id, depends_on_id
        FROM task_dependencies
        WHERE task_id IN ({}) AND depends_on_id IN ({})
    """.format(
            ",".join("?" * len(task_ids)), ",".join("?" * len(task_ids))
        ),
        task_ids + task_ids,
    ).fetchall()

    # Build adjacency list (depends_on -> tasks that depend on it)
    graph = {t["id"]: [] for t in tasks}
    in_degree = {t["id"]: 0 for t in tasks}

    for dep in deps:
        if dep["depends_on_id"] in graph:
            graph[dep["depends_on_id"]].append(dep["task_id"])
            in_degree[dep["task_id"]] = in_degree.get(dep["task_id"], 0) + 1

    # Find root tasks (no dependencies within pending set)
    roots = [tid for tid, deg in in_degree.items() if deg == 0]

    # DFS to find longest path
    def dfs(node, visited):
        if node in visited:
            return []
        visited.add(node)
        max_path = [node]
        for neighbor in graph.get(node, []):
            path = dfs(neighbor, visited.copy())
            if len(path) + 1 > len(max_path):
                max_path = [node] + path
        return max_path

    # Find longest path from any root
    longest_path = []
    for root in roots:
        path = dfs(root, set())
        if len(path) > len(longest_path):
            longest_path = path

    return longest_path


@app.route("/api/tasks/auto-prioritize", methods=["POST"])
@require_auth
def auto_prioritize_tasks():
    """Automatically adjust task priorities based on dependencies.

    Algorithm:
    1. Tasks that block many others get priority boosts
    2. Tasks on the critical path get additional boost
    3. Updates the priority field in task_queue

    Body (optional):
        apply: If true, actually update priorities (default: false, dry run)
        pending_only: Only consider pending tasks (default: true)
    """
    data = request.get_json() or {}
    apply_changes = data.get("apply", False)
    pending_only = data.get("pending_only", True)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Get tasks to process
        status_filter = "WHERE status = 'pending'" if pending_only else ""
        tasks = conn.execute(
            """
            SELECT id, task_type, priority, status, created_at
            FROM task_queue {status_filter}
        """
        ).fetchall()

        if not tasks:
            return jsonify(
                {
                    "success": True,
                    "message": "No tasks to prioritize",
                    "updates": [],
                }
            )

        # Find critical path
        critical_path = find_critical_path(conn)
        critical_path_set = set(critical_path)

        # Calculate new priorities
        updates = []
        for task in tasks:
            dep_info = calculate_dependency_priority(conn, task["id"])

            # Calculate new priority
            base_priority = task["priority"]
            dependency_boost = dep_info["dependency_boost"]
            critical_boost = (
                CRITICAL_PATH_BOOST if task["id"] in critical_path_set else 0
            )
            new_priority = base_priority + dependency_boost + critical_boost

            # Only include if priority would change
            if new_priority != task["priority"]:
                updates.append(
                    {
                        "task_id": task["id"],
                        "task_type": task["task_type"],
                        "old_priority": task["priority"],
                        "new_priority": new_priority,
                        "dependency_boost": dependency_boost,
                        "critical_path_boost": critical_boost,
                        "direct_dependents": dep_info["direct_dependents"],
                        "total_dependents": dep_info["total_dependents"],
                        "on_critical_path": task["id"] in critical_path_set,
                    }
                )

        # Apply updates if requested
        if apply_changes and updates:
            for update in updates:
                conn.execute(
                    "UPDATE task_queue SET priority = ? WHERE id = ?",
                    (update["new_priority"], update["task_id"]),
                )
            log_activity(
                "auto_prioritize",
                "task_queue",
                None,
                f"Updated {len(updates)} task priorities",
            )

        return jsonify(
            {
                "success": True,
                "applied": apply_changes,
                "tasks_analyzed": len(tasks),
                "tasks_updated": len(updates),
                "critical_path": critical_path,
                "critical_path_length": len(critical_path),
                "updates": updates,
                "config": {
                    "boost_per_blocked": DEPENDENCY_BOOST_PER_BLOCKED,
                    "max_boost": DEPENDENCY_BOOST_MAX,
                    "critical_path_boost": CRITICAL_PATH_BOOST,
                },
            }
        )


@app.route("/api/tasks/dependency-graph", methods=["GET"])
@require_auth
def get_dependency_graph():
    """Get the full dependency graph for visualization.

    Query params:
        pending_only: Only include pending tasks (default: false)
        include_completed: Include completed tasks (default: false)

    Returns:
        nodes: List of tasks with their properties
        edges: List of dependency relationships
        critical_path: List of task IDs on the critical path
    """
    pending_only = request.args.get("pending_only", "false").lower() == "true"
    include_completed = (
        request.args.get("include_completed", "false").lower() == "true"
    )

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Build status filter
        if pending_only:
            status_filter = "WHERE status = 'pending'"
        elif not include_completed:
            status_filter = "WHERE status NOT IN ('completed', 'failed')"
        else:
            status_filter = ""

        # Get all tasks
        tasks = conn.execute(
            """
            SELECT id, task_type, priority, status, story_points,
                   created_at, started_at, completed_at
            FROM task_queue {status_filter}
        """
        ).fetchall()

        task_ids = [t["id"] for t in tasks]

        if not task_ids:
            return jsonify(
                {
                    "nodes": [],
                    "edges": [],
                    "critical_path": [],
                    "stats": {"total_nodes": 0, "total_edges": 0},
                }
            )

        # Get all dependencies between these tasks
        deps = conn.execute(
            """
            SELECT id, task_id, depends_on_id, dependency_type, created_at
            FROM task_dependencies
            WHERE task_id IN ({}) OR depends_on_id IN ({})
        """.format(
                ",".join("?" * len(task_ids)), ",".join("?" * len(task_ids))
            ),
            task_ids + task_ids,
        ).fetchall()

        # Build nodes with dependency info
        nodes = []
        for task in tasks:
            dep_info = calculate_dependency_priority(conn, task["id"])
            nodes.append(
                {
                    "id": task["id"],
                    "task_type": task["task_type"],
                    "priority": task["priority"],
                    "status": task["status"],
                    "story_points": task["story_points"],
                    "dependency_boost": dep_info["dependency_boost"],
                    "direct_dependents": dep_info["direct_dependents"],
                    "total_dependents": dep_info["total_dependents"],
                    "chain_depth": dep_info["chain_depth"],
                }
            )

        # Build edges
        edges = []
        for dep in deps:
            edges.append(
                {
                    "id": dep["id"],
                    "source": dep[
                        "depends_on_id"
                    ],  # Task that must complete first
                    "target": dep["task_id"],  # Task that depends on source
                    "type": dep["dependency_type"],
                }
            )

        # Find critical path
        critical_path = find_critical_path(conn)

        return jsonify(
            {
                "nodes": nodes,
                "edges": edges,
                "critical_path": critical_path,
                "stats": {
                    "total_nodes": len(nodes),
                    "total_edges": len(edges),
                    "critical_path_length": len(critical_path),
                },
            }
        )


@app.route("/api/tasks/critical-path", methods=["GET"])
@require_auth
def get_critical_path():
    """Get the critical path through pending tasks.

    The critical path is the longest chain of dependent tasks.
    Tasks on the critical path should be prioritized.

    Returns:
        path: Ordered list of task IDs from root to leaf
        tasks: Detailed info for each task on the path
        total_story_points: Sum of story points on critical path
        total_estimated_hours: Sum of estimated hours
    """
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        critical_path = find_critical_path(conn)

        if not critical_path:
            return jsonify(
                {
                    "path": [],
                    "tasks": [],
                    "total_story_points": 0,
                    "total_estimated_hours": 0,
                    "message": "No pending tasks with dependencies",
                }
            )

        # Get detailed info for each task on the path
        tasks = conn.execute(
            """
            SELECT id, task_type, priority, status, story_points,
                   estimated_hours, created_at
            FROM task_queue
            WHERE id IN ({})
        """.format(
                ",".join("?" * len(critical_path))
            ),
            critical_path,
        ).fetchall()

        # Create lookup and preserve order
        task_lookup = {t["id"]: dict(t) for t in tasks}
        ordered_tasks = [
            task_lookup.get(tid) for tid in critical_path if tid in task_lookup
        ]

        # Calculate totals
        total_sp = sum(t.get("story_points") or 0 for t in ordered_tasks)
        total_hours = sum(t.get("estimated_hours") or 0 for t in ordered_tasks)

        return jsonify(
            {
                "path": critical_path,
                "tasks": ordered_tasks,
                "path_length": len(critical_path),
                "total_story_points": total_sp,
                "total_estimated_hours": total_hours,
            }
        )


@app.route("/api/tasks/<int:task_id>/priority-breakdown", methods=["GET"])
@require_auth
def get_task_priority_breakdown(task_id):
    """Get detailed breakdown of how a task's effective priority is calculated.

    Shows all factors affecting priority:
    - Base priority
    - Aging bonus
    - Dependency boost
    - Critical path boost
    """
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        task = conn.execute(
            """
            SELECT id, task_type, priority, status, created_at,
                ROUND((strftime('%s', 'now') - strftime('%s', created_at)) / 60.0, 2) as age_minutes
            FROM task_queue WHERE id = ?
        """,
            (task_id,),
        ).fetchone()

        if not task:
            return jsonify({"error": "Task not found"}), 404

        # Calculate aging bonus
        age_minutes = task["age_minutes"] or 0
        aging_bonus = min(
            TASK_PRIORITY_MAX_AGE_BONUS,
            age_minutes * TASK_PRIORITY_AGING_FACTOR,
        )

        # Calculate dependency boost
        dep_info = calculate_dependency_priority(conn, task_id)

        # Check if on critical path
        critical_path = find_critical_path(conn)
        on_critical_path = task_id in critical_path
        critical_boost = CRITICAL_PATH_BOOST if on_critical_path else 0

        # Calculate total effective priority
        base_priority = task["priority"]
        total_effective = (
            base_priority
            + aging_bonus
            + dep_info["dependency_boost"]
            + critical_boost
        )

        return jsonify(
            {
                "task_id": task_id,
                "task_type": task["task_type"],
                "status": task["status"],
                "priority_breakdown": {
                    "base_priority": base_priority,
                    "aging_bonus": round(aging_bonus, 2),
                    "dependency_boost": dep_info["dependency_boost"],
                    "critical_path_boost": critical_boost,
                    "total_effective_priority": round(total_effective, 2),
                },
                "factors": {
                    "age_minutes": round(age_minutes, 2),
                    "direct_dependents": dep_info["direct_dependents"],
                    "total_dependents": dep_info["total_dependents"],
                    "chain_depth": dep_info["chain_depth"],
                    "on_critical_path": on_critical_path,
                },
                "config": {
                    "aging_factor": TASK_PRIORITY_AGING_FACTOR,
                    "max_age_bonus": TASK_PRIORITY_MAX_AGE_BONUS,
                    "boost_per_blocked": DEPENDENCY_BOOST_PER_BLOCKED,
                    "max_dependency_boost": DEPENDENCY_BOOST_MAX,
                    "critical_path_boost": CRITICAL_PATH_BOOST,
                },
            }
        )


@app.route("/api/tasks/process", methods=["POST"])
@require_auth
def process_queue_task():
    """Process next pending claude_task from queue - send to tmux."""
    import json as json_module

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Get next pending claude_task
        task = conn.execute(
            """
            SELECT * FROM task_queue
            WHERE status = 'pending' AND task_type = 'claude_task'
            ORDER BY priority DESC, created_at ASC
            LIMIT 1
        """
        ).fetchone()

        if not task:
            return jsonify({"success": False, "message": "No pending tasks"})

        task_id = task["id"]
        task_data = json_module.loads(task["task_data"])

        session = task_data.get("session", "arch_env3")
        message = task_data.get("message", "")
        entity_type = task_data.get("entity_type")
        entity_id = task_data.get("entity_id")

        # Mark task as running
        conn.execute(
            """
            UPDATE task_queue SET status = 'running', started_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (task_id,),
        )

        # Send to tmux
        try:
            subprocess.run(
                ["tmux", "send-keys", "-t", session, message],
                check=True,
                timeout=5,
            )
            time.sleep(0.5)
            subprocess.run(
                ["tmux", "load-buffer", "-"],
                input=b"\r",
                check=True,
                timeout=5,
            )
            subprocess.run(
                ["tmux", "paste-buffer", "-t", session], check=True, timeout=5
            )

            # Mark task as completed
            conn.execute(
                """
                UPDATE task_queue SET status = 'completed', completed_at = CURRENT_TIMESTAMP
                WHERE id = ?
            """,
                (task_id,),
            )

            # Update entity status to in_progress
            if entity_type == "error":
                conn.execute(
                    "UPDATE errors SET status = 'in_progress' WHERE id = ?",
                    (entity_id,),
                )
            elif entity_type == "feature":
                conn.execute(
                    "UPDATE features SET status = 'in_progress' WHERE id = ?",
                    (entity_id,),
                )
            elif entity_type == "bug":
                conn.execute(
                    "UPDATE bugs SET status = 'in_progress' WHERE id = ?",
                    (entity_id,),
                )

            log_activity(
                "process_queue_task", "task", task_id, f"sent to {session}"
            )

            return jsonify(
                {
                    "success": True,
                    "task_id": task_id,
                    "session": session,
                    "entity_type": entity_type,
                    "entity_id": entity_id,
                }
            )

        except Exception as e:
            # Mark task as failed
            conn.execute(
                """
                UPDATE task_queue SET status = 'failed', error_message = ?
                WHERE id = ?
            """,
                (str(e), task_id),
            )
            return jsonify({"success": False, "error": str(e)}), 500


# ============================================================================
# SUGGESTED TASKS API (Claude can suggest, user confirms)
# ============================================================================


@app.route("/api/tasks/suggest", methods=["POST"])
def suggest_task():
    """Create a suggested task for user confirmation.

    Claude can call this to suggest follow-up tasks like browser testing.
    No auth required so Claude can call it from any session.
    """
    data = request.get_json()

    if not data.get("title"):
        return jsonify({"error": "title is required"}), 400

    # Calculate expiration time if timeout is set
    timeout = data.get("confirmation_timeout", 300)
    expires_at = None
    if timeout > 0:
        from datetime import timedelta

        expires_at = (datetime.now() + timedelta(seconds=timeout)).isoformat()

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        cursor = conn.execute(
            """
            INSERT INTO suggested_tasks (
                title, description, task_type, priority,
                source_session, context, requires_confirmation,
                confirmation_timeout, expires_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                data.get("title"),
                data.get("description"),
                data.get("task_type", "manual"),
                data.get("priority", "normal"),
                data.get("source_session"),
                (
                    json.dumps(data.get("context"))
                    if data.get("context")
                    else None
                ),
                1 if data.get("requires_confirmation", True) else 0,
                timeout,
                expires_at,
            ),
        )

        task_id = cursor.lastrowid
        log_activity(
            "suggest_task", "suggested_task", task_id, data.get("title")
        )

        return jsonify(
            {
                "success": True,
                "id": task_id,
                "message": "Task suggested, awaiting confirmation",
                "poll_url": f"/api/tasks/suggest/{task_id}/status",
            }
        )


@app.route("/api/tasks/suggest", methods=["GET"])
def get_suggested_tasks():
    """Get all pending suggested tasks."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Get pending tasks, ordered by priority and creation time
        tasks = conn.execute(
            """
            SELECT * FROM suggested_tasks
            WHERE status = 'pending'
            AND (expires_at IS NULL OR expires_at > CURRENT_TIMESTAMP)
            ORDER BY
                CASE priority
                    WHEN 'critical' THEN 0
                    WHEN 'high' THEN 1
                    WHEN 'normal' THEN 2
                    WHEN 'low' THEN 3
                END,
                created_at ASC
        """
        ).fetchall()

        return jsonify([dict(t) for t in tasks])


@app.route("/api/tasks/suggest/<int:task_id>/status", methods=["GET"])
def get_suggested_task_status(task_id):
    """Get status of a suggested task. Claude can poll this to wait for confirmation."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        task = conn.execute(
            "SELECT * FROM suggested_tasks WHERE id = ?", (task_id,)
        ).fetchone()

        if not task:
            return jsonify({"error": "Task not found"}), 404

        # Check if expired
        if task["expires_at"] and task["status"] == "pending":
            from datetime import datetime as dt

            try:
                expires = dt.fromisoformat(task["expires_at"])
                if dt.now() > expires:
                    conn.execute(
                        "UPDATE suggested_tasks SET status = 'expired' WHERE id = ?",
                        (task_id,),
                    )
                    return jsonify(
                        {
                            "id": task_id,
                            "status": "expired",
                            "message": "Task confirmation timed out",
                        }
                    )
            except (ValueError, sqlite3.Error):
                pass

        return jsonify(
            {
                "id": task_id,
                "status": task["status"],
                "title": task["title"],
                "confirmed_by": task["confirmed_by"],
                "confirmed_at": task["confirmed_at"],
                "result": (
                    json.loads(task["result"]) if task["result"] else None
                ),
            }
        )


@app.route("/api/tasks/suggest/<int:task_id>/confirm", methods=["POST"])
@require_auth
def confirm_suggested_task(task_id):
    """Confirm a suggested task. User approves the task to proceed."""
    data = request.get_json(silent=True) or {}

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        task = conn.execute(
            "SELECT * FROM suggested_tasks WHERE id = ?", (task_id,)
        ).fetchone()

        if not task:
            return jsonify({"error": "Task not found"}), 404

        if task["status"] != "pending":
            return jsonify({"error": f'Task already {task["status"]}'}), 400

        conn.execute(
            """
            UPDATE suggested_tasks
            SET status = 'confirmed',
                confirmed_by = ?,
                confirmed_at = CURRENT_TIMESTAMP,
                result = ?
            WHERE id = ?
        """,
            (
                session.get("username", "user"),
                json.dumps(data.get("notes")) if data.get("notes") else None,
                task_id,
            ),
        )

        log_activity("confirm_task", "suggested_task", task_id, "confirmed")

        return jsonify(
            {
                "success": True,
                "status": "confirmed",
                "message": f'Task "{task["title"]}" confirmed',
            }
        )


@app.route("/api/tasks/suggest/<int:task_id>/skip", methods=["POST"])
@require_auth
def skip_suggested_task(task_id):
    """Skip a suggested task. User decides not to do this task."""
    data = request.get_json(silent=True) or {}

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        task = conn.execute(
            "SELECT * FROM suggested_tasks WHERE id = ?", (task_id,)
        ).fetchone()

        if not task:
            return jsonify({"error": "Task not found"}), 404

        if task["status"] != "pending":
            return jsonify({"error": f'Task already {task["status"]}'}), 400

        conn.execute(
            """
            UPDATE suggested_tasks
            SET status = 'skipped',
                confirmed_by = ?,
                confirmed_at = CURRENT_TIMESTAMP,
                result = ?
            WHERE id = ?
        """,
            (
                session.get("username", "user"),
                json.dumps({"reason": data.get("reason", "User skipped")}),
                task_id,
            ),
        )

        log_activity("skip_task", "suggested_task", task_id, "skipped")

        return jsonify(
            {
                "success": True,
                "status": "skipped",
                "message": f'Task "{task["title"]}" skipped',
            }
        )


@app.route("/api/tasks/suggest/<int:task_id>/complete", methods=["POST"])
@require_auth
def complete_suggested_task(task_id):
    """Mark a confirmed task as completed with optional result notes."""
    data = request.get_json() or {}

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        task = conn.execute(
            "SELECT * FROM suggested_tasks WHERE id = ?", (task_id,)
        ).fetchone()

        if not task:
            return jsonify({"error": "Task not found"}), 404

        if task["status"] != "confirmed":
            return (
                jsonify({"error": "Task must be confirmed before completing"}),
                400,
            )

        conn.execute(
            """
            UPDATE suggested_tasks
            SET status = 'completed',
                completed_at = CURRENT_TIMESTAMP,
                result = ?
            WHERE id = ?
        """,
            (json.dumps(data.get("result", {"status": "completed"})), task_id),
        )

        log_activity("complete_task", "suggested_task", task_id, "completed")

        return jsonify(
            {
                "success": True,
                "status": "completed",
                "message": f'Task "{task["title"]}" completed',
            }
        )


# ============================================================================
# TASK EXPORT API
# ============================================================================


@app.route("/api/tasks/export", methods=["GET"])
@require_auth
def export_tasks():
    """Export tasks to CSV or JSON format.

    Query params:
        format: 'csv' or 'json' (default: json)
        status: filter by status (pending, running, completed, failed)
        type: filter by task_type
        from_date: filter tasks created after this date (ISO format)
        to_date: filter tasks created before this date (ISO format)
        include_data: include task_data field (default: true)
    """
    import csv
    import io
    from datetime import datetime

    export_format = request.args.get("format", "json").lower()
    status = request.args.get("status")
    task_type = request.args.get("type")
    from_date = request.args.get("from_date")
    to_date = request.args.get("to_date")
    include_data = request.args.get("include_data", "true").lower() == "true"

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        query = "SELECT * FROM task_queue WHERE 1=1"
        params = []

        if status:
            query += " AND status = ?"
            params.append(status)
        if task_type:
            query += " AND task_type = ?"
            params.append(task_type)
        if from_date:
            query += " AND created_at >= ?"
            params.append(from_date)
        if to_date:
            query += " AND created_at <= ?"
            params.append(to_date)

        query += " ORDER BY created_at DESC"

        tasks = conn.execute(query, params).fetchall()
        task_list = []

        for task in tasks:
            task_dict = dict(task)
            # Parse task_data JSON
            if task_dict.get("task_data"):
                try:
                    task_dict["task_data"] = json.loads(task_dict["task_data"])
                except json.JSONDecodeError:
                    pass
            if not include_data:
                task_dict.pop("task_data", None)
            task_list.append(task_dict)

    if export_format == "csv":
        # Generate CSV
        output = io.StringIO()
        if task_list:
            # Flatten task_data for CSV
            flat_tasks = []
            for task in task_list:
                flat_task = {k: v for k, v in task.items() if k != "task_data"}
                if include_data and task.get("task_data"):
                    if isinstance(task["task_data"], dict):
                        flat_task["task_data"] = json.dumps(task["task_data"])
                    else:
                        flat_task["task_data"] = str(task["task_data"])
                flat_tasks.append(flat_task)

            fieldnames = list(flat_tasks[0].keys()) if flat_tasks else []
            writer = csv.DictWriter(output, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(flat_tasks)

        response = make_response(output.getvalue())
        response.headers["Content-Type"] = "text/csv; charset=utf-8"
        response.headers["Content-Disposition"] = (
            f'attachment; filename=tasks_export_{
                datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
        )

        log_activity(
            "export", "tasks", None, f"CSV export: {len(task_list)} tasks"
        )
        return response

    else:
        # JSON format (default)
        export_data = {
            "exported_at": datetime.now().isoformat(),
            "total_count": len(task_list),
            "filters": {
                "status": status,
                "type": task_type,
                "from_date": from_date,
                "to_date": to_date,
            },
            "tasks": task_list,
        }

        response = make_response(
            json.dumps(export_data, indent=2, default=str)
        )
        response.headers["Content-Type"] = "application/json; charset=utf-8"
        response.headers["Content-Disposition"] = (
            f'attachment; filename=tasks_export_{
                datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        )

        log_activity(
            "export", "tasks", None, f"JSON export: {len(task_list)} tasks"
        )
        return response


@app.route("/api/tasks/export/summary", methods=["GET"])
@require_auth
def export_tasks_summary():
    """Get a summary of tasks for export preview.

    Returns counts by status, type, and date range.
    """
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Count by status
        status_counts = conn.execute(
            """
            SELECT status, COUNT(*) as count
            FROM task_queue
            GROUP BY status
        """
        ).fetchall()

        # Count by type
        type_counts = conn.execute(
            """
            SELECT task_type, COUNT(*) as count
            FROM task_queue
            GROUP BY task_type
            ORDER BY count DESC
            LIMIT 10
        """
        ).fetchall()

        # Date range
        date_range = conn.execute(
            """
            SELECT
                MIN(created_at) as earliest,
                MAX(created_at) as latest,
                COUNT(*) as total
            FROM task_queue
        """
        ).fetchone()

        # Recent activity (last 7 days)
        recent = conn.execute(
            """
            SELECT DATE(created_at) as date, COUNT(*) as count
            FROM task_queue
            WHERE created_at >= DATE('now', '-7 days')
            GROUP BY DATE(created_at)
            ORDER BY date DESC
        """
        ).fetchall()

        return jsonify(
            {
                "by_status": {
                    row["status"]: row["count"] for row in status_counts
                },
                "by_type": {
                    row["task_type"]: row["count"] for row in type_counts
                },
                "date_range": {
                    "earliest": date_range["earliest"],
                    "latest": date_range["latest"],
                    "total": date_range["total"],
                },
                "recent_7_days": {row["date"]: row["count"] for row in recent},
            }
        )


# ============================================================================
# EXCEL EXPORT API
# ============================================================================


def create_excel_workbook(data_dict):
    """Create Excel workbook from dict of sheet_name->rows."""
    try:
        from openpyxl import Workbook
        from openpyxl.styles import Border, Font, PatternFill, Side
        from openpyxl.utils import get_column_letter
    except ImportError:
        return None
    wb = Workbook()
    wb.remove(wb.active)
    hfont = Font(bold=True, color="FFFFFF")
    hfill = PatternFill(
        start_color="4472C4", end_color="4472C4", fill_type="solid"
    )
    border = Border(
        left=Side(style="thin"),
        right=Side(style="thin"),
        top=Side(style="thin"),
        bottom=Side(style="thin"),
    )
    for name, rows in data_dict.items():
        if not rows:
            continue
        ws = wb.create_sheet(title=name[:31])
        headers = list(rows[0].keys())
        for ci, h in enumerate(headers, 1):
            c = ws.cell(row=1, column=ci, value=h.replace("_", " ").title())
            c.font, c.fill, c.border = hfont, hfill, border
        for ri, rd in enumerate(rows, 2):
            for ci, h in enumerate(headers, 1):
                v = rd.get(h, "")
                if isinstance(v, (dict, list)):
                    v = json.dumps(v, default=str)
                ws.cell(row=ri, column=ci, value=v).border = border
        for ci, h in enumerate(headers, 1):
            ws.column_dimensions[get_column_letter(ci)].width = (
                min(max(len(h), 10), 50) + 2
            )
        ws.freeze_panes = "A2"
    if not wb.sheetnames:
        wb.create_sheet("Empty").cell(1, 1, "No data")
    out = io.BytesIO()
    wb.save(out)
    out.seek(0)
    return out


@app.route("/api/export/xlsx", methods=["GET"])
@require_auth
def export_to_excel():
    """Export data to Excel. Query: entities (csv list), from_date, to_date."""
    entities = [
        e.strip()
        for e in request.args.get(
            "entities", "projects,features,bugs,tasks,errors"
        ).split(",")
    ]
    from_date, to_date = request.args.get("from_date"), request.args.get(
        "to_date"
    )
    data = {}
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        df = ""
        dp = []
        if from_date:
            df += " AND created_at >= ?"
            dp.append(from_date)
        if to_date:
            df += " AND created_at <= ?"
            dp.append(to_date)
        if "projects" in entities:
            data["Projects"] = [
                dict(r)
                for r in conn.execute(
                    f"SELECT id, name, description, status, created_at FROM projects WHERE 1=1 {df}",
                    dp,
                ).fetchall()
            ]
        if "features" in entities:
            data["Features"] = [
                dict(r)
                for r in conn.execute(
                    f"SELECT f.id, p.name as project, f.name, f.status, f.priority, f.created_at FROM features f LEFT JOIN projects p ON f.project_id=p.id WHERE 1=1 {
                        df.replace(
                            'created_at',
                            'f.created_at')}",
                    dp,
                ).fetchall()
            ]
        if "bugs" in entities:
            data["Bugs"] = [
                dict(r)
                for r in conn.execute(
                    f"SELECT b.id, p.name as project, b.title, b.status, b.severity, b.created_at FROM bugs b LEFT JOIN projects p ON b.project_id=p.id WHERE 1=1 {
                        df.replace(
                            'created_at',
                            'b.created_at')}",
                    dp,
                ).fetchall()
            ]
        if "tasks" in entities:
            data["Tasks"] = [
                dict(r)
                for r in conn.execute(
                    f"SELECT id, task_type, status, priority, error_message, created_at, completed_at FROM task_queue WHERE 1=1 {df}",
                    dp,
                ).fetchall()
            ]
        if "errors" in entities:
            data["Errors"] = [
                dict(r)
                for r in conn.execute(
                    f"SELECT id, error_type, message, source, status, occurrence_count, last_seen FROM errors WHERE 1=1 {
                        df.replace(
                            'created_at',
                            'first_seen')}",
                    dp,
                ).fetchall()
            ]
    xlsx = create_excel_workbook(data)
    if not xlsx:
        return (
            jsonify({"error": "openpyxl required. pip install openpyxl"}),
            500,
        )
    resp = make_response(xlsx.read())
    resp.headers["Content-Type"] = (
        "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )
    resp.headers["Content-Disposition"] = (
        f'attachment; filename=export_{
            datetime.now().strftime("%Y%m%d_%H%M%S")}.xlsx'
    )
    return resp


@app.route("/api/export/xlsx/tasks", methods=["GET"])
@require_auth
def export_tasks_xlsx():
    """Export tasks to Excel."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        q, p = "SELECT * FROM task_queue WHERE 1=1", []
        if request.args.get("status"):
            q += " AND status=?"
            p.append(request.args["status"])
        if request.args.get("type"):
            q += " AND task_type=?"
            p.append(request.args["type"])
        tasks = [
            dict(r)
            for r in conn.execute(
                q + " ORDER BY created_at DESC", p
            ).fetchall()
        ]
    xlsx = create_excel_workbook({"Tasks": tasks})
    if not xlsx:
        return jsonify({"error": "openpyxl required"}), 500
    resp = make_response(xlsx.read())
    resp.headers["Content-Type"] = (
        "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )
    resp.headers["Content-Disposition"] = (
        f'attachment; filename=tasks_{
            datetime.now().strftime("%Y%m%d_%H%M%S")}.xlsx'
    )
    return resp


# ============================================================================
# PDF EXPORT API
# ============================================================================


def create_pdf_report(data: dict, title: str = "Architect Report") -> bytes:
    """Generate a PDF report from data. Returns PDF bytes or None if reportlab not available."""
    try:
        import io

        from reportlab.lib import colors
        from reportlab.lib.enums import TA_CENTER
        from reportlab.lib.pagesizes import letter
        from reportlab.lib.styles import ParagraphStyle, getSampleStyleSheet
        from reportlab.lib.units import inch
        from reportlab.platypus import Paragraph, SimpleDocTemplate, Spacer, Table, TableStyle

        buffer = io.BytesIO()
        doc = SimpleDocTemplate(
            buffer,
            pagesize=letter,
            rightMargin=72,
            leftMargin=72,
            topMargin=72,
            bottomMargin=72,
        )
        styles = getSampleStyleSheet()
        styles.add(
            ParagraphStyle(
                name="Title2",
                parent=styles["Heading1"],
                fontSize=18,
                spaceAfter=20,
                alignment=TA_CENTER,
            )
        )
        styles.add(
            ParagraphStyle(
                name="Section",
                parent=styles["Heading2"],
                fontSize=14,
                spaceBefore=15,
                spaceAfter=10,
                textColor=colors.darkblue,
            )
        )
        styles.add(
            ParagraphStyle(
                name="SubSection",
                parent=styles["Heading3"],
                fontSize=11,
                spaceBefore=10,
                spaceAfter=5,
            )
        )
        story = []

        # Title
        story.append(Paragraph(title, styles["Title2"]))
        story.append(
            Paragraph(
                f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                styles["Normal"],
            )
        )
        story.append(Spacer(1, 20))

        # Summary section
        if "summary" in data:
            story.append(Paragraph("Summary", styles["Section"]))
            summary = data["summary"]
            summary_data = [
                [k.replace("_", " ").title(), str(v)]
                for k, v in summary.items()
            ]
            if summary_data:
                t = Table(summary_data, colWidths=[2.5 * inch, 2 * inch])
                t.setStyle(
                    TableStyle(
                        [
                            ("BACKGROUND", (0, 0), (0, -1), colors.lightgrey),
                            ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                            ("FONTNAME", (0, 0), (-1, -1), "Helvetica"),
                            ("FONTSIZE", (0, 0), (-1, -1), 10),
                            ("PADDING", (0, 0), (-1, -1), 6),
                        ]
                    )
                )
                story.append(t)
            story.append(Spacer(1, 15))

        # Projects section
        if "projects" in data and data["projects"]:
            story.append(Paragraph("Projects", styles["Section"]))
            for proj in data["projects"][:20]:
                story.append(
                    Paragraph(
                        f"<b>{proj.get('name',
                                       'Unnamed')}</b> - {proj.get('status',
                                                                   'N/A')}",
                        styles["SubSection"],
                    )
                )
                if proj.get("description"):
                    story.append(
                        Paragraph(proj["description"][:200], styles["Normal"])
                    )
            story.append(Spacer(1, 10))

        # Milestones section
        if "milestones" in data and data["milestones"]:
            story.append(Paragraph("Milestones", styles["Section"]))
            ms_data = [
                ["Name", "Project", "Status", "Progress", "Target Date"]
            ]
            for m in data["milestones"][:30]:
                ms_data.append(
                    [
                        m.get("name", "")[:30],
                        str(m.get("project_id", ""))[:10],
                        m.get("status", ""),
                        f"{m.get('progress', 0)}%",
                        str(m.get("target_date", ""))[:10],
                    ]
                )
            t = Table(
                ms_data,
                colWidths=[
                    1.8 * inch,
                    0.8 * inch,
                    0.8 * inch,
                    0.7 * inch,
                    1 * inch,
                ],
            )
            t.setStyle(
                TableStyle(
                    [
                        ("BACKGROUND", (0, 0), (-1, 0), colors.grey),
                        ("TEXTCOLOR", (0, 0), (-1, 0), colors.whitesmoke),
                        ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                        ("FONTSIZE", (0, 0), (-1, -1), 8),
                        ("PADDING", (0, 0), (-1, -1), 4),
                    ]
                )
            )
            story.append(t)
            story.append(Spacer(1, 10))

        # Features section
        if "features" in data and data["features"]:
            story.append(Paragraph("Features", styles["Section"]))
            f_data = [["Title", "Status", "Priority", "Project"]]
            for f in data["features"][:50]:
                f_data.append(
                    [
                        f.get("title", "")[:35],
                        f.get("status", ""),
                        str(f.get("priority", 0)),
                        str(f.get("project_id", "")),
                    ]
                )
            t = Table(
                f_data,
                colWidths=[2.5 * inch, 1 * inch, 0.8 * inch, 0.8 * inch],
            )
            t.setStyle(
                TableStyle(
                    [
                        ("BACKGROUND", (0, 0), (-1, 0), colors.grey),
                        ("TEXTCOLOR", (0, 0), (-1, 0), colors.whitesmoke),
                        ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                        ("FONTSIZE", (0, 0), (-1, -1), 8),
                        ("PADDING", (0, 0), (-1, -1), 4),
                    ]
                )
            )
            story.append(t)
            story.append(Spacer(1, 10))

        # Bugs section
        if "bugs" in data and data["bugs"]:
            story.append(Paragraph("Bugs", styles["Section"]))
            b_data = [["Title", "Status", "Severity", "Project"]]
            for b in data["bugs"][:50]:
                b_data.append(
                    [
                        b.get("title", "")[:35],
                        b.get("status", ""),
                        b.get("severity", ""),
                        str(b.get("project_id", "")),
                    ]
                )
            t = Table(
                b_data,
                colWidths=[2.5 * inch, 1 * inch, 0.8 * inch, 0.8 * inch],
            )
            t.setStyle(
                TableStyle(
                    [
                        ("BACKGROUND", (0, 0), (-1, 0), colors.grey),
                        ("TEXTCOLOR", (0, 0), (-1, 0), colors.whitesmoke),
                        ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                        ("FONTSIZE", (0, 0), (-1, -1), 8),
                        ("PADDING", (0, 0), (-1, -1), 4),
                    ]
                )
            )
            story.append(t)
            story.append(Spacer(1, 10))

        # Tasks section
        if "tasks" in data and data["tasks"]:
            story.append(Paragraph("Tasks", styles["Section"]))
            t_data = [["Type", "Status", "Priority", "Created"]]
            for task in data["tasks"][:50]:
                t_data.append(
                    [
                        task.get("task_type", ""),
                        task.get("status", ""),
                        str(task.get("priority", 0)),
                        str(task.get("created_at", ""))[:10],
                    ]
                )
            t = Table(
                t_data, colWidths=[1.5 * inch, 1 * inch, 0.8 * inch, 1 * inch]
            )
            t.setStyle(
                TableStyle(
                    [
                        ("BACKGROUND", (0, 0), (-1, 0), colors.grey),
                        ("TEXTCOLOR", (0, 0), (-1, 0), colors.whitesmoke),
                        ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                        ("FONTSIZE", (0, 0), (-1, -1), 8),
                        ("PADDING", (0, 0), (-1, -1), 4),
                    ]
                )
            )
            story.append(t)

        doc.build(story)
        buffer.seek(0)
        return buffer.read()
    except ImportError:
        return None


@app.route("/api/export/pd", methods=["GET"])
@require_auth
def export_to_pdf():
    """Export project data to PDF report.

    Query params:
        project_id: Filter to specific project
        include: Comma-separated list of sections (projects,milestones,features,bugs,tasks,summary)
        title: Custom report title
    """
    project_id = request.args.get("project_id", type=int)
    include = request.args.get(
        "include", "summary,projects,milestones,features,bugs"
    ).split(",")
    title = request.args.get("title", "Architect Project Report")

    data = {}

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Summary stats
        if "summary" in include:
            data["summary"] = {
                "total_projects": conn.execute(
                    "SELECT COUNT(*) FROM projects WHERE status='active'"
                ).fetchone()[0],
                "total_features": conn.execute(
                    "SELECT COUNT(*) FROM features"
                ).fetchone()[0],
                "open_features": conn.execute(
                    "SELECT COUNT(*) FROM features WHERE status='open'"
                ).fetchone()[0],
                "total_bugs": conn.execute(
                    "SELECT COUNT(*) FROM bugs"
                ).fetchone()[0],
                "open_bugs": conn.execute(
                    "SELECT COUNT(*) FROM bugs WHERE status='open'"
                ).fetchone()[0],
                "total_milestones": conn.execute(
                    "SELECT COUNT(*) FROM milestones"
                ).fetchone()[0],
                "pending_tasks": conn.execute(
                    "SELECT COUNT(*) FROM task_queue WHERE status='pending'"
                ).fetchone()[0],
            }

        # Projects
        if "projects" in include:
            q = "SELECT * FROM projects WHERE status='active'"
            if project_id:
                q = f"SELECT * FROM projects WHERE id={project_id}"
            data["projects"] = [
                dict(r) for r in conn.execute(q + " ORDER BY name").fetchall()
            ]

        # Milestones
        if "milestones" in include:
            q = "SELECT * FROM milestones"
            if project_id:
                q += f" WHERE project_id={project_id}"
            data["milestones"] = [
                dict(r)
                for r in conn.execute(q + " ORDER BY target_date").fetchall()
            ]

        # Features
        if "features" in include:
            q = "SELECT * FROM features"
            if project_id:
                q += f" WHERE project_id={project_id}"
            data["features"] = [
                dict(r)
                for r in conn.execute(
                    q + " ORDER BY priority DESC, created_at DESC"
                ).fetchall()
            ]

        # Bugs
        if "bugs" in include:
            q = "SELECT * FROM bugs"
            if project_id:
                q += f" WHERE project_id={project_id}"
            data["bugs"] = [
                dict(r)
                for r in conn.execute(
                    q + " ORDER BY severity DESC, created_at DESC"
                ).fetchall()
            ]

        # Tasks
        if "tasks" in include:
            data["tasks"] = [
                dict(r)
                for r in conn.execute(
                    "SELECT * FROM task_queue ORDER BY created_at DESC LIMIT 100"
                ).fetchall()
            ]

    pdf_bytes = create_pdf_report(data, title)
    if not pdf_bytes:
        return (
            jsonify(
                {
                    "error": "reportlab required. Install with: pip install reportlab"
                }
            ),
            500,
        )

    response = make_response(pdf_bytes)
    response.headers["Content-Type"] = "application/pdf"
    response.headers["Content-Disposition"] = (
        f'attachment; filename=report_{
            datetime.now().strftime("%Y%m%d_%H%M%S")}.pdf'
    )
    log_activity("export", "pd", project_id, f"PDF report: {title}")
    return response


@app.route("/api/export/pdf/project/<int:project_id>", methods=["GET"])
@require_auth
def export_project_pdf(project_id):
    """Export a single project to detailed PDF report."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        project = conn.execute(
            "SELECT * FROM projects WHERE id=?", (project_id,)
        ).fetchone()
        if not project:
            return jsonify({"error": "Project not found"}), 404

        data = {
            "summary": {
                "project_name": project["name"],
                "status": project["status"],
                "created_at": project["created_at"],
                "milestones": conn.execute(
                    "SELECT COUNT(*) FROM milestones WHERE project_id=?",
                    (project_id,),
                ).fetchone()[0],
                "features": conn.execute(
                    "SELECT COUNT(*) FROM features WHERE project_id=?",
                    (project_id,),
                ).fetchone()[0],
                "open_features": conn.execute(
                    "SELECT COUNT(*) FROM features WHERE project_id=? AND status='open'",
                    (project_id,),
                ).fetchone()[0],
                "bugs": conn.execute(
                    "SELECT COUNT(*) FROM bugs WHERE project_id=?",
                    (project_id,),
                ).fetchone()[0],
                "open_bugs": conn.execute(
                    "SELECT COUNT(*) FROM bugs WHERE project_id=? AND status='open'",
                    (project_id,),
                ).fetchone()[0],
            },
            "projects": [dict(project)],
            "milestones": [
                dict(r)
                for r in conn.execute(
                    "SELECT * FROM milestones WHERE project_id=? ORDER BY target_date",
                    (project_id,),
                ).fetchall()
            ],
            "features": [
                dict(r)
                for r in conn.execute(
                    "SELECT * FROM features WHERE project_id=? ORDER BY priority DESC",
                    (project_id,),
                ).fetchall()
            ],
            "bugs": [
                dict(r)
                for r in conn.execute(
                    "SELECT * FROM bugs WHERE project_id=? ORDER BY severity DESC",
                    (project_id,),
                ).fetchall()
            ],
        }

    pdf_bytes = create_pdf_report(data, f"Project Report: {project['name']}")
    if not pdf_bytes:
        return (
            jsonify(
                {
                    "error": "reportlab required. Install with: pip install reportlab"
                }
            ),
            500,
        )

    response = make_response(pdf_bytes)
    response.headers["Content-Type"] = "application/pdf"
    response.headers["Content-Disposition"] = (
        f'attachment; filename=project_{project_id}_{
            datetime.now().strftime("%Y%m%d")}.pdf'
    )
    log_activity(
        "export", "pdf", project_id, f'Project PDF: {project["name"]}'
    )
    return response


@app.route("/api/export/pdf/tasks", methods=["GET"])
@require_auth
def export_tasks_pdf():
    """Export tasks to PDF report."""
    status = request.args.get("status")
    task_type = request.args.get("type")
    days = request.args.get("days", type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        q, p = "SELECT * FROM task_queue WHERE 1=1", []
        if status:
            q += " AND status=?"
            p.append(status)
        if task_type:
            q += " AND task_type=?"
            p.append(task_type)
        if days:
            q += " AND created_at >= datetime('now', '-' || ? || ' days')"
            p.append(days)
        tasks = [
            dict(r)
            for r in conn.execute(
                q + " ORDER BY created_at DESC LIMIT 200", p
            ).fetchall()
        ]

        # Summary by status
        status_counts = {}
        for t in tasks:
            s = t.get("status", "unknown")
            status_counts[s] = status_counts.get(s, 0) + 1

        data = {
            "summary": {
                "total_tasks": len(tasks),
                **{f"{k}_tasks": v for k, v in status_counts.items()},
            },
            "tasks": tasks,
        }

    pdf_bytes = create_pdf_report(data, "Task Queue Report")
    if not pdf_bytes:
        return jsonify({"error": "reportlab required"}), 500

    response = make_response(pdf_bytes)
    response.headers["Content-Type"] = "application/pdf"
    response.headers["Content-Disposition"] = (
        f'attachment; filename=tasks_{
            datetime.now().strftime("%Y%m%d_%H%M%S")}.pdf'
    )
    log_activity("export", "pd", None, f"Tasks PDF: {len(tasks)} tasks")
    return response


# ============================================================================
# iCAL EXPORT API
# ============================================================================


def _ical_uid(t, i):
    import hashlib

    return (
        hashlib.md5(f"{t}-{i}-architect".encode()).hexdigest() + "@architect"
    )


def _ical_esc(t):
    return (
        str(t or "")
        .replace("\\", "\\\\")
        .replace(";", "\\;")
        .replace(",", "\\,")
        .replace("\n", "\\n")
    )


def _ical_dt(d):
    if not d:
        return None
    return (
        str(d).replace("-", "")[:8]
        if isinstance(d, str)
        else d.strftime("%Y%m%d")
    )


def _ical_ev(uid, summ, st, end=None, desc=None, stat=None, pri=None):
    event_lines = [
        "BEGIN:VEVENT",
        f"UID:{uid}",
        f"DTSTAMP:{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}",
        f"DTSTART;VALUE=DATE:{st}",
        f"SUMMARY:{_ical_esc(summ)}",
    ]
    if end:
        event_lines.insert(4, f"DTEND;VALUE=DATE:{end}")
    if desc:
        event_lines.append(f"DESCRIPTION:{_ical_esc(desc)}")
    if stat:
        stat_map = {"completed": "COMPLETED", "in_progress": "IN-PROCESS"}
        event_lines.append(
            f"STATUS:{stat_map.get(stat.lower(), 'NEEDS-ACTION')}"
        )
    if pri:
        pri_map = {"critical": 1, "high": 3, "medium": 5, "low": 7}
        event_lines.append(f"PRIORITY:{pri_map.get(str(pri).lower(), 5)}")
    event_lines.append("END:VEVENT")
    return "\r\n".join(event_lines)


@app.route("/api/export/ical", methods=["GET"])
@require_auth
def export_ical():
    """Export milestones/tasks/features/bugs as iCalendar (.ics)."""
    inc = request.args.get("include", "milestones").split(",")
    pid = request.args.get("project_id", type=int)
    sf = request.args.get("status", "")
    fd, td = request.args.get("from_date"), request.args.get("to_date")
    cn = request.args.get("calendar_name", "Architect Dashboard")
    evs = []
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        if "milestones" in inc:
            q, p = (
                "SELECT m.id,m.name,m.description,m.target_date,m.status,p.name pn FROM milestones m LEFT JOIN projects p ON m.project_id=p.id WHERE m.target_date IS NOT NULL",
                [],
            )
            if pid:
                q += " AND m.project_id=?"
                p.append(pid)
            if sf:
                ss = [s.strip() for s in sf.split(",")]
                q += f" AND m.status IN ({','.join('?'*len(ss))})"
                p.extend(ss)
            if fd:
                q += " AND m.target_date>=?"
                p.append(fd)
            if td:
                q += " AND m.target_date<=?"
                p.append(td)
            for m in conn.execute(q, p).fetchall():
                s = _ical_dt(m["target_date"])
                if s:
                    evs.append(
                        _ical_ev(
                            _ical_uid("m", m["id"]),
                            (
                                f"[{m['pn']}] {m['name']}"
                                if m["pn"]
                                else f"[Milestone] {m['name']}"
                            ),
                            s,
                            desc=f"Status: {
                                m['status']}\\n{
                                m['description'] or ''}",
                            stat=m["status"],
                        )
                    )
        if "tasks" in inc:
            q, p = (
                "SELECT id,task_type,task_data,status,priority,created_at,completed_at FROM task_queue WHERE 1=1",
                [],
            )
            if sf:
                ss = [s.strip() for s in sf.split(",")]
                q += f" AND status IN ({','.join('?'*len(ss))})"
                p.extend(ss)
            if fd:
                q += " AND created_at>=?"
                p.append(fd)
            if td:
                q += " AND created_at<=?"
                p.append(td)
            for t in conn.execute(
                q + " ORDER BY created_at DESC LIMIT 500", p
            ).fetchall():
                td2 = json.loads(t["task_data"] or "{}")
                s = _ical_dt(t["created_at"])
                if s:
                    evs.append(
                        _ical_ev(
                            _ical_uid("t", t["id"]),
                            f"[Task] {td2.get('description', t['task_type'])}",
                            s,
                            _ical_dt(t["completed_at"]),
                            f"Type: {t['task_type']}\\nStatus: {t['status']}",
                            t["status"],
                            t["priority"],
                        )
                    )
        if "features" in inc:
            q, p = (
                "SELECT f.id,f.name,f.description,f.status,f.priority,f.created_at,p.name pn,m.target_date td FROM features f LEFT JOIN projects p ON f.project_id=p.id LEFT JOIN milestones m ON f.milestone_id=m.id WHERE 1=1",
                [],
            )
            if pid:
                q += " AND f.project_id=?"
                p.append(pid)
            if sf:
                ss = [s.strip() for s in sf.split(",")]
                q += f" AND f.status IN ({','.join('?'*len(ss))})"
                p.extend(ss)
            for f in conn.execute(q + " LIMIT 500", p).fetchall():
                s = _ical_dt(f["td"] or f["created_at"])
                if s:
                    evs.append(
                        _ical_ev(
                            _ical_uid("", f["id"]),
                            (
                                f"[{f['pn']}] {f['name']}"
                                if f["pn"]
                                else f"[Feature] {f['name']}"
                            ),
                            s,
                            desc=f"Status: {
                                f['status']}\\n{
                                f['description'] or ''}",
                            stat=f["status"],
                            pri=f["priority"],
                        )
                    )
        if "bugs" in inc:
            q, p = (
                "SELECT b.id,b.title,b.description,b.status,b.severity,b.created_at,p.name pn FROM bugs b LEFT JOIN projects p ON b.project_id=p.id WHERE 1=1",
                [],
            )
            if pid:
                q += " AND b.project_id=?"
                p.append(pid)
            if sf:
                ss = [s.strip() for s in sf.split(",")]
                q += f" AND b.status IN ({','.join('?'*len(ss))})"
                p.extend(ss)
            for b in conn.execute(q + " LIMIT 500", p).fetchall():
                s = _ical_dt(b["created_at"])
                if s:
                    evs.append(
                        _ical_ev(
                            _ical_uid("b", b["id"]),
                            (
                                f"[{b['pn']}] Bug: {b['title']}"
                                if b["pn"]
                                else f"[Bug] {b['title']}"
                            ),
                            s,
                            desc=f"Severity: {
                                b['severity']}\\nStatus: {
                                b['status']}\\n{
                                b['description'] or ''}",
                            stat=b["status"],
                            pri=b["severity"],
                        )
                    )
    ical = "\r\n".join(
        [
            "BEGIN:VCALENDAR",
            "VERSION:2.0",
            "PRODID:-//Architect Dashboard//EN",
            f"X-WR-CALNAME:{_ical_esc(cn)}",
            "CALSCALE:GREGORIAN",
            "METHOD:PUBLISH",
        ]
        + evs
        + ["END:VCALENDAR"]
    )
    log_activity("export", "ical", pid, f"iCal: {len(evs)} events")
    r = make_response(ical)
    r.headers["Content-Type"] = "text/calendar; charset=utf-8"
    r.headers["Content-Disposition"] = (
        f'attachment; filename="architect_{
            datetime.now().strftime("%Y%m%d")}.ics"'
    )
    return r


@app.route("/api/export/ical/project/<int:project_id>", methods=["GET"])
@require_auth
def export_project_ical(project_id):
    """Export project items as iCal."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        proj = conn.execute(
            "SELECT name FROM projects WHERE id=?", (project_id,)
        ).fetchone()
        if not proj:
            return api_error("Project not found", 404, "not_found")
        evs = []
        for m in conn.execute(
            "SELECT id,name,description,target_date,status FROM milestones WHERE project_id=? AND target_date IS NOT NULL",
            (project_id,),
        ).fetchall():
            s = _ical_dt(m["target_date"])
            if s:
                evs.append(
                    _ical_ev(
                        _ical_uid("m", m["id"]),
                        f"[Milestone] {m['name']}",
                        s,
                        desc=m["description"],
                        stat=m["status"],
                    )
                )
        for f in conn.execute(
            "SELECT f.id,f.name,f.description,f.status,f.priority,f.created_at,m.target_date td FROM features f LEFT JOIN milestones m ON f.milestone_id=m.id WHERE f.project_id=?",
            (project_id,),
        ).fetchall():
            s = _ical_dt(f["td"] or f["created_at"])
            if s:
                evs.append(
                    _ical_ev(
                        _ical_uid("", f["id"]),
                        f"[Feature] {f['name']}",
                        s,
                        desc=f["description"],
                        stat=f["status"],
                        pri=f["priority"],
                    )
                )
        for b in conn.execute(
            "SELECT id,title,description,status,severity,created_at FROM bugs WHERE project_id=?",
            (project_id,),
        ).fetchall():
            s = _ical_dt(b["created_at"])
            if s:
                evs.append(
                    _ical_ev(
                        _ical_uid("b", b["id"]),
                        f"[Bug] {b['title']}",
                        s,
                        desc=b["description"],
                        stat=b["status"],
                        pri=b["severity"],
                    )
                )
    ical = "\r\n".join(
        [
            "BEGIN:VCALENDAR",
            "VERSION:2.0",
            "PRODID:-//Architect Dashboard//EN",
            f"X-WR-CALNAME:{_ical_esc(proj['name'])}",
            "CALSCALE:GREGORIAN",
            "METHOD:PUBLISH",
        ]
        + evs
        + ["END:VCALENDAR"]
    )
    log_activity(
        "export", "ical", project_id, f"Project iCal: {len(evs)} events"
    )
    r = make_response(ical)
    r.headers["Content-Type"] = "text/calendar; charset=utf-8"
    r.headers["Content-Disposition"] = (
        f'attachment; filename="{
            proj["name"].replace(
                " ", "_")}_{
            datetime.now().strftime("%Y%m%d")}.ics"'
    )
    return r


@app.route("/api/export/ical/milestones", methods=["GET"])
@require_auth
def export_milestones_ical():
    """Export milestones as iCal."""
    pid = request.args.get("project_id", type=int)
    up = request.args.get("upcoming", "false").lower() == "true"
    evs = []
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        q, p = (
            "SELECT m.id,m.name,m.description,m.target_date,m.status,p.name pn FROM milestones m LEFT JOIN projects p ON m.project_id=p.id WHERE m.target_date IS NOT NULL",
            [],
        )
        if pid:
            q += " AND m.project_id=?"
            p.append(pid)
        if up:
            q += " AND m.target_date>=date('now') AND m.status!='completed'"
        for m in conn.execute(q + " ORDER BY m.target_date", p).fetchall():
            s = _ical_dt(m["target_date"])
            if s:
                evs.append(
                    _ical_ev(
                        _ical_uid("m", m["id"]),
                        f"[{m['pn']}] {m['name']}" if m["pn"] else m["name"],
                        s,
                        desc=m["description"],
                        stat=m["status"],
                    )
                )
    ical = "\r\n".join(
        [
            "BEGIN:VCALENDAR",
            "VERSION:2.0",
            "PRODID:-//Architect Dashboard//EN",
            "X-WR-CALNAME:Architect Milestones",
            "CALSCALE:GREGORIAN",
            "METHOD:PUBLISH",
        ]
        + evs
        + ["END:VCALENDAR"]
    )
    log_activity("export", "ical", pid, f"Milestones iCal: {len(evs)} events")
    r = make_response(ical)
    r.headers["Content-Type"] = "text/calendar; charset=utf-8"
    r.headers["Content-Disposition"] = (
        f'attachment; filename="milestones_{
            datetime.now().strftime("%Y%m%d")}.ics"'
    )
    return r


# ============================================================================
# WEB CRAWL API (For Crawler Service)
# ============================================================================


@app.route("/api/crawl/<int:task_id>/result", methods=["GET"])
@require_auth
def get_crawl_result(task_id):
    """Get the result of a web crawl task."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        result = conn.execute(
            """
            SELECT cr.*, tq.status as task_status, tq.task_data
            FROM crawl_results cr
            JOIN task_queue tq ON cr.task_id = tq.id
            WHERE cr.task_id = ?
        """,
            (task_id,),
        ).fetchone()

        if not result:
            # Check if task exists but has no result yet
            task = conn.execute(
                "SELECT * FROM task_queue WHERE id = ?", (task_id,)
            ).fetchone()

            if task:
                return jsonify(
                    {
                        "task_id": task_id,
                        "status": task["status"],
                        "result": None,
                        "message": "Task found but no result yet",
                    }
                )

            return jsonify({"error": "Task not found"}), 404

        # Parse JSON fields
        result_dict = dict(result)
        for field in [
            "extracted_data",
            "action_history",
            "screenshots",
            "task_data",
        ]:
            if result_dict.get(field):
                try:
                    result_dict[field] = json.loads(result_dict[field])
                except (json.JSONDecodeError, TypeError):
                    pass

        return jsonify(result_dict)


@app.route("/api/crawl/history", methods=["GET"])
@require_auth
def get_crawl_history():
    """Get history of crawl results with optional filtering."""
    limit = request.args.get("limit", 50, type=int)
    offset = request.args.get("offset", 0, type=int)
    success_only = request.args.get("success", None)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        query = """
            SELECT cr.*, tq.task_type, tq.status as task_status
            FROM crawl_results cr
            JOIN task_queue tq ON cr.task_id = tq.id
            WHERE 1=1
        """
        params = []

        if success_only is not None:
            query += " AND cr.success = ?"
            params.append(1 if success_only.lower() == "true" else 0)

        query += " ORDER BY cr.created_at DESC LIMIT ? OFFSET ?"
        params.extend([limit, offset])

        results = conn.execute(query, params).fetchall()

        # Parse JSON fields for each result
        output = []
        for r in results:
            r_dict = dict(r)
            for field in ["extracted_data", "action_history", "screenshots"]:
                if r_dict.get(field):
                    try:
                        r_dict[field] = json.loads(r_dict[field])
                    except (json.JSONDecodeError, TypeError):
                        pass
            output.append(r_dict)

        # Get total count
        total = conn.execute(
            "SELECT COUNT(*) as count FROM crawl_results"
        ).fetchone()["count"]

        return jsonify(
            {
                "results": output,
                "total": total,
                "limit": limit,
                "offset": offset,
            }
        )


@app.route("/api/crawl/submit", methods=["POST"])
@require_auth
def submit_crawl_task():
    """Submit a new web crawl task to the queue.

    Expected JSON body:
    {
        "prompt": "Go to amazon.com and find the price of iPhone 15",
        "url": "https://amazon.com",  // optional starting URL
        "extract_fields": ["price", "title"],  // optional fields to extract
        "screenshot": true,  // optional, default true
        "timeout": 60,  // optional, default 60
        "priority": 5  // optional, default 5
    }
    """
    data = request.get_json() or {}

    prompt = data.get("prompt")
    if not prompt:
        return jsonify({"error": "prompt is required"}), 400

    task_data = {
        "prompt": prompt,
        "url": data.get("url", ""),
        "extract_fields": data.get("extract_fields", []),
        "screenshot": data.get("screenshot", True),
        "timeout": data.get("timeout", 60),
    }

    priority = data.get("priority", 5)

    with get_db_connection() as conn:
        cursor = conn.execute(
            """
            INSERT INTO task_queue (task_type, task_data, priority, max_retries)
            VALUES ('web_crawl', ?, ?, 3)
        """,
            (json.dumps(task_data), priority),
        )

        task_id = cursor.lastrowid

        log_activity("create_task", "web_crawl", task_id, prompt[:100])

        return jsonify(
            {
                "success": True,
                "task_id": task_id,
                "status": "pending",
                "message": "Web crawl task submitted",
                "result_url": f"/api/crawl/{task_id}/result",
            }
        )


@app.route("/api/crawl/stats", methods=["GET"])
@require_auth
def get_crawl_stats():
    """Get statistics about web crawl tasks."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Task counts by status
        task_stats = conn.execute(
            """
            SELECT status, COUNT(*) as count
            FROM task_queue
            WHERE task_type = 'web_crawl'
            GROUP BY status
        """
        ).fetchall()

        # Result stats
        result_stats = conn.execute(
            """
            SELECT
                COUNT(*) as total,
                SUM(CASE WHEN success = 1 THEN 1 ELSE 0 END) as successful,
                SUM(CASE WHEN success = 0 THEN 1 ELSE 0 END) as failed,
                AVG(duration_seconds) as avg_duration,
                MAX(duration_seconds) as max_duration
            FROM crawl_results
        """
        ).fetchone()

        # Provider usage
        provider_stats = conn.execute(
            """
            SELECT llm_provider, COUNT(*) as count
            FROM crawl_results
            WHERE llm_provider IS NOT NULL
            GROUP BY llm_provider
        """
        ).fetchall()

        return jsonify(
            {
                "tasks": {row["status"]: row["count"] for row in task_stats},
                "results": {
                    "total": result_stats["total"] or 0,
                    "successful": result_stats["successful"] or 0,
                    "failed": result_stats["failed"] or 0,
                    "success_rate": (
                        (
                            result_stats["successful"]
                            / result_stats["total"]
                            * 100
                        )
                        if result_stats["total"]
                        else 0
                    ),
                    "avg_duration": result_stats["avg_duration"] or 0,
                    "max_duration": result_stats["max_duration"] or 0,
                },
                "providers": {
                    row["llm_provider"]: row["count"] for row in provider_stats
                },
            }
        )


# ============================================================================
# WORKER MANAGEMENT API
# ============================================================================


@app.route("/api/workers", methods=["GET"])
@require_auth
def get_workers():
    """Get all registered workers.

    Returns workers with their node info and current task assignment.

    Query Parameters:
        status (str): Filter by status (idle, busy, offline)
        page (int): Page number (default: 1)
        per_page (int): Items per page, max 100 (default: 50)
        paginate (str): Set to 'true' for pagination response

    Returns:
        200: List of workers or paginated result

    Example Request:
        GET /api/workers
        GET /api/workers?status=busy
        GET /api/workers?paginate=true

    Example Response:
        [
            {
                "id": "worker-abc123",
                "worker_type": "task",
                "node_id": 1,
                "node_hostname": "worker-01",
                "status": "busy",
                "current_task_id": 42,
                "current_task_type": "shell",
                "tasks_completed": 156,
                "tasks_failed": 3,
                "last_heartbeat": "2024-01-15T14:30:00",
                "started_at": "2024-01-15T08:00:00"
            }
        ]

    cURL Example:
        curl -X GET "http://localhost:8080/api/workers?status=idle" \\
             -H "Cookie: session=<session_cookie>"
    """
    status_filter = request.args.get("status")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        query = """
            SELECT w.*, n.hostname as node_hostname, t.task_type as current_task_type
            FROM workers w
            LEFT JOIN nodes n ON w.node_id = n.id
            LEFT JOIN task_queue t ON w.current_task_id = t.id
        """
        params = []

        if status_filter:
            query += " WHERE w.status = ?"
            params.append(status_filter)

        query += " ORDER BY w.node_id, w.worker_type"

        workers = conn.execute(query, params).fetchall()
        worker_list = [dict(w) for w in workers]

        # Support pagination if requested
        if request.args.get("paginate", "").lower() == "true":
            page, per_page = get_pagination_params()
            result = paginate_query(worker_list, page, per_page)
            return jsonify(result)

        return jsonify(worker_list)


@app.route("/api/workers/stats", methods=["GET"])
@require_auth
def get_worker_stats():
    """Get worker performance statistics and metrics.

    Returns aggregated statistics about workers including:
    - Total count and counts by status
    - Task completion metrics per worker type
    - Average task processing times
    - Throughput (tasks per hour/day)
    - Error rates and retry statistics

    Query params:
        worker_type: Filter stats by worker type
        hours: Time window for recent stats (default 24)
    """
    worker_type_filter = request.args.get("worker_type")
    hours = request.args.get("hours", 24, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Build WHERE clause for worker type filter
        worker_where = ""
        worker_params = []
        if worker_type_filter:
            worker_where = "WHERE worker_type = ?"
            worker_params = [worker_type_filter]

        # Total workers and by status
        total_workers = conn.execute(
            f"SELECT COUNT(*) as count FROM workers {worker_where}",
            worker_params,
        ).fetchone()["count"]

        status_query = """
            SELECT status, COUNT(*) as count
            FROM workers {worker_where}
            GROUP BY status
        """
        status_rows = conn.execute(status_query, worker_params).fetchall()
        by_status = {row["status"]: row["count"] for row in status_rows}

        # Workers by type
        type_query = """
            SELECT worker_type, COUNT(*) as count,
                   SUM(CASE WHEN status = 'idle' THEN 1 ELSE 0 END) as idle,
                   SUM(CASE WHEN status = 'busy' THEN 1 ELSE 0 END) as busy,
                   SUM(CASE WHEN status = 'offline' THEN 1 ELSE 0 END) as offline
            FROM workers
            GROUP BY worker_type
        """
        type_rows = conn.execute(type_query).fetchall()
        by_type = {
            row["worker_type"]: {
                "total": row["count"],
                "idle": row["idle"],
                "busy": row["busy"],
                "offline": row["offline"],
            }
            for row in type_rows
        }

        # Task completion stats (from task_queue)
        task_stats_query = """
            SELECT
                COUNT(*) as total_tasks,
                SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completed,
                SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed,
                SUM(CASE WHEN status = 'running' THEN 1 ELSE 0 END) as running,
                SUM(CASE WHEN status = 'pending' THEN 1 ELSE 0 END) as pending,
                AVG(CASE WHEN status = 'completed' AND started_at IS NOT NULL AND completed_at IS NOT NULL
                    THEN (JULIANDAY(completed_at) - JULIANDAY(started_at)) * 86400
                    ELSE NULL END) as avg_duration_seconds,
                SUM(retries) as total_retries
            FROM task_queue
            WHERE created_at > datetime('now', '-' || ? || ' hours')
        """
        task_stats = conn.execute(task_stats_query, [hours]).fetchone()

        # Tasks completed in time window
        completed_count = task_stats["completed"] or 0
        failed_count = task_stats["failed"] or 0
        total_processed = completed_count + failed_count

        # Calculate success rate
        success_rate = (
            round((completed_count / total_processed * 100), 1)
            if total_processed > 0
            else 100.0
        )

        # Throughput calculations
        tasks_per_hour = round(total_processed / hours, 2) if hours > 0 else 0

        # Top performing workers (by completed tasks)
        top_workers_query = """
            SELECT assigned_worker, COUNT(*) as completed_tasks,
                   AVG((JULIANDAY(completed_at) - JULIANDAY(started_at)) * 86400) as avg_seconds
            FROM task_queue
            WHERE status = 'completed'
              AND assigned_worker IS NOT NULL
              AND completed_at > datetime('now', '-' || ? || ' hours')
            GROUP BY assigned_worker
            ORDER BY completed_tasks DESC
            LIMIT 10
        """
        top_workers = conn.execute(top_workers_query, [hours]).fetchall()
        top_performers = [
            {
                "worker_id": row["assigned_worker"],
                "completed_tasks": row["completed_tasks"],
                "avg_duration_seconds": round(row["avg_seconds"] or 0, 2),
            }
            for row in top_workers
        ]

        # Tasks by type in time window
        tasks_by_type_query = """
            SELECT task_type, COUNT(*) as count,
                   SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completed,
                   SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed
            FROM task_queue
            WHERE created_at > datetime('now', '-' || ? || ' hours')
            GROUP BY task_type
            ORDER BY count DESC
        """
        tasks_by_type = conn.execute(tasks_by_type_query, [hours]).fetchall()
        task_type_breakdown = {
            row["task_type"]: {
                "total": row["count"],
                "completed": row["completed"],
                "failed": row["failed"],
            }
            for row in tasks_by_type
        }

        # Worker availability (last heartbeat within threshold)
        heartbeat_threshold = 120  # 2 minutes
        available_query = """
            SELECT COUNT(*) as count
            FROM workers
            WHERE last_heartbeat > datetime('now', '-' || ? || ' seconds')
        """
        available_count = conn.execute(
            available_query, [heartbeat_threshold]
        ).fetchone()["count"]
        availability_rate = (
            round((available_count / total_workers * 100), 1)
            if total_workers > 0
            else 0
        )

        # Recent errors
        error_query = """
            SELECT COUNT(*) as count
            FROM task_queue
            WHERE status = 'failed'
              AND completed_at > datetime('now', '-' || ? || ' hours')
        """
        recent_errors = conn.execute(error_query, [hours]).fetchone()["count"]

        return jsonify(
            {
                "workers": {
                    "total": total_workers,
                    "by_status": by_status,
                    "by_type": by_type,
                    "available": available_count,
                    "availability_rate": availability_rate,
                },
                "tasks": {
                    "time_window_hours": hours,
                    "total": task_stats["total_tasks"] or 0,
                    "completed": completed_count,
                    "failed": failed_count,
                    "running": task_stats["running"] or 0,
                    "pending": task_stats["pending"] or 0,
                    "success_rate": success_rate,
                    "total_retries": task_stats["total_retries"] or 0,
                },
                "performance": {
                    "avg_duration_seconds": round(
                        task_stats["avg_duration_seconds"] or 0, 2
                    ),
                    "tasks_per_hour": tasks_per_hour,
                    "tasks_per_day": round(tasks_per_hour * 24, 2),
                },
                "top_performers": top_performers,
                "task_type_breakdown": task_type_breakdown,
                "recent_errors": recent_errors,
                "worker_type_filter": worker_type_filter,
            }
        )


@app.route("/api/workers/register", methods=["POST"])
@rate_limit(requests_per_minute=20)  # Prevent worker registration spam
def register_worker():
    """Register a new worker."""
    data = request.get_json()

    if not data:
        return api_error("Request body is required", 400, "validation_error")

    worker_id = data.get("id") or str(uuid.uuid4())

    try:
        with get_db_connection() as conn:
            conn.execute(
                """
                INSERT OR REPLACE INTO workers (id, node_id, worker_type, status, last_heartbeat)
                VALUES (?, ?, ?, 'idle', CURRENT_TIMESTAMP)
            """,
                (
                    worker_id,
                    data.get("node_id", "local"),
                    data.get("worker_type", "general"),
                ),
            )

            return jsonify({"id": worker_id, "success": True})
    except sqlite3.Error as e:
        logger.error(f"Database error registering worker: {e}")
        return api_error("Failed to register worker", 500, "database_error")


@app.route("/api/workers/<worker_id>/heartbeat", methods=["POST"])
def worker_heartbeat(worker_id):
    """Receive heartbeat from a worker."""
    data = request.get_json(silent=True) or {}

    with get_db_connection() as conn:
        conn.execute(
            """
            UPDATE workers SET
                status = ?,
                last_heartbeat = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (data.get("status", "idle"), worker_id),
        )

        return jsonify({"success": True})


# ============================================================================
# Skill-Based Task Assignment Endpoints
# ============================================================================


@app.route("/api/workers/<worker_id>/skills", methods=["GET", "POST"])
@require_auth
def worker_skills(worker_id):
    """Get or add skills for a worker.

    GET: Returns all skills for the worker
    POST: Add a new skill to the worker
        Body: {skill_name, proficiency (0-100)}
    """
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        if request.method == "GET":
            skills = conn.execute(
                """
                SELECT skill_name, proficiency, tasks_completed,
                       avg_duration_seconds, last_used, created_at
                FROM worker_skills
                WHERE worker_id = ?
                ORDER BY proficiency DESC
            """,
                [worker_id],
            ).fetchall()

            return jsonify(
                {
                    "worker_id": worker_id,
                    "skills": [dict(s) for s in skills],
                    "skill_count": len(skills),
                }
            )

        else:  # POST
            data = request.get_json()
            if not data or not data.get("skill_name"):
                return api_error(
                    "skill_name is required", 400, "missing_field"
                )

            skill_name = data["skill_name"].lower().strip()
            proficiency = min(100, max(0, data.get("proficiency", 50)))

            conn.execute(
                """
                INSERT INTO worker_skills (worker_id, skill_name, proficiency)
                VALUES (?, ?, ?)
                ON CONFLICT(worker_id, skill_name)
                DO UPDATE SET proficiency = excluded.proficiency
            """,
                [worker_id, skill_name, proficiency],
            )

            log_activity(
                conn,
                "skill_added",
                "worker",
                worker_id,
                f"Added skill {skill_name} (proficiency: {proficiency})",
            )

            return jsonify(
                {
                    "success": True,
                    "worker_id": worker_id,
                    "skill_name": skill_name,
                    "proficiency": proficiency,
                }
            )


@app.route("/api/workers/<worker_id>/skills/<skill_name>", methods=["DELETE"])
@require_auth
def delete_worker_skill(worker_id, skill_name):
    """Remove a skill from a worker."""
    with get_db_connection() as conn:
        result = conn.execute(
            """
            DELETE FROM worker_skills
            WHERE worker_id = ? AND skill_name = ?
        """,
            [worker_id, skill_name.lower()],
        )

        if result.rowcount == 0:
            return api_error("Skill not found for worker", 404, "not_found")

        log_activity(
            conn,
            "skill_removed",
            "worker",
            worker_id,
            f"Removed skill {skill_name}",
        )

        return jsonify({"success": True, "deleted": skill_name})


@app.route("/api/workers/<worker_id>/skills/match", methods=["GET"])
@require_auth
def get_worker_skill_match(worker_id):
    """Get skill match scores for a worker against a task type.

    Query params:
        task_type: The task type to match against (required)
    """
    task_type = request.args.get("task_type")
    if not task_type:
        return api_error(
            "task_type query param is required", 400, "missing_param"
        )

    from services.skill_matching import calculate_skill_match_score

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        match = calculate_skill_match_score(worker_id, task_type, conn)
        return jsonify(match)


@app.route("/api/tasks/skill-requirements", methods=["GET", "POST"])
@require_auth
def task_skill_requirements():
    """Get or add skill requirements for task types.

    GET: List all task skill requirements
        Query params: task_type (optional filter)
    POST: Add a skill requirement
        Body: {task_type, skill_name, min_proficiency, priority}
    """
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        if request.method == "GET":
            task_type = request.args.get("task_type")

            if task_type:
                requirements = conn.execute(
                    """
                    SELECT id, task_type, skill_name, min_proficiency, priority, created_at
                    FROM task_skill_requirements
                    WHERE task_type = ?
                    ORDER BY priority DESC
                """,
                    [task_type],
                ).fetchall()
            else:
                requirements = conn.execute(
                    """
                    SELECT id, task_type, skill_name, min_proficiency, priority, created_at
                    FROM task_skill_requirements
                    ORDER BY task_type, priority DESC
                """
                ).fetchall()

            # Group by task type
            by_type = {}
            for r in requirements:
                tt = r["task_type"]
                if tt not in by_type:
                    by_type[tt] = []
                by_type[tt].append(dict(r))

            return jsonify(
                {
                    "requirements": [dict(r) for r in requirements],
                    "by_task_type": by_type,
                    "total": len(requirements),
                }
            )

        else:  # POST
            data = request.get_json()
            if (
                not data
                or not data.get("task_type")
                or not data.get("skill_name")
            ):
                return api_error(
                    "task_type and skill_name are required",
                    400,
                    "missing_field",
                )

            conn.execute(
                """
                INSERT INTO task_skill_requirements
                    (task_type, skill_name, min_proficiency, priority)
                VALUES (?, ?, ?, ?)
                ON CONFLICT(task_type, skill_name)
                DO UPDATE SET
                    min_proficiency = excluded.min_proficiency,
                    priority = excluded.priority
            """,
                [
                    data["task_type"],
                    data["skill_name"].lower(),
                    data.get("min_proficiency", 0),
                    data.get("priority", 1),
                ],
            )

            return jsonify(
                {
                    "success": True,
                    "task_type": data["task_type"],
                    "skill_name": data["skill_name"],
                }
            )


@app.route("/api/tasks/skill-requirements/<int:req_id>", methods=["DELETE"])
@require_auth
def delete_skill_requirement(req_id):
    """Delete a task skill requirement."""
    with get_db_connection() as conn:
        result = conn.execute(
            "DELETE FROM task_skill_requirements WHERE id = ?", [req_id]
        )

        if result.rowcount == 0:
            return api_error("Requirement not found", 404, "not_found")

        return jsonify({"success": True, "deleted_id": req_id})


@app.route("/api/tasks/find-worker", methods=["POST"])
@require_auth
def find_worker_for_task():
    """Find the best worker for a task based on skill matching.

    Body: {
        task_type: The type of task (required)
        min_score: Minimum acceptable match score 0-100 (default 0)
        status_filter: Only consider workers with this status (default 'idle')
    }
    """
    data = request.get_json()
    if not data or not data.get("task_type"):
        return api_error("task_type is required", 400, "missing_field")

    from services.skill_matching import find_best_worker_for_task

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Get available workers
        status_filter = data.get("status_filter", "idle")
        workers = conn.execute(
            """
            SELECT id, worker_type, node_id, status
            FROM workers
            WHERE status = ?
        """,
            [status_filter],
        ).fetchall()

        available = [dict(w) for w in workers]

        if not available:
            return jsonify(
                {
                    "best_worker": None,
                    "candidates": [],
                    "qualified_count": 0,
                    "total_evaluated": 0,
                    "message": f"No workers available with status: {status_filter}",
                }
            )

        result = find_best_worker_for_task(
            data["task_type"],
            available,
            conn,
            min_score=data.get("min_score", 0),
        )

        return jsonify(result)


@app.route("/api/skills", methods=["GET"])
@require_auth
def list_all_skills():
    """List all skills in the system with statistics.

    Query params:
        limit: Max results (default 50)
    """
    limit = min(request.args.get("limit", 50, type=int), 200)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Get all unique skills with worker counts
        skills = conn.execute(
            """
            SELECT
                skill_name,
                COUNT(DISTINCT worker_id) as worker_count,
                AVG(proficiency) as avg_proficiency,
                SUM(tasks_completed) as total_tasks,
                MAX(last_used) as last_used
            FROM worker_skills
            GROUP BY skill_name
            ORDER BY worker_count DESC, total_tasks DESC
            LIMIT ?
        """,
            [limit],
        ).fetchall()

        # Get skill requirements
        required_skills = conn.execute(
            """
            SELECT skill_name, COUNT(DISTINCT task_type) as task_types
            FROM task_skill_requirements
            GROUP BY skill_name
        """
        ).fetchall()

        req_map = {r["skill_name"]: r["task_types"] for r in required_skills}

        skill_list = []
        for s in skills:
            skill_list.append(
                {
                    "skill_name": s["skill_name"],
                    "worker_count": s["worker_count"],
                    "avg_proficiency": round(s["avg_proficiency"] or 0, 1),
                    "total_tasks_completed": s["total_tasks"] or 0,
                    "last_used": s["last_used"],
                    "required_by_task_types": req_map.get(s["skill_name"], 0),
                }
            )

        return jsonify({"skills": skill_list, "total": len(skill_list)})


@app.route("/api/skills/<skill_name>/leaderboard", methods=["GET"])
@require_auth
def skill_leaderboard(skill_name):
    """Get top workers for a specific skill.

    Query params:
        limit: Max results (default 10)
    """
    from services.skill_matching import get_skill_leaderboard

    limit = min(request.args.get("limit", 10, type=int), 50)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        leaders = get_skill_leaderboard(skill_name, conn, limit)

        return jsonify(
            {
                "skill_name": skill_name,
                "leaderboard": leaders,
                "count": len(leaders),
            }
        )


@app.route("/api/workers/health", methods=["GET"])
def get_workers_health():
    """Aggregate health status from all worker processes.

    Checks PID files and process status for all known workers.
    Returns running/stopped status, uptime, and resource usage.
    """

    # Define all known workers with their PID/lock files and log files
    WORKERS = {
        "project_orchestrator": {
            "pid_file": "/tmp/project_orchestrator.pid",
            "log_file": "/tmp/project_orchestrator.log",
            "description": "Manages autopilot runs and project orchestration",
        },
        "auto_confirm": {
            "pid_file": "/tmp/auto_confirm.lock",  # Uses lock file
            "log_file": "/tmp/auto_confirm.log",
            "description": "Auto-confirms Claude permission prompts",
        },
        "assigner": {
            "pid_file": "/tmp/architect_assigner_worker.pid",
            "log_file": "/tmp/architect_assigner_worker.log",
            "description": "Assigns prompts to Claude sessions",
        },
        "milestone": {
            "pid_file": "/tmp/architect_milestone_worker.pid",
            "log_file": "/tmp/architect_milestone_worker.log",
            "description": "Scans projects and generates milestones",
        },
        "health_monitor": {
            "pid_file": "/tmp/health_monitor.pid",
            "log_file": "/tmp/health_monitor.log",
            "description": "Monitors system and service health",
        },
        "service_checker": {
            "pid_file": "/tmp/service_checker.pid",
            "log_file": "/tmp/service_checker.log",
            "description": "Checks service availability",
        },
        "test_worker": {
            "pid_file": "/tmp/architect_test_worker.pid",
            "log_file": "/tmp/architect_test_worker.log",
            "description": "Runs automated tests",
        },
        "perplexity_sheets": {
            "pid_file": "/tmp/perplexity_sheets.pid",
            "log_file": "/tmp/perplexity_sheets.log",
            "description": "Syncs with Perplexity and Google Sheets",
        },
    }

    def check_process(pid):
        """Check if a process is running by PID."""
        try:
            os.kill(pid, 0)
            return True
        except (OSError, ProcessLookupError):
            return False

    def get_process_info(pid):
        """Get process info using psutil if available."""
        try:
            import psutil

            proc = psutil.Process(pid)
            return {
                "cpu_percent": round(proc.cpu_percent(interval=0.1), 1),
                "memory_mb": round(proc.memory_info().rss / 1024 / 1024, 1),
                "uptime_seconds": round(time.time() - proc.create_time(), 0),
                "threads": proc.num_threads(),
                "status": proc.status(),
            }
        except Exception:
            return None

    def get_last_log_line(log_file):
        """Get the last line of a log file."""
        try:
            path = Path(log_file)
            if path.exists():
                with open(path, "rb") as f:
                    # Seek to end and read last 500 bytes
                    f.seek(0, 2)
                    size = f.tell()
                    if size > 500:
                        f.seek(-500, 2)
                    else:
                        f.seek(0)
                    lines = (
                        f.read()
                        .decode("utf-8", errors="ignore")
                        .strip()
                        .split("\n")
                    )
                    return lines[-1] if lines else None
        except Exception:
            return None

    results = {
        "timestamp": datetime.now().isoformat(),
        "workers": {},
        "summary": {"total": len(WORKERS), "running": 0, "stopped": 0},
    }

    for name, config in WORKERS.items():
        pid_file = Path(config["pid_file"])
        worker_info = {
            "name": name,
            "description": config["description"],
            "status": "stopped",
            "pid": None,
            "pid_file": config["pid_file"],
            "log_file": config["log_file"],
        }

        if pid_file.exists():
            try:
                pid_content = pid_file.read_text().strip()
                # Handle lock files that might contain more than just PID
                pid = int(pid_content.split("\n")[0].strip())
                worker_info["pid"] = pid

                if check_process(pid):
                    worker_info["status"] = "running"
                    results["summary"]["running"] += 1

                    # Get detailed process info
                    proc_info = get_process_info(pid)
                    if proc_info:
                        worker_info.update(proc_info)
                else:
                    worker_info["status"] = (
                        "stale"  # PID file exists but process dead
                    )
                    results["summary"]["stopped"] += 1
            except (ValueError, IOError) as e:
                worker_info["status"] = "error"
                worker_info["error"] = str(e)
                results["summary"]["stopped"] += 1
        else:
            results["summary"]["stopped"] += 1

        # Get last log entry
        last_log = get_last_log_line(config["log_file"])
        if last_log:
            worker_info["last_log"] = last_log[:200]  # Truncate long lines

        # Check if log file exists and get its modification time
        log_path = Path(config["log_file"])
        if log_path.exists():
            worker_info["log_modified"] = datetime.fromtimestamp(
                log_path.stat().st_mtime
            ).isoformat()

        results["workers"][name] = worker_info

    # Add overall health status
    running_ratio = results["summary"]["running"] / results["summary"]["total"]
    if running_ratio >= 0.8:
        results["health"] = "healthy"
    elif running_ratio >= 0.5:
        results["health"] = "degraded"
    else:
        results["health"] = "unhealthy"

    return jsonify(results)


# ============================================================================
# WORKER LOAD BALANCER API
# ============================================================================


@app.route("/api/workers/load", methods=["GET"])
@require_auth
def get_workers_load_distribution():
    """Get load distribution across all workers.

    Returns:
        - total_workers: Total registered workers
        - healthy_workers: Workers with recent heartbeat
        - total_capacity: Combined task capacity
        - total_current_tasks: Tasks being processed
        - avg_load_percentage: Average load across workers
        - workers: List of individual worker loads
        - by_node: Load grouped by cluster node
        - by_type: Load grouped by worker type
        - imbalance_score: 0-100 score (0 = balanced)
        - recommendations: Suggested actions
    """
    from services.load_balancer import get_load_balancer

    try:
        lb = get_load_balancer(DB_PATH)
        distribution = lb.get_load_distribution()

        return jsonify(
            {
                "success": True,
                "distribution": {
                    "total_workers": distribution.total_workers,
                    "healthy_workers": distribution.healthy_workers,
                    "total_capacity": distribution.total_capacity,
                    "total_current_tasks": distribution.total_current_tasks,
                    "avg_load_percentage": round(
                        distribution.avg_load_percentage, 1
                    ),
                    "max_load_percentage": round(
                        distribution.max_load_percentage, 1
                    ),
                    "min_load_percentage": round(
                        distribution.min_load_percentage, 1
                    ),
                    "imbalance_score": round(distribution.imbalance_score, 1),
                    "workers": distribution.workers,
                    "by_node": distribution.by_node,
                    "by_type": distribution.by_type,
                    "recommendations": distribution.recommendations,
                },
            }
        )
    except Exception as e:
        logger.error(f"Failed to get load distribution: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/workers/select", methods=["POST"])
@require_auth
def select_worker_for_task():
    """Select the best worker for a task using load balancing.

    Request body:
        - task_type: Type of task (optional, for skill matching)
        - strategy: Load balancing strategy (optional)
            - round_robin: Simple rotation
            - least_loaded: Worker with lowest current load (default)
            - weighted: Consider worker weights and capacity
            - skill_based: Match worker skills to task
            - adaptive: Combine multiple factors
        - worker_type: Filter by worker type (optional)
        - exclude_workers: List of worker IDs to exclude (optional)
        - min_capacity: Minimum required capacity (optional, default 1)

    Returns:
        - worker_id: Selected worker ID
        - worker_type: Type of selected worker
        - node_id: Node where worker runs
        - strategy_used: Strategy that was applied
        - score: Selection score
        - load_before: Worker's current load %
        - estimated_load_after: Projected load after assignment
        - alternatives: Other candidate workers
    """
    from services.load_balancer import LoadBalancingStrategy, get_load_balancer

    data = request.get_json() or {}
    task_type = data.get("task_type")
    strategy_name = data.get("strategy", "least_loaded")
    worker_type = data.get("worker_type")
    exclude_workers = data.get("exclude_workers", [])
    min_capacity = data.get("min_capacity", 1)

    try:
        strategy = LoadBalancingStrategy(strategy_name)
    except ValueError:
        return (
            jsonify(
                {
                    "success": False,
                    "error": f"Invalid strategy: {strategy_name}. Valid: round_robin, least_loaded, weighted, skill_based, adaptive",
                }
            ),
            400,
        )

    try:
        lb = get_load_balancer(DB_PATH)
        selection = lb.select_worker(
            task_type=task_type,
            strategy=strategy,
            worker_type=worker_type,
            exclude_workers=exclude_workers,
            min_capacity=min_capacity,
        )

        if not selection.worker_id:
            return (
                jsonify(
                    {
                        "success": False,
                        "error": selection.reason,
                        "strategy_used": selection.strategy_used,
                    }
                ),
                404,
            )

        return jsonify(
            {
                "success": True,
                "selection": {
                    "worker_id": selection.worker_id,
                    "worker_type": selection.worker_type,
                    "node_id": selection.node_id,
                    "strategy_used": selection.strategy_used,
                    "score": round(selection.score, 2),
                    "reason": selection.reason,
                    "load_before": round(selection.load_before, 1),
                    "estimated_load_after": round(
                        selection.estimated_load_after, 1
                    ),
                    "alternatives": selection.alternatives[:5],
                },
            }
        )
    except Exception as e:
        logger.error(f"Failed to select worker: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/workers/<worker_id>/load", methods=["GET"])
@require_auth
def get_worker_load(worker_id):
    """Get detailed load information for a specific worker.

    Returns:
        - worker_id: Worker identifier
        - worker_type: Type of worker
        - node_id: Node identifier
        - status: Worker status
        - current_tasks: Running tasks count
        - pending_tasks: Pending assigned tasks
        - completed_today: Tasks completed today
        - failed_today: Tasks failed today
        - capacity: Maximum concurrent tasks
        - weight: Load balancing weight
        - load_percentage: Current load %
        - available_capacity: Remaining capacity
        - avg_task_duration_ms: Average task duration
        - success_rate: Task success rate %
        - is_healthy: Health status
        - is_draining: Drain status
    """
    from services.load_balancer import get_load_balancer

    try:
        lb = get_load_balancer(DB_PATH)
        load = lb.get_worker_load(worker_id)

        if not load:
            return (
                jsonify(
                    {
                        "success": False,
                        "error": f"Worker not found: {worker_id}",
                    }
                ),
                404,
            )

        return jsonify(
            {
                "success": True,
                "load": {
                    "worker_id": load.worker_id,
                    "worker_type": load.worker_type,
                    "node_id": load.node_id,
                    "status": load.status,
                    "current_tasks": load.current_tasks,
                    "pending_tasks": load.pending_tasks,
                    "completed_today": load.completed_today,
                    "failed_today": load.failed_today,
                    "capacity": load.capacity,
                    "weight": load.weight,
                    "load_percentage": round(load.load_percentage, 1),
                    "available_capacity": load.available_capacity,
                    "effective_weight": round(load.effective_weight, 2),
                    "avg_task_duration_ms": round(
                        load.avg_task_duration_ms, 0
                    ),
                    "success_rate": round(load.success_rate, 1),
                    "is_healthy": load.is_healthy,
                    "is_draining": load.is_draining,
                    "last_heartbeat": load.last_heartbeat,
                },
            }
        )
    except Exception as e:
        logger.error(f"Failed to get worker load: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/workers/<worker_id>/capacity", methods=["POST"])
@require_auth
def set_worker_capacity(worker_id):
    """Set the maximum task capacity for a worker.

    Request body:
        - capacity: Maximum concurrent tasks (integer, >= 1)
    """
    from services.load_balancer import get_load_balancer

    data = request.get_json() or {}
    capacity = data.get("capacity")

    if capacity is None or not isinstance(capacity, int) or capacity < 1:
        return (
            jsonify(
                {
                    "success": False,
                    "error": "capacity must be a positive integer",
                }
            ),
            400,
        )

    try:
        lb = get_load_balancer(DB_PATH)
        success = lb.set_worker_capacity(worker_id, capacity)

        if success:
            log_activity(
                "worker_config",
                f"Set capacity to {capacity} for worker {worker_id}",
            )
            return jsonify(
                {
                    "success": True,
                    "message": f"Capacity set to {capacity} for worker {worker_id}",
                }
            )
        else:
            return (
                jsonify({"success": False, "error": "Failed to set capacity"}),
                500,
            )
    except Exception as e:
        logger.error(f"Failed to set worker capacity: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/workers/<worker_id>/weight", methods=["POST"])
@require_auth
def set_worker_weight(worker_id):
    """Set the load balancing weight for a worker.

    Request body:
        - weight: Load balancing weight (float, > 0)
            Higher weight = more tasks assigned
    """
    from services.load_balancer import get_load_balancer

    data = request.get_json() or {}
    weight = data.get("weight")

    if weight is None or not isinstance(weight, (int, float)) or weight <= 0:
        return (
            jsonify(
                {"success": False, "error": "weight must be a positive number"}
            ),
            400,
        )

    try:
        lb = get_load_balancer(DB_PATH)
        success = lb.set_worker_weight(worker_id, float(weight))

        if success:
            log_activity(
                "worker_config",
                f"Set weight to {weight} for worker {worker_id}",
            )
            return jsonify(
                {
                    "success": True,
                    "message": f"Weight set to {weight} for worker {worker_id}",
                }
            )
        else:
            return (
                jsonify({"success": False, "error": "Failed to set weight"}),
                500,
            )
    except Exception as e:
        logger.error(f"Failed to set worker weight: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/workers/<worker_id>/drain", methods=["POST"])
@require_auth
def drain_worker(worker_id):
    """Mark a worker as draining (finish current tasks, no new assignments).

    Used for graceful worker shutdown or maintenance.
    """
    from services.load_balancer import get_load_balancer

    try:
        lb = get_load_balancer(DB_PATH)
        success = lb.drain_worker(worker_id)

        if success:
            log_activity("worker_drain", f"Drained worker {worker_id}")
            return jsonify(
                {
                    "success": True,
                    "message": f"Worker {worker_id} is now draining",
                }
            )
        else:
            return (
                jsonify({"success": False, "error": "Failed to drain worker"}),
                500,
            )
    except Exception as e:
        logger.error(f"Failed to drain worker: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/workers/<worker_id>/undrain", methods=["POST"])
@require_auth
def undrain_worker(worker_id):
    """Remove draining status from a worker, making it available for new tasks."""
    from services.load_balancer import get_load_balancer

    try:
        lb = get_load_balancer(DB_PATH)
        success = lb.undrain_worker(worker_id)

        if success:
            log_activity("worker_undrain", f"Undrained worker {worker_id}")
            return jsonify(
                {
                    "success": True,
                    "message": f"Worker {worker_id} is now available",
                }
            )
        else:
            return (
                jsonify(
                    {"success": False, "error": "Failed to undrain worker"}
                ),
                500,
            )
    except Exception as e:
        logger.error(f"Failed to undrain worker: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/workers/rebalance/suggest", methods=["GET"])
@require_auth
def suggest_rebalance():
    """Get suggestions for rebalancing tasks across workers.

    Analyzes current load distribution and suggests task moves
    from overloaded workers to underutilized ones.

    Returns:
        - suggestions: List of suggested task moves
            - task_id: Task to move
            - from_worker: Current worker
            - to_worker: Suggested target
            - reason: Why this move is suggested
        - imbalance_score: Current imbalance (0-100)
    """
    from services.load_balancer import get_load_balancer

    try:
        lb = get_load_balancer(DB_PATH)
        suggestions = lb.suggest_rebalance()
        distribution = lb.get_load_distribution()

        return jsonify(
            {
                "success": True,
                "imbalance_score": round(distribution.imbalance_score, 1),
                "suggestions": suggestions,
                "suggestion_count": len(suggestions),
            }
        )
    except Exception as e:
        logger.error(f"Failed to suggest rebalance: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/workers/rebalance/execute", methods=["POST"])
@require_auth
def execute_rebalance():
    """Execute a task rebalance move.

    Request body:
        - task_id: ID of task to move
        - to_worker: Target worker ID

    Only pending tasks can be rebalanced. Running tasks will not be moved.
    """
    from services.load_balancer import get_load_balancer

    data = request.get_json() or {}
    task_id = data.get("task_id")
    to_worker = data.get("to_worker")

    if not task_id or not to_worker:
        return (
            jsonify(
                {
                    "success": False,
                    "error": "task_id and to_worker are required",
                }
            ),
            400,
        )

    try:
        lb = get_load_balancer(DB_PATH)
        success = lb.execute_rebalance(task_id, to_worker)

        if success:
            log_activity(
                "task_rebalance", f"Moved task {task_id} to worker {to_worker}"
            )
            return jsonify(
                {
                    "success": True,
                    "message": f"Task {task_id} reassigned to {to_worker}",
                }
            )
        else:
            return (
                jsonify(
                    {
                        "success": False,
                        "error": "Failed to rebalance task (may already be running)",
                    }
                ),
                400,
            )
    except Exception as e:
        logger.error(f"Failed to execute rebalance: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/workers/load/strategies", methods=["GET"])
def get_load_balancing_strategies():
    """Get available load balancing strategies and their descriptions."""
    return jsonify(
        {
            "success": True,
            "strategies": [
                {
                    "name": "round_robin",
                    "description": "Simple rotation through available workers",
                },
                {
                    "name": "least_loaded",
                    "description": "Select worker with lowest current load percentage",
                },
                {
                    "name": "weighted",
                    "description": "Consider worker weights and effective capacity",
                },
                {
                    "name": "skill_based",
                    "description": "Match worker skills to task type for best fit",
                },
                {
                    "name": "least_connections",
                    "description": "Select worker with fewest active task connections",
                },
                {
                    "name": "adaptive",
                    "description": "Combine load, success rate, and skills dynamically",
                },
            ],
            "default": "least_loaded",
        }
    )


# ============================================================================
# SECURE VAULT API (Encrypted Secrets Storage)
# ============================================================================


def encrypt_secret(value: str) -> str:
    """Encrypt a secret value using app secret key."""
    import base64

    key = (
        app.secret_key.encode()
        if isinstance(app.secret_key, str)
        else app.secret_key
    )
    # Simple XOR encryption with key rotation
    key_bytes = key * (len(value) // len(key) + 1)
    encrypted = bytes([ord(c) ^ key_bytes[i] for i, c in enumerate(value)])
    return base64.b64encode(encrypted).decode()


def decrypt_secret(encrypted_value: str) -> str:
    """Decrypt a secret value using app secret key."""
    import base64

    key = (
        app.secret_key.encode()
        if isinstance(app.secret_key, str)
        else app.secret_key
    )
    encrypted = base64.b64decode(encrypted_value.encode())
    key_bytes = key * (len(encrypted) // len(key) + 1)
    decrypted = "".join(
        [chr(b ^ key_bytes[i]) for i, b in enumerate(encrypted)]
    )
    return decrypted


@app.route("/api/secrets", methods=["GET"])
@require_auth
def get_secrets():
    """Get all secrets (values are NOT returned for security)."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        secrets = conn.execute(
            """
            SELECT s.id, s.name, s.category, s.description, s.project_id,
                   s.expires_at, s.last_accessed, s.access_count,
                   s.created_by, s.created_at, s.updated_at,
                   p.name as project_name
            FROM secrets s
            LEFT JOIN projects p ON s.project_id = p.id
            ORDER BY s.category, s.name
        """
        ).fetchall()
        return jsonify([dict(s) for s in secrets])


@app.route("/api/secrets", methods=["POST"])
@require_auth
def create_secret():
    """Create a new secret."""
    data = request.get_json(silent=True) or {}

    name = data.get("name")
    value = data.get("value")

    if not name or not value:
        return jsonify({"error": "Name and value are required"}), 400

    encrypted_value = encrypt_secret(value)

    with get_db_connection() as conn:
        try:
            cursor = conn.execute(
                """
                INSERT INTO secrets (name, category, encrypted_value, description,
                                    project_id, expires_at, created_by)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    name,
                    data.get("category", "general"),
                    encrypted_value,
                    data.get("description"),
                    data.get("project_id"),
                    data.get("expires_at"),
                    session.get("username", "system"),
                ),
            )
            secret_id = cursor.lastrowid

            log_activity(
                "create", "secret", secret_id, json.dumps({"name": name})
            )

            return jsonify({"success": True, "id": secret_id})
        except sqlite3.IntegrityError:
            return (
                jsonify({"error": "A secret with this name already exists"}),
                409,
            )


@app.route("/api/secrets/<int:secret_id>", methods=["GET"])
@require_auth
def get_secret_value(secret_id):
    """Get a secret's decrypted value (logged for security)."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        secret = conn.execute(
            "SELECT * FROM secrets WHERE id = ?", (secret_id,)
        ).fetchone()

        if not secret:
            return jsonify({"error": "Secret not found"}), 404

        # Update access tracking
        conn.execute(
            """
            UPDATE secrets SET
                last_accessed = CURRENT_TIMESTAMP,
                access_count = access_count + 1
            WHERE id = ?
        """,
            (secret_id,),
        )

        # Log the access
        log_activity(
            "access",
            "secret",
            secret_id,
            json.dumps(
                {
                    "name": secret["name"],
                    "accessed_by": session.get("username", "unknown"),
                }
            ),
        )

        decrypted_value = decrypt_secret(secret["encrypted_value"])

        return jsonify(
            {
                "id": secret["id"],
                "name": secret["name"],
                "value": decrypted_value,
                "category": secret["category"],
            }
        )


@app.route("/api/secrets/<int:secret_id>", methods=["PUT"])
@require_auth
def update_secret(secret_id):
    """Update a secret."""
    data = request.get_json(silent=True) or {}

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        existing = conn.execute(
            "SELECT * FROM secrets WHERE id = ?", (secret_id,)
        ).fetchone()

        if not existing:
            return jsonify({"error": "Secret not found"}), 404

        # Build update query dynamically
        updates = ["updated_at = CURRENT_TIMESTAMP"]
        params = []

        if "name" in data:
            updates.append("name = ?")
            params.append(data["name"])

        if "value" in data:
            updates.append("encrypted_value = ?")
            params.append(encrypt_secret(data["value"]))

        if "category" in data:
            updates.append("category = ?")
            params.append(data["category"])

        if "description" in data:
            updates.append("description = ?")
            params.append(data["description"])

        if "project_id" in data:
            updates.append("project_id = ?")
            params.append(data["project_id"])

        if "expires_at" in data:
            updates.append("expires_at = ?")
            params.append(data["expires_at"])

        params.append(secret_id)

        try:
            conn.execute(
                f"UPDATE secrets SET {', '.join(updates)} WHERE id = ?", params
            )
            log_activity(
                "update",
                "secret",
                secret_id,
                json.dumps({"name": existing["name"]}),
            )

            return jsonify({"success": True})
        except sqlite3.IntegrityError:
            return (
                jsonify({"error": "A secret with this name already exists"}),
                409,
            )


@app.route("/api/secrets/<int:secret_id>", methods=["DELETE"])
@require_auth
def delete_secret(secret_id):
    """Delete a secret."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        existing = conn.execute(
            "SELECT name FROM secrets WHERE id = ?", (secret_id,)
        ).fetchone()

        if not existing:
            return jsonify({"error": "Secret not found"}), 404

        conn.execute("DELETE FROM secrets WHERE id = ?", (secret_id,))

        log_activity(
            "delete",
            "secret",
            secret_id,
            json.dumps({"name": existing["name"]}),
        )

        return jsonify({"success": True})


# ============================================================================
# NOTIFICATIONS API
# ============================================================================


@app.route("/api/notifications", methods=["GET"])
@require_auth
def get_notifications():
    """Get notification log with optional filters."""
    from db import DB_PATHS
    from services.notifications import get_notification_service

    service = get_notification_service(str(DB_PATHS["main"]))
    notifications = service.get_notification_log(
        limit=int(request.args.get("limit", 50)),
        offset=int(request.args.get("offset", 0)),
        severity=request.args.get("severity"),
        notification_type=request.args.get("type"),
        source=request.args.get("source"),
        acknowledged=(
            request.args.get("acknowledged") == "true"
            if request.args.get("acknowledged")
            else None
        ),
    )
    return jsonify(notifications)


@app.route(
    "/api/notifications/<int:notification_id>/acknowledge", methods=["POST"]
)
@require_auth
def acknowledge_notification(notification_id):
    """Mark a notification as acknowledged."""
    from db import DB_PATHS
    from services.notifications import get_notification_service

    service = get_notification_service(str(DB_PATHS["main"]))
    user = session.get("user", "unknown")
    success = service.acknowledge_notification(notification_id, user)

    if success:
        return jsonify({"success": True})
    return jsonify({"error": "Failed to acknowledge notification"}), 400


@app.route("/api/notifications/settings", methods=["GET"])
@require_auth
def get_notification_settings():
    """Get all notification settings."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        settings = conn.execute(
            """
            SELECT id, name, category, config, enabled, notify_on, created_at, updated_at
            FROM notification_settings
            ORDER BY category, name
        """
        ).fetchall()

        result = []
        for row in settings:
            setting = dict(row)
            try:
                setting["config"] = json.loads(setting["config"])
                # Hide sensitive data in webhook URLs
                if (
                    setting["category"] == "webhook"
                    and "url" in setting["config"]
                ):
                    url = setting["config"]["url"]
                    if len(url) > 40:
                        setting["config"]["url_preview"] = (
                            url[:20] + "..." + url[-15:]
                        )
            except json.JSONDecodeError:
                pass
            result.append(setting)

        return jsonify(result)


@app.route("/api/notifications/settings", methods=["POST"])
@require_auth
def create_notification_setting():
    """Create a new notification setting (webhook)."""
    from db import DB_PATHS
    from services.notifications import get_notification_service

    data = request.get_json()
    if not data.get("name") or not data.get("url"):
        return jsonify({"error": "Name and URL are required"}), 400

    service = get_notification_service(str(DB_PATHS["main"]))
    success = service.add_webhook(
        name=data["name"],
        url=data["url"],
        webhook_type=data.get("type", "generic"),
        notify_on=data.get("notify_on", "all"),
        headers=data.get("headers"),
        template=data.get("template"),
    )

    if success:
        log_activity("create", "notification_setting", None, data["name"])
        return jsonify({"success": True})
    return jsonify({"error": "Failed to create webhook"}), 400


@app.route("/api/notifications/settings/<int:setting_id>", methods=["PUT"])
@require_auth
def update_notification_setting(setting_id):
    """Update a notification setting."""
    data = request.get_json()

    with get_db_connection() as conn:
        # Build update query
        updates = []
        params = []

        if "enabled" in data:
            updates.append("enabled = ?")
            params.append(1 if data["enabled"] else 0)

        if "notify_on" in data:
            updates.append("notify_on = ?")
            params.append(data["notify_on"])

        if "config" in data:
            updates.append("config = ?")
            params.append(json.dumps(data["config"]))

        if not updates:
            return jsonify({"error": "No fields to update"}), 400

        updates.append("updated_at = CURRENT_TIMESTAMP")
        params.append(setting_id)

        conn.execute(
            f"UPDATE notification_settings SET {
                ', '.join(updates)} WHERE id = ?",
            params,
        )

        log_activity("update", "notification_setting", setting_id)
        return jsonify({"success": True})


@app.route("/api/notifications/settings/<int:setting_id>", methods=["DELETE"])
@require_auth
def delete_notification_setting(setting_id):
    """Delete a notification setting."""
    with get_db_connection() as conn:
        conn.execute(
            "DELETE FROM notification_settings WHERE id = ?", (setting_id,)
        )
        log_activity("delete", "notification_setting", setting_id)
        return jsonify({"success": True})


@app.route("/api/notifications/test", methods=["POST"])
@require_auth
def test_notification():
    """Send a test notification."""
    from db import DB_PATHS
    from services.notifications import get_notification_service

    data = request.get_json() or {}
    setting_name = data.get("setting")

    service = get_notification_service(str(DB_PATHS["main"]))

    if setting_name:
        # Test specific webhook
        result = service.test_webhook(setting_name)
    else:
        # Send test through all channels
        result = service.notify(
            notification_type="test",
            title="Test Notification",
            message="This is a test notification from Architect Dashboard",
            source="api",
            severity="info",
            metadata={
                "test": True,
                "triggered_by": session.get("user", "unknown"),
            },
        )

    return jsonify(result)


@app.route("/api/notifications/send", methods=["POST"])
@require_auth
def send_notification():
    """Manually send a notification."""
    from db import DB_PATHS
    from services.notifications import get_notification_service

    data = request.get_json()
    if not data.get("title"):
        return jsonify({"error": "Title is required"}), 400

    service = get_notification_service(str(DB_PATHS["main"]))
    result = service.notify(
        notification_type=data.get("type", "manual"),
        title=data["title"],
        message=data.get("message"),
        source=data.get("source", "manual"),
        severity=data.get("severity", "info"),
        metadata=data.get("metadata"),
    )

    log_activity("send", "notification", None, data["title"])
    return jsonify(result)


# ============================================================================
# TASK WEBHOOKS API
# ============================================================================


@app.route("/api/webhooks/events", methods=["GET"])
@require_auth
def get_webhook_events():
    """Get list of available task events for webhooks."""
    return jsonify(
        {
            "events": [
                {"name": name, "description": desc}
                for name, desc in task_webhooks.TASK_EVENTS.items()
            ]
        }
    )


@app.route("/api/webhooks", methods=["GET"])
@require_auth
def list_webhooks():
    """List all task webhooks."""
    enabled_only = request.args.get("enabled_only", "false").lower() == "true"
    with get_db_connection() as conn:
        webhooks = task_webhooks.get_webhooks(conn, enabled_only=enabled_only)
    return jsonify({"webhooks": webhooks, "count": len(webhooks)})


@app.route("/api/webhooks", methods=["POST"])
@require_auth
def create_webhook():
    """Create a new task webhook."""
    data = request.get_json() or {}
    if not data.get("name") or not data.get("url"):
        return jsonify({"error": "name and url are required"}), 400
    try:
        with get_db_connection() as conn:
            webhook_id = task_webhooks.create_webhook(
                conn,
                name=data["name"],
                url=data["url"],
                events=data.get("events"),
                task_types=data.get("task_types"),
                secret=data.get("secret"),
                enabled=data.get("enabled", True),
                retry_count=data.get("retry_count", 3),
                timeout_seconds=data.get("timeout_seconds", 10),
            )
            log_activity("create_webhook", "webhook", webhook_id, data["name"])
        return jsonify({"id": webhook_id, "success": True})
    except ValueError as e:
        return jsonify({"error": str(e)}), 400


@app.route("/api/webhooks/<int:webhook_id>", methods=["GET"])
@require_auth
def get_webhook(webhook_id):
    """Get a specific webhook."""
    with get_db_connection() as conn:
        webhook = task_webhooks.get_webhook(conn, webhook_id)
    if not webhook:
        return jsonify({"error": "Webhook not found"}), 404
    return jsonify(webhook)


@app.route("/api/webhooks/<int:webhook_id>", methods=["PUT"])
@require_auth
def update_webhook(webhook_id):
    """Update a webhook."""
    data = request.get_json() or {}
    try:
        with get_db_connection() as conn:
            updated = task_webhooks.update_webhook(conn, webhook_id, **data)
            if not updated:
                return (
                    jsonify({"error": "Webhook not found or no changes"}),
                    404,
                )
            log_activity("update_webhook", "webhook", webhook_id)
        return jsonify({"success": True})
    except ValueError as e:
        return jsonify({"error": str(e)}), 400


@app.route("/api/webhooks/<int:webhook_id>", methods=["DELETE"])
@require_auth
def delete_webhook(webhook_id):
    """Delete a webhook."""
    with get_db_connection() as conn:
        deleted = task_webhooks.delete_webhook(conn, webhook_id)
        if not deleted:
            return jsonify({"error": "Webhook not found"}), 404
        log_activity("delete_webhook", "webhook", webhook_id)
    return jsonify({"success": True})


@app.route("/api/webhooks/<int:webhook_id>/test", methods=["POST"])
@require_auth
def test_webhook_endpoint(webhook_id):
    """Send a test event to a webhook."""
    with get_db_connection() as conn:
        result = task_webhooks.test_webhook(conn, webhook_id)
    return jsonify(result)


@app.route("/api/webhooks/<int:webhook_id>/toggle", methods=["POST"])
@require_auth
def toggle_webhook(webhook_id):
    """Toggle a webhook enabled/disabled."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        current = conn.execute(
            "SELECT enabled FROM task_webhooks WHERE id=?", (webhook_id,)
        ).fetchone()
        if not current:
            return jsonify({"error": "Webhook not found"}), 404
        new_state = 0 if current["enabled"] else 1
        conn.execute(
            "UPDATE task_webhooks SET enabled=?, updated_at=CURRENT_TIMESTAMP WHERE id=?",
            (new_state, webhook_id),
        )
        log_activity(
            "toggle_webhook", "webhook", webhook_id, f"enabled={new_state}"
        )
    return jsonify({"success": True, "enabled": bool(new_state)})


@app.route("/api/webhooks/deliveries", methods=["GET"])
@require_auth
def get_webhook_deliveries():
    """Get webhook delivery history."""
    webhook_id = request.args.get("webhook_id", type=int)
    task_id = request.args.get("task_id", type=int)
    event = request.args.get("event")
    success = request.args.get("success")
    limit = min(int(request.args.get("limit", 50)), 200)
    offset = int(request.args.get("offset", 0))
    success_bool = None
    if success is not None:
        success_bool = success.lower() == "true"
    with get_db_connection() as conn:
        deliveries = task_webhooks.get_deliveries(
            conn,
            webhook_id=webhook_id,
            task_id=task_id,
            event=event,
            success=success_bool,
            limit=limit,
            offset=offset,
        )
    return jsonify({"deliveries": deliveries, "count": len(deliveries)})


@app.route("/api/webhooks/stats", methods=["GET"])
@require_auth
def get_webhook_stats():
    """Get webhook delivery statistics."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        total = conn.execute(
            "SELECT COUNT(*) as c FROM task_webhooks"
        ).fetchone()["c"]
        enabled = conn.execute(
            "SELECT COUNT(*) as c FROM task_webhooks WHERE enabled=1"
        ).fetchone()["c"]
        deliveries_24h = conn.execute(
            """
            SELECT COUNT(*) as total,
                   SUM(CASE WHEN success=1 THEN 1 ELSE 0 END) as successful,
                   AVG(duration_seconds) as avg_duration
            FROM webhook_deliveries
            WHERE created_at > datetime('now', '-24 hours')
        """
        ).fetchone()
        by_event = conn.execute(
            """
            SELECT event, COUNT(*) as count, SUM(CASE WHEN success=1 THEN 1 ELSE 0 END) as successful
            FROM webhook_deliveries
            WHERE created_at > datetime('now', '-24 hours')
            GROUP BY event ORDER BY count DESC
        """
        ).fetchall()
    return jsonify(
        {
            "webhooks": {"total": total, "enabled": enabled},
            "deliveries_24h": {
                "total": deliveries_24h["total"] or 0,
                "successful": deliveries_24h["successful"] or 0,
                "failed": (deliveries_24h["total"] or 0)
                - (deliveries_24h["successful"] or 0),
                "avg_duration_seconds": round(
                    deliveries_24h["avg_duration"] or 0, 3
                ),
            },
            "by_event": [dict(r) for r in by_event],
        }
    )


# ============================================================================
# USER ROLES API
# ============================================================================


@app.route("/api/users/roles", methods=["GET"])
@require_auth
def list_roles():
    """List all roles."""
    include_system = (
        request.args.get("include_system", "true").lower() == "true"
    )
    with get_db_connection() as conn:
        roles = user_roles.get_roles(conn, include_system=include_system)
    return jsonify({"roles": roles, "count": len(roles)})


@app.route("/api/users/roles/permissions", methods=["GET"])
@require_auth
def list_available_permissions():
    """List all available permissions."""
    permissions = []
    for resource, actions in user_roles.AVAILABLE_PERMISSIONS.items():
        for action in actions:
            permissions.append(
                {
                    "permission": f"{resource}.{action}",
                    "resource": resource,
                    "action": action,
                }
            )
        permissions.append(
            {
                "permission": f"{resource}.*",
                "resource": resource,
                "action": "*",
                "description": f"All {resource} permissions",
            }
        )
    return jsonify(
        {
            "permissions": permissions,
            "resources": list(user_roles.AVAILABLE_PERMISSIONS.keys()),
        }
    )


@app.route("/api/users/roles", methods=["POST"])
@require_auth
def create_role():
    """Create a new role."""
    data = request.get_json() or {}
    if not data.get("name"):
        return jsonify({"error": "name is required"}), 400
    try:
        with get_db_connection() as conn:
            role_id = user_roles.create_role(
                conn,
                name=data["name"],
                description=data.get("description"),
                permissions=data.get("permissions", []),
                created_by=session.get("user", "unknown"),
            )
            log_activity("create_role", "role", role_id, data["name"])
        return jsonify({"id": role_id, "success": True})
    except ValueError as e:
        return jsonify({"error": str(e)}), 400
    except sqlite3.IntegrityError:
        return jsonify({"error": "Role name already exists"}), 409


@app.route("/api/users/roles/<int:role_id>", methods=["GET"])
@require_auth
def get_role_detail(role_id):
    """Get a specific role."""
    with get_db_connection() as conn:
        role = user_roles.get_role(conn, role_id=role_id)
        if not role:
            return jsonify({"error": "Role not found"}), 404
        # Get users with this role
        role["users"] = user_roles.get_role_users(conn, role_id)
    return jsonify(role)


@app.route("/api/users/roles/<int:role_id>", methods=["PUT"])
@require_auth
def update_role_endpoint(role_id):
    """Update a role."""
    data = request.get_json() or {}
    try:
        with get_db_connection() as conn:
            updated = user_roles.update_role(
                conn,
                role_id,
                name=data.get("name"),
                description=data.get("description"),
                permissions=data.get("permissions"),
                updated_by=session.get("user", "unknown"),
            )
            if not updated:
                return jsonify({"error": "Role not found"}), 404
            log_activity("update_role", "role", role_id)
        return jsonify({"success": True})
    except ValueError as e:
        return jsonify({"error": str(e)}), 400


@app.route("/api/users/roles/<int:role_id>", methods=["DELETE"])
@require_auth
def delete_role_endpoint(role_id):
    """Delete a role."""
    try:
        with get_db_connection() as conn:
            deleted = user_roles.delete_role(
                conn, role_id, deleted_by=session.get("user", "unknown")
            )
            if not deleted:
                return jsonify({"error": "Role not found"}), 404
            log_activity("delete_role", "role", role_id)
        return jsonify({"success": True})
    except ValueError as e:
        return jsonify({"error": str(e)}), 400


@app.route("/api/users/<int:user_id>/roles", methods=["GET"])
@require_auth
def get_user_roles_endpoint(user_id):
    """Get roles assigned to a user."""
    with get_db_connection() as conn:
        roles = user_roles.get_user_roles(conn, user_id)
        permissions = user_roles.get_user_permissions(conn, user_id)
    return jsonify(
        {"roles": roles, "permissions": permissions, "user_id": user_id}
    )


@app.route("/api/users/<int:user_id>/roles", methods=["POST"])
@require_auth
def assign_role_endpoint(user_id):
    """Assign a role to a user."""
    data = request.get_json() or {}
    role_id = data.get("role_id")
    if not role_id:
        return jsonify({"error": "role_id is required"}), 400
    try:
        with get_db_connection() as conn:
            assigned = user_roles.assign_role(
                conn,
                user_id,
                role_id,
                assigned_by=session.get("user", "unknown"),
            )
            if not assigned:
                return jsonify(
                    {"success": True, "message": "Role already assigned"}
                )
            log_activity("assign_role", "user", user_id, f"role_id={role_id}")
        return jsonify({"success": True})
    except ValueError as e:
        return jsonify({"error": str(e)}), 400


@app.route("/api/users/<int:user_id>/roles/<int:role_id>", methods=["DELETE"])
@require_auth
def revoke_role_endpoint(user_id, role_id):
    """Revoke a role from a user."""
    with get_db_connection() as conn:
        revoked = user_roles.revoke_role(
            conn, user_id, role_id, revoked_by=session.get("user", "unknown")
        )
        if not revoked:
            return jsonify({"error": "Role not assigned to user"}), 404
        log_activity("revoke_role", "user", user_id, f"role_id={role_id}")
    return jsonify({"success": True})


@app.route("/api/users/<int:user_id>/permissions", methods=["GET"])
@require_auth
def get_user_permissions_endpoint(user_id):
    """Get all permissions for a user."""
    with get_db_connection() as conn:
        permissions = user_roles.get_user_permissions(conn, user_id)
    return jsonify({"permissions": permissions, "user_id": user_id})


@app.route("/api/users/<int:user_id>/permissions/check", methods=["POST"])
@require_auth
def check_user_permission(user_id):
    """Check if a user has a specific permission."""
    data = request.get_json() or {}
    permission = data.get("permission")
    if not permission:
        return jsonify({"error": "permission is required"}), 400
    with get_db_connection() as conn:
        has_perm = user_roles.has_permission(conn, user_id, permission)
    return jsonify(
        {
            "has_permission": has_perm,
            "permission": permission,
            "user_id": user_id,
        }
    )


@app.route("/api/users/roles/audit", methods=["GET"])
@require_auth
def get_roles_audit_log():
    """Get role audit log."""
    target_type = request.args.get("target_type")
    target_id = request.args.get("target_id", type=int)
    action = request.args.get("action")
    limit = min(int(request.args.get("limit", 50)), 200)
    offset = int(request.args.get("offset", 0))
    with get_db_connection() as conn:
        entries = user_roles.get_role_audit_log(
            conn,
            target_type=target_type,
            target_id=target_id,
            action=action,
            limit=limit,
            offset=offset,
        )
    return jsonify({"entries": entries, "count": len(entries)})


@app.route("/api/users/roles/default", methods=["GET"])
@require_auth
def get_default_roles():
    """Get the default role definitions."""
    return jsonify({"roles": user_roles.DEFAULT_ROLES})


# ============================================================================
# USER NOTIFICATIONS API (In-App Notifications)
# ============================================================================


@app.route("/api/notifications/user", methods=["GET"])
@require_auth
def get_user_notifications():
    """Get notifications for the current user.

    Query params:
        unread_only: Only return unread notifications (default false)
        category: Filter by category
        type: Filter by notification type (info, success, warning, error)
        limit: Max results (default 50)
        offset: Pagination offset (default 0)
    """
    user_id = session.get("user_id")
    unread_only = request.args.get("unread_only", "").lower() == "true"
    category = request.args.get("category")
    notif_type = request.args.get("type")
    limit = min(request.args.get("limit", 50, type=int), 100)
    offset = request.args.get("offset", 0, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        query = "SELECT * FROM user_notifications WHERE (user_id = ? OR user_id IS NULL)"
        params = [user_id]

        if unread_only:
            query += " AND is_read = 0"
        if category:
            query += " AND category = ?"
            params.append(category)
        if notif_type:
            query += " AND notification_type = ?"
            params.append(notif_type)

        # Exclude expired notifications
        query += " AND (expires_at IS NULL OR expires_at > CURRENT_TIMESTAMP)"
        query += " AND is_dismissed = 0"
        query += " ORDER BY created_at DESC LIMIT ? OFFSET ?"
        params.extend([limit, offset])

        notifications = conn.execute(query, params).fetchall()

        # Get unread count
        unread_count = conn.execute(
            "SELECT COUNT(*) as cnt FROM user_notifications WHERE (user_id = ? OR user_id IS NULL) AND is_read = 0 AND is_dismissed = 0 AND (expires_at IS NULL OR expires_at > CURRENT_TIMESTAMP)",
            (user_id,),
        ).fetchone()["cnt"]

        return jsonify(
            {
                "notifications": [dict(n) for n in notifications],
                "unread_count": unread_count,
                "total": len(notifications),
            }
        )


@app.route("/api/notifications/user", methods=["POST"])
@require_auth
def create_user_notification():
    """Create a notification for a user or all users.

    Request body:
        title: Notification title (required)
        message: Notification body text
        notification_type: info, success, warning, error (default: info)
        priority: low, normal, high, urgent (default: normal)
        category: Category for filtering (default: system)
        user_id: Target user ID (null for all users)
        link: Optional link URL
        link_text: Optional link text
        entity_type: Related entity type (project, feature, bug, etc.)
        entity_id: Related entity ID
        expires_at: Expiration timestamp
    """
    data = request.get_json() or {}

    if not data.get("title"):
        return jsonify({"error": "title is required"}), 400

    valid_types = ["info", "success", "warning", "error"]
    notif_type = data.get("notification_type", "info")
    if notif_type not in valid_types:
        return (
            jsonify(
                {"error": f"notification_type must be one of: {valid_types}"}
            ),
            400,
        )

    with get_db_connection() as conn:
        cursor = conn.execute(
            """
            INSERT INTO user_notifications
                (user_id, title, message, notification_type, priority, category, link, link_text, entity_type, entity_id, expires_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                data.get("user_id"),
                data["title"],
                data.get("message"),
                notif_type,
                data.get("priority", "normal"),
                data.get("category", "system"),
                data.get("link"),
                data.get("link_text"),
                data.get("entity_type"),
                data.get("entity_id"),
                data.get("expires_at"),
            ),
        )

        log_activity("create", "notification", cursor.lastrowid, data["title"])
        return jsonify({"success": True, "id": cursor.lastrowid})


@app.route(
    "/api/notifications/user/<int:notification_id>/read", methods=["PUT"]
)
@require_auth
def mark_notification_read(notification_id):
    """Mark a notification as read."""
    user_id = session.get("user_id")

    with get_db_connection() as conn:
        conn.execute(
            """
            UPDATE user_notifications SET is_read = 1, read_at = CURRENT_TIMESTAMP
            WHERE id = ? AND (user_id = ? OR user_id IS NULL)
        """,
            (notification_id, user_id),
        )

        if conn.total_changes > 0:
            return jsonify({"success": True})
        return jsonify({"error": "Notification not found"}), 404


@app.route("/api/notifications/user/<int:notification_id>", methods=["DELETE"])
@require_auth
def dismiss_user_notification(notification_id):
    """Dismiss (soft delete) a notification."""
    user_id = session.get("user_id")

    with get_db_connection() as conn:
        conn.execute(
            """
            UPDATE user_notifications SET is_dismissed = 1, dismissed_at = CURRENT_TIMESTAMP
            WHERE id = ? AND (user_id = ? OR user_id IS NULL)
        """,
            (notification_id, user_id),
        )

        if conn.total_changes > 0:
            return jsonify({"success": True})
        return jsonify({"error": "Notification not found"}), 404


@app.route("/api/notifications/user/read-all", methods=["POST"])
@require_auth
def mark_all_notifications_read():
    """Mark all notifications as read for the current user."""
    user_id = session.get("user_id")
    category = request.args.get("category")

    with get_db_connection() as conn:
        if category:
            conn.execute(
                """
                UPDATE user_notifications SET is_read = 1, read_at = CURRENT_TIMESTAMP
                WHERE (user_id = ? OR user_id IS NULL) AND is_read = 0 AND category = ?
            """,
                (user_id, category),
            )
        else:
            conn.execute(
                """
                UPDATE user_notifications SET is_read = 1, read_at = CURRENT_TIMESTAMP
                WHERE (user_id = ? OR user_id IS NULL) AND is_read = 0
            """,
                (user_id,),
            )

        return jsonify({"success": True, "updated": conn.total_changes})


@app.route("/api/notifications/user/unread-count", methods=["GET"])
@require_auth
def get_unread_notification_count():
    """Get count of unread notifications for the current user."""
    user_id = session.get("user_id")

    with get_db_connection() as conn:
        counts = conn.execute(
            """
            SELECT
                COUNT(*) as total,
                SUM(CASE WHEN priority = 'urgent' THEN 1 ELSE 0 END) as urgent,
                SUM(CASE WHEN priority = 'high' THEN 1 ELSE 0 END) as high,
                SUM(CASE WHEN notification_type = 'error' THEN 1 ELSE 0 END) as errors,
                SUM(CASE WHEN notification_type = 'warning' THEN 1 ELSE 0 END) as warnings
            FROM user_notifications
            WHERE (user_id = ? OR user_id IS NULL)
                AND is_read = 0 AND is_dismissed = 0
                AND (expires_at IS NULL OR expires_at > CURRENT_TIMESTAMP)
        """,
            (user_id,),
        ).fetchone()

        return jsonify(
            {
                "total": counts[0] or 0,
                "urgent": counts[1] or 0,
                "high": counts[2] or 0,
                "errors": counts[3] or 0,
                "warnings": counts[4] or 0,
            }
        )


# Default notification preferences
DEFAULT_NOTIFICATION_PREFS = {
    "channels": {"in_app": True, "email": False, "webhook": False},
    "categories": {
        "system": {"enabled": True, "priority_threshold": "low"},
        "errors": {"enabled": True, "priority_threshold": "low"},
        "tasks": {"enabled": True, "priority_threshold": "normal"},
        "deployments": {"enabled": True, "priority_threshold": "low"},
        "milestones": {"enabled": True, "priority_threshold": "normal"},
        "features": {"enabled": True, "priority_threshold": "normal"},
        "bugs": {"enabled": True, "priority_threshold": "normal"},
        "security": {"enabled": True, "priority_threshold": "low"},
        "workers": {"enabled": False, "priority_threshold": "high"},
    },
    "quiet_hours": {
        "enabled": False,
        "start": "22:00",
        "end": "08:00",
        "timezone": "UTC",
    },
    "digest": {"enabled": False, "frequency": "daily", "time": "09:00"},
    "sounds": {"enabled": True, "urgent_only": False},
}


@app.route("/api/notifications/preferences", methods=["GET"])
@require_auth
def get_notification_preferences():
    """Get notification preferences for the current user."""
    user_id = session.get("user_id") or session.get("user", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        row = conn.execute(
            "SELECT preference_value, updated_at FROM user_preferences WHERE user_id = ? AND preference_key = 'notification_preferences' AND category = 'notifications'",
            (user_id,),
        ).fetchone()
        if row:
            try:
                prefs = json.loads(row["preference_value"])
                merged = {
                    k: (
                        {
                            **DEFAULT_NOTIFICATION_PREFS.get(k, {}),
                            **prefs.get(k, {}),
                        }
                        if isinstance(DEFAULT_NOTIFICATION_PREFS.get(k), dict)
                        else prefs.get(k, DEFAULT_NOTIFICATION_PREFS.get(k))
                    )
                    for k in DEFAULT_NOTIFICATION_PREFS
                }
                return jsonify(
                    {
                        "preferences": merged,
                        "updated_at": row["updated_at"],
                        "is_default": False,
                    }
                )
            except json.JSONDecodeError:
                pass
        return jsonify(
            {
                "preferences": DEFAULT_NOTIFICATION_PREFS,
                "updated_at": None,
                "is_default": True,
            }
        )


@app.route("/api/notifications/preferences", methods=["PUT"])
@require_auth
def update_notification_preferences():
    """Update notification preferences for the current user."""
    user_id = session.get("user_id") or session.get("user", "anonymous")
    data = request.get_json() or {}
    if not data:
        return jsonify({"error": "No preferences provided"}), 400

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        row = conn.execute(
            "SELECT preference_value FROM user_preferences WHERE user_id = ? AND preference_key = 'notification_preferences' AND category = 'notifications'",
            (user_id,),
        ).fetchone()
        current = json.loads(row["preference_value"]) if row else {}
        merged = {
            k: (
                {**DEFAULT_NOTIFICATION_PREFS.get(k, {}), **current.get(k, {})}
                if isinstance(DEFAULT_NOTIFICATION_PREFS.get(k), dict)
                else current.get(k, DEFAULT_NOTIFICATION_PREFS.get(k))
            )
            for k in DEFAULT_NOTIFICATION_PREFS
        }
        for key in data:
            if key in merged:
                merged[key] = (
                    {**merged[key], **data[key]}
                    if isinstance(merged[key], dict)
                    and isinstance(data[key], dict)
                    else data[key]
                )
        conn.execute(
            "INSERT INTO user_preferences (user_id, preference_key, preference_value, category, updated_at) VALUES (?, 'notification_preferences', ?, 'notifications', CURRENT_TIMESTAMP) ON CONFLICT(user_id, preference_key) DO UPDATE SET preference_value = excluded.preference_value, updated_at = CURRENT_TIMESTAMP",
            (user_id, json.dumps(merged)),
        )
        log_activity(
            "update",
            "notification_preferences",
            None,
            "Updated notification preferences",
        )
        return jsonify({"success": True, "preferences": merged})


@app.route("/api/notifications/preferences/reset", methods=["POST"])
@require_auth
def reset_notification_preferences():
    """Reset notification preferences to defaults."""
    user_id = session.get("user_id") or session.get("user", "anonymous")
    with get_db_connection() as conn:
        conn.execute(
            "DELETE FROM user_preferences WHERE user_id = ? AND preference_key = 'notification_preferences' AND category = 'notifications'",
            (user_id,),
        )
        log_activity(
            "reset", "notification_preferences", None, "Reset to defaults"
        )
        return jsonify(
            {"success": True, "preferences": DEFAULT_NOTIFICATION_PREFS}
        )


@app.route("/api/notifications/preferences/categories", methods=["GET"])
@require_auth
def get_notification_categories():
    """Get available notification categories with descriptions."""
    categories = {
        "system": {
            "name": "System",
            "description": "System-wide announcements and maintenance notices",
        },
        "errors": {
            "name": "Errors",
            "description": "Error alerts and critical issues",
        },
        "tasks": {
            "name": "Tasks",
            "description": "Task queue updates and completions",
        },
        "deployments": {
            "name": "Deployments",
            "description": "Deployment status and results",
        },
        "milestones": {
            "name": "Milestones",
            "description": "Milestone progress and deadlines",
        },
        "features": {
            "name": "Features",
            "description": "Feature status changes",
        },
        "bugs": {"name": "Bugs", "description": "Bug reports and resolutions"},
        "security": {
            "name": "Security",
            "description": "Security alerts and audit events",
        },
        "workers": {
            "name": "Workers",
            "description": "Worker status and health alerts",
        },
        "nodes": {
            "name": "Nodes",
            "description": "Cluster node status changes",
        },
        "autopilot": {
            "name": "Autopilot",
            "description": "Autopilot run results",
        },
    }
    return jsonify(
        {
            "categories": categories,
            "priorities": ["low", "normal", "high", "urgent"],
        }
    )


@app.route("/api/notifications/preferences/channel/<channel>", methods=["PUT"])
@require_auth
def toggle_notification_channel(channel):
    """Quick toggle for a notification channel (in_app, email, webhook)."""
    if channel not in ["in_app", "email", "webhook"]:
        return jsonify({"error": "Invalid channel"}), 400
    data = request.get_json() or {}
    enabled = data.get("enabled", True)
    user_id = session.get("user_id") or session.get("user", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        row = conn.execute(
            "SELECT preference_value FROM user_preferences WHERE user_id = ? AND preference_key = 'notification_preferences' AND category = 'notifications'",
            (user_id,),
        ).fetchone()
        prefs = json.loads(row["preference_value"]) if row else {}
        if "channels" not in prefs:
            prefs["channels"] = {**DEFAULT_NOTIFICATION_PREFS["channels"]}
        prefs["channels"][channel] = bool(enabled)
        conn.execute(
            "INSERT INTO user_preferences (user_id, preference_key, preference_value, category, updated_at) VALUES (?, 'notification_preferences', ?, 'notifications', CURRENT_TIMESTAMP) ON CONFLICT(user_id, preference_key) DO UPDATE SET preference_value = excluded.preference_value, updated_at = CURRENT_TIMESTAMP",
            (user_id, json.dumps(prefs)),
        )
        return jsonify(
            {"success": True, "channel": channel, "enabled": bool(enabled)}
        )


@app.route(
    "/api/notifications/preferences/category/<category>", methods=["PUT"]
)
@require_auth
def update_category_preference(category):
    """Update preferences for a specific notification category."""
    data = request.get_json() or {}
    if not data:
        return jsonify({"error": "No settings provided"}), 400
    user_id = session.get("user_id") or session.get("user", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        row = conn.execute(
            "SELECT preference_value FROM user_preferences WHERE user_id = ? AND preference_key = 'notification_preferences' AND category = 'notifications'",
            (user_id,),
        ).fetchone()
        prefs = json.loads(row["preference_value"]) if row else {}
        if "categories" not in prefs:
            prefs["categories"] = {**DEFAULT_NOTIFICATION_PREFS["categories"]}
        if category not in prefs["categories"]:
            prefs["categories"][category] = {
                "enabled": True,
                "priority_threshold": "normal",
            }
        if "enabled" in data:
            prefs["categories"][category]["enabled"] = bool(data["enabled"])
        if "priority_threshold" in data:
            if data["priority_threshold"] not in [
                "low",
                "normal",
                "high",
                "urgent",
            ]:
                return jsonify({"error": "Invalid priority_threshold"}), 400
            prefs["categories"][category]["priority_threshold"] = data[
                "priority_threshold"
            ]
        conn.execute(
            "INSERT INTO user_preferences (user_id, preference_key, preference_value, category, updated_at) VALUES (?, 'notification_preferences', ?, 'notifications', CURRENT_TIMESTAMP) ON CONFLICT(user_id, preference_key) DO UPDATE SET preference_value = excluded.preference_value, updated_at = CURRENT_TIMESTAMP",
            (user_id, json.dumps(prefs)),
        )
        return jsonify(
            {
                "success": True,
                "category": category,
                "settings": prefs["categories"][category],
            }
        )


# ============================================================================
# DASHBOARD THEME PREFERENCES API
# ============================================================================

# Default theme preferences
DEFAULT_THEME_PREFS = {
    "mode": "system",  # 'light', 'dark', 'system'
    "primary_color": "#3B82F6",  # Blue
    "accent_color": "#10B981",  # Emerald
    "sidebar_style": "default",  # 'default', 'compact', 'expanded'
    "font_size": "medium",  # 'small', 'medium', 'large'
    "density": "comfortable",  # 'compact', 'comfortable', 'spacious'
    "animations": True,
    "blur_effects": True,
    "high_contrast": False,
    "reduce_motion": False,
    "custom_css": "",
}

# Available theme presets
THEME_PRESETS = {
    "default": {
        "name": "Default",
        "description": "Clean and modern default theme",
        "mode": "system",
        "primary_color": "#3B82F6",
        "accent_color": "#10B981",
    },
    "dark_professional": {
        "name": "Dark Professional",
        "description": "Sleek dark theme for reduced eye strain",
        "mode": "dark",
        "primary_color": "#6366F1",
        "accent_color": "#8B5CF6",
    },
    "light_minimal": {
        "name": "Light Minimal",
        "description": "Clean light theme with minimal distractions",
        "mode": "light",
        "primary_color": "#0EA5E9",
        "accent_color": "#14B8A6",
    },
    "high_contrast": {
        "name": "High Contrast",
        "description": "Maximum readability with high contrast colors",
        "mode": "dark",
        "primary_color": "#FBBF24",
        "accent_color": "#F97316",
        "high_contrast": True,
    },
    "nature": {
        "name": "Nature",
        "description": "Calming green tones inspired by nature",
        "mode": "light",
        "primary_color": "#059669",
        "accent_color": "#84CC16",
    },
    "sunset": {
        "name": "Sunset",
        "description": "Warm orange and pink tones",
        "mode": "dark",
        "primary_color": "#F97316",
        "accent_color": "#EC4899",
    },
}


@app.route("/api/dashboard/theme", methods=["GET"])
@require_auth
def get_theme_preferences():
    """Get theme preferences for the current user."""
    user_id = session.get("user_id") or session.get("user", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        row = conn.execute(
            """
            SELECT preference_value, updated_at FROM user_preferences
            WHERE user_id = ? AND preference_key = 'theme_preferences' AND category = 'theme'
        """,
            (user_id,),
        ).fetchone()

        if row:
            try:
                user_prefs = json.loads(row["preference_value"])
                merged = {**DEFAULT_THEME_PREFS, **user_prefs}
                return jsonify(
                    {
                        "preferences": merged,
                        "updated_at": row["updated_at"],
                        "is_default": False,
                    }
                )
            except json.JSONDecodeError:
                pass

        return jsonify(
            {
                "preferences": DEFAULT_THEME_PREFS,
                "updated_at": None,
                "is_default": True,
            }
        )


@app.route("/api/dashboard/theme", methods=["PUT"])
@require_auth
def update_theme_preferences():
    """Update theme preferences for the current user."""
    data = request.get_json() or {}

    if not data:
        return api_error("No preferences provided", 400, "missing_field")

    # Validate mode
    if "mode" in data and data["mode"] not in ["light", "dark", "system"]:
        return api_error(
            "Invalid mode. Must be 'light', 'dark', or 'system'",
            400,
            "invalid_value",
        )

    # Validate sidebar_style
    if "sidebar_style" in data and data["sidebar_style"] not in [
        "default",
        "compact",
        "expanded",
    ]:
        return api_error("Invalid sidebar_style", 400, "invalid_value")

    # Validate font_size
    if "font_size" in data and data["font_size"] not in [
        "small",
        "medium",
        "large",
    ]:
        return api_error("Invalid font_size", 400, "invalid_value")

    # Validate density
    if "density" in data and data["density"] not in [
        "compact",
        "comfortable",
        "spacious",
    ]:
        return api_error("Invalid density", 400, "invalid_value")

    # Validate colors (basic hex check)
    for color_key in ["primary_color", "accent_color"]:
        if color_key in data:
            color = data[color_key]
            if (
                not isinstance(color, str)
                or not color.startswith("#")
                or len(color) not in [4, 7]
            ):
                return api_error(
                    f"Invalid {color_key}. Must be hex color",
                    400,
                    "invalid_value",
                )

    # Validate custom_css length
    if "custom_css" in data and len(data.get("custom_css", "")) > 10240:
        return api_error(
            "custom_css exceeds maximum length of 10KB", 400, "limit_exceeded"
        )

    user_id = session.get("user_id") or session.get("user", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        row = conn.execute(
            """
            SELECT preference_value FROM user_preferences
            WHERE user_id = ? AND preference_key = 'theme_preferences' AND category = 'theme'
        """,
            (user_id,),
        ).fetchone()
        existing = json.loads(row["preference_value"]) if row else {}
        merged = {**DEFAULT_THEME_PREFS, **existing, **data}

        conn.execute(
            """
            INSERT INTO user_preferences (user_id, preference_key, preference_value, category, updated_at)
            VALUES (?, 'theme_preferences', ?, 'theme', CURRENT_TIMESTAMP)
            ON CONFLICT(user_id, preference_key) DO UPDATE SET preference_value = excluded.preference_value, updated_at = CURRENT_TIMESTAMP
        """,
            (user_id, json.dumps(merged)),
        )

        log_activity(
            conn,
            "update",
            "theme_preferences",
            None,
            "Updated theme preferences",
        )
        return jsonify({"success": True, "preferences": merged})


@app.route("/api/dashboard/theme/reset", methods=["POST"])
@require_auth
def reset_theme_preferences():
    """Reset theme preferences to defaults."""
    user_id = session.get("user_id") or session.get("user", "anonymous")

    with get_db_connection() as conn:
        conn.execute(
            """
            DELETE FROM user_preferences
            WHERE user_id = ? AND preference_key = 'theme_preferences' AND category = 'theme'
        """,
            (user_id,),
        )
        log_activity(
            conn, "reset", "theme_preferences", None, "Reset theme to defaults"
        )
        return jsonify({"success": True, "preferences": DEFAULT_THEME_PREFS})


@app.route("/api/dashboard/theme/presets", methods=["GET"])
@require_auth
def get_theme_presets():
    """Get available theme presets."""
    return jsonify({"presets": THEME_PRESETS, "default_preset": "default"})


@app.route("/api/dashboard/theme/presets/<preset_name>", methods=["POST"])
@require_auth
def apply_theme_preset(preset_name):
    """Apply a theme preset to the current user's preferences."""
    if preset_name not in THEME_PRESETS:
        return api_error("Preset not found", 404, "not_found")

    preset = THEME_PRESETS[preset_name]
    user_id = session.get("user_id") or session.get("user", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        row = conn.execute(
            """
            SELECT preference_value FROM user_preferences
            WHERE user_id = ? AND preference_key = 'theme_preferences' AND category = 'theme'
        """,
            (user_id,),
        ).fetchone()
        existing = json.loads(row["preference_value"]) if row else {}

        preset_values = {
            k: v for k, v in preset.items() if k not in ["name", "description"]
        }
        merged = {**DEFAULT_THEME_PREFS, **existing, **preset_values}

        conn.execute(
            """
            INSERT INTO user_preferences (user_id, preference_key, preference_value, category, updated_at)
            VALUES (?, 'theme_preferences', ?, 'theme', CURRENT_TIMESTAMP)
            ON CONFLICT(user_id, preference_key) DO UPDATE SET preference_value = excluded.preference_value, updated_at = CURRENT_TIMESTAMP
        """,
            (user_id, json.dumps(merged)),
        )

        log_activity(
            conn,
            "apply_preset",
            "theme_preferences",
            None,
            f"Applied theme preset: {preset_name}",
        )
        return jsonify(
            {
                "success": True,
                "preset_applied": preset_name,
                "preferences": merged,
            }
        )


@app.route("/api/dashboard/theme/export", methods=["GET"])
@require_auth
def export_theme_preferences():
    """Export theme preferences as a downloadable JSON file."""
    user_id = session.get("user_id") or session.get("user", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        row = conn.execute(
            """
            SELECT preference_value FROM user_preferences
            WHERE user_id = ? AND preference_key = 'theme_preferences' AND category = 'theme'
        """,
            (user_id,),
        ).fetchone()
        prefs = (
            json.loads(row["preference_value"]) if row else DEFAULT_THEME_PREFS
        )

        export_data = {
            "version": "1.0",
            "exported_at": datetime.now().isoformat(),
            "preferences": prefs,
        }
        return (
            json.dumps(export_data, indent=2),
            200,
            {
                "Content-Type": "application/json",
                "Content-Disposition": 'attachment; filename="theme_preferences.json"',
            },
        )


@app.route("/api/dashboard/theme/import", methods=["POST"])
@require_auth
def import_theme_preferences():
    """Import theme preferences from a JSON object."""
    data = request.get_json() or {}
    preferences = data.get("preferences")
    merge = data.get("merge", True)

    if not preferences or not isinstance(preferences, dict):
        return api_error(
            "preferences object is required", 400, "missing_field"
        )

    valid_keys = set(DEFAULT_THEME_PREFS.keys())
    filtered_prefs = {k: v for k, v in preferences.items() if k in valid_keys}

    if not filtered_prefs:
        return api_error(
            "No valid preference keys found", 400, "invalid_value"
        )

    user_id = session.get("user_id") or session.get("user", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        if merge:
            row = conn.execute(
                """
                SELECT preference_value FROM user_preferences
                WHERE user_id = ? AND preference_key = 'theme_preferences' AND category = 'theme'
            """,
                (user_id,),
            ).fetchone()
            existing = json.loads(row["preference_value"]) if row else {}
            merged = {**DEFAULT_THEME_PREFS, **existing, **filtered_prefs}
        else:
            merged = {**DEFAULT_THEME_PREFS, **filtered_prefs}

        conn.execute(
            """
            INSERT INTO user_preferences (user_id, preference_key, preference_value, category, updated_at)
            VALUES (?, 'theme_preferences', ?, 'theme', CURRENT_TIMESTAMP)
            ON CONFLICT(user_id, preference_key) DO UPDATE SET preference_value = excluded.preference_value, updated_at = CURRENT_TIMESTAMP
        """,
            (user_id, json.dumps(merged)),
        )

        log_activity(
            conn,
            "import",
            "theme_preferences",
            None,
            "Imported theme preferences",
        )
        return jsonify(
            {
                "success": True,
                "preferences": merged,
                "imported_keys": list(filtered_prefs.keys()),
            }
        )


# ============================================================================
# DASHBOARD WIDGET ORDER API
# ============================================================================

DEFAULT_WIDGET_ORDER = [
    "stats",
    "queue",
    "milestones",
    "features",
    "bugs",
    "errors",
    "nodes",
    "workers",
    "activity",
]


@app.route("/api/dashboard/widgets/order", methods=["GET"])
@require_auth
def get_widget_order():
    """Get user's dashboard widget order."""
    user_id = session.get("user_id") or session.get("user", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        row = conn.execute(
            "SELECT preference_value FROM user_preferences WHERE user_id = ? AND preference_key = 'widget_order' AND category = 'dashboard'",
            (user_id,),
        ).fetchone()
        if row:
            order = json.loads(row["preference_value"])
            return jsonify(
                {
                    "order": order.get("order", DEFAULT_WIDGET_ORDER),
                    "hidden": order.get("hidden", []),
                    "is_default": False,
                }
            )
        return jsonify(
            {"order": DEFAULT_WIDGET_ORDER, "hidden": [], "is_default": True}
        )


@app.route("/api/dashboard/widgets/order", methods=["PUT"])
@require_auth
def update_widget_order():
    """Update user's dashboard widget order."""
    data = request.get_json() or {}
    order = data.get("order")
    hidden = data.get("hidden", [])
    if not order or not isinstance(order, list):
        return api_error("order array is required", 400, "missing_field")
    user_id = session.get("user_id") or session.get("user", "anonymous")
    with get_db_connection() as conn:
        conn.execute(
            """
            INSERT INTO user_preferences (user_id, preference_key, preference_value, category, updated_at)
            VALUES (?, 'widget_order', ?, 'dashboard', CURRENT_TIMESTAMP)
            ON CONFLICT(user_id, preference_key) DO UPDATE SET preference_value = excluded.preference_value, updated_at = CURRENT_TIMESTAMP
        """,
            (user_id, json.dumps({"order": order, "hidden": hidden})),
        )
        log_activity(
            conn,
            "update",
            "widget_order",
            None,
            f"Updated widget order: {len(order)} widgets",
        )
        return jsonify({"success": True, "order": order, "hidden": hidden})


@app.route("/api/dashboard/widgets/order/reset", methods=["POST"])
@require_auth
def reset_widget_order():
    """Reset widget order to defaults."""
    user_id = session.get("user_id") or session.get("user", "anonymous")
    with get_db_connection() as conn:
        conn.execute(
            "DELETE FROM user_preferences WHERE user_id = ? AND preference_key = 'widget_order' AND category = 'dashboard'",
            (user_id,),
        )
        return jsonify(
            {"success": True, "order": DEFAULT_WIDGET_ORDER, "hidden": []}
        )


# ============================================================================
# DRAG-AND-DROP TASK PRIORITIZATION API
# ============================================================================


@app.route("/api/tasks/reorder", methods=["POST"])
@require_auth
def reorder_tasks():
    """Reorder tasks via drag-and-drop. Updates priorities based on new positions."""
    data = request.get_json() or {}
    task_ids = data.get("task_ids", [])
    base_priority = data.get("base_priority", 100)
    if not task_ids:
        return api_error("task_ids array is required", 400, "missing_field")
    with get_db_connection() as conn:
        for idx, task_id in enumerate(task_ids):
            new_priority = base_priority - idx
            conn.execute(
                "UPDATE task_queue SET priority = ? WHERE id = ?",
                (new_priority, task_id),
            )
        log_activity(
            conn, "reorder", "tasks", None, f"Reordered {len(task_ids)} tasks"
        )
    broadcast_queue()
    return jsonify({"success": True, "reordered_count": len(task_ids)})


@app.route("/api/tasks/<int:task_id>/priority", methods=["PUT"])
@require_auth
def update_task_priority(task_id):
    """Update a single task's priority."""
    data = request.get_json() or {}
    priority = data.get("priority")
    if priority is None:
        return api_error("priority is required", 400, "missing_field")
    with get_db_connection() as conn:
        result = conn.execute(
            "UPDATE task_queue SET priority = ? WHERE id = ?",
            (priority, task_id),
        )
        if result.rowcount == 0:
            return api_error("Task not found", 404, "not_found")
        log_activity(
            conn,
            "update_priority",
            "task",
            task_id,
            f"Set priority to {priority}",
        )
    broadcast_queue()
    return jsonify({"success": True, "task_id": task_id, "priority": priority})


@app.route("/api/tasks/bulk-priority", methods=["POST"])
@require_auth
def bulk_update_task_priorities():
    """Update priorities for multiple tasks at once."""
    data = request.get_json() or {}
    updates = data.get("updates", [])
    if not updates:
        return api_error("updates array is required", 400, "missing_field")
    with get_db_connection() as conn:
        for upd in updates:
            if "task_id" in upd and "priority" in upd:
                conn.execute(
                    "UPDATE task_queue SET priority = ? WHERE id = ?",
                    (upd["priority"], upd["task_id"]),
                )
        log_activity(
            conn,
            "bulk_priority",
            "tasks",
            None,
            f"Updated priorities for {len(updates)} tasks",
        )
    broadcast_queue()
    return jsonify({"success": True, "updated_count": len(updates)})


# ============================================================================
# PROJECT TEMPLATES API
# ============================================================================

DEFAULT_PROJECT_TEMPLATES = {
    "web_app": {
        "name": "Web Application",
        "description": "Standard web application project",
        "default_milestones": ["Setup", "MVP", "Beta", "Launch"],
        "default_features": ["Authentication", "Dashboard", "API"],
    },
    "mobile_app": {
        "name": "Mobile Application",
        "description": "iOS/Android mobile app",
        "default_milestones": [
            "Design",
            "Prototype",
            "Alpha",
            "Beta",
            "Release",
        ],
        "default_features": ["Onboarding", "Home Screen", "Settings"],
    },
    "api_service": {
        "name": "API Service",
        "description": "Backend API service",
        "default_milestones": [
            "Design",
            "Implementation",
            "Testing",
            "Deployment",
        ],
        "default_features": [
            "Authentication",
            "Rate Limiting",
            "Documentation",
        ],
    },
    "data_pipeline": {
        "name": "Data Pipeline",
        "description": "ETL/Data processing pipeline",
        "default_milestones": [
            "Design",
            "Extraction",
            "Transformation",
            "Loading",
            "Monitoring",
        ],
        "default_features": ["Data Ingestion", "Processing", "Output"],
    },
    "blank": {
        "name": "Blank Project",
        "description": "Empty project template",
        "default_milestones": [],
        "default_features": [],
    },
}


@app.route("/api/projects/templates", methods=["GET"])
@require_auth
def list_project_starter_templates():
    """Get available project templates (built-in and custom)."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        custom = conn.execute(
            "SELECT id, name, description, template_data, created_at FROM project_templates WHERE is_active = 1 ORDER BY name"
        ).fetchall()
        custom_templates = {
            f"custom_{t['id']}": {
                "id": t["id"],
                "name": t["name"],
                "description": t["description"],
                "is_custom": True,
                **json.loads(t["template_data"] or "{}"),
            }
            for t in custom
        }
    return jsonify(
        {"built_in": DEFAULT_PROJECT_TEMPLATES, "custom": custom_templates}
    )


@app.route("/api/projects/templates", methods=["POST"])
@require_auth
def create_project_starter_template():
    """Create a custom project template."""
    data = request.get_json() or {}
    name = data.get("name")
    if not name:
        return api_error("name is required", 400, "missing_field")
    template_data = {
        "default_milestones": data.get("default_milestones", []),
        "default_features": data.get("default_features", []),
        "default_structure": data.get("default_structure", {}),
        "settings": data.get("settings", {}),
    }
    with get_db_connection() as conn:
        cursor = conn.execute(
            "INSERT INTO project_templates (name, description, template_data, created_by) VALUES (?, ?, ?, ?)",
            (
                name,
                data.get("description", ""),
                json.dumps(template_data),
                session.get("user_id"),
            ),
        )
        log_activity(
            conn,
            "create",
            "project_template",
            cursor.lastrowid,
            f"Created template: {name}",
        )
        return jsonify({"success": True, "template_id": cursor.lastrowid})


@app.route("/api/projects/templates/<int:template_id>", methods=["PUT"])
@require_auth
def update_project_starter_template(template_id):
    """Update a custom project template."""
    data = request.get_json() or {}
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        existing = conn.execute(
            "SELECT template_data FROM project_templates WHERE id = ?",
            (template_id,),
        ).fetchone()
        if not existing:
            return api_error("Template not found", 404, "not_found")
        current = json.loads(existing["template_data"] or "{}")
        if "default_milestones" in data:
            current["default_milestones"] = data["default_milestones"]
        if "default_features" in data:
            current["default_features"] = data["default_features"]
        if "settings" in data:
            current["settings"] = data["settings"]
        updates = []
        params = []
        if "name" in data:
            updates.append("name = ?")
            params.append(data["name"])
        if "description" in data:
            updates.append("description = ?")
            params.append(data["description"])
        updates.append("template_data = ?")
        params.append(json.dumps(current))
        params.append(template_id)
        conn.execute(
            f"UPDATE project_templates SET {
                ', '.join(updates)}, updated_at = CURRENT_TIMESTAMP WHERE id = ?",
            params,
        )
        return jsonify({"success": True, "template_id": template_id})


@app.route("/api/projects/templates/<int:template_id>", methods=["DELETE"])
@require_auth
def delete_project_starter_template(template_id):
    """Delete a custom project template."""
    with get_db_connection() as conn:
        conn.execute(
            "UPDATE project_templates SET is_active = 0 WHERE id = ?",
            (template_id,),
        )
        return jsonify({"success": True})


@app.route("/api/projects/from-starter-template", methods=["POST"])
@require_auth
def create_project_from_starter_template():
    """Create a new project from a starter template."""
    data = request.get_json() or {}
    template_key = data.get("template")
    project_name = data.get("name")
    if not project_name:
        return api_error("name is required", 400, "missing_field")

    template = None
    if template_key in DEFAULT_PROJECT_TEMPLATES:
        template = DEFAULT_PROJECT_TEMPLATES[template_key]
    elif template_key and template_key.startswith("custom_"):
        tid = int(template_key.replace("custom_", ""))
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            row = conn.execute(
                "SELECT template_data FROM project_templates WHERE id = ?",
                (tid,),
            ).fetchone()
            if row:
                template = json.loads(row["template_data"] or "{}")

    with get_db_connection() as conn:
        cursor = conn.execute(
            "INSERT INTO projects (name, description, source_path, status) VALUES (?, ?, ?, 'active')",
            (
                project_name,
                data.get(
                    "description",
                    template.get("description", "") if template else "",
                ),
                data.get("source_path"),
            ),
        )
        project_id = cursor.lastrowid

        if template:
            for idx, ms_name in enumerate(
                template.get("default_milestones", [])
            ):
                conn.execute(
                    "INSERT INTO milestones (project_id, name, status) VALUES (?, ?, 'pending')",
                    (project_id, ms_name),
                )
            for feat_name in template.get("default_features", []):
                conn.execute(
                    "INSERT INTO features (project_id, name, status) VALUES (?, ?, 'planned')",
                    (project_id, feat_name),
                )

        log_activity(
            conn,
            "create_from_template",
            "project",
            project_id,
            f"Created from template: {template_key}",
        )
        return jsonify(
            {
                "success": True,
                "project_id": project_id,
                "template_used": template_key,
            }
        )


# ============================================================================
# TASK TIME ESTIMATES WITH ROLLUP API
# ============================================================================


@app.route("/api/tasks/<int:task_id>/estimate/rollup", methods=["GET"])
@require_auth
def get_task_estimate_with_rollup(task_id):
    """Get time estimate for a task including rollup from subtasks."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        task = conn.execute(
            "SELECT id, task_type, task_data, status FROM task_queue WHERE id = ?",
            (task_id,),
        ).fetchone()
        if not task:
            return api_error("Task not found", 404, "not_found")
        task_data = json.loads(task["task_data"] or "{}")
        estimate = task_data.get("estimate_hours", 0)
        actual = task_data.get("actual_hours", 0)

        deps = conn.execute(
            "SELECT task_id FROM task_dependencies WHERE depends_on_id = ?",
            (task_id,),
        ).fetchall()
        subtask_estimate = 0
        subtask_actual = 0
        for dep in deps:
            sub = conn.execute(
                "SELECT task_data FROM task_queue WHERE id = ?",
                (dep["task_id"],),
            ).fetchone()
            if sub:
                sub_data = json.loads(sub["task_data"] or "{}")
                subtask_estimate += sub_data.get("estimate_hours", 0)
                subtask_actual += sub_data.get("actual_hours", 0)

        return jsonify(
            {
                "task_id": task_id,
                "estimate_hours": estimate,
                "actual_hours": actual,
                "subtask_estimate_hours": subtask_estimate,
                "subtask_actual_hours": subtask_actual,
                "total_estimate_hours": estimate + subtask_estimate,
                "total_actual_hours": actual + subtask_actual,
                "variance_hours": (estimate + subtask_estimate)
                - (actual + subtask_actual),
            }
        )


@app.route("/api/tasks/<int:task_id>/hours", methods=["PUT"])
@require_auth
def update_task_hours(task_id):
    """Update time tracking hours in task_data JSON."""
    data = request.get_json() or {}
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        task = conn.execute(
            "SELECT task_data FROM task_queue WHERE id = ?", (task_id,)
        ).fetchone()
        if not task:
            return api_error("Task not found", 404, "not_found")
        task_data = json.loads(task["task_data"] or "{}")
        if "estimate_hours" in data:
            task_data["estimate_hours"] = float(data["estimate_hours"])
        if "actual_hours" in data:
            task_data["actual_hours"] = float(data["actual_hours"])
        if "estimate_unit" in data:
            task_data["estimate_unit"] = data["estimate_unit"]
        conn.execute(
            "UPDATE task_queue SET task_data = ? WHERE id = ?",
            (json.dumps(task_data), task_id),
        )
        log_activity(
            conn,
            "update_hours",
            "task",
            task_id,
            f"Est: {
                task_data.get('estimate_hours')}h, Act: {
                task_data.get('actual_hours')}h",
        )
        return jsonify(
            {
                "success": True,
                "task_id": task_id,
                "estimate_hours": task_data.get("estimate_hours"),
                "actual_hours": task_data.get("actual_hours"),
            }
        )


@app.route("/api/tasks/estimates/rollup", methods=["GET"])
@require_auth
def get_estimates_rollup():
    """Get aggregated time estimates for all tasks or by filter."""
    status = request.args.get("status")
    task_type = request.args.get("task_type")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        q, p = (
            "SELECT task_data, status, task_type FROM task_queue WHERE 1=1",
            [],
        )
        if status:
            q += " AND status = ?"
            p.append(status)
        if task_type:
            q += " AND task_type = ?"
            p.append(task_type)
        tasks = conn.execute(q, p).fetchall()

        total_estimate = 0
        total_actual = 0
        by_status = {}
        by_type = {}

        for t in tasks:
            td = json.loads(t["task_data"] or "{}")
            est = td.get("estimate_hours", 0)
            act = td.get("actual_hours", 0)
            total_estimate += est
            total_actual += act

            s = t["status"]
            if s not in by_status:
                by_status[s] = {"estimate": 0, "actual": 0, "count": 0}
            by_status[s]["estimate"] += est
            by_status[s]["actual"] += act
            by_status[s]["count"] += 1

            tt = t["task_type"]
            if tt not in by_type:
                by_type[tt] = {"estimate": 0, "actual": 0, "count": 0}
            by_type[tt]["estimate"] += est
            by_type[tt]["actual"] += act
            by_type[tt]["count"] += 1

        return jsonify(
            {
                "total_estimate_hours": total_estimate,
                "total_actual_hours": total_actual,
                "variance_hours": total_estimate - total_actual,
                "task_count": len(tasks),
                "by_status": by_status,
                "by_type": by_type,
            }
        )


@app.route("/api/milestones/<int:milestone_id>/estimate", methods=["GET"])
@require_auth
def get_milestone_estimate(milestone_id):
    """Get rolled-up time estimates for a milestone."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        milestone = conn.execute(
            "SELECT id, name FROM milestones WHERE id = ?", (milestone_id,)
        ).fetchone()
        if not milestone:
            return api_error("Milestone not found", 404, "not_found")

        features = conn.execute(
            "SELECT id FROM features WHERE milestone_id = ?", (milestone_id,)
        ).fetchall()
        total_est, total_act, feature_count = 0, 0, len(features)

        for f in features:
            tasks = conn.execute(
                "SELECT task_data FROM task_queue WHERE task_data LIKE ?",
                (f'%"feature_id": {f["id"]}%',),
            ).fetchall()
            for t in tasks:
                td = json.loads(t["task_data"] or "{}")
                total_est += td.get("estimate_hours", 0)
                total_act += td.get("actual_hours", 0)

        return jsonify(
            {
                "milestone_id": milestone_id,
                "milestone_name": milestone["name"],
                "total_estimate_hours": total_est,
                "total_actual_hours": total_act,
                "variance_hours": total_est - total_act,
                "feature_count": feature_count,
            }
        )


# ============================================================================
# TASK DUE DATE REMINDERS API
# ============================================================================


@app.route("/api/tasks/due", methods=["GET"])
@require_auth
def get_tasks_by_due_date():
    """Get tasks filtered by due date status.

    Query params:
        filter: 'overdue', 'today', 'tomorrow', 'this_week', 'upcoming', 'all' (default 'upcoming')
        status: Filter by task status (comma-separated)
        project_id: Filter by project
        assigned_to: Filter by assigned worker
        limit: Max results (default 50)
    """
    date_filter = request.args.get("filter", "upcoming")
    status_filter = request.args.get("status", "")
    project_id = request.args.get("project_id", type=int)
    assigned_to = request.args.get("assigned_to")
    limit = min(request.args.get("limit", 50, type=int), 200)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        conditions = ["status NOT IN ('completed', 'failed', 'cancelled')"]
        params = []

        if date_filter == "overdue":
            conditions.append(
                "due_date < DATE('now') AND due_date IS NOT NULL"
            )
        elif date_filter == "today":
            conditions.append("DATE(due_date) = DATE('now')")
        elif date_filter == "tomorrow":
            conditions.append("DATE(due_date) = DATE('now', '+1 day')")
        elif date_filter == "this_week":
            conditions.append(
                "due_date BETWEEN DATE('now') AND DATE('now', '+7 days')"
            )
        elif date_filter == "upcoming":
            conditions.append(
                "due_date >= DATE('now') AND due_date IS NOT NULL"
            )
        elif date_filter != "all":
            conditions.append("due_date IS NOT NULL")

        if status_filter:
            statuses = [s.strip() for s in status_filter.split(",")]
            conditions.append(f"status IN ({','.join('?' * len(statuses))})")
            params.extend(statuses)

        if project_id:
            conditions.append("task_data LIKE ?")
            params.append(f'%"project_id": {project_id}%')

        if assigned_to:
            conditions.append("assigned_worker = ?")
            params.append(assigned_to)

        params.append(limit)

        tasks = conn.execute(
            """
            SELECT id, task_type, task_data, priority, status, due_date,
                   assigned_worker, created_at, started_at,
                   CASE
                       WHEN due_date IS NULL THEN NULL
                       WHEN due_date < DATE('now') THEN 'overdue'
                       WHEN DATE(due_date) = DATE('now') THEN 'due_today'
                       WHEN DATE(due_date) = DATE('now', '+1 day') THEN 'due_tomorrow'
                       WHEN due_date <= DATE('now', '+7 days') THEN 'due_this_week'
                       ELSE 'upcoming'
                   END as due_status,
                   CAST(julianday(due_date) - julianday('now') AS INTEGER) as days_until_due
            FROM task_queue
            WHERE {' AND '.join(conditions)}
            ORDER BY CASE WHEN due_date IS NULL THEN 1 ELSE 0 END, due_date ASC, priority DESC
            LIMIT ?
        """,
            params,
        ).fetchall()

        result = []
        for task in tasks:
            task_dict = dict(task)
            task_dict["task_data"] = json.loads(task["task_data"] or "{}")
            result.append(task_dict)

        summary = conn.execute(
            """
            SELECT
                COUNT(CASE WHEN due_date < DATE('now') AND due_date IS NOT NULL THEN 1 END) as overdue,
                COUNT(CASE WHEN DATE(due_date) = DATE('now') THEN 1 END) as due_today,
                COUNT(CASE WHEN DATE(due_date) = DATE('now', '+1 day') THEN 1 END) as due_tomorrow,
                COUNT(CASE WHEN due_date BETWEEN DATE('now') AND DATE('now', '+7 days') THEN 1 END) as due_this_week
            FROM task_queue WHERE status NOT IN ('completed', 'failed', 'cancelled')
        """
        ).fetchone()

        return jsonify(
            {
                "tasks": result,
                "filter": date_filter,
                "count": len(result),
                "summary": dict(summary),
            }
        )


@app.route("/api/tasks/<int:task_id>/due-date", methods=["GET"])
@require_auth
def get_task_due_date(task_id):
    """Get due date information for a specific task."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        task = conn.execute(
            """
            SELECT id, task_type, due_date, status, priority,
                   CASE WHEN due_date IS NULL THEN NULL
                        WHEN due_date < DATE('now') THEN 'overdue'
                        WHEN DATE(due_date) = DATE('now') THEN 'due_today'
                        ELSE 'upcoming' END as due_status,
                   CAST(julianday(due_date) - julianday('now') AS INTEGER) as days_until_due,
                   CAST((julianday(due_date) - julianday('now')) * 24 AS INTEGER) as hours_until_due
            FROM task_queue WHERE id = ?
        """,
            (task_id,),
        ).fetchone()

        if not task:
            return api_error("Task not found", 404, "not_found")
        return jsonify(dict(task))


@app.route("/api/tasks/<int:task_id>/due-date", methods=["PUT"])
@require_auth
def set_task_due_date(task_id):
    """Set or update the due date for a task."""
    data = request.get_json() or {}
    due_date = data.get("due_date")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        task = conn.execute(
            "SELECT id FROM task_queue WHERE id = ?", (task_id,)
        ).fetchone()
        if not task:
            return api_error("Task not found", 404, "not_found")

        if due_date:
            try:
                parsed = datetime.strptime(due_date, "%Y-%m-%d")
                due_date = parsed.strftime("%Y-%m-%d")
            except ValueError:
                try:
                    parsed = datetime.fromisoformat(
                        due_date.replace("Z", "+00:00")
                    )
                    due_date = parsed.strftime("%Y-%m-%d %H:%M:%S")
                except ValueError:
                    return api_error(
                        "Invalid date format. Use YYYY-MM-DD.",
                        400,
                        "invalid_date",
                    )

        conn.execute(
            "UPDATE task_queue SET due_date = ? WHERE id = ?",
            (due_date, task_id),
        )
        user_id = session.get("user_id", session.get("user", "anonymous"))
        log_activity(
            conn,
            user_id,
            "set_task_due_date",
            "task",
            task_id,
            f"Due date: {due_date}" if due_date else "Removed due date",
        )

        return jsonify(
            {"success": True, "task_id": task_id, "due_date": due_date}
        )


@app.route("/api/tasks/bulk-due-date", methods=["PUT"])
@require_auth
def bulk_set_due_dates():
    """Set due dates for multiple tasks at once."""
    data = request.get_json() or {}
    task_ids = data.get("task_ids", [])
    due_date = data.get("due_date")

    if not task_ids:
        return api_error("task_ids is required", 400, "missing_field")

    if due_date:
        try:
            parsed = datetime.strptime(due_date, "%Y-%m-%d")
            due_date = parsed.strftime("%Y-%m-%d")
        except ValueError:
            return api_error(
                "Invalid date format. Use YYYY-MM-DD.", 400, "invalid_date"
            )

    with get_db_connection() as conn:
        placeholders = ",".join("?" * len(task_ids))
        conn.execute(
            f"UPDATE task_queue SET due_date = ? WHERE id IN ({placeholders})",
            [due_date] + task_ids,
        )
        user_id = session.get("user_id", session.get("user", "anonymous"))
        log_activity(
            conn,
            user_id,
            "bulk_set_due_dates",
            "task",
            None,
            f"Set due date for {len(task_ids)} tasks",
        )

        return jsonify(
            {
                "success": True,
                "updated_count": len(task_ids),
                "due_date": due_date,
            }
        )


@app.route("/api/tasks/reminders/settings", methods=["GET"])
@require_auth
def get_reminder_settings():
    """Get user's task reminder settings."""
    user_id = session.get("user_id", session.get("user", "anonymous"))

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        settings = conn.execute(
            "SELECT * FROM task_reminder_settings WHERE user_id = ?",
            (user_id,),
        ).fetchone()

        if settings:
            return jsonify({"settings": dict(settings)})

        return jsonify(
            {
                "settings": {
                    "user_id": user_id,
                    "reminder_enabled": True,
                    "remind_days_before": 1,
                    "remind_hours_before": 24,
                    "remind_on_due_day": True,
                    "remind_when_overdue": True,
                    "email_notifications": False,
                    "in_app_notifications": True,
                    "slack_notifications": False,
                }
            }
        )


@app.route("/api/tasks/reminders/settings", methods=["PUT"])
@require_auth
def update_reminder_settings():
    """Update user's task reminder settings."""
    user_id = session.get("user_id", session.get("user", "anonymous"))
    data = request.get_json() or {}

    with get_db_connection() as conn:
        conn.execute(
            """
            INSERT INTO task_reminder_settings
                (user_id, reminder_enabled, remind_days_before, remind_hours_before,
                 remind_on_due_day, remind_when_overdue, email_notifications,
                 in_app_notifications, slack_notifications, quiet_hours_start, quiet_hours_end)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(user_id) DO UPDATE SET
                reminder_enabled = excluded.reminder_enabled,
                remind_days_before = excluded.remind_days_before,
                remind_hours_before = excluded.remind_hours_before,
                remind_on_due_day = excluded.remind_on_due_day,
                remind_when_overdue = excluded.remind_when_overdue,
                email_notifications = excluded.email_notifications,
                in_app_notifications = excluded.in_app_notifications,
                slack_notifications = excluded.slack_notifications,
                quiet_hours_start = excluded.quiet_hours_start,
                quiet_hours_end = excluded.quiet_hours_end,
                updated_at = CURRENT_TIMESTAMP
        """,
            (
                user_id,
                data.get("reminder_enabled", True),
                data.get("remind_days_before", 1),
                data.get("remind_hours_before", 24),
                data.get("remind_on_due_day", True),
                data.get("remind_when_overdue", True),
                data.get("email_notifications", False),
                data.get("in_app_notifications", True),
                data.get("slack_notifications", False),
                data.get("quiet_hours_start"),
                data.get("quiet_hours_end"),
            ),
        )

        return jsonify(
            {"success": True, "message": "Reminder settings updated"}
        )


@app.route("/api/tasks/reminders/check", methods=["POST"])
@require_auth
def check_and_send_reminders():
    """Check for tasks needing reminders and create notifications."""
    user_id = session.get("user_id", session.get("user", "anonymous"))

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        settings = conn.execute(
            "SELECT * FROM task_reminder_settings WHERE user_id = ?",
            (user_id,),
        ).fetchone()
        if not settings or not settings["reminder_enabled"]:
            return jsonify({"message": "Reminders disabled", "sent": 0})

        remind_hours = settings["remind_hours_before"] if settings else 24
        reminders_sent = 0

        tasks = conn.execute(
            """
            SELECT t.id, t.task_type, t.task_data, t.due_date, t.priority,
                   CAST((julianday(t.due_date) - julianday('now')) * 24 AS REAL) as hours_until_due,
                   CASE WHEN t.due_date < datetime('now') THEN 'overdue'
                        WHEN t.due_date < datetime('now', '+' || ? || ' hours') THEN 'due_soon'
                        ELSE 'upcoming' END as reminder_type
            FROM task_queue t
            WHERE t.status NOT IN ('completed', 'failed', 'cancelled')
              AND t.due_date IS NOT NULL
              AND (t.due_date < datetime('now') OR t.due_date < datetime('now', '+' || ? || ' hours'))
              AND NOT EXISTS (SELECT 1 FROM task_reminders r WHERE r.task_id = t.id
                             AND r.user_id = ? AND r.sent_at > datetime('now', '-24 hours'))
            LIMIT 50
        """,
            (remind_hours, remind_hours, user_id),
        ).fetchall()

        for task in tasks:
            task_data = json.loads(task["task_data"] or "{}")
            task_name = (
                task_data.get("name")
                or task_data.get("title")
                or f"Task #{task['id']}"
            )
            hours_left = task["hours_until_due"]

            if task["reminder_type"] == "overdue":
                title, message = (
                    f"Task Overdue: {task_name}",
                    f"Task was due {abs(int(hours_left))} hours ago.",
                )
                notif_type, priority = "warning", "high"
            else:
                title, message = (
                    f"Task Due Soon: {task_name}",
                    f"Task is due in {int(hours_left)} hours.",
                )
                notif_type, priority = "info", "normal"

            if settings["in_app_notifications"]:
                conn.execute(
                    """
                    INSERT INTO user_notifications (user_id, title, message, notification_type, priority, category, entity_type, entity_id)
                    VALUES (?, ?, ?, ?, ?, 'task_reminder', 'task', ?)
                """,
                    (
                        user_id,
                        title,
                        message,
                        notif_type,
                        priority,
                        task["id"],
                    ),
                )

            conn.execute(
                "INSERT INTO task_reminders (task_id, user_id, reminder_type, reminder_method) VALUES (?, ?, ?, 'in_app')",
                (task["id"], user_id, task["reminder_type"]),
            )
            reminders_sent += 1

        return jsonify({"success": True, "sent": reminders_sent})


@app.route("/api/tasks/reminders/upcoming", methods=["GET"])
@require_auth
def get_upcoming_reminders():
    """Get list of upcoming task reminders for the user."""
    user_id = session.get("user_id", session.get("user", "anonymous"))
    hours_ahead = request.args.get("hours", 72, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        tasks = conn.execute(
            """
            SELECT t.id, t.task_type, t.task_data, t.due_date, t.priority, t.status,
                   CAST((julianday(t.due_date) - julianday('now')) * 24 AS REAL) as hours_until_due,
                   CASE WHEN t.due_date < datetime('now') THEN 'overdue'
                        WHEN DATE(t.due_date) = DATE('now') THEN 'due_today'
                        ELSE 'upcoming' END as due_status
            FROM task_queue t
            WHERE t.status NOT IN ('completed', 'failed', 'cancelled')
              AND t.due_date IS NOT NULL AND t.due_date <= datetime('now', '+' || ? || ' hours')
              AND (t.assigned_worker = ? OR t.assigned_worker IS NULL)
            ORDER BY t.due_date ASC
        """,
            (hours_ahead, user_id),
        ).fetchall()

        result = [
            dict(t)
            | {
                "task_data": json.loads(t["task_data"] or "{}"),
                "hours_until_due": round(t["hours_until_due"], 1),
            }
            for t in tasks
        ]
        return jsonify(
            {"tasks": result, "hours_ahead": hours_ahead, "count": len(result)}
        )


@app.route("/api/tasks/overdue/summary", methods=["GET"])
@require_auth
def get_overdue_summary():
    """Get summary of overdue tasks."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        summary = conn.execute(
            """
            SELECT COUNT(*) as total_overdue,
                   COUNT(CASE WHEN priority >= 8 THEN 1 END) as critical,
                   COUNT(CASE WHEN priority >= 5 AND priority < 8 THEN 1 END) as high,
                   COUNT(CASE WHEN priority < 5 THEN 1 END) as normal,
                   MIN(due_date) as oldest_due_date,
                   AVG(julianday('now') - julianday(due_date)) as avg_days_overdue
            FROM task_queue
            WHERE status NOT IN ('completed', 'failed', 'cancelled')
              AND due_date < DATE('now') AND due_date IS NOT NULL
        """
        ).fetchone()

        by_type = conn.execute(
            """
            SELECT task_type, COUNT(*) as count FROM task_queue
            WHERE status NOT IN ('completed', 'failed', 'cancelled')
              AND due_date < DATE('now') AND due_date IS NOT NULL
            GROUP BY task_type ORDER BY count DESC
        """
        ).fetchall()

        return jsonify(
            {
                "summary": {
                    "total": summary["total_overdue"],
                    "critical": summary["critical"],
                    "high": summary["high"],
                    "normal": summary["normal"],
                    "oldest_due_date": summary["oldest_due_date"],
                    "avg_days_overdue": round(
                        summary["avg_days_overdue"] or 0, 1
                    ),
                },
                "by_type": [dict(t) for t in by_type],
            }
        )


# ============================================================================
# BATCH TASK CREATION API
# ============================================================================


@app.route("/api/tasks/templates", methods=["GET"])
@require_auth
@api_error_handler
def list_task_templates():
    """List available task templates."""
    category = request.args.get("category")
    task_type = request.args.get("type")
    active_only = request.args.get("active_only", "true").lower() == "true"

    with get_db_connection() as conn:
        templates = batch_tasks.list_templates(
            conn, category, task_type, active_only
        )
        return jsonify({"templates": templates, "count": len(templates)})


@app.route("/api/tasks/templates/<int:template_id>", methods=["GET"])
@require_auth
@api_error_handler
def get_task_template(template_id):
    """Get a specific task template."""
    with get_db_connection() as conn:
        template = batch_tasks.get_template(conn, template_id)
        if not template:
            raise APIError("Template not found", 404)

        # Extract variables
        variables = batch_tasks.extract_variables(
            template["task_data_template"]
        )
        template["variables"] = variables

        return jsonify({"template": template})


@app.route("/api/tasks/templates", methods=["POST"])
@require_auth
@api_error_handler
def create_task_template():
    """Create a new task template."""
    data = request.get_json()
    validate_required_fields(data, ["name", "task_type", "task_data_template"])

    user_id = session.get("user_id")

    with get_db_connection() as conn:
        template_id = batch_tasks.create_template(
            conn,
            name=sanitize_string(data["name"]),
            task_type=data["task_type"],
            task_data_template=data["task_data_template"],
            description=data.get("description"),
            default_priority=data.get("default_priority", 0),
            default_max_retries=data.get("default_max_retries", 3),
            default_timeout_seconds=data.get("default_timeout_seconds"),
            category=data.get("category", "general"),
            icon=data.get("icon"),
            created_by=user_id,
        )
        conn.commit()

        log_activity(
            conn,
            user_id,
            "create_template",
            "task_template",
            template_id,
            {"name": data["name"], "task_type": data["task_type"]},
        )

        template = batch_tasks.get_template(conn, template_id)
        return (
            jsonify(
                {
                    "success": True,
                    "template": template,
                    "message": "Template created successfully",
                }
            ),
            201,
        )


@app.route("/api/tasks/templates/<int:template_id>", methods=["PUT"])
@require_auth
@api_error_handler
def update_task_template(template_id):
    """Update a task template."""
    data = request.get_json()
    user_id = session.get("user_id")

    with get_db_connection() as conn:
        success = batch_tasks.update_template(conn, template_id, **data)
        conn.commit()

        if success:
            log_activity(
                conn,
                user_id,
                "update_template",
                "task_template",
                template_id,
                data,
            )
            template = batch_tasks.get_template(conn, template_id)
            return jsonify(
                {
                    "success": True,
                    "template": template,
                    "message": "Template updated",
                }
            )
        else:
            return jsonify(
                {
                    "success": False,
                    "message": "No changes or template not found",
                }
            )


@app.route("/api/tasks/templates/<int:template_id>", methods=["DELETE"])
@require_auth
@api_error_handler
def delete_task_template(template_id):
    """Delete (deactivate) a task template."""
    user_id = session.get("user_id")
    hard_delete = request.args.get("hard", "false").lower() == "true"

    with get_db_connection() as conn:
        success = batch_tasks.delete_template(conn, template_id, hard_delete)
        conn.commit()

        if success:
            log_activity(
                conn,
                user_id,
                "delete_template",
                "task_template",
                template_id,
                {"hard_delete": hard_delete},
            )
            return jsonify(
                {
                    "success": True,
                    "message": (
                        "Template deleted"
                        if hard_delete
                        else "Template deactivated"
                    ),
                }
            )
        else:
            raise APIError("Template not found", 404)


@app.route("/api/tasks/templates/<int:template_id>/stats", methods=["GET"])
@require_auth
@api_error_handler
def get_template_stats(template_id):
    """Get usage statistics for a template."""
    with get_db_connection() as conn:
        stats = batch_tasks.get_template_stats(conn, template_id)
        if not stats:
            raise APIError("Template not found", 404)

        return jsonify({"success": True, "stats": stats})


@app.route("/api/tasks/templates/<int:template_id>/variables", methods=["GET"])
@require_auth
@api_error_handler
def get_template_variables(template_id):
    """Get variables required by a template."""
    with get_db_connection() as conn:
        template = batch_tasks.get_template(conn, template_id)
        if not template:
            raise APIError("Template not found", 404)

        variables = batch_tasks.extract_variables(
            template["task_data_template"]
        )
        return jsonify(
            {
                "template_id": template_id,
                "template_name": template["name"],
                "variables": variables,
                "count": len(variables),
            }
        )


@app.route("/api/tasks/batch", methods=["POST"])
@require_auth
@api_error_handler
def create_batch_tasks():
    """Create multiple tasks from a template.

    Request body:
        template_id: Template ID to use
        items: List of variable dicts for each task
        batch_name: Optional batch name
        batch_description: Optional description
        default_priority: Default priority for all tasks
        stagger_seconds: Seconds between task starts
    """
    data = request.get_json()
    validate_required_fields(data, ["template_id", "items"])

    if not isinstance(data["items"], list) or len(data["items"]) == 0:
        raise APIError("items must be a non-empty list", 400)

    user_id = session.get("user_id")
    username = session.get("username", "unknown")

    with get_db_connection() as conn:
        try:
            result = batch_tasks.create_batch_from_template(
                conn,
                template_id=data["template_id"],
                items=data["items"],
                batch_name=data.get("batch_name"),
                batch_description=data.get("batch_description"),
                default_priority=data.get("default_priority"),
                stagger_seconds=data.get("stagger_seconds", 0),
                created_by=username,
            )
            conn.commit()

            log_activity(
                conn,
                user_id,
                "create_batch",
                "task_batch",
                None,
                {
                    "batch_id": result["batch_id"],
                    "template_id": data["template_id"],
                    "count": result["created_count"],
                },
            )

            return (
                jsonify({"success": result["status"] != "failed", **result}),
                201,
            )

        except ValueError as e:
            raise APIError(str(e), 400)


@app.route("/api/tasks/batch/<batch_id>", methods=["GET"])
@require_auth
@api_error_handler
def get_batch_info(batch_id):
    """Get information about a task batch."""
    with get_db_connection() as conn:
        batch = batch_tasks.get_batch(conn, batch_id)
        if not batch:
            raise APIError("Batch not found", 404)

        return jsonify({"success": True, "batch": batch})


@app.route("/api/tasks/batch/<batch_id>/tasks", methods=["GET"])
@require_auth
@api_error_handler
def get_batch_tasks_list(batch_id):
    """Get all tasks in a batch."""
    with get_db_connection() as conn:
        tasks = batch_tasks.get_batch_tasks(conn, batch_id)
        return jsonify(
            {"batch_id": batch_id, "tasks": tasks, "count": len(tasks)}
        )


@app.route("/api/tasks/batch/<batch_id>/cancel", methods=["POST"])
@require_auth
@api_error_handler
def cancel_batch_tasks(batch_id):
    """Cancel all pending tasks in a batch."""
    user_id = session.get("user_id")

    with get_db_connection() as conn:
        result = batch_tasks.cancel_batch(conn, batch_id)
        conn.commit()

        log_activity(
            conn,
            user_id,
            "cancel_batch",
            "task_batch",
            None,
            {"batch_id": batch_id, "cancelled": result["cancelled_count"]},
        )

        return jsonify({"success": True, **result})


@app.route("/api/tasks/batch/<batch_id>/retry", methods=["POST"])
@require_auth
@api_error_handler
def retry_batch_tasks(batch_id):
    """Retry all failed tasks in a batch."""
    user_id = session.get("user_id")

    with get_db_connection() as conn:
        result = batch_tasks.retry_failed_batch_tasks(conn, batch_id)
        conn.commit()

        log_activity(
            conn,
            user_id,
            "retry_batch",
            "task_batch",
            None,
            {"batch_id": batch_id, "retried": result["retried_count"]},
        )

        return jsonify({"success": True, **result})


@app.route("/api/tasks/batches", methods=["GET"])
@require_auth
@api_error_handler
def list_task_batches():
    """List task batches."""
    status = request.args.get("status")
    template_id = request.args.get("template_id", type=int)
    limit = request.args.get("limit", 50, type=int)

    with get_db_connection() as conn:
        batches = batch_tasks.list_batches(
            conn, status, template_id, limit=limit
        )
        return jsonify({"batches": batches, "count": len(batches)})


@app.route("/api/tasks/from-template", methods=["POST"])
@require_auth
@api_error_handler
def create_single_task_from_template():
    """Create a single task from a template.

    Request body:
        template_id: Template ID to use
        variables: Variable values for substitution
        priority: Optional priority override
        max_retries: Optional max retries override
        timeout_seconds: Optional timeout override
    """
    data = request.get_json()
    validate_required_fields(data, ["template_id"])

    user_id = session.get("user_id")

    with get_db_connection() as conn:
        try:
            task = batch_tasks.create_task_from_template(
                conn,
                template_id=data["template_id"],
                variables=data.get("variables", {}),
                priority_override=data.get("priority"),
                max_retries_override=data.get("max_retries"),
                timeout_override=data.get("timeout_seconds"),
            )
            conn.commit()

            log_activity(
                conn,
                user_id,
                "create_task_from_template",
                "task",
                task["id"],
                {
                    "template_id": data["template_id"],
                    "template_name": task["template_name"],
                },
            )

            return (
                jsonify(
                    {
                        "success": True,
                        "task": task,
                        "message": "Task created from template",
                    }
                ),
                201,
            )

        except ValueError as e:
            raise APIError(str(e), 400)


# ============================================================================
# ACTIVITY TIMELINE API
# ============================================================================


@app.route("/api/activity/timeline", methods=["GET"])
@require_auth
@api_error_handler
def get_activity_timeline():
    """Get activity timeline with filtering and pagination.

    Query params:
        start_date: Filter from date (ISO format)
        end_date: Filter to date (ISO format)
        user_id: Filter by user
        entity_type: Filter by entity type
        entity_id: Filter by entity ID
        action: Filter by action (supports wildcards with *)
        category: Filter by action category
        limit: Maximum results (default 100)
        offset: Pagination offset
    """
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")
    user_id = request.args.get("user_id", type=int)
    entity_type = request.args.get("entity_type")
    entity_id = request.args.get("entity_id", type=int)
    action = request.args.get("action")
    category = request.args.get("category")
    limit = request.args.get("limit", 100, type=int)
    offset = request.args.get("offset", 0, type=int)

    # Parse dates
    start_dt = datetime.fromisoformat(start_date) if start_date else None
    end_dt = datetime.fromisoformat(end_date) if end_date else None

    with get_db_connection() as conn:
        result = activity_timeline.get_timeline(
            conn,
            start_date=start_dt,
            end_date=end_dt,
            user_id=user_id,
            entity_type=entity_type,
            entity_id=entity_id,
            action=action,
            action_category=category,
            limit=limit,
            offset=offset,
        )

        return jsonify(result)


@app.route("/api/activity/timeline/grouped", methods=["GET"])
@require_auth
@api_error_handler
def get_grouped_activity_timeline():
    """Get activity timeline grouped by time period.

    Query params:
        group_by: Grouping period (hour, day, week, month)
        start_date: Filter from date
        end_date: Filter to date
        user_id: Filter by user
        entity_type: Filter by entity type
        limit_per_group: Max activities per group
    """
    group_by = request.args.get("group_by", "day")
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")
    user_id = request.args.get("user_id", type=int)
    entity_type = request.args.get("entity_type")
    limit_per_group = request.args.get("limit_per_group", 10, type=int)

    start_dt = datetime.fromisoformat(start_date) if start_date else None
    end_dt = datetime.fromisoformat(end_date) if end_date else None

    with get_db_connection() as conn:
        result = activity_timeline.get_timeline_grouped(
            conn,
            group_by=group_by,
            start_date=start_dt,
            end_date=end_dt,
            user_id=user_id,
            entity_type=entity_type,
            limit_per_group=limit_per_group,
        )

        return jsonify(result)


@app.route("/api/activity/stats", methods=["GET"])
@require_auth
@api_error_handler
def get_activity_stats():
    """Get activity statistics.

    Query params:
        start_date: Filter from date
        end_date: Filter to date
        user_id: Filter by user
    """
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")
    user_id = request.args.get("user_id", type=int)

    start_dt = datetime.fromisoformat(start_date) if start_date else None
    end_dt = datetime.fromisoformat(end_date) if end_date else None

    with get_db_connection() as conn:
        stats = activity_timeline.get_activity_stats(
            conn, start_dt, end_dt, user_id
        )
        return jsonify({"success": True, "stats": stats})


@app.route("/api/activity/feed", methods=["GET"])
@require_auth
@api_error_handler
def get_activity_feed():
    """Get recent activity feed with enriched data.

    Query params:
        limit: Maximum results
        user_id: Filter by user
        exclude: Comma-separated actions to exclude
    """
    limit = request.args.get("limit", 50, type=int)
    user_id = request.args.get("user_id", type=int)
    exclude = request.args.get("exclude", "")
    exclude_actions = (
        [a.strip() for a in exclude.split(",")] if exclude else None
    )

    with get_db_connection() as conn:
        feed = activity_timeline.get_recent_activity_feed(
            conn, limit=limit, user_id=user_id, exclude_actions=exclude_actions
        )

        return jsonify({"feed": feed, "count": len(feed)})


@app.route(
    "/api/activity/entity/<entity_type>/<int:entity_id>", methods=["GET"]
)
@require_auth
@api_error_handler
def get_entity_activity_history(entity_type, entity_id):
    """Get activity history for a specific entity."""
    limit = request.args.get("limit", 50, type=int)

    with get_db_connection() as conn:
        result = activity_timeline.get_entity_activity(
            conn, entity_type, entity_id, limit
        )
        return jsonify(result)


@app.route("/api/activity/user/<int:user_id>", methods=["GET"])
@require_auth
@api_error_handler
def get_user_activity_history(user_id):
    """Get activity history for a specific user."""
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")
    limit = request.args.get("limit", 100, type=int)

    start_dt = datetime.fromisoformat(start_date) if start_date else None
    end_dt = datetime.fromisoformat(end_date) if end_date else None

    with get_db_connection() as conn:
        result = activity_timeline.get_user_activity(
            conn, user_id, start_dt, end_dt, limit
        )

        if result is None:
            raise APIError("User not found", 404)

        return jsonify(result)


@app.route("/api/activity/search", methods=["GET"])
@require_auth
@api_error_handler
def search_activities():
    """Search activity log.

    Query params:
        q: Search query
        start_date: Filter from date
        end_date: Filter to date
        limit: Maximum results
    """
    query = request.args.get("q", "")
    if not query or len(query) < 2:
        raise APIError("Search query must be at least 2 characters", 400)

    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")
    limit = request.args.get("limit", 50, type=int)

    start_dt = datetime.fromisoformat(start_date) if start_date else None
    end_dt = datetime.fromisoformat(end_date) if end_date else None

    with get_db_connection() as conn:
        results = activity_timeline.search_activity(
            conn, query, start_dt, end_dt, limit
        )
        return jsonify(
            {"query": query, "results": results, "count": len(results)}
        )


@app.route("/api/activity/export", methods=["GET"])
@require_auth
@api_error_handler
def export_activity_log():
    """Export activity log.

    Query params:
        start_date: Start date (required)
        end_date: End date (required)
        format: Export format (json or csv)
    """
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")
    export_format = request.args.get("format", "json")

    if not start_date or not end_date:
        raise APIError("start_date and end_date are required", 400)

    start_dt = datetime.fromisoformat(start_date)
    end_dt = datetime.fromisoformat(end_date)

    with get_db_connection() as conn:
        result = activity_timeline.export_activity(
            conn, start_dt, end_dt, export_format
        )

        if export_format == "csv":
            response = make_response(result)
            response.headers["Content-Type"] = "text/csv"
            response.headers["Content-Disposition"] = (
                f"attachment; filename=activity_{start_date}_{end_date}.csv"
            )
            return response

        return jsonify(result)


@app.route("/api/activity/categories", methods=["GET"])
@require_auth
@api_error_handler
def get_activity_categories():
    """Get available action categories."""
    return jsonify(
        {
            "categories": activity_timeline.ACTION_CATEGORIES,
            "entity_types": activity_timeline.ENTITY_DISPLAY_NAMES,
        }
    )


# ============================================================================
# TASK WORKLOG API
# ============================================================================


@app.route("/api/worklog", methods=["POST"])
@require_auth
@api_error_handler
def create_worklog_entry():
    """Create a new worklog entry for a task."""
    data = request.get_json() or {}
    task_id = data.get("task_id")
    if not task_id:
        return api_error("task_id is required", 400, "missing_field")

    time_spent = data.get("time_spent_minutes")
    if not time_spent or time_spent <= 0:
        return api_error(
            "time_spent_minutes must be positive", 400, "invalid_value"
        )

    user_id = session.get("user_id", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = task_worklog.create_worklog_entry(
            conn,
            task_id,
            user_id,
            description=data.get("description"),
            time_spent_minutes=time_spent,
            work_date=data.get("work_date"),
            work_type=data.get("work_type", "general"),
            billable=data.get("billable", True),
            metadata=data.get("metadata"),
        )
        if "error" in result:
            return api_error(result["error"], 400, "worklog_error")

        log_activity(
            conn,
            user_id,
            "create",
            "worklog",
            result["id"],
            {"task_id": task_id, "minutes": time_spent},
        )
        return jsonify(result), 201


@app.route("/api/worklog/<int:worklog_id>", methods=["GET"])
@require_auth
@api_error_handler
def get_worklog_entry(worklog_id):
    """Get a single worklog entry."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        entry = task_worklog.get_worklog_entry(conn, worklog_id)
        if not entry:
            return api_error("Worklog entry not found", 404, "not_found")
        return jsonify(entry)


@app.route("/api/worklog/<int:worklog_id>", methods=["PUT"])
@require_auth
@api_error_handler
def update_worklog_entry(worklog_id):
    """Update a worklog entry (only by creator)."""
    data = request.get_json() or {}
    user_id = session.get("user_id", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = task_worklog.update_worklog_entry(
            conn, worklog_id, user_id, data
        )
        if "error" in result:
            return api_error(result["error"], 400, "worklog_error")

        log_activity(conn, user_id, "update", "worklog", worklog_id, data)
        return jsonify(result)


@app.route("/api/worklog/<int:worklog_id>", methods=["DELETE"])
@require_auth
@api_error_handler
def delete_worklog_entry(worklog_id):
    """Delete a worklog entry (only by creator)."""
    user_id = session.get("user_id", "anonymous")
    force = request.args.get("force", "false").lower() == "true"

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = task_worklog.delete_worklog_entry(
            conn, worklog_id, user_id, force
        )
        if "error" in result:
            return api_error(result["error"], 400, "worklog_error")

        log_activity(conn, user_id, "delete", "worklog", worklog_id, {})
        return jsonify(result)


@app.route("/api/tasks/<int:task_id>/worklog", methods=["GET"])
@require_auth
@api_error_handler
def get_task_worklog(task_id):
    """Get all worklog entries for a task."""
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")
    user_id = request.args.get("user_id")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = task_worklog.get_task_worklog(
            conn, task_id, start_date, end_date, user_id
        )
        return jsonify(result)


@app.route("/api/users/<user_id>/worklog", methods=["GET"])
@require_auth
@api_error_handler
def get_user_worklog(user_id):
    """Get all worklog entries for a user."""
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")
    task_id = request.args.get("task_id")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = task_worklog.get_user_worklog(
            conn, user_id, start_date, end_date, task_id
        )
        return jsonify(result)


@app.route("/api/worklog/summary", methods=["GET"])
@require_auth
@api_error_handler
def get_worklog_summary():
    """Get worklog summary grouped by task, user, project, date, or work_type."""
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")
    group_by = request.args.get("group_by", "task")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = task_worklog.get_worklog_summary(
            conn, start_date, end_date, group_by
        )
        if "error" in result:
            return api_error(result["error"], 400, "invalid_group_by")
        return jsonify(result)


@app.route("/api/worklog/types", methods=["GET"])
@require_auth
@api_error_handler
def get_work_types():
    """Get available work types."""
    return jsonify({"work_types": task_worklog.get_work_types()})


# Timer endpoints for real-time tracking


@app.route("/api/timer/start", methods=["POST"])
@require_auth
@api_error_handler
def start_timer():
    """Start a timer for a task."""
    data = request.get_json() or {}
    task_id = data.get("task_id")
    if not task_id:
        return api_error("task_id is required", 400, "missing_field")

    user_id = session.get("user_id", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = task_worklog.start_timer(
            conn,
            task_id,
            user_id,
            description=data.get("description"),
            work_type=data.get("work_type", "general"),
        )
        if "error" in result:
            return api_error(result["error"], 400, "timer_error")

        log_activity(
            conn,
            user_id,
            "start",
            "timer",
            result["timer_id"],
            {"task_id": task_id},
        )
        return jsonify(result), 201


@app.route("/api/timer/stop", methods=["POST"])
@require_auth
@api_error_handler
def stop_timer():
    """Stop the active timer and create worklog entry."""
    data = request.get_json() or {}
    user_id = session.get("user_id", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = task_worklog.stop_timer(
            conn,
            user_id,
            billable=data.get("billable", True),
            description=data.get("description"),
        )
        if "error" in result:
            return api_error(result["error"], 400, "timer_error")

        log_activity(
            conn,
            user_id,
            "stop",
            "timer",
            result["timer_id"],
            {
                "worklog_id": result["worklog_id"],
                "duration_minutes": result["duration_minutes"],
            },
        )
        return jsonify(result)


@app.route("/api/timer/active", methods=["GET"])
@require_auth
@api_error_handler
def get_active_timer():
    """Get user's active timer if any."""
    user_id = session.get("user_id", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        timer = task_worklog.get_active_timer(conn, user_id)
        if not timer:
            return jsonify({"active": False})
        return jsonify({"active": True, "timer": timer})


@app.route("/api/timer/discard", methods=["POST"])
@require_auth
@api_error_handler
def discard_timer():
    """Discard active timer without creating worklog entry."""
    user_id = session.get("user_id", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = task_worklog.discard_timer(conn, user_id)
        if "error" in result:
            return api_error(result["error"], 400, "timer_error")

        log_activity(
            conn, user_id, "discard", "timer", result["discarded_timer_id"], {}
        )
        return jsonify(result)


# ============================================================================
# RESOURCE ALLOCATION API
# ============================================================================


@app.route("/api/reports/resource-allocation", methods=["GET"])
@require_auth
@api_error_handler
def get_resource_allocation_report():
    """Get overall resource allocation report."""
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")
    include_unassigned = (
        request.args.get("include_unassigned", "true").lower() == "true"
    )

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = resource_allocation.get_team_allocation(
            conn, start_date, end_date, include_unassigned
        )
        return jsonify(result)


@app.route("/api/reports/resource-allocation/user/<user_id>", methods=["GET"])
@require_auth
@api_error_handler
def get_user_resource_allocation(user_id):
    """Get resource allocation for a specific user."""
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = resource_allocation.get_user_allocation(
            conn, user_id, start_date, end_date
        )
        return jsonify(result)


@app.route(
    "/api/reports/resource-allocation/project/<int:project_id>",
    methods=["GET"],
)
@require_auth
@api_error_handler
def get_project_resource_allocation(project_id):
    """Get resource allocation for a specific project."""
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = resource_allocation.get_project_allocation(
            conn, project_id, start_date, end_date
        )
        if "error" in result:
            return api_error(result["error"], 404, "not_found")
        return jsonify(result)


@app.route("/api/reports/resource-allocation/capacity", methods=["GET"])
@require_auth
@api_error_handler
def get_capacity_report():
    """Get capacity vs allocation report."""
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")
    hours_per_day = request.args.get("hours_per_day", 8, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = resource_allocation.get_capacity_report(
            conn, start_date, end_date, hours_per_day
        )
        return jsonify(result)


@app.route("/api/reports/resource-allocation/forecast", methods=["GET"])
@require_auth
@api_error_handler
def get_workload_forecast():
    """Get workload forecast for upcoming weeks."""
    weeks = request.args.get("weeks", 4, type=int)
    hours_per_day = request.args.get("hours_per_day", 8, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = resource_allocation.get_workload_forecast(
            conn, weeks, hours_per_day
        )
        return jsonify(result)


@app.route("/api/reports/resource-allocation/conflicts", methods=["GET"])
@require_auth
@api_error_handler
def get_resource_conflicts():
    """Identify resource conflicts and overallocations."""
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = resource_allocation.get_resource_conflicts(
            conn, start_date, end_date
        )
        return jsonify(result)


@app.route("/api/reports/resource-allocation/by-skill", methods=["GET"])
@require_auth
@api_error_handler
def get_allocation_by_skill():
    """Get resource allocation grouped by task type/skill."""
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = resource_allocation.get_allocation_by_skill(
            conn, start_date, end_date
        )
        return jsonify(result)


@app.route("/api/reports/resource-allocation/suggestions", methods=["GET"])
@require_auth
@api_error_handler
def get_reallocation_suggestions():
    """Get suggestions for task reallocation to balance workload."""
    user_id = request.args.get("user_id")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = resource_allocation.suggest_reallocation(conn, user_id)
        return jsonify(result)


# ============================================================================
# PROJECT COST TRACKING API
# ============================================================================


@app.route("/api/projects/<int:project_id>/budget", methods=["GET"])
@require_auth
@api_error_handler
def get_project_budget(project_id):
    """Get project budget with spending summary."""
    fiscal_year = request.args.get("fiscal_year", type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = project_costs.get_project_budget(
            conn, project_id, fiscal_year
        )
        if not result:
            return api_error(
                "Budget not found for this project/year", 404, "not_found"
            )
        return jsonify(result)


@app.route("/api/projects/<int:project_id>/budget", methods=["POST", "PUT"])
@require_auth
@api_error_handler
def set_project_budget(project_id):
    """Create or update project budget."""
    data = request.get_json() or {}
    budget_amount = data.get("budget_amount")
    if budget_amount is None or budget_amount < 0:
        return api_error(
            "budget_amount is required and must be non-negative",
            400,
            "invalid_value",
        )

    user_id = session.get("user_id", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = project_costs.create_project_budget(
            conn,
            project_id,
            budget_amount,
            currency=data.get("currency", "USD"),
            fiscal_year=data.get("fiscal_year"),
            notes=data.get("notes"),
            created_by=user_id,
        )
        if "error" in result:
            return api_error(result["error"], 400, "budget_error")

        log_activity(
            conn,
            user_id,
            "set",
            "budget",
            result["id"],
            {"project_id": project_id, "amount": budget_amount},
        )
        return jsonify(result), 201


@app.route("/api/projects/<int:project_id>/costs", methods=["GET"])
@require_auth
@api_error_handler
def get_project_costs(project_id):
    """Get all cost entries for a project."""
    fiscal_year = request.args.get("fiscal_year", type=int)
    category = request.args.get("category")
    status = request.args.get("status")
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = project_costs.get_project_costs(
            conn,
            project_id,
            fiscal_year,
            category,
            status,
            start_date,
            end_date,
        )
        return jsonify(result)


@app.route("/api/projects/<int:project_id>/costs", methods=["POST"])
@require_auth
@api_error_handler
def add_project_cost(project_id):
    """Add a cost entry to a project."""
    data = request.get_json() or {}
    amount = data.get("amount")
    if amount is None or amount <= 0:
        return api_error(
            "amount is required and must be positive", 400, "invalid_value"
        )

    category = data.get("category")
    if not category:
        return api_error("category is required", 400, "missing_field")

    user_id = session.get("user_id", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = project_costs.add_cost_entry(
            conn,
            project_id,
            amount,
            category,
            description=data.get("description"),
            cost_date=data.get("cost_date"),
            status=data.get("status", "planned"),
            vendor=data.get("vendor"),
            invoice_ref=data.get("invoice_ref"),
            fiscal_year=data.get("fiscal_year"),
            metadata=data.get("metadata"),
            created_by=user_id,
        )
        if "error" in result:
            return api_error(result["error"], 400, "cost_error")

        log_activity(
            conn,
            user_id,
            "create",
            "cost",
            result["id"],
            {"project_id": project_id, "amount": amount, "category": category},
        )
        return jsonify(result), 201


@app.route("/api/costs/<int:cost_id>", methods=["GET"])
@require_auth
@api_error_handler
def get_cost_entry(cost_id):
    """Get a single cost entry."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        entry = project_costs.get_cost_entry(conn, cost_id)
        if not entry:
            return api_error("Cost entry not found", 404, "not_found")
        return jsonify(entry)


@app.route("/api/costs/<int:cost_id>", methods=["PUT"])
@require_auth
@api_error_handler
def update_cost_entry(cost_id):
    """Update a cost entry."""
    data = request.get_json() or {}
    user_id = session.get("user_id", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = project_costs.update_cost_entry(conn, cost_id, data)
        if "error" in result:
            return api_error(result["error"], 400, "cost_error")

        log_activity(conn, user_id, "update", "cost", cost_id, data)
        return jsonify(result)


@app.route("/api/costs/<int:cost_id>", methods=["DELETE"])
@require_auth
@api_error_handler
def delete_cost_entry(cost_id):
    """Delete a cost entry."""
    user_id = session.get("user_id", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = project_costs.delete_cost_entry(conn, cost_id)
        if "error" in result:
            return api_error(result["error"], 400, "cost_error")

        log_activity(conn, user_id, "delete", "cost", cost_id, {})
        return jsonify(result)


@app.route("/api/reports/costs/summary", methods=["GET"])
@require_auth
@api_error_handler
def get_cost_summary():
    """Get cost summary across all projects."""
    fiscal_year = request.args.get("fiscal_year", type=int)
    group_by = request.args.get("group_by", "project")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = project_costs.get_cost_summary(conn, fiscal_year, group_by)
        if "error" in result:
            return api_error(result["error"], 400, "invalid_group_by")
        return jsonify(result)


@app.route("/api/reports/costs/budget-vs-actual", methods=["GET"])
@require_auth
@api_error_handler
def get_budget_vs_actual():
    """Get budget vs actual spending comparison."""
    project_id = request.args.get("project_id", type=int)
    fiscal_year = request.args.get("fiscal_year", type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = project_costs.get_budget_vs_actual(
            conn, project_id, fiscal_year
        )
        return jsonify(result)


@app.route("/api/reports/costs/forecast/<int:project_id>", methods=["GET"])
@require_auth
@api_error_handler
def get_cost_forecast(project_id):
    """Get cost forecast for a project."""
    months = request.args.get("months", 3, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = project_costs.get_cost_forecast(conn, project_id, months)
        if "error" in result:
            return api_error(result["error"], 400, "forecast_error")
        return jsonify(result)


@app.route("/api/reports/costs/labor/<int:project_id>", methods=["GET"])
@require_auth
@api_error_handler
def get_labor_costs(project_id):
    """Calculate labor costs from worklog entries."""
    hourly_rate = request.args.get("hourly_rate", type=float)
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = project_costs.get_labor_costs_from_worklog(
            conn, project_id, hourly_rate, start_date, end_date
        )
        return jsonify(result)


@app.route("/api/costs/categories", methods=["GET"])
@require_auth
@api_error_handler
def get_cost_categories():
    """Get available cost categories."""
    return jsonify({"categories": project_costs.get_categories()})


@app.route("/api/costs/statuses", methods=["GET"])
@require_auth
@api_error_handler
def get_cost_statuses():
    """Get available cost statuses."""
    return jsonify({"statuses": project_costs.get_statuses()})


# ============================================================================
# NOTIFICATION RULES API
# ============================================================================


@app.route("/api/notifications/rules", methods=["GET"])
@require_auth
@api_error_handler
def list_notification_rules():
    """List notification rules with optional filters."""
    user_id = request.args.get("user_id") or session.get("user_id")
    project_id = request.args.get("project_id", type=int)
    event_type = request.args.get("event_type")
    enabled = request.args.get("enabled")

    if enabled is not None:
        enabled = enabled.lower() == "true"

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = notification_rules.list_rules(
            conn, user_id, project_id, event_type, enabled
        )
        return jsonify(result)


@app.route("/api/notifications/rules", methods=["POST"])
@require_auth
@api_error_handler
def create_notification_rule():
    """Create a new notification rule."""
    data = request.get_json() or {}
    name = data.get("name")
    if not name:
        return api_error("name is required", 400, "missing_field")

    event_type = data.get("event_type")
    if not event_type:
        return api_error("event_type is required", 400, "missing_field")

    channels = data.get("channels")
    if not channels:
        return api_error("channels is required", 400, "missing_field")

    user_id = session.get("user_id", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = notification_rules.create_rule(
            conn,
            name,
            event_type,
            channels,
            user_id=data.get("user_id", user_id),
            project_id=data.get("project_id"),
            conditions=data.get("conditions"),
            frequency=data.get("frequency", "immediate"),
            enabled=data.get("enabled", True),
            quiet_hours_start=data.get("quiet_hours_start"),
            quiet_hours_end=data.get("quiet_hours_end"),
            description=data.get("description"),
            created_by=user_id,
        )
        if "error" in result:
            return api_error(result["error"], 400, "rule_error")

        log_activity(
            conn,
            user_id,
            "create",
            "notification_rule",
            result["id"],
            {"name": name, "event_type": event_type},
        )
        return jsonify(result), 201


@app.route("/api/notifications/rules/<int:rule_id>", methods=["GET"])
@require_auth
@api_error_handler
def get_notification_rule(rule_id):
    """Get a single notification rule."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        rule = notification_rules.get_rule(conn, rule_id)
        if not rule:
            return api_error("Rule not found", 404, "not_found")
        return jsonify(rule)


@app.route("/api/notifications/rules/<int:rule_id>", methods=["PUT"])
@require_auth
@api_error_handler
def update_notification_rule(rule_id):
    """Update a notification rule."""
    data = request.get_json() or {}
    user_id = session.get("user_id", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = notification_rules.update_rule(conn, rule_id, data)
        if "error" in result:
            return api_error(result["error"], 400, "rule_error")

        log_activity(
            conn, user_id, "update", "notification_rule", rule_id, data
        )
        return jsonify(result)


@app.route("/api/notifications/rules/<int:rule_id>", methods=["DELETE"])
@require_auth
@api_error_handler
def delete_notification_rule(rule_id):
    """Delete a notification rule."""
    user_id = session.get("user_id", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = notification_rules.delete_rule(conn, rule_id)
        if "error" in result:
            return api_error(result["error"], 400, "rule_error")

        log_activity(conn, user_id, "delete", "notification_rule", rule_id, {})
        return jsonify(result)


@app.route("/api/notifications/rules/<int:rule_id>/toggle", methods=["POST"])
@require_auth
@api_error_handler
def toggle_notification_rule(rule_id):
    """Enable or disable a notification rule."""
    data = request.get_json() or {}
    enabled = data.get("enabled", True)
    user_id = session.get("user_id", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = notification_rules.toggle_rule(conn, rule_id, enabled)
        if "error" in result:
            return api_error(result["error"], 400, "rule_error")

        log_activity(
            conn,
            user_id,
            "toggle",
            "notification_rule",
            rule_id,
            {"enabled": enabled},
        )
        return jsonify(result)


@app.route("/api/notifications/rules/defaults", methods=["POST"])
@require_auth
@api_error_handler
def create_default_notification_rules():
    """Create default notification rules for the current user."""
    user_id = session.get("user_id", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = notification_rules.create_default_rules(conn, user_id)
        return jsonify(result), 201


@app.route("/api/notifications/queue", methods=["GET"])
@require_auth
@api_error_handler
def get_notification_queue():
    """Get pending notifications."""
    frequency = request.args.get("frequency", "immediate")
    limit = request.args.get("limit", 100, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        notifications = notification_rules.get_pending_notifications(
            conn, frequency, limit
        )
        return jsonify(
            {"notifications": notifications, "count": len(notifications)}
        )


@app.route("/api/notifications/stats", methods=["GET"])
@require_auth
@api_error_handler
def get_notification_stats():
    """Get notification delivery statistics."""
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")
    user_id = request.args.get("user_id")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = notification_rules.get_notification_stats(
            conn, start_date, end_date, user_id
        )
        return jsonify(result)


@app.route("/api/notifications/event-types", methods=["GET"])
@require_auth
@api_error_handler
def get_notification_event_types():
    """Get available event types for notifications."""
    return jsonify({"event_types": notification_rules.get_event_types()})


@app.route("/api/notifications/channels", methods=["GET"])
@require_auth
@api_error_handler
def get_notification_channels():
    """Get available notification channels."""
    return jsonify({"channels": notification_rules.get_channels()})


@app.route("/api/notifications/operators", methods=["GET"])
@require_auth
@api_error_handler
def get_notification_operators():
    """Get available condition operators."""
    return jsonify({"operators": notification_rules.get_operators()})


@app.route("/api/notifications/frequencies", methods=["GET"])
@require_auth
@api_error_handler
def get_notification_frequencies():
    """Get available notification frequencies."""
    return jsonify({"frequencies": notification_rules.get_frequencies()})


# ============================================================================
# TASK CONVERSION API
# ============================================================================


@app.route("/api/tasks/convert", methods=["POST"])
@require_auth
@api_error_handler
def convert_task_type():
    """Convert a task from one type to another."""
    data = request.get_json() or {}
    source_type = data.get("source_type")
    source_id = data.get("source_id")
    target_type = data.get("target_type")

    if not source_type:
        return api_error("source_type is required", 400, "missing_field")
    if not source_id:
        return api_error("source_id is required", 400, "missing_field")
    if not target_type:
        return api_error("target_type is required", 400, "missing_field")

    user_id = session.get("user_id", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = task_convert.convert_task(
            conn,
            source_type,
            source_id,
            target_type,
            additional_fields=data.get("additional_fields"),
            keep_original=data.get("keep_original", False),
            created_by=user_id,
        )
        if "error" in result:
            return api_error(result["error"], 400, "conversion_error")

        log_activity(
            conn,
            user_id,
            "convert",
            "task",
            source_id,
            {
                "from": source_type,
                "to": target_type,
                "new_id": result["target"]["id"],
            },
        )
        return jsonify(result), 201


@app.route("/api/tasks/convert/bulk", methods=["POST"])
@require_auth
@api_error_handler
def bulk_convert_tasks():
    """Convert multiple tasks at once."""
    data = request.get_json() or {}
    source_type = data.get("source_type")
    source_ids = data.get("source_ids")
    target_type = data.get("target_type")

    if not source_type:
        return api_error("source_type is required", 400, "missing_field")
    if not source_ids or not isinstance(source_ids, list):
        return api_error(
            "source_ids must be a non-empty list", 400, "invalid_value"
        )
    if not target_type:
        return api_error("target_type is required", 400, "missing_field")

    user_id = session.get("user_id", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = task_convert.bulk_convert(
            conn,
            source_type,
            source_ids,
            target_type,
            additional_fields=data.get("additional_fields"),
            keep_original=data.get("keep_original", False),
            created_by=user_id,
        )

        log_activity(
            conn,
            user_id,
            "bulk_convert",
            "task",
            None,
            {
                "from": source_type,
                "to": target_type,
                "count": result["successful_count"],
            },
        )
        return jsonify(result)


@app.route("/api/tasks/convert/preview", methods=["POST"])
@require_auth
@api_error_handler
def preview_task_conversion():
    """Preview a task conversion without executing it."""
    data = request.get_json() or {}
    source_type = data.get("source_type")
    source_id = data.get("source_id")
    target_type = data.get("target_type")

    if not source_type:
        return api_error("source_type is required", 400, "missing_field")
    if not source_id:
        return api_error("source_id is required", 400, "missing_field")
    if not target_type:
        return api_error("target_type is required", 400, "missing_field")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = task_convert.preview_conversion(
            conn, source_type, source_id, target_type
        )
        if "error" in result:
            return api_error(result["error"], 400, "preview_error")
        return jsonify(result)


@app.route("/api/tasks/convert/history", methods=["GET"])
@require_auth
@api_error_handler
def get_conversion_history():
    """Get history of task conversions."""
    source_type = request.args.get("source_type")
    source_id = request.args.get("source_id", type=int)
    target_type = request.args.get("target_type")
    limit = request.args.get("limit", 50, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = task_convert.get_conversion_history(
            conn, source_type, source_id, target_type, limit
        )
        return jsonify(result)


@app.route("/api/tasks/convert/stats", methods=["GET"])
@require_auth
@api_error_handler
def get_conversion_stats():
    """Get statistics on task conversions."""
    start_date = request.args.get("start_date")
    end_date = request.args.get("end_date")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = task_convert.get_conversion_stats(conn, start_date, end_date)
        return jsonify(result)


@app.route("/api/tasks/convert/types", methods=["GET"])
@require_auth
@api_error_handler
def get_convertible_types():
    """Get available task types for conversion."""
    source_type = request.args.get("source_type")

    types = task_convert.get_task_types()

    if source_type:
        convertible = task_convert.get_convertible_types(source_type)
        return jsonify(
            {
                "source_type": source_type,
                "convertible_to": {
                    k: types[k] for k in convertible if k in types
                },
            }
        )

    return jsonify({"types": types})


@app.route("/api/tasks/monitor", methods=["GET", "OPTIONS"])
def get_tasks_monitor():
    """Get all tasks/prompts for monitoring dashboard.

    Public endpoint (no auth) that returns all prompts from the assigner database.
    Used by the monitor.html page to display real-time task status.

    Returns:
        JSON array of tasks with:
        - id: Task ID
        - content: Task description
        - status: pending/assigned/in_progress/completed/failed
        - assigned_session: Session working on it
        - created_at: Creation timestamp
        - completed_at: Completion timestamp
        - priority: Task priority
        - target_session: Intended target
        - provider: Provider type (codex/comet/claude)
    """
    # Handle CORS preflight OPTIONS request
    if request.method == "OPTIONS":
        response = make_response("", 204)
        response.headers["Access-Control-Allow-Origin"] = "*"
        response.headers["Access-Control-Allow-Methods"] = "GET, OPTIONS"
        response.headers["Access-Control-Allow-Headers"] = "Content-Type"
        response.headers["Access-Control-Max-Age"] = "3600"
        return response

    import sqlite3

    # Path to assigner database
    assigner_db = Path("data/assigner/assigner.db")

    if not assigner_db.exists():
        return jsonify([])

    try:
        with sqlite3.connect(str(assigner_db)) as conn:
            conn.row_factory = sqlite3.Row

            # Get archived filter from query params
            show_archived = (
                request.args.get("archived", "false").lower() == "true"
            )

            # Get all prompts (filtered by archived status)
            cursor = conn.execute(
                """
                SELECT
                    id,
                    content,
                    status,
                    assigned_session,
                    target_session,
                    priority,
                    created_at,
                    assigned_at,
                    completed_at,
                    timeout_minutes,
                    retry_count,
                    max_retries,
                    error,
                    archived
                FROM prompts
                WHERE archived = ?
                ORDER BY
                    CASE
                        WHEN status = 'in_progress' THEN 1
                        WHEN status = 'assigned' THEN 2
                        WHEN status = 'pending' THEN 3
                        WHEN status = 'completed' THEN 4
                        ELSE 5
                    END,
                    priority DESC,
                    id DESC
                LIMIT 100
            """,
                (1 if show_archived else 0,),
            )

            tasks = []
            for row in cursor.fetchall():
                # Determine provider from session name or target
                session = (
                    row["assigned_session"] or row["target_session"] or ""
                )
                provider = "unknown"
                if "codex" in session.lower():
                    provider = "codex"
                elif "comet" in session.lower():
                    provider = "comet"
                elif (
                    "claude" in session.lower()
                    or "concurrent" in session.lower()
                ):
                    provider = "claude"

                # Determine progress percentage
                progress = 0
                if row["status"] == "completed":
                    progress = 100
                elif row["status"] == "in_progress":
                    progress = 50
                elif row["status"] == "assigned":
                    progress = 25

                # Format timestamps
                created = row["created_at"] or ""
                completed = row["completed_at"] or ""

                tasks.append(
                    {
                        "id": row["id"],
                        "desc": row["content"][:100]
                        + ("..." if len(row["content"]) > 100 else ""),
                        "content": row["content"],
                        "status": row["status"],
                        "session": row["assigned_session"] or "Unassigned",
                        "type": provider,
                        "progress": progress,
                        "priority": row["priority"] or 0,
                        "target_session": row["target_session"],
                        "created_at": created,
                        "completed_at": completed,
                        "timeout_minutes": row["timeout_minutes"],
                        "retry_count": row["retry_count"],
                        "max_retries": row["max_retries"],
                        "archived": row["archived"],
                        "step": f"Status: {row['status']}",
                    }
                )

            return jsonify(tasks)

    except Exception as e:
        logger.error(f"Error reading assigner database: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/monitor/session/<session_name>", methods=["GET"])
def get_monitor_session_output(session_name):
    """Get terminal output from a session for monitor inspection.

    Public endpoint (no auth) for monitor to view session output.
    Captures last 100 lines of tmux session output.
    """
    try:
        import subprocess

        # Capture last 100 lines from tmux session
        result = subprocess.run(
            ["tmux", "capture-pane", "-t", session_name, "-p", "-S", "-100"],
            capture_output=True,
            text=True,
            timeout=5,
        )

        if result.returncode == 0:
            output = result.stdout
            # Add CORS headers
            response = jsonify(
                {
                    "session": session_name,
                    "output": output,
                    "lines": len(output.split("\n")),
                }
            )
            response.headers["Access-Control-Allow-Origin"] = "*"
            return response
        else:
            return (
                jsonify({"error": "Session not found or not accessible"}),
                404,
            )

    except subprocess.TimeoutExpired:
        return jsonify({"error": "Capture timed out"}), 500
    except Exception as e:
        logger.error(f"Error capturing session {session_name}: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/monitor/live", methods=["GET"])
def get_live_monitor_sessions():
    """Get active tmux sessions with live output and current task info.

    Public endpoint (no auth) for monitor to display real-time session state.
    """
    import sqlite3
    import subprocess

    def parse_timestamp(value):
        if value is None:
            return None
        if isinstance(value, (int, float)):
            try:
                return datetime.fromtimestamp(float(value))
            except (ValueError, OSError):
                return None
        if isinstance(value, str):
            raw = value.strip()
            if not raw:
                return None
            if raw.isdigit():
                try:
                    return datetime.fromtimestamp(int(raw))
                except (ValueError, OSError):
                    return None
            raw = raw.replace("Z", "+00:00")
            try:
                return datetime.fromisoformat(raw)
            except ValueError:
                pass
            for fmt in ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M:%S.%f"):
                try:
                    return datetime.strptime(raw, fmt)
                except ValueError:
                    continue
        return None

    lines = request.args.get("lines", 120, type=int)
    lines = max(20, min(lines or 120, 400))

    try:
        list_result = subprocess.run(
            [
                "tmux",
                "list-sessions",
                "-F",
                "#{session_name}|#{session_windows}|#{session_attached}|#{session_activity}|#{session_created}",
            ],
            capture_output=True,
            text=True,
            timeout=5,
        )

        if list_result.returncode != 0:
            if (
                "no server running" in list_result.stderr
                or "no sessions" in list_result.stderr.lower()
            ):
                response = jsonify(
                    {
                        "sessions": [],
                        "count": 0,
                        "message": "No active tmux sessions",
                    }
                )
                response.headers["Access-Control-Allow-Origin"] = "*"
                return response
            return jsonify({"error": f"tmux error: {list_result.stderr}"}), 500

        sessions = []
        session_names = []
        for line in list_result.stdout.strip().split("\n"):
            if not line:
                continue
            parts = line.split("|")
            session_name = parts[0]
            session_names.append(session_name)

            activity_ts = (
                int(parts[3])
                if len(parts) > 3 and parts[3].isdigit()
                else None
            )
            created_ts = (
                int(parts[4])
                if len(parts) > 4 and parts[4].isdigit()
                else None
            )

            sessions.append(
                {
                    "session": session_name,
                    "windows": (
                        int(parts[1])
                        if len(parts) > 1 and parts[1].isdigit()
                        else 0
                    ),
                    "attached": parts[2] == "1" if len(parts) > 2 else False,
                    "activity": (
                        datetime.fromtimestamp(activity_ts).isoformat()
                        if activity_ts
                        else None
                    ),
                    "created": (
                        datetime.fromtimestamp(created_ts).isoformat()
                        if created_ts
                        else None
                    ),
                    "output": "",
                    "lines": 0,
                }
            )

        # Map active tasks to sessions
        tasks_by_session = {}
        assigner_db = Path("data/assigner/assigner.db")
        if assigner_db.exists():
            with sqlite3.connect(str(assigner_db)) as conn:
                conn.row_factory = sqlite3.Row
                rows = conn.execute(
                    """
                    SELECT id, content, status, assigned_session, target_session,
                           created_at, assigned_at
                    FROM prompts
                    WHERE archived = 0 AND status IN ('assigned', 'in_progress', 'working')
                    ORDER BY COALESCE(assigned_at, created_at) DESC
                """
                ).fetchall()

                for row in rows:
                    session_name = (
                        row["assigned_session"] or row["target_session"]
                    )
                    if not session_name:
                        continue
                    if session_name in tasks_by_session:
                        continue
                    started_at = parse_timestamp(
                        row["assigned_at"]
                    ) or parse_timestamp(row["created_at"])
                    elapsed_seconds = None
                    if started_at:
                        elapsed_seconds = max(
                            0,
                            int((datetime.now() - started_at).total_seconds()),
                        )

                    tasks_by_session[session_name] = {
                        "id": row["id"],
                        "content": row["content"],
                        "status": row["status"],
                        "assigned_at": row["assigned_at"],
                        "created_at": row["created_at"],
                        "elapsed_seconds": elapsed_seconds,
                    }

        # Capture output for each session
        for sess in sessions:
            try:
                result = subprocess.run(
                    [
                        "tmux",
                        "capture-pane",
                        "-t",
                        sess["session"],
                        "-p",
                        "-S",
                        f"-{lines}",
                    ],
                    capture_output=True,
                    text=True,
                    timeout=5,
                )
                if result.returncode == 0:
                    output = result.stdout or ""
                    sess["output"] = output
                    sess["lines"] = len(output.split("\n"))
                else:
                    sess["output"] = ""
                    sess["lines"] = 0
                    sess["output_error"] = (
                        result.stderr.strip() or "Capture failed"
                    )
            except subprocess.TimeoutExpired:
                sess["output"] = ""
                sess["lines"] = 0
                sess["output_error"] = "Capture timed out"

            task_info = tasks_by_session.get(session["session"])
            if task_info:
                session["task"] = task_info

        response = jsonify(
            {
                "sessions": sessions,
                "count": len(sessions),
                "timestamp": datetime.now().isoformat(),
            }
        )
        response.headers["Access-Control-Allow-Origin"] = "*"
        return response

    except FileNotFoundError:
        return jsonify({"error": "tmux not installed"}), 500
    except Exception as e:
        logger.error(f"Error getting live sessions: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/monitor/session/<session_name>/send", methods=["POST"])
def send_to_monitor_session(session_name):
    """Send input to a session from the monitor.

    Public endpoint (no auth) for monitor to interact with sessions.
    Sends text to the tmux session.
    """
    try:
        import subprocess

        data = request.get_json() or {}
        text = data.get("text", "")
        send_enter = data.get("send_enter", True)

        # Allow empty text for Enter key only
        if text is None:
            return jsonify({"error": "No text provided"}), 400

        # Build tmux command
        cmd = ["tmux", "send-keys", "-t", session_name]
        if text:  # Only add text if not empty
            cmd.append(text)
        if send_enter:
            cmd.append("Enter")

        result = subprocess.run(cmd, capture_output=True, text=True, timeout=5)

        if result.returncode == 0:
            response = jsonify(
                {"success": True, "session": session_name, "sent": text}
            )
            response.headers["Access-Control-Allow-Origin"] = "*"
            return response
        else:
            return jsonify({"error": f"Failed to send: {result.stderr}"}), 500

    except Exception as e:
        logger.error(f"Error sending to session {session_name}: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/monitor/<int:task_id>/archive", methods=["POST"])
def archive_monitor_task(task_id):
    """Archive a task in the monitor.

    Public endpoint (no auth) for monitor to archive tasks.
    Sets archived=1 for the specified task.
    """
    try:
        import sqlite3

        assigner_db = Path("data/assigner/assigner.db")
        if not assigner_db.exists():
            return jsonify({"error": "Database not found"}), 404

        with sqlite3.connect(str(assigner_db)) as conn:
            cursor = conn.execute(
                """
                UPDATE prompts
                SET archived = 1
                WHERE id = ?
            """,
                (task_id,),
            )

            if cursor.rowcount == 0:
                return jsonify({"error": "Task not found"}), 404

            conn.commit()

            response = jsonify(
                {"success": True, "task_id": task_id, "archived": True}
            )
            response.headers["Access-Control-Allow-Origin"] = "*"
            return response

    except Exception as e:
        logger.error(f"Error archiving task {task_id}: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/monitor/archive/bulk", methods=["POST"])
def bulk_archive_monitor_tasks():
    """Bulk archive tasks by status.

    Public endpoint (no auth) for monitor to archive multiple tasks.
    Request body: {"status": "completed"} or {"status": "failed"}
    """
    try:
        import sqlite3

        data = request.get_json() or {}
        status = data.get("status")

        if not status:
            return jsonify({"error": "Status required"}), 400

        if status not in ["completed", "failed"]:
            return (
                jsonify({"error": "Status must be completed or failed"}),
                400,
            )

        assigner_db = Path("data/assigner/assigner.db")
        if not assigner_db.exists():
            return jsonify({"error": "Database not found"}), 404

        with sqlite3.connect(str(assigner_db)) as conn:
            # First, get count of tasks to be archived
            cursor = conn.execute(
                """
                SELECT COUNT(*) FROM prompts
                WHERE status = ? AND archived = 0
            """,
                (status,),
            )
            count = cursor.fetchone()[0]

            if count == 0:
                return jsonify(
                    {
                        "success": True,
                        "count": 0,
                        "message": f"No {status} tasks to archive",
                    }
                )

            # Archive them
            conn.execute(
                """
                UPDATE prompts
                SET archived = 1
                WHERE status = ? AND archived = 0
            """,
                (status,),
            )

            conn.commit()

            response = jsonify(
                {
                    "success": True,
                    "count": count,
                    "status": status,
                    "message": f"Archived {count} {status} task(s)",
                }
            )
            response.headers["Access-Control-Allow-Origin"] = "*"
            return response

    except Exception as e:
        logger.error(f"Error bulk archiving tasks: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/monitor/<int:task_id>/unarchive", methods=["POST"])
def unarchive_monitor_task(task_id):
    """Unarchive a task in the monitor.

    Public endpoint (no auth) for monitor to unarchive tasks.
    Sets archived=0 for the specified task.
    """
    try:
        import sqlite3

        assigner_db = Path("data/assigner/assigner.db")
        if not assigner_db.exists():
            return jsonify({"error": "Database not found"}), 404

        with sqlite3.connect(str(assigner_db)) as conn:
            cursor = conn.execute(
                """
                UPDATE prompts
                SET archived = 0
                WHERE id = ?
            """,
                (task_id,),
            )

            if cursor.rowcount == 0:
                return jsonify({"error": "Task not found"}), 404

            conn.commit()

            response = jsonify(
                {"success": True, "task_id": task_id, "archived": False}
            )
            response.headers["Access-Control-Allow-Origin"] = "*"
            return response

    except Exception as e:
        logger.error(f"Error unarchiving task {task_id}: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/monitor/screenshot/<session_name>", methods=["GET"])
def get_monitor_screenshot(session_name):
    """Get latest screenshot for browser-based sessions (comet).

    Public endpoint (no auth) for monitor to view browser screenshots.
    Returns the most recent screenshot if available.
    """
    try:
        import base64

        # Look for screenshots in common locations
        screenshot_dirs = [
            Path("screenshots"),
            Path("data/screenshots"),
            Path(f"data/screenshots/{session_name}"),
        ]

        latest_screenshot = None
        latest_time = 0

        for screenshot_dir in screenshot_dirs:
            if screenshot_dir.exists():
                # Find most recent .png file
                for img_file in screenshot_dir.glob("*.png"):
                    if img_file.stat().st_mtime > latest_time:
                        latest_time = img_file.stat().st_mtime
                        latest_screenshot = img_file

        if latest_screenshot and latest_screenshot.exists():
            import time

            age_seconds = time.time() - latest_time
            age_minutes = int(age_seconds / 60)

            with open(latest_screenshot, "rb") as f:
                img_data = base64.b64encode(f.read()).decode("utf-8")

            response = jsonify(
                {
                    "session": session_name,
                    "screenshot": f"data:image/png;base64,{img_data}",
                    "timestamp": latest_time,
                    "filename": latest_screenshot.name,
                    "age_minutes": age_minutes,
                    "warning": (
                        f"Screenshot is {age_minutes} minutes old"
                        if age_minutes > 5
                        else None
                    ),
                }
            )
            response.headers["Access-Control-Allow-Origin"] = "*"
            return response
        else:
            return (
                jsonify(
                    {
                        "error": "No screenshots found",
                        "message": "Comet sessions do not currently capture screenshots automatically. Terminal output is available instead.",
                    }
                ),
                404,
            )

    except Exception as e:
        logger.error(f"Error getting screenshot for {session_name}: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/broadcast-status", methods=["POST"])
def broadcast_task_status():
    """Broadcast task status updates via WebSocket.

    Called by task_monitor.py to push live status updates to connected dashboard clients.
    No authentication required - this is an internal monitoring endpoint.

    Expected payload:
    {
        "updates": [
            {
                "task_id": 1,
                "session": "session_name",
                "type": "codex",
                "is_working": false,
                "progress": 0,
                "current_step": "...",
                "status": "idle",
                "last_activity": "..."
            }
        ]
    }
    """
    try:
        data = request.get_json()
        if not data or "updates" not in data:
            return jsonify({"error": "Invalid payload - missing updates"}), 400

        updates = data["updates"]

        # Broadcast to all clients subscribed to the 'tasks' room
        socketio.emit(
            "task_status_update",
            {"timestamp": datetime.now().isoformat(), "updates": updates},
            room="tasks",
        )

        logger.info(f"Broadcasted {len(updates)} task status updates")

        return jsonify({"success": True, "broadcasted": len(updates)})

    except Exception as e:
        logger.error(f"Error broadcasting task status: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/tasks/live-status", methods=["GET"])
def get_live_task_status():
    """Get current live status for monitored tasks.

    Returns status for tasks 1, 2, 4, 5, 9, 10 as monitored by task_monitor.py.
    No authentication required - this is for dashboard display.

    Returns:
        {
            "tasks": [
                {
                    "task_id": 1,
                    "description": "Task description",
                    "session": "session_name",
                    "status": "working",
                    "progress": 50,
                    "current_step": "...",
                    "last_activity": "..."
                }
            ]
        }
    """
    try:
        with get_db_connection() as db:
            db.row_factory = sqlite3.Row

            # Get task IDs being monitored
            monitored_task_ids = [1, 2, 4, 5, 9, 10]

            tasks = []
            for task_id in monitored_task_ids:
                task_row = db.execute(
                """
                SELECT id, type, description, status, task_data, created_at
                FROM task_queue
                WHERE id = ?
            """,
                (task_id,),
            ).fetchone()

            if task_row:
                task_data = (
                    json.loads(task_row["task_data"])
                    if task_row["task_data"]
                    else {}
                )

                # Get live status from task_data (set by task_monitor.py)
                live_status = task_data.get("live_status", "unknown")
                live_progress = task_data.get("live_progress", 0)
                live_step = task_data.get("live_step", "No activity detected")
                live_updated = task_data.get(
                    "live_updated", task_row["created_at"]
                )

                # Map session from task_id
                session_map = {
                    1: "codex",
                    2: "codex_edu",
                    4: "concurrent_worker1",
                    5: "concurrent_worker2",
                    9: "comet",
                    10: "concurrent_worker3",
                }

                tasks.append(
                    {
                        "task_id": task_row["id"],
                        "description": task_row["description"]
                        or f"Task #{task_row['id']}",
                        "session": session_map.get(task_row["id"], "unknown"),
                        "status": live_status,
                        "progress": live_progress,
                        "current_step": live_step,
                        "last_activity": live_updated,
                        "overall_status": task_row["status"],
                    }
                )
            else:
                # Task doesn't exist yet, show as unknown
                tasks.append(
                    {
                        "task_id": task_id,
                        "description": f"Task #{task_id}",
                        "session": session_map.get(task_id, "unknown"),
                        "status": "unknown",
                        "progress": 0,
                        "current_step": "Task not found",
                        "last_activity": "",
                        "overall_status": "pending",
                    }
                )

        return jsonify(
            {
                "success": True,
                "tasks": tasks,
                "timestamp": datetime.now().isoformat(),
            }
        )

    except Exception as e:
        logger.error(f"Error getting live task status: {e}")
        return jsonify({"error": str(e)}), 500


# ============================================================================
# DASHBOARD LAYOUT API
# ============================================================================


@app.route("/api/dashboard/layout", methods=["GET"])
@require_auth
@api_error_handler
def get_user_dashboard_layout():
    """Get the user's default dashboard layout."""
    user_id = session.get("user_id", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        layout = dashboard_layout.get_default_layout(conn, user_id)
        return jsonify(layout)


@app.route("/api/dashboard/layout", methods=["POST"])
@require_auth
@api_error_handler
def save_dashboard_layout():
    """Save a new dashboard layout."""
    data = request.get_json() or {}
    name = data.get("name")
    if not name:
        return api_error("name is required", 400, "missing_field")

    layout_config = data.get("layout_config")
    if not layout_config:
        return api_error("layout_config is required", 400, "missing_field")

    user_id = session.get("user_id", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = dashboard_layout.save_layout(
            conn,
            user_id,
            name,
            layout_config,
            is_default=data.get("is_default", False),
            description=data.get("description"),
        )
        if "error" in result:
            return api_error(result["error"], 400, "layout_error")

        log_activity(
            conn,
            user_id,
            "create",
            "dashboard_layout",
            result["id"],
            {"name": name},
        )
        return jsonify(result), 201


@app.route("/api/dashboard/layout/<layout_id>", methods=["GET"])
@require_auth
@api_error_handler
def get_dashboard_layout(layout_id):
    """Get a specific dashboard layout."""
    # Handle preset layouts
    if layout_id.startswith("preset_"):
        preset_id = layout_id.replace("preset_", "")
        presets = dashboard_layout.get_presets()
        if preset_id not in presets:
            return api_error("Preset not found", 404, "not_found")
        preset = presets[preset_id]
        return jsonify(
            {
                "id": layout_id,
                "name": preset["name"],
                "description": preset["description"],
                "is_preset": True,
                "layout_config": {
                    "columns": preset["columns"],
                    "panels": preset["panels"],
                },
            }
        )

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        layout = dashboard_layout.get_layout(conn, int(layout_id))
        if not layout:
            return api_error("Layout not found", 404, "not_found")
        return jsonify(layout)


@app.route("/api/dashboard/layout/<int:layout_id>", methods=["DELETE"])
@require_auth
@api_error_handler
def delete_dashboard_layout(layout_id):
    """Delete a dashboard layout."""
    user_id = session.get("user_id", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = dashboard_layout.delete_layout(conn, layout_id, user_id)
        if "error" in result:
            return api_error(result["error"], 400, "layout_error")

        log_activity(
            conn, user_id, "delete", "dashboard_layout", layout_id, {}
        )
        return jsonify(result)


@app.route("/api/dashboard/layouts", methods=["GET"])
@require_auth
@api_error_handler
def list_dashboard_layouts():
    """List all layouts for the current user."""
    user_id = session.get("user_id", "anonymous")
    include_presets = (
        request.args.get("include_presets", "true").lower() == "true"
    )

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = dashboard_layout.get_user_layouts(
            conn, user_id, include_presets
        )
        return jsonify(result)


@app.route("/api/dashboard/layout/default", methods=["POST"])
@require_auth
@api_error_handler
def set_default_dashboard_layout():
    """Set a layout as the user's default."""
    data = request.get_json() or {}
    layout_id = data.get("layout_id")
    if not layout_id:
        return api_error("layout_id is required", 400, "missing_field")

    user_id = session.get("user_id", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = dashboard_layout.set_default_layout(conn, layout_id, user_id)
        if "error" in result:
            return api_error(result["error"], 400, "layout_error")

        log_activity(
            conn, user_id, "set_default", "dashboard_layout", layout_id, {}
        )
        return jsonify(result)


@app.route("/api/dashboard/layout/panel", methods=["PUT"])
@require_auth
@api_error_handler
def update_dashboard_panel():
    """Update a single panel in the user's layout."""
    data = request.get_json() or {}
    panel_id = data.get("panel_id")
    if not panel_id:
        return api_error("panel_id is required", 400, "missing_field")

    updates = {
        k: v
        for k, v in data.items()
        if k in ["size", "order", "visible", "title", "collapsed"]
    }
    if not updates:
        return api_error("No valid updates provided", 400, "invalid_value")

    user_id = session.get("user_id", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = dashboard_layout.update_panel(
            conn, user_id, panel_id, updates
        )
        if "error" in result:
            return api_error(result["error"], 400, "layout_error")
        return jsonify(result)


@app.route("/api/dashboard/layout/reorder", methods=["POST"])
@require_auth
@api_error_handler
def reorder_dashboard_panels():
    """Reorder panels in the user's layout."""
    data = request.get_json() or {}
    panel_order = data.get("panel_order")
    if not panel_order or not isinstance(panel_order, list):
        return api_error(
            "panel_order must be a list of panel IDs", 400, "invalid_value"
        )

    user_id = session.get("user_id", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = dashboard_layout.reorder_panels(conn, user_id, panel_order)
        if "error" in result:
            return api_error(result["error"], 400, "layout_error")
        return jsonify(result)


@app.route("/api/dashboard/layout/panel/<panel_id>/toggle", methods=["POST"])
@require_auth
@api_error_handler
def toggle_dashboard_panel(panel_id):
    """Toggle panel visibility."""
    data = request.get_json() or {}
    visible = data.get("visible")

    user_id = session.get("user_id", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = dashboard_layout.toggle_panel(
            conn, user_id, panel_id, visible
        )
        if "error" in result:
            return api_error(result["error"], 400, "layout_error")
        return jsonify(result)


@app.route("/api/dashboard/layout/<layout_id>/duplicate", methods=["POST"])
@require_auth
@api_error_handler
def duplicate_dashboard_layout(layout_id):
    """Duplicate an existing layout."""
    data = request.get_json() or {}
    new_name = data.get("name")
    if not new_name:
        return api_error(
            "name is required for the new layout", 400, "missing_field"
        )

    user_id = session.get("user_id", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = dashboard_layout.duplicate_layout(
            conn, layout_id, user_id, new_name
        )
        if "error" in result:
            return api_error(result["error"], 400, "layout_error")

        log_activity(
            conn,
            user_id,
            "duplicate",
            "dashboard_layout",
            layout_id,
            {"new_name": new_name},
        )
        return jsonify(result), 201


@app.route("/api/dashboard/layout/presets", methods=["GET"])
@require_auth
@api_error_handler
def get_layout_presets():
    """Get available layout presets."""
    return jsonify({"presets": dashboard_layout.get_presets()})


# DUPLICATE REMOVED: @app.route('/api/dashboard/layout/panels', methods=['GET'])
# DUPLICATE REMOVED: @require_auth
# DUPLICATE REMOVED: @api_error_handler
# DUPLICATE REMOVED: def get_available_panels():
# DUPLICATE REMOVED:     """Get list of available panels."""
# DUPLICATE REMOVED:     return jsonify({'panels':
# dashboard_layout.get_available_panels()})


@app.route("/api/dashboard/layout/sizes", methods=["GET"])
@require_auth
@api_error_handler
def get_panel_sizes():
    """Get available panel sizes."""
    return jsonify({"sizes": dashboard_layout.get_panel_sizes()})


@app.route("/api/dashboard/layouts/public", methods=["GET"])
@require_auth
@api_error_handler
def get_public_layouts():
    """Get publicly shared layouts."""
    limit = request.args.get("limit", 20, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = dashboard_layout.get_public_layouts(conn, limit)
        return jsonify(result)


# ============================================================================
# SPRINT PLANNING BOARD API
# ============================================================================


@app.route("/api/sprints", methods=["GET"])
@require_auth
@api_error_handler
def list_sprints():
    """List sprints with optional filters."""
    project_id = request.args.get("project_id", type=int)
    status = request.args.get("status")
    limit = request.args.get("limit", 20, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = sprint_board.list_sprints(conn, project_id, status, limit)
        return jsonify(result)


@app.route("/api/sprints", methods=["POST"])
@require_auth
@api_error_handler
def create_sprint():
    """Create a new sprint."""
    data = request.get_json() or {}
    name = data.get("name")
    project_id = data.get("project_id")
    start_date = data.get("start_date")
    end_date = data.get("end_date")

    if not name:
        return api_error("name is required", 400, "missing_field")
    if not project_id:
        return api_error("project_id is required", 400, "missing_field")
    if not start_date or not end_date:
        return api_error(
            "start_date and end_date are required", 400, "missing_field"
        )

    user_id = session.get("user_id", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = sprint_board.create_sprint(
            conn,
            name,
            project_id,
            start_date,
            end_date,
            goal=data.get("goal"),
            capacity_hours=data.get("capacity_hours"),
            created_by=user_id,
        )
        if "error" in result:
            return api_error(result["error"], 400, "sprint_error")

        log_activity(
            conn, user_id, "create", "sprint", result["id"], {"name": name}
        )
        return jsonify(result), 201


@app.route("/api/sprints/<int:sprint_id>", methods=["GET"])
@require_auth
@api_error_handler
def get_sprint_details(sprint_id):
    """Get sprint details."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        sprint = sprint_board.get_sprint(conn, sprint_id)
        if not sprint:
            return api_error("Sprint not found", 404, "not_found")
        return jsonify(sprint)


@app.route("/api/sprints/<int:sprint_id>", methods=["PUT"])
@require_auth
@api_error_handler
def update_sprint(sprint_id):
    """Update sprint details."""
    data = request.get_json() or {}
    user_id = session.get("user_id", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = sprint_board.update_sprint(conn, sprint_id, data)
        if "error" in result:
            return api_error(result["error"], 400, "sprint_error")

        log_activity(conn, user_id, "update", "sprint", sprint_id, data)
        return jsonify(result)


@app.route("/api/sprints/<int:sprint_id>", methods=["DELETE"])
@require_auth
@api_error_handler
def delete_sprint(sprint_id):
    """Delete a sprint."""
    user_id = session.get("user_id", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = sprint_board.delete_sprint(conn, sprint_id)
        if "error" in result:
            return api_error(result["error"], 400, "sprint_error")

        log_activity(conn, user_id, "delete", "sprint", sprint_id, {})
        return jsonify(result)


@app.route("/api/sprints/<int:sprint_id>/start", methods=["POST"])
@require_auth
@api_error_handler
def start_sprint(sprint_id):
    """Start a sprint."""
    user_id = session.get("user_id", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = sprint_board.start_sprint(conn, sprint_id)
        if "error" in result:
            return api_error(result["error"], 400, "sprint_error")

        log_activity(conn, user_id, "start", "sprint", sprint_id, {})
        return jsonify(result)


@app.route("/api/sprints/<int:sprint_id>/complete", methods=["POST"])
@require_auth
@api_error_handler
def complete_sprint(sprint_id):
    """Complete a sprint."""
    data = request.get_json() or {}
    move_incomplete_to = data.get("move_incomplete_to")
    user_id = session.get("user_id", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = sprint_board.complete_sprint(
            conn, sprint_id, move_incomplete_to
        )
        if "error" in result:
            return api_error(result["error"], 400, "sprint_error")

        log_activity(conn, user_id, "complete", "sprint", sprint_id, result)
        return jsonify(result)


@app.route("/api/sprints/<int:sprint_id>/board", methods=["GET"])
@require_auth
@api_error_handler
def get_sprint_board(sprint_id):
    """Get sprint board view with tasks organized by columns."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = sprint_board.get_board_view(conn, sprint_id)
        if "error" in result:
            return api_error(result["error"], 404, "not_found")
        return jsonify(result)


@app.route("/api/sprints/<int:sprint_id>/tasks", methods=["POST"])
@require_auth
@api_error_handler
def add_task_to_sprint(sprint_id):
    """Add a task to a sprint."""
    data = request.get_json() or {}
    task_id = data.get("task_id")

    if not task_id:
        return api_error("task_id is required", 400, "missing_field")

    user_id = session.get("user_id", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = sprint_board.add_task_to_sprint(conn, sprint_id, task_id)
        if "error" in result:
            return api_error(result["error"], 400, "sprint_error")

        log_activity(
            conn,
            user_id,
            "add_to_sprint",
            "task",
            task_id,
            {"sprint_id": sprint_id},
        )
        return jsonify(result)


@app.route("/api/sprints/<int:sprint_id>/tasks/bulk", methods=["POST"])
@require_auth
@api_error_handler
def bulk_add_tasks_to_sprint(sprint_id):
    """Add multiple tasks to a sprint."""
    data = request.get_json() or {}
    task_ids = data.get("task_ids")

    if not task_ids or not isinstance(task_ids, list):
        return api_error(
            "task_ids must be a non-empty list", 400, "invalid_value"
        )

    user_id = session.get("user_id", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = sprint_board.bulk_add_tasks_to_sprint(
            conn, sprint_id, task_ids
        )
        if "error" in result:
            return api_error(result["error"], 400, "sprint_error")

        log_activity(
            conn,
            user_id,
            "bulk_add_to_sprint",
            "sprint",
            sprint_id,
            {"added_count": result["added_count"]},
        )
        return jsonify(result)


@app.route("/api/tasks/<int:task_id>/sprint", methods=["DELETE"])
@require_auth
@api_error_handler
def remove_task_from_sprint(task_id):
    """Remove a task from its sprint."""
    user_id = session.get("user_id", "anonymous")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = sprint_board.remove_task_from_sprint(conn, task_id)
        if "error" in result:
            return api_error(result["error"], 400, "sprint_error")

        log_activity(
            conn, user_id, "remove_from_sprint", "task", task_id, result
        )
        return jsonify(result)


@app.route("/api/board/move", methods=["POST"])
@require_auth
@api_error_handler
def move_task_on_board():
    """Move a task to a different column on the board."""
    data = request.get_json() or {}
    task_id = data.get("task_id")
    target_column = data.get("target_column")

    if not task_id:
        return api_error("task_id is required", 400, "missing_field")
    if not target_column:
        return api_error("target_column is required", 400, "missing_field")

    user_id = session.get("user_id", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = sprint_board.move_task_on_board(conn, task_id, target_column)
        if "error" in result:
            return api_error(result["error"], 400, "board_error")

        log_activity(conn, user_id, "move_on_board", "task", task_id, result)
        return jsonify(result)


@app.route("/api/sprints/<int:sprint_id>/burndown", methods=["GET"])
@require_auth
@api_error_handler
def get_sprint_burndown(sprint_id):
    """Get burndown chart data for a sprint."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = sprint_board.get_sprint_burndown(conn, sprint_id)
        if "error" in result:
            return api_error(result["error"], 404, "not_found")
        return jsonify(result)


@app.route("/api/projects/<int:project_id>/velocity", methods=["GET"])
@require_auth
@api_error_handler
def get_project_velocity(project_id):
    """Get velocity data for recent sprints."""
    num_sprints = request.args.get("num_sprints", 5, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = sprint_board.get_sprint_velocity(
            conn, project_id, num_sprints
        )
        return jsonify(result)


@app.route("/api/projects/<int:project_id>/backlog", methods=["GET"])
@require_auth
@api_error_handler
def get_project_backlog(project_id):
    """Get project backlog (tasks not in any sprint)."""
    exclude_sprint_tasks = (
        request.args.get("exclude_sprint_tasks", "true").lower() == "true"
    )

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = sprint_board.get_backlog(
            conn, project_id, exclude_sprint_tasks
        )
        return jsonify(result)


@app.route("/api/projects/<int:project_id>/sprint/active", methods=["GET"])
@require_auth
@api_error_handler
def get_active_sprint(project_id):
    """Get the currently active sprint for a project."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        sprint = sprint_board.get_active_sprint(conn, project_id)
        if not sprint:
            return jsonify({"active": False})
        return jsonify({"active": True, "sprint": sprint})


@app.route("/api/sprints/statuses", methods=["GET"])
@require_auth
@api_error_handler
def get_sprint_statuses():
    """Get available sprint statuses."""
    return jsonify({"statuses": sprint_board.get_sprint_statuses()})


@app.route("/api/board/columns", methods=["GET"])
@require_auth
@api_error_handler
def get_board_columns():
    """Get default board columns."""
    return jsonify({"columns": sprint_board.get_default_columns()})


# ============================================================================
# CUSTOM REPORT BUILDER API
# ============================================================================


@app.route("/api/reports/custom", methods=["GET"])
@require_auth
def list_custom_reports():
    """List all saved custom reports."""
    user_id = session.get("user_id")
    include_shared = (
        request.args.get("include_shared", "true").lower() == "true"
    )
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        if include_shared:
            reports = conn.execute(
                "SELECT id, name, description, report_type, is_shared, created_by, created_at FROM custom_reports WHERE created_by = ? OR is_shared = 1 ORDER BY name",
                (user_id,),
            ).fetchall()
        else:
            reports = conn.execute(
                "SELECT id, name, description, report_type, is_shared, created_by, created_at FROM custom_reports WHERE created_by = ? ORDER BY name",
                (user_id,),
            ).fetchall()
        return jsonify({"reports": [dict(r) for r in reports]})


# DUPLICATE REMOVED: @app.route('/api/reports/custom', methods=['POST'])
# DUPLICATE REMOVED: @require_auth
# DUPLICATE REMOVED: def create_custom_report():
# DUPLICATE REMOVED:     """Create a new custom report definition."""
# DUPLICATE REMOVED:     data = request.get_json() or {}
# DUPLICATE REMOVED:     name = data.get('name')
# DUPLICATE REMOVED:     if not name:
# DUPLICATE REMOVED:         return api_error("name is required", 400,
# "missing_field")

# DUPLICATE REMOVED:     report_config = {
# DUPLICATE REMOVED:         'data_source': data.get('data_source', 'tasks'),
# DUPLICATE REMOVED:         'filters': data.get('filters', []),
# DUPLICATE REMOVED:         'columns': data.get('columns', []),
# DUPLICATE REMOVED:         'grouping': data.get('grouping'),
# DUPLICATE REMOVED:         'sorting': data.get('sorting', []),
# DUPLICATE REMOVED:         'aggregations': data.get('aggregations', []),
# DUPLICATE REMOVED:         'chart_type': data.get('chart_type'),
# DUPLICATE REMOVED:         'date_range': data.get('date_range')
# DUPLICATE REMOVED:     }

# DUPLICATE REMOVED:     with get_db_connection() as conn:
# DUPLICATE REMOVED:         cursor = conn.execute("""
# DUPLICATE REMOVED:             INSERT INTO custom_reports (name, description, report_type, report_config, is_shared, created_by)
# DUPLICATE REMOVED:             VALUES (?, ?, ?, ?, ?, ?)
# DUPLICATE REMOVED:         """, (name, data.get('description', ''), data.get('report_type', 'table'), json.dumps(report_config), data.get('is_shared', False), session.get('user_id')))
# DUPLICATE REMOVED:         log_activity(conn, 'create', 'custom_report', cursor.lastrowid, f"Created report: {name}")
# DUPLICATE REMOVED:         return jsonify({'success': True, 'report_id':
# cursor.lastrowid})

# DUPLICATE REMOVED: @app.route('/api/reports/custom/<int:report_id>', methods=['GET'])
# DUPLICATE REMOVED: @require_auth
# DUPLICATE REMOVED: def get_custom_report(report_id):
# DUPLICATE REMOVED:     """Get a custom report definition."""
# DUPLICATE REMOVED:     with get_db_connection() as conn:
# DUPLICATE REMOVED:         conn.row_factory = sqlite3.Row
# DUPLICATE REMOVED:         report = conn.execute("SELECT * FROM custom_reports WHERE id = ?", (report_id,)).fetchone()
# DUPLICATE REMOVED:         if not report:
# DUPLICATE REMOVED:             return api_error("Report not found", 404, "not_found")
# DUPLICATE REMOVED:         result = dict(report)
# DUPLICATE REMOVED:         result['report_config'] = json.loads(result['report_config'] or '{}')
# DUPLICATE REMOVED:         return jsonify(result)

# DUPLICATE REMOVED: @app.route('/api/reports/custom/<int:report_id>', methods=['PUT'])
# DUPLICATE REMOVED: @require_auth
# DUPLICATE REMOVED: def update_custom_report(report_id):
# DUPLICATE REMOVED:     """Update a custom report definition."""
# DUPLICATE REMOVED:     data = request.get_json() or {}
# DUPLICATE REMOVED:     with get_db_connection() as conn:
# DUPLICATE REMOVED:         conn.row_factory = sqlite3.Row
# DUPLICATE REMOVED:         existing = conn.execute("SELECT report_config, created_by FROM custom_reports WHERE id = ?", (report_id,)).fetchone()
# DUPLICATE REMOVED:         if not existing:
# DUPLICATE REMOVED:             return api_error("Report not found", 404,
# "not_found")

# DUPLICATE REMOVED:         config = json.loads(existing['report_config'] or '{}')
# DUPLICATE REMOVED:         for key in ['data_source', 'filters', 'columns', 'grouping', 'sorting', 'aggregations', 'chart_type', 'date_range']:
# DUPLICATE REMOVED:             if key in data: config[key] = data[key]

# DUPLICATE REMOVED:         updates, params = ["report_config = ?"], [json.dumps(config)]
# DUPLICATE REMOVED:         if 'name' in data: updates.append("name = ?"); params.append(data['name'])
# DUPLICATE REMOVED:         if 'description' in data: updates.append("description = ?"); params.append(data['description'])
# DUPLICATE REMOVED:         if 'is_shared' in data: updates.append("is_shared = ?"); params.append(data['is_shared'])
# DUPLICATE REMOVED:         params.append(report_id)
# DUPLICATE REMOVED:         conn.execute(f"UPDATE custom_reports SET {', '.join(updates)}, updated_at = CURRENT_TIMESTAMP WHERE id = ?", params)
# DUPLICATE REMOVED:         return jsonify({'success': True, 'report_id':
# report_id})

# DUPLICATE REMOVED: @app.route('/api/reports/custom/<int:report_id>', methods=['DELETE'])
# DUPLICATE REMOVED: @require_auth
# DUPLICATE REMOVED: def delete_custom_report(report_id):
# DUPLICATE REMOVED:     """Delete a custom report."""
# DUPLICATE REMOVED:     with get_db_connection() as conn:
# DUPLICATE REMOVED:         conn.execute("DELETE FROM custom_reports WHERE id = ?", (report_id,))
# DUPLICATE REMOVED:         return jsonify({'success': True})

# DUPLICATE REMOVED: @app.route('/api/reports/custom/<int:report_id>/run', methods=['POST'])
# DUPLICATE REMOVED: @require_auth
# DUPLICATE REMOVED: def run_custom_report(report_id):
# DUPLICATE REMOVED:     """Execute a custom report and return results."""
# DUPLICATE REMOVED:     with get_db_connection() as conn:
# DUPLICATE REMOVED:         conn.row_factory = sqlite3.Row
# DUPLICATE REMOVED:         report = conn.execute("SELECT report_config FROM custom_reports WHERE id = ?", (report_id,)).fetchone()
# DUPLICATE REMOVED:         if not report:
# DUPLICATE REMOVED:             return api_error("Report not found", 404,
# "not_found")

# DUPLICATE REMOVED:         config = json.loads(report['report_config'] or '{}')
# DUPLICATE REMOVED:         data_source = config.get('data_source', 'tasks')
# DUPLICATE REMOVED:         filters = config.get('filters', [])
# DUPLICATE REMOVED:         columns = config.get('columns', [])
# DUPLICATE REMOVED:         grouping = config.get('grouping')
# DUPLICATE REMOVED:         sorting = config.get('sorting', [])
# DUPLICATE REMOVED:         aggregations = config.get('aggregations', [])

# DUPLICATE REMOVED:         table_map = {'tasks': 'task_queue', 'projects': 'projects', 'milestones': 'milestones', 'features': 'features', 'bugs': 'bugs', 'errors': 'errors'}
# DUPLICATE REMOVED:         table = table_map.get(data_source, 'task_queue')

# DUPLICATE REMOVED:         q = f"SELECT * FROM {table} WHERE 1=1"
# DUPLICATE REMOVED:         params = []

# DUPLICATE REMOVED:         for f in filters:
# DUPLICATE REMOVED:             field, op, val = f.get('field'), f.get('operator', '='), f.get('value')
# DUPLICATE REMOVED:             if field and val is not None:
# DUPLICATE REMOVED:                 if op == 'contains': q += f" AND {field} LIKE ?"; params.append(f'%{val}%')
# DUPLICATE REMOVED:                 elif op == 'in': q += f" AND {field} IN ({','.join('?' * len(val))})"; params.extend(val)
# DUPLICATE REMOVED:                 else: q += f" AND {field} {op} ?";
# params.append(val)

# DUPLICATE REMOVED:         if sorting:
# DUPLICATE REMOVED:             sort_clauses = [f"{s['field']} {s.get('direction', 'ASC')}" for s in sorting if 'field' in s]
# DUPLICATE REMOVED:             if sort_clauses: q += f" ORDER BY {',
# '.join(sort_clauses)}"

# DUPLICATE REMOVED:         q += " LIMIT 1000"
# DUPLICATE REMOVED:         rows = conn.execute(q, params).fetchall()
# DUPLICATE REMOVED:         results = [dict(r) for r in rows]

# DUPLICATE REMOVED:         agg_results = {}
# DUPLICATE REMOVED:         if aggregations:
# DUPLICATE REMOVED:             for agg in aggregations:
# DUPLICATE REMOVED:                 func, field = agg.get('function', 'count'), agg.get('field', '*')
# DUPLICATE REMOVED:                 if func == 'count': agg_results[f"count_{field}"] = len(results)
# DUPLICATE REMOVED:                 elif func == 'sum' and field != '*':
# DUPLICATE REMOVED:                     agg_results[f"sum_{field}"] =
# sum(r.get(field, 0) or 0 for r in results if isinstance(r.get(field),
# (int, float)))

# DUPLICATE REMOVED:         conn.execute("UPDATE custom_reports SET
# last_run_at = CURRENT_TIMESTAMP WHERE id = ?", (report_id,))

# DUPLICATE REMOVED:         return jsonify({'report_id': report_id,
# 'row_count': len(results), 'data': results, 'aggregations':
# agg_results})

# DUPLICATE REMOVED: @app.route('/api/reports/custom/<int:report_id>/export', methods=['GET'])
# DUPLICATE REMOVED: @require_auth
# DUPLICATE REMOVED: def export_custom_report(report_id):
# DUPLICATE REMOVED:     """Export custom report results as CSV."""
# DUPLICATE REMOVED:     with get_db_connection() as conn:
# DUPLICATE REMOVED:         conn.row_factory = sqlite3.Row
# DUPLICATE REMOVED:         report = conn.execute("SELECT name, report_config FROM custom_reports WHERE id = ?", (report_id,)).fetchone()
# DUPLICATE REMOVED:         if not report:
# DUPLICATE REMOVED:             return api_error("Report not found", 404,
# "not_found")

# DUPLICATE REMOVED:         config = json.loads(report['report_config'] or '{}')
# DUPLICATE REMOVED:         table_map = {'tasks': 'task_queue', 'projects': 'projects', 'milestones': 'milestones', 'features': 'features', 'bugs': 'bugs'}
# DUPLICATE REMOVED:         table =
# table_map.get(config.get('data_source', 'tasks'), 'task_queue')

# DUPLICATE REMOVED:         rows = conn.execute(f"SELECT * FROM {table} LIMIT 1000").fetchall()
# DUPLICATE REMOVED:         if not rows:
# DUPLICATE REMOVED:             return "No data", 200, {'Content-Type':
# 'text/csv'}

# DUPLICATE REMOVED:         columns = rows[0].keys()
# DUPLICATE REMOVED:         lines = [','.join(columns)]
# DUPLICATE REMOVED:         for r in rows:
# DUPLICATE REMOVED:             lines.append(','.join(str(r[c] or
# '').replace(',', ';').replace('\n', ' ') for c in columns))

# DUPLICATE REMOVED:         csv_content = '\n'.join(lines)
# DUPLICATE REMOVED:         return csv_content, 200, {
# DUPLICATE REMOVED:             'Content-Type': 'text/csv',
# DUPLICATE REMOVED:             'Content-Disposition': f'attachment; filename="{report["name"].replace(" ", "_")}_{datetime.now().strftime("%Y%m%d")}.csv"'
# DUPLICATE REMOVED:         }


# ============================================================================
# REAL-TIME COLLABORATION API
# ============================================================================

# Presence timeout in seconds (users are considered offline after this)
PRESENCE_TIMEOUT_SECONDS = 60

# Avatar colors for random assignment
AVATAR_COLORS = [
    '#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7',
    '#DDA0DD', '#98D8C8', '#F7DC6F', '#BB8FCE', '#85C1E9'
]


@app.route("/api/collaboration/presence", methods=["POST"])
@require_auth
def update_presence():
    """Register or update user presence (heartbeat).

    Body:
        session_id: Unique session identifier (required)
        display_name: User display name (optional)
        entity_type: Current entity type being viewed (optional)
        entity_id: Current entity ID being viewed (optional)
        action: Current action - 'viewing', 'editing', 'idle' (default 'viewing')
    """
    import hashlib

    data = request.get_json() or {}
    session_id = data.get("session_id")

    if not session_id:
        return api_error("session_id is required", 400, "missing_field")

    user_id = session.get("user", "anonymous")
    display_name = data.get("display_name", user_id)
    entity_type = data.get("entity_type")
    entity_id = data.get("entity_id")
    action = data.get("action", "viewing")

    # Generate consistent avatar color from user_id
    color_index = int(hashlib.md5(user_id.encode()).hexdigest(), 16) % len(
        AVATAR_COLORS
    )
    avatar_color = AVATAR_COLORS[color_index]

    user_agent = request.headers.get("User-Agent", "")[:200]
    ip_address = request.remote_addr

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Update or insert presence
        conn.execute(
            """
            INSERT INTO user_presence
                (user_id, session_id, display_name, avatar_color, status,
                 current_entity_type, current_entity_id, current_action,
                 last_heartbeat, user_agent, ip_address)
            VALUES (?, ?, ?, ?, 'online', ?, ?, ?, CURRENT_TIMESTAMP, ?, ?)
            ON CONFLICT(session_id) DO UPDATE SET
                display_name = excluded.display_name,
                status = 'online',
                current_entity_type = excluded.current_entity_type,
                current_entity_id = excluded.current_entity_id,
                current_action = excluded.current_action,
                last_heartbeat = CURRENT_TIMESTAMP
        """,
            (
                user_id,
                session_id,
                display_name,
                avatar_color,
                entity_type,
                entity_id,
                action,
                user_agent,
                ip_address,
            ),
        )

        # Clean up stale sessions (older than 5 minutes)
        conn.execute(
            """
            DELETE FROM user_presence
            WHERE last_heartbeat < datetime('now', '-5 minutes')
        """
        )

        # Get collaborators on same entity
        collaborators = []
        if entity_type and entity_id:
            others = conn.execute(
                """
                SELECT user_id, session_id, display_name, avatar_color,
                       current_action, last_heartbeat
                FROM user_presence
                WHERE current_entity_type = ? AND current_entity_id = ?
                  AND session_id != ?
                  AND last_heartbeat > datetime('now', '-' || ? || ' seconds')
                ORDER BY last_heartbeat DESC
            """,
                (entity_type, entity_id, session_id, PRESENCE_TIMEOUT_SECONDS),
            ).fetchall()
            collaborators = [dict(o) for o in others]

        return jsonify(
            {
                "success": True,
                "session_id": session_id,
                "collaborators": collaborators,
                "collaborator_count": len(collaborators),
            }
        )


@app.route("/api/collaboration/presence", methods=["DELETE"])
@require_auth
def remove_presence():
    """Remove user presence (disconnect).

    Body:
        session_id: Session to remove (required)
    """
    data = request.get_json() or {}
    session_id = data.get("session_id")

    if not session_id:
        return api_error("session_id is required", 400, "missing_field")

    with get_db_connection() as conn:
        conn.execute(
            "DELETE FROM user_presence WHERE session_id = ?", (session_id,)
        )

    return jsonify({"success": True, "disconnected": session_id})


@app.route("/api/collaboration/active-users", methods=["GET"])
@require_auth
def get_active_users():
    """Get all currently active users.

    Query params:
        entity_type: Filter by entity type
        entity_id: Filter by entity ID
        include_idle: Include idle users (default true)
    """
    entity_type = request.args.get("entity_type")
    entity_id = request.args.get("entity_id", type=int)
    include_idle = request.args.get("include_idle", "true").lower() == "true"

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        conditions = [
            f"last_heartbeat > datetime('now', '-{PRESENCE_TIMEOUT_SECONDS} seconds')"
        ]
        params = []

        if entity_type:
            conditions.append("current_entity_type = ?")
            params.append(entity_type)
        if entity_id:
            conditions.append("current_entity_id = ?")
            params.append(entity_id)
        if not include_idle:
            conditions.append("current_action != 'idle'")

        users = conn.execute(
            """
            SELECT user_id, session_id, display_name, avatar_color, status,
                   current_entity_type, current_entity_id, current_action,
                   last_heartbeat, connected_at
            FROM user_presence
            WHERE {' AND '.join(conditions)}
            ORDER BY last_heartbeat DESC
        """,
            params,
        ).fetchall()

        # Group by user
        by_user = {}
        for u in users:
            uid = u["user_id"]
            if uid not in by_user:
                by_user[uid] = {
                    "user_id": uid,
                    "display_name": u["display_name"],
                    "avatar_color": u["avatar_color"],
                    "sessions": [],
                }
            by_user[uid]["sessions"].append(
                {
                    "session_id": u["session_id"],
                    "entity_type": u["current_entity_type"],
                    "entity_id": u["current_entity_id"],
                    "action": u["current_action"],
                    "last_heartbeat": u["last_heartbeat"],
                }
            )

        return jsonify(
            {
                "active_users": list(by_user.values()),
                "total_users": len(by_user),
                "total_sessions": len(users),
            }
        )


@app.route(
    "/api/collaboration/entity/<entity_type>/<int:entity_id>", methods=["GET"]
)
@require_auth
def get_entity_collaborators(entity_type, entity_id):
    """Get users currently viewing or editing a specific entity.

    Returns list of users with their actions (viewing/editing).
    """
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        users = conn.execute(
            """
            SELECT user_id, session_id, display_name, avatar_color,
                   current_action, last_heartbeat
            FROM user_presence
            WHERE current_entity_type = ? AND current_entity_id = ?
              AND last_heartbeat > datetime('now', '-{PRESENCE_TIMEOUT_SECONDS} seconds')
            ORDER BY
                CASE current_action
                    WHEN 'editing' THEN 1
                    WHEN 'viewing' THEN 2
                    ELSE 3
                END,
                last_heartbeat DESC
        """,
            (entity_type, entity_id),
        ).fetchall()

        collaborators = []
        for u in users:
            collaborators.append(
                {
                    "user_id": u["user_id"],
                    "display_name": u["display_name"],
                    "avatar_color": u["avatar_color"],
                    "action": u["current_action"],
                    "last_seen": u["last_heartbeat"],
                }
            )

        # Count by action
        viewing = sum(1 for c in collaborators if c["action"] == "viewing")
        editing = sum(1 for c in collaborators if c["action"] == "editing")

        return jsonify(
            {
                "entity_type": entity_type,
                "entity_id": entity_id,
                "collaborators": collaborators,
                "counts": {
                    "total": len(collaborators),
                    "viewing": viewing,
                    "editing": editing,
                },
            }
        )


# DUPLICATE REMOVED: @app.route('/api/collaboration/stats', methods=['GET'])
# DUPLICATE REMOVED: @require_auth
# DUPLICATE REMOVED: def get_collaboration_stats():
# DUPLICATE REMOVED:     """Get overall collaboration statistics."""
# DUPLICATE REMOVED:     with get_db_connection() as conn:
# DUPLICATE REMOVED:         conn.row_factory = sqlite3.Row

# Active users count
# DUPLICATE REMOVED:         active = conn.execute("""
# DUPLICATE REMOVED:             SELECT COUNT(DISTINCT user_id) as users,
# DUPLICATE REMOVED:                    COUNT(*) as sessions
# DUPLICATE REMOVED:             FROM user_presence
# DUPLICATE REMOVED:             WHERE last_heartbeat > datetime('now', '-{PRESENCE_TIMEOUT_SECONDS} seconds')
# DUPLICATE REMOVED:         """).fetchone()

# Users by action
# DUPLICATE REMOVED:         by_action = conn.execute("""
# DUPLICATE REMOVED:             SELECT current_action, COUNT(DISTINCT user_id) as users
# DUPLICATE REMOVED:             FROM user_presence
# DUPLICATE REMOVED:             WHERE last_heartbeat > datetime('now', '-{PRESENCE_TIMEOUT_SECONDS} seconds')
# DUPLICATE REMOVED:             GROUP BY current_action
# DUPLICATE REMOVED:         """).fetchall()

# Most active entities
# DUPLICATE REMOVED:         hot_entities = conn.execute("""
# DUPLICATE REMOVED:             SELECT current_entity_type, current_entity_id,
# DUPLICATE REMOVED:                    COUNT(DISTINCT user_id) as user_count,
# DUPLICATE REMOVED:                    GROUP_CONCAT(DISTINCT display_name) as users
# DUPLICATE REMOVED:             FROM user_presence
# DUPLICATE REMOVED:             WHERE current_entity_type IS NOT NULL
# DUPLICATE REMOVED:               AND last_heartbeat > datetime('now', '-{PRESENCE_TIMEOUT_SECONDS} seconds')
# DUPLICATE REMOVED:             GROUP BY current_entity_type, current_entity_id
# DUPLICATE REMOVED:             HAVING user_count > 1
# DUPLICATE REMOVED:             ORDER BY user_count DESC
# DUPLICATE REMOVED:             LIMIT 10
# DUPLICATE REMOVED:         """).fetchall()

# Recent activity (last hour)
# DUPLICATE REMOVED:         hourly = conn.execute("""
# DUPLICATE REMOVED:             SELECT strftime('%H:%M', connected_at) as time,
# DUPLICATE REMOVED:                    COUNT(DISTINCT user_id) as users
# DUPLICATE REMOVED:             FROM user_presence
# DUPLICATE REMOVED:             WHERE connected_at > datetime('now', '-1 hour')
# DUPLICATE REMOVED:             GROUP BY strftime('%H', connected_at)
# DUPLICATE REMOVED:             ORDER BY time
# DUPLICATE REMOVED:         """).fetchall()

# DUPLICATE REMOVED:         return jsonify({
# DUPLICATE REMOVED:             'current': {
# DUPLICATE REMOVED:                 'active_users': active['users'],
# DUPLICATE REMOVED:                 'active_sessions': active['sessions']
# DUPLICATE REMOVED:             },
# DUPLICATE REMOVED:             'by_action': {r['current_action'] or 'unknown': r['users'] for r in by_action},
# DUPLICATE REMOVED:             'hot_entities': [
# DUPLICATE REMOVED:                 {
# DUPLICATE REMOVED:                     'entity_type': e['current_entity_type'],
# DUPLICATE REMOVED:                     'entity_id': e['current_entity_id'],
# DUPLICATE REMOVED:                     'user_count': e['user_count'],
# DUPLICATE REMOVED:                     'users': e['users'].split(',') if e['users'] else []
# DUPLICATE REMOVED:                 }
# DUPLICATE REMOVED:                 for e in hot_entities
# DUPLICATE REMOVED:],
# DUPLICATE REMOVED:             'hourly_activity': [dict(h) for h in hourly]
# DUPLICATE REMOVED:         })


@app.route("/api/collaboration/broadcast", methods=["POST"])
@require_auth
def broadcast_to_collaborators():
    """Broadcast a message to collaborators on an entity.

    This stores a notification that collaborators can poll for.

    Body:
        entity_type: Target entity type (required)
        entity_id: Target entity ID (required)
        message_type: Type of message (required) - 'update', 'lock', 'unlock', 'comment', 'custom'
        message: Message content (optional)
        data: Additional data payload (optional)
    """
    data = request.get_json() or {}
    entity_type = data.get("entity_type")
    entity_id = data.get("entity_id")
    message_type = data.get("message_type")

    if not entity_type or not entity_id or not message_type:
        return api_error(
            "entity_type, entity_id, and message_type are required",
            400,
            "missing_field",
        )

    user_id = session.get("user", "anonymous")
    message = data.get("message", "")
    extra_data = json.dumps(data.get("data", {}))

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Get collaborators
        collaborators = conn.execute(
            """
            SELECT DISTINCT user_id
            FROM user_presence
            WHERE current_entity_type = ? AND current_entity_id = ?
              AND user_id != ?
              AND last_heartbeat > datetime('now', '-{PRESENCE_TIMEOUT_SECONDS} seconds')
        """,
            (entity_type, entity_id, user_id),
        ).fetchall()

        # Create notifications for each collaborator
        notified = 0
        for collab in collaborators:
            conn.execute(
                """
                INSERT INTO user_notifications
                    (user_id, title, message, notification_type, category,
                     entity_type, entity_id, created_at)
                VALUES (?, ?, ?, ?, 'collaboration', ?, ?, CURRENT_TIMESTAMP)
            """,
                (
                    collab["user_id"],
                    f"Collaboration: {message_type}",
                    message
                    or f"{user_id} performed {message_type} on {entity_type} #{entity_id}",
                    message_type,
                    entity_type,
                    entity_id,
                ),
            )
            notified += 1

        log_activity(
            conn,
            "collaboration_broadcast",
            entity_type,
            entity_id,
            f"Broadcast {message_type} to {notified} collaborators",
        )

        return jsonify(
            {
                "success": True,
                "message_type": message_type,
                "collaborators_notified": notified,
            }
        )


# ============================================================================
# USER PREFERENCES API
# ============================================================================


@app.route("/api/preferences", methods=["GET"])
@require_auth
def get_user_preferences():
    """Get user preferences for the current user."""
    try:
        user_id = session.get("user", "anonymous")
        category = request.args.get("category")

        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            if category:
                rows = conn.execute(
                    """
                    SELECT preference_key, preference_value, category, updated_at
                    FROM user_preferences
                    WHERE user_id = ? AND category = ?
                    ORDER BY preference_key
                """,
                    (user_id, category),
                ).fetchall()
            else:
                rows = conn.execute(
                    """
                    SELECT preference_key, preference_value, category, updated_at
                    FROM user_preferences
                    WHERE user_id = ?
                    ORDER BY category, preference_key
                """,
                    (user_id,),
                ).fetchall()

            preferences = {}
            for row in rows:
                key = row["preference_key"]
                value = row["preference_value"]
                # Try to parse JSON values
                try:
                    value = json.loads(value)
                except (json.JSONDecodeError, TypeError):
                    pass
                preferences[key] = {
                    "value": value,
                    "category": row["category"],
                    "updated_at": row["updated_at"],
                }

            return jsonify({"preferences": preferences})
    except Exception as e:
        logger.error(f"Failed to get preferences: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/preferences", methods=["PUT"])
@require_auth
def set_user_preference():
    """Set a user preference."""
    try:
        user_id = session.get("user", "anonymous")
        data = request.get_json()

        if not data.get("key"):
            return jsonify({"error": "Preference key is required"}), 400

        key = data["key"]
        value = data.get("value")
        category = data.get("category", "general")

        # Serialize value to JSON if it's not a string
        if not isinstance(value, str):
            value = json.dumps(value)

        with get_db_connection() as conn:
            conn.execute(
                """
                INSERT INTO user_preferences (user_id, preference_key, preference_value, category, updated_at)
                VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP)
                ON CONFLICT(user_id, preference_key) DO UPDATE SET
                    preference_value = excluded.preference_value,
                    category = excluded.category,
                    updated_at = CURRENT_TIMESTAMP
            """,
                (user_id, key, value, category),
            )
            conn.commit()

        log_activity(
            "set",
            "preference",
            None,
            f"{key}={value[:50] if value else 'null'}",
        )
        return jsonify({"success": True, "key": key})
    except Exception as e:
        logger.error(f"Failed to set preference: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/preferences/<key>", methods=["DELETE"])
@require_auth
def delete_user_preference(key):
    """Delete a user preference."""
    try:
        user_id = session.get("user", "anonymous")

        with get_db_connection() as conn:
            result = conn.execute(
                """
                DELETE FROM user_preferences
                WHERE user_id = ? AND preference_key = ?
            """,
                (user_id, key),
            )
            conn.commit()

            if result.rowcount == 0:
                return jsonify({"error": "Preference not found"}), 404

        log_activity("delete", "preference", None, key)
        return jsonify({"success": True, "key": key})
    except Exception as e:
        logger.error(f"Failed to delete preference: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/preferences/bulk", methods=["PUT"])
@require_auth
def set_bulk_preferences():
    """Set multiple preferences at once."""
    try:
        user_id = session.get("user", "anonymous")
        data = request.get_json()

        if not data.get("preferences") or not isinstance(
            data["preferences"], dict
        ):
            return jsonify({"error": "Preferences object is required"}), 400

        category = data.get("category", "general")

        with get_db_connection() as conn:
            for key, value in data["preferences"].items():
                # Serialize value to JSON if it's not a string
                if not isinstance(value, str):
                    value = json.dumps(value)

                conn.execute(
                    """
                    INSERT INTO user_preferences (user_id, preference_key, preference_value, category, updated_at)
                    VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP)
                    ON CONFLICT(user_id, preference_key) DO UPDATE SET
                        preference_value = excluded.preference_value,
                        category = excluded.category,
                        updated_at = CURRENT_TIMESTAMP
                """,
                    (user_id, key, value, category),
                )
            conn.commit()

        log_activity(
            "set",
            "preferences",
            None,
            f"bulk update: {len(data['preferences'])} items",
        )
        return jsonify({"success": True, "count": len(data["preferences"])})
    except Exception as e:
        logger.error(f"Failed to set bulk preferences: {e}")
        return jsonify({"error": str(e)}), 500


# ============================================================================
# DASHBOARD CONFIG API
# ============================================================================

DEFAULT_DASHBOARD_CONFIG = {
    "theme": "dark",
    "panels": {
        "projects": {
            "visible": True,
            "order": 1,
            "collapsed": False,
            "width": "full",
        },
        "milestones": {
            "visible": True,
            "order": 2,
            "collapsed": False,
            "width": "half",
        },
        "features": {
            "visible": True,
            "order": 3,
            "collapsed": False,
            "width": "half",
        },
        "bugs": {
            "visible": True,
            "order": 4,
            "collapsed": False,
            "width": "half",
        },
        "errors": {
            "visible": True,
            "order": 5,
            "collapsed": False,
            "width": "half",
        },
        "tasks": {
            "visible": True,
            "order": 6,
            "collapsed": False,
            "width": "full",
        },
        "tmux": {
            "visible": True,
            "order": 7,
            "collapsed": True,
            "width": "full",
        },
        "nodes": {
            "visible": True,
            "order": 8,
            "collapsed": True,
            "width": "full",
        },
    },
    "sidebar": {"collapsed": False, "width": 250},
    "refresh_interval": 30,
    "notifications": {"enabled": True, "sound": False},
    "table_settings": {"rows_per_page": 25},
}


@app.route("/api/dashboard/config", methods=["GET"])
@require_auth
def get_dashboard_config():
    """Get user's dashboard configuration."""
    user_id = session.get("user", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        row = conn.execute(
            "SELECT preference_value, updated_at FROM user_preferences WHERE user_id=? AND preference_key='dashboard_layout' AND category='dashboard'",
            (user_id,),
        ).fetchone()
        if row:
            try:
                config = json.loads(row["preference_value"])
                merged = {
                    **DEFAULT_DASHBOARD_CONFIG,
                    **{k: v for k, v in config.items() if k != "panels"},
                }
                if "panels" in config:
                    merged["panels"] = {
                        **DEFAULT_DASHBOARD_CONFIG["panels"],
                        **config["panels"],
                    }
                return jsonify(
                    {
                        "config": merged,
                        "updated_at": row["updated_at"],
                        "is_default": False,
                    }
                )
            except Exception:
                pass
        return jsonify(
            {"config": DEFAULT_DASHBOARD_CONFIG, "is_default": True}
        )


@app.route("/api/dashboard/config", methods=["PUT"])
@require_auth
def save_dashboard_config():
    """Save user's dashboard configuration."""
    user_id = session.get("user", "anonymous")
    data = request.get_json() or {}
    config = data.get("config", data)
    if not isinstance(config, dict):
        return jsonify({"error": "Config must be an object"}), 400
    with get_db_connection() as conn:
        conn.execute(
            "INSERT INTO user_preferences (user_id,preference_key,preference_value,category,updated_at) VALUES (?,'dashboard_layout',?,'dashboard',CURRENT_TIMESTAMP) ON CONFLICT(user_id,preference_key) DO UPDATE SET preference_value=excluded.preference_value,updated_at=CURRENT_TIMESTAMP",
            (user_id, json.dumps(config)),
        )
    log_activity("save_dashboard_config", "dashboard", None, f"user={user_id}")
    return jsonify({"success": True})


@app.route("/api/dashboard/config/reset", methods=["POST"])
@require_auth
def reset_dashboard_config():
    """Reset dashboard configuration to defaults."""
    user_id = session.get("user", "anonymous")
    with get_db_connection() as conn:
        conn.execute(
            "DELETE FROM user_preferences WHERE user_id=? AND preference_key='dashboard_layout' AND category='dashboard'",
            (user_id,),
        )
    return jsonify({"success": True, "config": DEFAULT_DASHBOARD_CONFIG})


@app.route("/api/dashboard/config/layouts", methods=["GET"])
@require_auth
def get_saved_layouts():
    """Get all saved dashboard layouts."""
    user_id = session.get("user", "anonymous")
    layouts = []
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        for row in conn.execute(
            "SELECT preference_key,preference_value,updated_at FROM user_preferences WHERE user_id=? AND category='dashboard_layout' ORDER BY updated_at DESC",
            (user_id,),
        ).fetchall():
            try:
                layouts.append(
                    {
                        "name": row["preference_key"],
                        "config": json.loads(row["preference_value"]),
                        "updated_at": row["updated_at"],
                        "is_shared": False,
                    }
                )
            except Exception:
                continue
        for row in conn.execute(
            "SELECT preference_key,preference_value,updated_at FROM user_preferences WHERE user_id='shared' AND category='dashboard_layout'"
        ).fetchall():
            try:
                layouts.append(
                    {
                        "name": row["preference_key"],
                        "config": json.loads(row["preference_value"]),
                        "updated_at": row["updated_at"],
                        "is_shared": True,
                    }
                )
            except Exception:
                continue
    return jsonify({"layouts": layouts, "count": len(layouts)})


@app.route("/api/dashboard/config/layouts", methods=["POST"])
@require_auth
def save_named_layout():
    """Save a named dashboard layout."""
    user_id = session.get("user", "anonymous")
    data = request.get_json() or {}
    name = (data.get("name") or "").strip()
    if not name:
        return jsonify({"error": "Layout name is required"}), 400
    config = data.get("config")
    if not config or not isinstance(config, dict):
        return jsonify({"error": "Config required"}), 400
    target_user = "shared" if data.get("shared") else user_id
    with get_db_connection() as conn:
        existing = conn.execute(
            "SELECT 1 FROM user_preferences WHERE user_id=? AND preference_key=? AND category='dashboard_layout'",
            (target_user, name),
        ).fetchone()
        conn.execute(
            "INSERT INTO user_preferences (user_id,preference_key,preference_value,category,updated_at) VALUES (?,?,?,'dashboard_layout',CURRENT_TIMESTAMP) ON CONFLICT(user_id,preference_key) DO UPDATE SET preference_value=excluded.preference_value,updated_at=CURRENT_TIMESTAMP",
            (target_user, name, json.dumps(config)),
        )
    log_activity("save_layout", "dashboard_layout", None, f"name={name}")
    return jsonify({"success": True, "name": name, "is_new": not existing})


@app.route("/api/dashboard/config/layouts/<layout_name>", methods=["GET"])
@require_auth
def get_named_layout(layout_name):
    """Get a specific named layout."""
    user_id = session.get("user", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        row = conn.execute(
            "SELECT preference_value,updated_at FROM user_preferences WHERE user_id=? AND preference_key=? AND category='dashboard_layout'",
            (user_id, layout_name),
        ).fetchone()
        if row:
            return jsonify(
                {
                    "name": layout_name,
                    "config": json.loads(row["preference_value"]),
                    "updated_at": row["updated_at"],
                    "is_shared": False,
                }
            )
        row = conn.execute(
            "SELECT preference_value,updated_at FROM user_preferences WHERE user_id='shared' AND preference_key=? AND category='dashboard_layout'",
            (layout_name,),
        ).fetchone()
        if row:
            return jsonify(
                {
                    "name": layout_name,
                    "config": json.loads(row["preference_value"]),
                    "updated_at": row["updated_at"],
                    "is_shared": True,
                }
            )
        return jsonify({"error": "Layout not found"}), 404


@app.route("/api/dashboard/config/layouts/<layout_name>", methods=["DELETE"])
@require_auth
def delete_named_layout(layout_name):
    """Delete a named dashboard layout."""
    user_id = session.get("user", "anonymous")
    with get_db_connection() as conn:
        result = conn.execute(
            "DELETE FROM user_preferences WHERE user_id=? AND preference_key=? AND category='dashboard_layout'",
            (user_id, layout_name),
        )
        if result.rowcount == 0:
            return jsonify({"error": "Layout not found"}), 404
    log_activity(
        "delete_layout", "dashboard_layout", None, f"name={layout_name}"
    )
    return jsonify({"success": True, "name": layout_name})


@app.route(
    "/api/dashboard/config/layouts/<layout_name>/apply", methods=["POST"]
)
@require_auth
def apply_named_layout(layout_name):
    """Apply a saved layout as the current dashboard configuration."""
    user_id = session.get("user", "anonymous")
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        row = conn.execute(
            "SELECT preference_value FROM user_preferences WHERE user_id=? AND preference_key=? AND category='dashboard_layout'",
            (user_id, layout_name),
        ).fetchone()
        if not row:
            row = conn.execute(
                "SELECT preference_value FROM user_preferences WHERE user_id='shared' AND preference_key=? AND category='dashboard_layout'",
                (layout_name,),
            ).fetchone()
        if not row:
            return jsonify({"error": "Layout not found"}), 404
        config = row["preference_value"]
        conn.execute(
            "INSERT INTO user_preferences (user_id,preference_key,preference_value,category,updated_at) VALUES (?,'dashboard_layout',?,'dashboard',CURRENT_TIMESTAMP) ON CONFLICT(user_id,preference_key) DO UPDATE SET preference_value=excluded.preference_value,updated_at=CURRENT_TIMESTAMP",
            (user_id, config),
        )
    log_activity("apply_layout", "dashboard", None, f"layout={layout_name}")
    return jsonify(
        {
            "success": True,
            "applied_layout": layout_name,
            "config": json.loads(config),
        }
    )


@app.route("/api/dashboard/config/defaults", methods=["GET"])
@require_auth
def get_dashboard_defaults():
    """Get default dashboard configuration."""
    return jsonify({"config": DEFAULT_DASHBOARD_CONFIG})


@app.route("/api/dashboard/config/panels", methods=["GET"])
@require_auth
def get_available_panels():
    """Get list of available dashboard panels."""
    panels = [
        {"id": "projects", "name": "Projects", "icon": "folder"},
        {"id": "milestones", "name": "Milestones", "icon": "flag"},
        {"id": "features", "name": "Features", "icon": "star"},
        {"id": "bugs", "name": "Bugs", "icon": "bug"},
        {"id": "errors", "name": "Errors", "icon": "alert-circle"},
        {"id": "tasks", "name": "Task Queue", "icon": "list"},
        {"id": "tmux", "name": "tmux Sessions", "icon": "terminal"},
        {"id": "nodes", "name": "Cluster Nodes", "icon": "server"},
        {"id": "workers", "name": "Workers", "icon": "cpu"},
        {"id": "activity", "name": "Activity Log", "icon": "activity"},
    ]
    return jsonify({"panels": panels})


# ============================================================================
# DASHBOARD ONBOARDING TOUR API
# ============================================================================

# Default tour steps for the dashboard
DEFAULT_TOUR_STEPS = [
    {
        "id": "welcome",
        "title": "Welcome to Architect Dashboard",
        "content": "This tour will guide you through the main features of the dashboard. Click Next to continue.",
        "target": None,
        "placement": "center",
        "order": 1,
    },
    {
        "id": "sidebar",
        "title": "Navigation Sidebar",
        "content": "Use the sidebar to navigate between different sections. You can collapse it by clicking the toggle button.",
        "target": "#sidebar",
        "placement": "right",
        "order": 2,
    },
    {
        "id": "projects",
        "title": "Projects Panel",
        "content": "View and manage your projects here. Click on a project to see its milestones, features, and bugs.",
        "target": "#projects-panel",
        "placement": "bottom",
        "order": 3,
    },
    {
        "id": "milestones",
        "title": "Milestones",
        "content": "Track project milestones with target dates. Milestones group related features and bugs together.",
        "target": "#milestones-panel",
        "placement": "bottom",
        "order": 4,
    },
    {
        "id": "features",
        "title": "Features",
        "content": "Create and track features. Assign them to milestones and monitor their progress through different stages.",
        "target": "#features-panel",
        "placement": "bottom",
        "order": 5,
    },
    {
        "id": "bugs",
        "title": "Bug Tracking",
        "content": "Report and manage bugs. Set severity levels and track resolution status.",
        "target": "#bugs-panel",
        "placement": "bottom",
        "order": 6,
    },
    {
        "id": "errors",
        "title": "Error Aggregation",
        "content": "Errors from all nodes are collected and grouped here. You can convert recurring errors into bug reports.",
        "target": "#errors-panel",
        "placement": "top",
        "order": 7,
    },
    {
        "id": "tasks",
        "title": "Task Queue",
        "content": "Background tasks are managed here. Workers process tasks asynchronously.",
        "target": "#tasks-panel",
        "placement": "top",
        "order": 8,
    },
    {
        "id": "tmux",
        "title": "tmux Sessions",
        "content": "Manage tmux sessions across nodes. Send commands and capture output from active sessions.",
        "target": "#tmux-panel",
        "placement": "top",
        "order": 9,
    },
    {
        "id": "nodes",
        "title": "Cluster Nodes",
        "content": "Monitor your distributed cluster nodes. View CPU, memory, and disk metrics.",
        "target": "#nodes-panel",
        "placement": "top",
        "order": 10,
    },
    {
        "id": "complete",
        "title": "Tour Complete!",
        "content": "You now know the basics of the Architect Dashboard. You can restart this tour anytime from the help menu.",
        "target": None,
        "placement": "center",
        "order": 11,
    },
]


@app.route("/api/dashboard/tour", methods=["GET"])
@require_auth
def get_tour_steps():
    """Get onboarding tour steps.

    Returns the list of tour steps with their content and targets.
    Can be customized per user if custom steps exist.

    Query params:
        include_custom: Include user's custom tour steps (default true)
    """
    user_id = session.get("user", "anonymous")
    include_custom = (
        request.args.get("include_custom", "true").lower() == "true"
    )

    steps = list(DEFAULT_TOUR_STEPS)

    if include_custom:
        try:
            with get_db_connection() as conn:
                conn.row_factory = sqlite3.Row
                row = conn.execute(
                    """
                    SELECT preference_value FROM user_preferences
                    WHERE user_id = ? AND preference_key = 'custom_tour_steps'
                    AND category = 'tour'
                """,
                    (user_id,),
                ).fetchone()

                if row:
                    custom_steps = json.loads(row["preference_value"])
                    if isinstance(custom_steps, list):
                        # Merge custom steps - custom steps can override or add
                        # new ones
                        custom_by_id = {
                            s["id"]: s for s in custom_steps if "id" in s
                        }
                        for i, step in enumerate(steps):
                            if step["id"] in custom_by_id:
                                steps[i] = {**step, **custom_by_id[step["id"]]}
                                del custom_by_id[step["id"]]
                        # Add remaining custom steps
                        steps.extend(custom_by_id.values())
                        steps.sort(key=lambda s: s.get("order", 999))
        except Exception as e:
            logger.warning(f"Failed to load custom tour steps: {e}")

    return jsonify(
        {"steps": steps, "total_steps": len(steps), "version": "1.0"}
    )


@app.route("/api/dashboard/tour/progress", methods=["GET"])
@require_auth
def get_tour_progress():
    """Get user's tour progress.

    Returns the current step, completion status, and history.
    """
    user_id = session.get("user", "anonymous")

    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            row = conn.execute(
                """
                SELECT preference_value, updated_at FROM user_preferences
                WHERE user_id = ? AND preference_key = 'tour_progress'
                AND category = 'tour'
            """,
                (user_id,),
            ).fetchone()

            if row:
                progress = json.loads(row["preference_value"])
                return jsonify(
                    {"progress": progress, "updated_at": row["updated_at"]}
                )

            # Default progress for new users
            default_progress = {
                "current_step": 0,
                "completed_steps": [],
                "is_complete": False,
                "skipped": False,
                "started_at": None,
                "completed_at": None,
            }
            return jsonify({"progress": default_progress, "is_new_user": True})
    except Exception as e:
        logger.error(f"Failed to get tour progress: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/dashboard/tour/progress", methods=["PUT"])
@require_auth
def update_tour_progress():
    """Update user's tour progress.

    Request body:
        current_step: Current step index (int)
        completed_steps: List of completed step IDs
        skipped: Whether the tour was skipped
    """
    user_id = session.get("user", "anonymous")
    data = request.get_json() or {}

    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            # Get existing progress
            row = conn.execute(
                """
                SELECT preference_value FROM user_preferences
                WHERE user_id = ? AND preference_key = 'tour_progress'
                AND category = 'tour'
            """,
                (user_id,),
            ).fetchone()

            if row:
                progress = json.loads(row["preference_value"])
            else:
                progress = {
                    "current_step": 0,
                    "completed_steps": [],
                    "is_complete": False,
                    "skipped": False,
                    "started_at": datetime.now().isoformat(),
                    "completed_at": None,
                }

            # Update progress with provided data
            if "current_step" in data:
                progress["current_step"] = data["current_step"]
            if "completed_steps" in data:
                progress["completed_steps"] = data["completed_steps"]
            if "skipped" in data:
                progress["skipped"] = data["skipped"]

            # Save progress
            conn.execute(
                """
                INSERT INTO user_preferences (user_id, preference_key, preference_value, category, updated_at)
                VALUES (?, 'tour_progress', ?, 'tour', CURRENT_TIMESTAMP)
                ON CONFLICT(user_id, preference_key) DO UPDATE SET
                    preference_value = excluded.preference_value,
                    updated_at = CURRENT_TIMESTAMP
            """,
                (user_id, json.dumps(progress)),
            )
            conn.commit()

        log_activity(
            "update_tour_progress",
            "tour",
            None,
            f"step={progress.get('current_step')}",
        )
        return jsonify({"success": True, "progress": progress})
    except Exception as e:
        logger.error(f"Failed to update tour progress: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/dashboard/tour/complete", methods=["POST"])
@require_auth
def complete_tour():
    """Mark the tour as completed.

    Request body:
        feedback: Optional feedback about the tour
        rating: Optional rating (1-5)
    """
    user_id = session.get("user", "anonymous")
    data = request.get_json() or {}

    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            # Get existing progress
            row = conn.execute(
                """
                SELECT preference_value FROM user_preferences
                WHERE user_id = ? AND preference_key = 'tour_progress'
                AND category = 'tour'
            """,
                (user_id,),
            ).fetchone()

            if row:
                progress = json.loads(row["preference_value"])
            else:
                progress = {
                    "current_step": len(DEFAULT_TOUR_STEPS) - 1,
                    "completed_steps": [s["id"] for s in DEFAULT_TOUR_STEPS],
                    "started_at": datetime.now().isoformat(),
                }

            # Mark as complete
            progress["is_complete"] = True
            progress["completed_at"] = datetime.now().isoformat()
            progress["current_step"] = len(DEFAULT_TOUR_STEPS) - 1

            # Add feedback if provided
            if data.get("feedback"):
                progress["feedback"] = data["feedback"]
            if data.get("rating"):
                progress["rating"] = min(max(int(data["rating"]), 1), 5)

            # Save progress
            conn.execute(
                """
                INSERT INTO user_preferences (user_id, preference_key, preference_value, category, updated_at)
                VALUES (?, 'tour_progress', ?, 'tour', CURRENT_TIMESTAMP)
                ON CONFLICT(user_id, preference_key) DO UPDATE SET
                    preference_value = excluded.preference_value,
                    updated_at = CURRENT_TIMESTAMP
            """,
                (user_id, json.dumps(progress)),
            )
            conn.commit()

        log_activity("complete_tour", "tour", None, f"user={user_id}")
        return jsonify(
            {
                "success": True,
                "message": "Tour completed successfully",
                "progress": progress,
            }
        )
    except Exception as e:
        logger.error(f"Failed to complete tour: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/dashboard/tour/reset", methods=["POST"])
@require_auth
def reset_tour():
    """Reset tour progress for the user.

    This allows the user to restart the onboarding tour.
    """
    user_id = session.get("user", "anonymous")

    try:
        with get_db_connection() as conn:
            conn.execute(
                """
                DELETE FROM user_preferences
                WHERE user_id = ? AND preference_key = 'tour_progress'
                AND category = 'tour'
            """,
                (user_id,),
            )
            conn.commit()

        log_activity("reset_tour", "tour", None, f"user={user_id}")
        return jsonify(
            {
                "success": True,
                "message": "Tour progress reset. You can start the tour again.",
            }
        )
    except Exception as e:
        logger.error(f"Failed to reset tour: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/dashboard/tour/skip", methods=["POST"])
@require_auth
def skip_tour():
    """Skip the onboarding tour.

    Marks the tour as skipped so it won't show again automatically.
    """
    user_id = session.get("user", "anonymous")

    try:
        progress = {
            "current_step": 0,
            "completed_steps": [],
            "is_complete": False,
            "skipped": True,
            "skipped_at": datetime.now().isoformat(),
        }

        with get_db_connection() as conn:
            conn.execute(
                """
                INSERT INTO user_preferences (user_id, preference_key, preference_value, category, updated_at)
                VALUES (?, 'tour_progress', ?, 'tour', CURRENT_TIMESTAMP)
                ON CONFLICT(user_id, preference_key) DO UPDATE SET
                    preference_value = excluded.preference_value,
                    updated_at = CURRENT_TIMESTAMP
            """,
                (user_id, json.dumps(progress)),
            )
            conn.commit()

        log_activity("skip_tour", "tour", None, f"user={user_id}")
        return jsonify(
            {
                "success": True,
                "message": "Tour skipped. You can restart it anytime from the help menu.",
            }
        )
    except Exception as e:
        logger.error(f"Failed to skip tour: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/dashboard/tour/steps", methods=["POST"])
@require_auth
def add_custom_tour_step():
    """Add a custom tour step for the user.

    Request body:
        id: Unique step ID
        title: Step title
        content: Step content/description
        target: CSS selector for the target element
        placement: Tooltip placement (top, bottom, left, right, center)
        order: Step order (optional, defaults to end)
    """
    user_id = session.get("user", "anonymous")
    data = request.get_json() or {}

    required = ["id", "title", "content"]
    missing = [f for f in required if not data.get(f)]
    if missing:
        return (
            jsonify(
                {"error": f'Missing required fields: {", ".join(missing)}'}
            ),
            400,
        )

    try:
        new_step = {
            "id": data["id"],
            "title": data["title"],
            "content": data["content"],
            "target": data.get("target"),
            "placement": data.get("placement", "bottom"),
            "order": data.get("order", 100),
            "custom": True,
        }

        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            # Get existing custom steps
            row = conn.execute(
                """
                SELECT preference_value FROM user_preferences
                WHERE user_id = ? AND preference_key = 'custom_tour_steps'
                AND category = 'tour'
            """,
                (user_id,),
            ).fetchone()

            if row:
                custom_steps = json.loads(row["preference_value"])
            else:
                custom_steps = []

            # Check for duplicate ID
            existing_ids = [s["id"] for s in custom_steps]
            if new_step["id"] in existing_ids:
                return (
                    jsonify(
                        {
                            "error": f'Step with ID {new_step["id"]} already exists'
                        }
                    ),
                    409,
                )

            custom_steps.append(new_step)
            custom_steps.sort(key=lambda s: s.get("order", 999))

            conn.execute(
                """
                INSERT INTO user_preferences (user_id, preference_key, preference_value, category, updated_at)
                VALUES (?, 'custom_tour_steps', ?, 'tour', CURRENT_TIMESTAMP)
                ON CONFLICT(user_id, preference_key) DO UPDATE SET
                    preference_value = excluded.preference_value,
                    updated_at = CURRENT_TIMESTAMP
            """,
                (user_id, json.dumps(custom_steps)),
            )
            conn.commit()

        log_activity(
            "add_tour_step", "tour", None, f"step_id={new_step['id']}"
        )
        return jsonify(
            {
                "success": True,
                "step": new_step,
                "total_custom_steps": len(custom_steps),
            }
        )
    except Exception as e:
        logger.error(f"Failed to add custom tour step: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/dashboard/tour/steps/<step_id>", methods=["DELETE"])
@require_auth
def delete_custom_tour_step(step_id):
    """Delete a custom tour step.

    Note: Only custom steps can be deleted, not default steps.
    """
    user_id = session.get("user", "anonymous")

    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            row = conn.execute(
                """
                SELECT preference_value FROM user_preferences
                WHERE user_id = ? AND preference_key = 'custom_tour_steps'
                AND category = 'tour'
            """,
                (user_id,),
            ).fetchone()

            if not row:
                return jsonify({"error": "No custom steps found"}), 404

            custom_steps = json.loads(row["preference_value"])
            original_len = len(custom_steps)
            custom_steps = [s for s in custom_steps if s.get("id") != step_id]

            if len(custom_steps) == original_len:
                return (
                    jsonify({"error": f"Custom step {step_id} not found"}),
                    404,
                )

            conn.execute(
                """
                UPDATE user_preferences SET preference_value = ?, updated_at = CURRENT_TIMESTAMP
                WHERE user_id = ? AND preference_key = 'custom_tour_steps' AND category = 'tour'
            """,
                (json.dumps(custom_steps), user_id),
            )
            conn.commit()

        log_activity("delete_tour_step", "tour", None, f"step_id={step_id}")
        return jsonify(
            {
                "success": True,
                "deleted_step_id": step_id,
                "remaining_custom_steps": len(custom_steps),
            }
        )
    except Exception as e:
        logger.error(f"Failed to delete custom tour step: {e}")
        return jsonify({"error": str(e)}), 500


# ============================================================================
# GLOBAL SEARCH API
# ============================================================================


@app.route("/api/search", methods=["GET"])
@require_auth
def global_search():
    """Search across all entities in the system.

    Searches projects, milestones, features, bugs, tasks, errors, and tmux sessions.
    Results are grouped by entity type and ranked by relevance.

    Query params:
        q: Search query (required, min 2 chars)
        types: Comma-separated entity types to search (default: all)
               Options: projects, milestones, features, bugs, tasks, errors, sessions
        limit: Max results per type (default 10, max 50)
        include_archived: Include archived items (default false)
    """
    query = request.args.get("q", "").strip()
    if len(query) < 2:
        return (
            jsonify({"error": "Search query must be at least 2 characters"}),
            400,
        )

    types_param = request.args.get("types", "")
    requested_types = (
        [t.strip() for t in types_param.split(",") if t.strip()]
        if types_param
        else None
    )
    limit = min(request.args.get("limit", 10, type=int), 50)
    include_archived = (
        request.args.get("include_archived", "").lower() == "true"
    )

    all_types = [
        "projects",
        "milestones",
        "features",
        "bugs",
        "tasks",
        "errors",
        "sessions",
    ]
    search_types = requested_types if requested_types else all_types

    invalid_types = [t for t in search_types if t not in all_types]
    if invalid_types:
        return (
            jsonify({"error": f'Invalid types: {", ".join(invalid_types)}'}),
            400,
        )

    results = {}
    total_count = 0
    search_pattern = f"%{query}%"

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        if "projects" in search_types:
            q = "SELECT id, name, description, status, source_path, created_at, 'project' as entity_type FROM projects WHERE (name LIKE ? OR description LIKE ? OR source_path LIKE ?)"
            if not include_archived:
                q += " AND status != 'archived'"
            q += " ORDER BY CASE WHEN name LIKE ? THEN 0 ELSE 1 END, name LIMIT ?"
            rows = conn.execute(
                q,
                [
                    search_pattern,
                    search_pattern,
                    search_pattern,
                    search_pattern,
                    limit,
                ],
            ).fetchall()
            results["projects"] = [dict(r) for r in rows]
            total_count += len(results["projects"])

        if "milestones" in search_types:
            q = "SELECT m.id, m.name, m.description, m.status, m.target_date, m.project_id, p.name as project_name, 'milestone' as entity_type FROM milestones m LEFT JOIN projects p ON m.project_id = p.id WHERE (m.name LIKE ? OR m.description LIKE ?)"
            if not include_archived:
                q += " AND m.status != 'archived'"
            q += " ORDER BY CASE WHEN m.name LIKE ? THEN 0 ELSE 1 END, m.name LIMIT ?"
            rows = conn.execute(
                q, [search_pattern, search_pattern, search_pattern, limit]
            ).fetchall()
            results["milestones"] = [dict(r) for r in rows]
            total_count += len(results["milestones"])

        if "features" in search_types:
            q = "SELECT f.id, f.name, f.description, f.status, f.priority, f.project_id, p.name as project_name, f.milestone_id, 'feature' as entity_type FROM features f LEFT JOIN projects p ON f.project_id = p.id WHERE (f.name LIKE ? OR f.description LIKE ?)"
            if not include_archived:
                q += " AND f.status != 'archived'"
            q += " ORDER BY CASE WHEN f.name LIKE ? THEN 0 ELSE 1 END, f.priority DESC LIMIT ?"
            rows = conn.execute(
                q, [search_pattern, search_pattern, search_pattern, limit]
            ).fetchall()
            results["features"] = [dict(r) for r in rows]
            total_count += len(results["features"])

        if "bugs" in search_types:
            q = "SELECT b.id, b.title, b.description, b.status, b.severity, b.priority, b.project_id, p.name as project_name, 'bug' as entity_type FROM bugs b LEFT JOIN projects p ON b.project_id = p.id WHERE (b.title LIKE ? OR b.description LIKE ?)"
            if not include_archived:
                q += " AND b.status != 'archived'"
            q += " ORDER BY CASE WHEN b.title LIKE ? THEN 0 ELSE 1 END, b.severity DESC LIMIT ?"
            rows = conn.execute(
                q, [search_pattern, search_pattern, search_pattern, limit]
            ).fetchall()
            results["bugs"] = [dict(r) for r in rows]
            total_count += len(results["bugs"])

        if "tasks" in search_types:
            q = "SELECT id, task_type, task_data, status, priority, created_at, assigned_worker, 'task' as entity_type FROM task_queue WHERE (task_type LIKE ? OR task_data LIKE ?)"
            q += " ORDER BY created_at DESC LIMIT ?"
            rows = conn.execute(
                q, [search_pattern, search_pattern, limit]
            ).fetchall()
            results["tasks"] = [dict(r) for r in rows]
            total_count += len(results["tasks"])

        if "errors" in search_types:
            q = "SELECT id, error_type, message, source, status, severity, occurrence_count, 'error' as entity_type FROM errors WHERE (error_type LIKE ? OR message LIKE ? OR source LIKE ?)"
            if not include_archived:
                q += " AND status != 'archived'"
            q += " ORDER BY occurrence_count DESC LIMIT ?"
            rows = conn.execute(
                q, [search_pattern, search_pattern, search_pattern, limit]
            ).fetchall()
            results["errors"] = [dict(r) for r in rows]
            total_count += len(results["errors"])

        if "sessions" in search_types:
            q = "SELECT ts.id, ts.session_name, ts.node_id, ts.status, ts.last_activity, 'session' as entity_type FROM tmux_sessions ts WHERE ts.session_name LIKE ? ORDER BY ts.last_activity DESC LIMIT ?"
            rows = conn.execute(q, [search_pattern, limit]).fetchall()
            results["sessions"] = [dict(r) for r in rows]
            total_count += len(results["sessions"])

    return jsonify(
        {
            "query": query,
            "total_count": total_count,
            "results": results,
            "searched_types": search_types,
            "limit_per_type": limit,
        }
    )


@app.route("/api/search/quick", methods=["GET"])
@require_auth
def quick_search():
    """Quick search for autocomplete - returns minimal data.

    Query params:
        q: Search query (required, min 2 chars)
        limit: Max total results (default 20, max 50)
    """
    query = request.args.get("q", "").strip()
    if len(query) < 2:
        return (
            jsonify({"error": "Search query must be at least 2 characters"}),
            400,
        )

    limit = min(request.args.get("limit", 20, type=int), 50)
    search_pattern = f"%{query}%"
    exact_pattern = f"{query}%"

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        quick_query = """
            SELECT id, name as title, 'project' as type, status FROM projects WHERE name LIKE ? AND status != 'archived'
            UNION ALL
            SELECT id, name as title, 'milestone' as type, status FROM milestones WHERE name LIKE ? AND status != 'archived'
            UNION ALL
            SELECT id, name as title, 'feature' as type, status FROM features WHERE name LIKE ? AND status != 'archived'
            UNION ALL
            SELECT id, title, 'bug' as type, status FROM bugs WHERE title LIKE ? AND status != 'archived'
            UNION ALL
            SELECT id, session_name as title, 'session' as type, status FROM tmux_sessions WHERE session_name LIKE ?
            ORDER BY CASE WHEN title LIKE ? THEN 0 ELSE 1 END, title LIMIT ?
        """
        rows = conn.execute(
            quick_query, [search_pattern] * 5 + [exact_pattern, limit]
        ).fetchall()
        results = [
            {
                "id": r["id"],
                "title": r["title"],
                "type": r["type"],
                "status": r["status"],
            }
            for r in rows
        ]

    return jsonify({"query": query, "results": results, "count": len(results)})


# ============================================================================
# RATE LIMITING DASHBOARD API
# ============================================================================


@app.route("/api/rate-limits", methods=["GET"])
@require_auth
def get_rate_limit_dashboard():
    """Get rate limiting dashboard overview."""
    now = time.time()
    window_start = now - 60

    with _rate_limit_lock:
        # Current active IPs and their request counts
        active_ips = {}
        endpoint_stats = {}

        for key, timestamps in _rate_limit_store.items():
            recent = [ts for ts in timestamps if ts > window_start]
            if not recent:
                continue

            parts = key.split(":", 1)
            ip = parts[0]
            endpoint = parts[1] if len(parts) > 1 else "unknown"

            if ip not in active_ips:
                active_ips[ip] = {"total_requests": 0, "endpoints": {}}
            active_ips[ip]["total_requests"] += len(recent)
            active_ips[ip]["endpoints"][endpoint] = len(recent)

            if endpoint not in endpoint_stats:
                endpoint_stats[endpoint] = {"requests": 0, "unique_ips": set()}
            endpoint_stats[endpoint]["requests"] += len(recent)
            endpoint_stats[endpoint]["unique_ips"].add(ip)

        # Convert sets to counts
        for ep in endpoint_stats:
            endpoint_stats[ep]["unique_ips"] = len(
                endpoint_stats[ep]["unique_ips"]
            )

        # Get top IPs by request count
        top_ips = sorted(
            active_ips.items(),
            key=lambda x: x[1]["total_requests"],
            reverse=True,
        )[:20]

        # Violation summary
        violation_ips = {}
        recent_violations = []
        violation_cutoff = now - 3600  # Last hour

        for ip, violations in _rate_limit_violations.items():
            recent = [
                v for v in violations if v["timestamp"] > violation_cutoff
            ]
            if recent:
                violation_ips[ip] = len(recent)
                for v in recent[-10:]:  # Last 10 per IP
                    recent_violations.append(
                        {
                            "ip": ip,
                            "endpoint": v["endpoint"],
                            "limit": v["limit"],
                            "timestamp": datetime.fromtimestamp(
                                v["timestamp"]
                            ).isoformat(),
                            "seconds_ago": int(now - v["timestamp"]),
                        }
                    )

        recent_violations.sort(key=lambda x: x["seconds_ago"])
        recent_violations = recent_violations[:50]

        # Stats summary
        uptime_seconds = now - _rate_limit_stats["start_time"]
        stats = {
            "total_requests": _rate_limit_stats["total_requests"],
            "total_violations": _rate_limit_stats["total_violations"],
            "violation_rate": round(
                _rate_limit_stats["total_violations"]
                / max(_rate_limit_stats["total_requests"], 1)
                * 100,
                2,
            ),
            "uptime_seconds": int(uptime_seconds),
            "requests_per_minute": round(
                _rate_limit_stats["total_requests"]
                / max(uptime_seconds / 60, 1),
                2,
            ),
            "active_ips": len(active_ips),
            "active_endpoints": len(endpoint_stats),
        }

        return jsonify(
            {
                "stats": stats,
                "top_ips": [{"ip": ip, **data} for ip, data in top_ips],
                "endpoint_stats": endpoint_stats,
                "violation_ips": violation_ips,
                "recent_violations": recent_violations,
                "config": RATE_LIMIT_CONFIG,
            }
        )


@app.route("/api/rate-limits/violations", methods=["GET"])
@require_auth
def get_rate_limit_violations():
    """Get detailed rate limit violations."""
    ip_filter = request.args.get("ip")
    endpoint_filter = request.args.get("endpoint")
    hours = request.args.get("hours", 24, type=int)

    now = time.time()
    cutoff = now - (hours * 3600)

    violations = []
    with _rate_limit_lock:
        for ip, ip_violations in _rate_limit_violations.items():
            if ip_filter and ip != ip_filter:
                continue
            for v in ip_violations:
                if v["timestamp"] < cutoff:
                    continue
                if endpoint_filter and v["endpoint"] != endpoint_filter:
                    continue
                violations.append(
                    {
                        "ip": ip,
                        "endpoint": v["endpoint"],
                        "limit": v["limit"],
                        "request_count": v.get("request_count", 0),
                        "timestamp": datetime.fromtimestamp(
                            v["timestamp"]
                        ).isoformat(),
                        "seconds_ago": int(now - v["timestamp"]),
                    }
                )

    violations.sort(key=lambda x: x["seconds_ago"])

    # Aggregate by IP
    ip_summary = {}
    for v in violations:
        ip = v["ip"]
        if ip not in ip_summary:
            ip_summary[ip] = {
                "count": 0,
                "endpoints": set(),
                "first_seen": v["timestamp"],
                "last_seen": v["timestamp"],
            }
        ip_summary[ip]["count"] += 1
        ip_summary[ip]["endpoints"].add(v["endpoint"])
        ip_summary[ip]["last_seen"] = v["timestamp"]

    for ip in ip_summary:
        ip_summary[ip]["endpoints"] = list(ip_summary[ip]["endpoints"])

    return jsonify(
        {
            "violations": violations[:500],
            "total": len(violations),
            "ip_summary": ip_summary,
            "filters": {
                "ip": ip_filter,
                "endpoint": endpoint_filter,
                "hours": hours,
            },
        }
    )


@app.route("/api/rate-limits/ip/<ip_address>", methods=["GET"])
@require_auth
def get_rate_limit_ip_details(ip_address):
    """Get rate limit details for a specific IP."""
    now = time.time()
    window_start = now - 60

    with _rate_limit_lock:
        # Current requests
        current_requests = {}
        for key, timestamps in _rate_limit_store.items():
            if not key.startswith(f"{ip_address}:"):
                continue
            endpoint = key.split(":", 1)[1] if ":" in key else "unknown"
            recent = [ts for ts in timestamps if ts > window_start]
            if recent:
                current_requests[endpoint] = {
                    "count": len(recent),
                    "oldest": datetime.fromtimestamp(min(recent)).isoformat(),
                    "newest": datetime.fromtimestamp(max(recent)).isoformat(),
                }

        # Violations for this IP
        violations = _rate_limit_violations.get(ip_address, [])
        recent_violations = [
            {
                "endpoint": v["endpoint"],
                "limit": v["limit"],
                "timestamp": datetime.fromtimestamp(
                    v["timestamp"]
                ).isoformat(),
                "seconds_ago": int(now - v["timestamp"]),
            }
            for v in violations
            if v["timestamp"] > now - 86400  # Last 24 hours
        ]

        return jsonify(
            {
                "ip": ip_address,
                "current_requests": current_requests,
                "total_current": sum(
                    r["count"] for r in current_requests.values()
                ),
                "violations_24h": len(recent_violations),
                "violations": recent_violations[-50:],
                "is_blocked": False,  # Placeholder for future IP blocking feature
            }
        )


@app.route("/api/rate-limits/config", methods=["GET"])
@require_auth
def get_rate_limit_config():
    """Get current rate limit configuration."""
    return jsonify({"config": RATE_LIMIT_CONFIG})


@app.route("/api/rate-limits/config", methods=["PUT"])
@require_auth
def update_rate_limit_config():
    """Update rate limit configuration."""
    data = request.get_json() or {}

    updated = []
    for key, value in data.items():
        if isinstance(value, int) and value > 0:
            RATE_LIMIT_CONFIG[key] = value
            updated.append(key)

    log_activity(
        "update_rate_limit_config",
        "config",
        None,
        f"Updated: {', '.join(updated)}",
    )
    return jsonify(
        {"success": True, "updated": updated, "config": RATE_LIMIT_CONFIG}
    )


@app.route("/api/rate-limits/reset", methods=["POST"])
@require_auth
def reset_rate_limit_stats():
    """Reset rate limit statistics and optionally clear violations."""
    data = request.get_json() or {}
    clear_violations = data.get("clear_violations", False)
    clear_requests = data.get("clear_requests", False)

    with _rate_limit_lock:
        if clear_violations:
            _rate_limit_violations.clear()

        if clear_requests:
            _rate_limit_store.clear()

        _rate_limit_stats["total_requests"] = 0
        _rate_limit_stats["total_violations"] = 0
        _rate_limit_stats["start_time"] = time.time()

    log_activity(
        "reset_rate_limits",
        "config",
        None,
        f"violations={
            'cleared' if clear_violations else 'kept'}, requests={
            'cleared' if clear_requests else 'kept'}",
    )

    return jsonify(
        {
            "success": True,
            "cleared_violations": clear_violations,
            "cleared_requests": clear_requests,
        }
    )


@app.route("/api/rate-limits/endpoints", methods=["GET"])
@require_auth
def get_rate_limited_endpoints():
    """Get list of all rate-limited endpoints with their limits."""
    # Scan app routes for rate-limited endpoints
    endpoints = []
    for rule in app.url_map.iter_rules():
        if rule.endpoint and not rule.endpoint.startswith("static"):
            view_func = app.view_functions.get(rule.endpoint)
            if view_func:
                # Check if decorated with rate_limit (look for X-RateLimit
                # headers in response)
                endpoints.append(
                    {
                        "endpoint": rule.endpoint,
                        "path": rule.rule,
                        "methods": list(rule.methods - {"HEAD", "OPTIONS"}),
                        "default_limit": RATE_LIMIT_CONFIG.get("default", 60),
                    }
                )

    return jsonify({"endpoints": endpoints, "total": len(endpoints)})


# ============================================================================
# AUDIT API
# ============================================================================


@app.route("/api/audit", methods=["GET"])
@require_auth
def get_audit_log():
    """Get security audit log with filtering and pagination."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            # Filters
            user_id = request.args.get("user_id", type=int)
            action = request.args.get("action")
            entity_type = request.args.get("entity_type")
            ip_address = request.args.get("ip_address")
            start_date = request.args.get("start_date")
            end_date = request.args.get("end_date")
            security_only = (
                request.args.get("security_only", "false").lower() == "true"
            )

            # Pagination
            page = request.args.get("page", 1, type=int)
            per_page = min(request.args.get("per_page", 50, type=int), 500)
            offset = (page - 1) * per_page

            # Security-relevant actions
            security_actions = [
                "login",
                "logout",
                "login_failed",
                "password_change",
                "user_create",
                "user_delete",
                "permission_change",
                "api_key_create",
                "api_key_revoke",
                "session_expired",
            ]

            # Build query
            where_clauses = []
            params = []

            if user_id:
                where_clauses.append("a.user_id = ?")
                params.append(user_id)
            if action:
                where_clauses.append("a.action LIKE ?")
                params.append(f"%{action}%")
            if entity_type:
                where_clauses.append("a.entity_type = ?")
                params.append(entity_type)
            if ip_address:
                where_clauses.append("a.ip_address LIKE ?")
                params.append(f"%{ip_address}%")
            if start_date:
                where_clauses.append("a.created_at >= ?")
                params.append(start_date)
            if end_date:
                where_clauses.append("a.created_at <= ?")
                params.append(end_date)
            if security_only:
                placeholders = ",".join(["?" for _ in security_actions])
                where_clauses.append(f"a.action IN ({placeholders})")
                params.extend(security_actions)

            where_sql = " AND ".join(where_clauses) if where_clauses else "1=1"

            # Get total count
            count_sql = f"SELECT COUNT(*) as total FROM activity_log a WHERE {where_sql}"
            total = conn.execute(count_sql, params).fetchone()["total"]

            # Get paginated results
            query = """
                SELECT a.id, a.user_id, a.action, a.entity_type, a.entity_id,
                       a.details, a.ip_address, a.created_at
                FROM activity_log a
                WHERE {where_sql}
                ORDER BY a.created_at DESC
                LIMIT ? OFFSET ?
            """
            rows = conn.execute(query, params + [per_page, offset]).fetchall()

            entries = []
            for row in rows:
                entries.append(
                    {
                        "id": row["id"],
                        "user_id": row["user_id"],
                        "action": row["action"],
                        "entity_type": row["entity_type"],
                        "entity_id": row["entity_id"],
                        "details": row["details"],
                        "ip_address": row["ip_address"],
                        "created_at": row["created_at"],
                    }
                )

            return jsonify(
                {
                    "entries": entries,
                    "pagination": {
                        "page": page,
                        "per_page": per_page,
                        "total": total,
                        "pages": (total + per_page - 1) // per_page,
                    },
                }
            )
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/audit/stats", methods=["GET"])
@require_auth
def get_audit_stats():
    """Get security audit statistics and summaries."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            # Time range
            days = request.args.get("days", 7, type=int)
            cutoff = (datetime.now() - timedelta(days=days)).isoformat()

            stats = {}

            # Total activity count
            stats["total_activities"] = conn.execute(
                "SELECT COUNT(*) as c FROM activity_log WHERE created_at >= ?",
                (cutoff,),
            ).fetchone()["c"]

            # Activity by action type
            action_counts = conn.execute(
                """
                SELECT action, COUNT(*) as count
                FROM activity_log WHERE created_at >= ?
                GROUP BY action ORDER BY count DESC LIMIT 20
            """,
                (cutoff,),
            ).fetchall()
            stats["actions"] = {r["action"]: r["count"] for r in action_counts}

            # Activity by entity type
            entity_counts = conn.execute(
                """
                SELECT entity_type, COUNT(*) as count
                FROM activity_log WHERE created_at >= ? AND entity_type IS NOT NULL
                GROUP BY entity_type ORDER BY count DESC
            """,
                (cutoff,),
            ).fetchall()
            stats["entities"] = {
                r["entity_type"]: r["count"] for r in entity_counts
            }

            # Unique IPs
            unique_ips = conn.execute(
                """
                SELECT COUNT(DISTINCT ip_address) as c
                FROM activity_log WHERE created_at >= ? AND ip_address IS NOT NULL
            """,
                (cutoff,),
            ).fetchone()["c"]
            stats["unique_ips"] = unique_ips

            # Top IPs by activity
            top_ips = conn.execute(
                """
                SELECT ip_address, COUNT(*) as count
                FROM activity_log WHERE created_at >= ? AND ip_address IS NOT NULL
                GROUP BY ip_address ORDER BY count DESC LIMIT 10
            """,
                (cutoff,),
            ).fetchall()
            stats["top_ips"] = [
                {"ip": r["ip_address"], "count": r["count"]} for r in top_ips
            ]

            # Security events
            security_actions = [
                "login",
                "logout",
                "login_failed",
                "password_change",
                "user_create",
                "user_delete",
                "permission_change",
            ]
            placeholders = ",".join(["?" for _ in security_actions])
            security_counts = conn.execute(
                """
                SELECT action, COUNT(*) as count
                FROM activity_log WHERE created_at >= ? AND action IN ({placeholders})
                GROUP BY action
            """,
                [cutoff] + security_actions,
            ).fetchall()
            stats["security_events"] = {
                r["action"]: r["count"] for r in security_counts
            }

            # Failed logins (potential security concern)
            failed_logins = conn.execute(
                """
                SELECT ip_address, COUNT(*) as count
                FROM activity_log WHERE created_at >= ? AND action = 'login_failed'
                GROUP BY ip_address ORDER BY count DESC LIMIT 10
            """,
                (cutoff,),
            ).fetchall()
            stats["failed_login_ips"] = [
                {"ip": r["ip_address"], "count": r["count"]}
                for r in failed_logins
            ]

            # Activity timeline (hourly for last 24 hours)
            timeline = conn.execute(
                """
                SELECT strftime('%Y-%m-%d %H:00', created_at) as hour, COUNT(*) as count
                FROM activity_log WHERE created_at >= datetime('now', '-24 hours')
                GROUP BY hour ORDER BY hour
            """
            ).fetchall()
            stats["timeline_24h"] = [
                {"hour": r["hour"], "count": r["count"]} for r in timeline
            ]

            # Daily activity for the period
            daily = conn.execute(
                """
                SELECT date(created_at) as day, COUNT(*) as count
                FROM activity_log WHERE created_at >= ?
                GROUP BY day ORDER BY day
            """,
                (cutoff,),
            ).fetchall()
            stats["daily_activity"] = [
                {"day": r["day"], "count": r["count"]} for r in daily
            ]

            stats["period_days"] = days

            return jsonify(stats)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/audit/export", methods=["GET"])
@require_auth
def export_audit_log():
    """Export audit log to CSV format."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            # Date range (default last 30 days)
            days = request.args.get("days", 30, type=int)
            cutoff = (datetime.now() - timedelta(days=days)).isoformat()

            rows = conn.execute(
                """
                SELECT id, user_id, action, entity_type, entity_id, details, ip_address, created_at
                FROM activity_log WHERE created_at >= ?
                ORDER BY created_at DESC
            """,
                (cutoff,),
            ).fetchall()

            # Build CSV
            import io

            output = io.StringIO()
            output.write(
                "id,user_id,action,entity_type,entity_id,details,ip_address,created_at\n"
            )
            for row in rows:
                details = (row["details"] or "").replace('"', '""')
                output.write(
                    f"{
                        row['id']},{
                        row['user_id'] or ''},{
                        row['action']},"
                    f"{
                        row['entity_type'] or ''},{
                        row['entity_id'] or ''},\"{details}\","
                    f"{
                            row['ip_address'] or ''},{
                                row['created_at']}\n"
                )

            from flask import Response

            return Response(
                output.getvalue(),
                mimetype="text/csv",
                headers={
                    "Content-Disposition": f'attachment; filename=audit_log_{datetime.now().strftime("%Y%m%d")}.csv'
                },
            )
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ============================================================================
# STATISTICS API
# ============================================================================


@app.route("/api/stats", methods=["GET"])
@require_auth
def get_stats():
    """Get dashboard statistics summary.

    Returns aggregated counts for projects, features, bugs, errors,
    tasks, nodes, and workers in the system.

    Returns:
        200: Statistics object with counts

    Example Request:
        GET /api/stats

    Example Response:
        {
            "projects": 5,
            "features": {"total": 42, "in_progress": 12, "completed": 25},
            "bugs": {"total": 18, "open": 5, "resolved": 13},
            "errors": {"open": 3, "total": 150},
            "tasks": {"pending": 8, "running": 2},
            "nodes": {"total": 3, "online": 3},
            "workers": {"total": 4, "active": 2}
        }

    cURL Example:
        curl -X GET "http://localhost:8080/api/stats" \\
             -H "Cookie: session=<session_cookie>"
    """
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            stats = {}

            # Project stats
            stats["projects"] = conn.execute(
                "SELECT COUNT(*) as count FROM projects WHERE status = 'active'"
            ).fetchone()["count"]

            # Feature stats
            feature_stats = conn.execute(
                """
                SELECT status, COUNT(*) as count
                FROM features
                GROUP BY status
            """
            ).fetchall()
            stats["features"] = {
                row["status"]: row["count"] for row in feature_stats
            }
            stats["features"]["total"] = sum(stats["features"].values())

            # Bug stats
            bug_stats = conn.execute(
                """
                SELECT status, COUNT(*) as count
                FROM bugs
                GROUP BY status
            """
            ).fetchall()
            stats["bugs"] = {row["status"]: row["count"] for row in bug_stats}
            stats["bugs"]["total"] = sum(stats["bugs"].values())

            # Error stats
            error_stats = conn.execute(
                """
                SELECT status, COUNT(*) as count, SUM(occurrence_count) as occurrences
                FROM errors
                GROUP BY status
            """
            ).fetchall()
            stats["errors"] = {
                row["status"]: {
                    "count": row["count"],
                    "occurrences": row["occurrences"],
                }
                for row in error_stats
            }

            # Node stats
            node_stats = conn.execute(
                """
                SELECT status, COUNT(*) as count
                FROM nodes
                GROUP BY status
            """
            ).fetchall()
            stats["nodes"] = {
                row["status"]: row["count"] for row in node_stats
            }

            # Task queue stats
            task_stats = conn.execute(
                """
                SELECT status, COUNT(*) as count
                FROM task_queue
                GROUP BY status
            """
            ).fetchall()
            stats["tasks"] = {
                row["status"]: row["count"] for row in task_stats
            }

            # tmux session count
            tmux_count = conn.execute(
                "SELECT COUNT(*) as count FROM tmux_sessions"
            ).fetchone()["count"]
            stats["tmux_sessions"] = tmux_count

            # Recent activity
            recent = conn.execute(
                """
                SELECT action, entity_type, entity_id, details, created_at
                FROM activity_log
                ORDER BY created_at DESC
                LIMIT 20
            """
            ).fetchall()
            stats["recent_activity"] = [dict(r) for r in recent]

            return jsonify(stats)
    except sqlite3.Error as e:
        logger.error(f"Database error fetching stats: {e}")
        return api_error("Failed to fetch statistics", 500, "database_error")


# ============================================================================
# HEALTH MONITORING API
# ============================================================================


@app.route("/api/health/system", methods=["GET"])
@require_auth
def get_system_health():
    """Get system health metrics over time.

    Query Parameters:
        hours (int): Number of hours to retrieve (default: 24, max: 720 for 30 days)
        interval (str): Data aggregation interval - 'minute', 'hour', 'day' (default: 'minute')

    Returns:
        200: Time series data for CPU, memory, disk usage

    Example:
        GET /api/health/system?hours=24&interval=minute
    """
    try:
        hours = min(int(request.args.get("hours", 24)), 720)  # Max 30 days
        interval = request.args.get("interval", "minute")

        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            # Determine time grouping based on interval
            if interval == "hour":
                time_format = "strftime('%Y-%m-%d %H:00:00', timestamp)"
            elif interval == "day":
                time_format = "strftime('%Y-%m-%d 00:00:00', timestamp)"
            else:  # minute
                time_format = "timestamp"

            rows = conn.execute(
                """
                SELECT
                    {time_format} as time,
                    AVG(cpu_percent) as cpu_percent,
                    AVG(memory_percent) as memory_percent,
                    AVG(disk_percent) as disk_percent,
                    AVG(load_avg_1) as load_avg_1,
                    AVG(services_running) as services_running,
                    AVG(workers_active) as workers_active
                FROM system_health
                WHERE timestamp >= datetime('now', '-{hours} hours')
                GROUP BY {time_format}
                ORDER BY time ASC
            """
            ).fetchall()

            return jsonify(
                {
                    "hours": hours,
                    "interval": interval,
                    "data": [dict(r) for r in rows],
                }
            )
    except Exception as e:
        logger.error(f"Error fetching system health: {e}")
        return api_error("Failed to fetch system health", 500, str(e))


@app.route("/api/health/services", methods=["GET"])
@require_auth
def get_service_health():
    """Get individual service health metrics over time.

    Query Parameters:
        hours (int): Number of hours to retrieve (default: 24)
        service (str): Optional service name filter

    Returns:
        200: Service status and resource usage over time
    """
    try:
        hours = min(int(request.args.get("hours", 24)), 720)
        service_filter = request.args.get("service")

        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            query = """
                SELECT
                    service_name,
                    service_type,
                    timestamp,
                    status,
                    cpu_percent,
                    memory_mb,
                    port
                FROM service_health
                WHERE timestamp >= datetime('now', '-{} hours')
            """.format(
                hours
            )

            if service_filter:
                query += " AND service_name = ?"
                rows = conn.execute(query, (service_filter,)).fetchall()
            else:
                rows = conn.execute(query).fetchall()

            # Group by service name
            services = {}
            for row in rows:
                name = row["service_name"]
                if name not in services:
                    services[name] = {
                        "service_name": name,
                        "service_type": row["service_type"],
                        "port": row["port"],
                        "data": [],
                    }
                services[name]["data"].append(
                    {
                        "timestamp": row["timestamp"],
                        "status": row["status"],
                        "cpu_percent": row["cpu_percent"],
                        "memory_mb": row["memory_mb"],
                    }
                )

            return jsonify(
                {"hours": hours, "services": list(services.values())}
            )
    except Exception as e:
        logger.error(f"Error fetching service health: {e}")
        return api_error("Failed to fetch service health", 500, str(e))


@app.route("/api/health/current", methods=["GET"])
@require_auth
def get_current_health():
    """Get current system and service health status.

    Returns:
        200: Latest metrics for system resources and all services
    """
    try:
        import subprocess

        import psutil

        # Get latest system metrics
        cpu_percent = psutil.cpu_percent(interval=1)
        mem = psutil.virtual_memory()
        disk = psutil.disk_usage("/")

        services = []
        service_configs = [
            {"name": "Pharma Dashboard", "port": 7085, "type": "web"},
            {"name": "Architect Dashboard", "port": 8081, "type": "web"},
            {"name": "Ollama", "port": 11434, "type": "llm"},
            {"name": "AnythingLLM", "port": 3001, "type": "llm"},
            {"name": "Chrome/Comet", "port": 9222, "type": "other"},
        ]

        for svc in service_configs:
            try:
                result = subprocess.run(
                    ["lso", "-ti", f":{svc['port']}"],
                    capture_output=True,
                    text=True,
                    timeout=2,
                )
                pid = result.stdout.strip() if result.returncode == 0 else None

                if pid:
                    proc = psutil.Process(int(pid))
                    services.append(
                        {
                            "name": svc["name"],
                            "type": svc["type"],
                            "port": svc["port"],
                            "status": "running",
                            "pid": int(pid),
                            "cpu_percent": proc.cpu_percent(),
                            "memory_mb": proc.memory_info().rss
                            / (1024 * 1024),
                        }
                    )
                else:
                    services.append(
                        {
                            "name": svc["name"],
                            "type": svc["type"],
                            "port": svc["port"],
                            "status": "stopped",
                        }
                    )
            except Exception:
                services.append(
                    {
                        "name": svc["name"],
                        "type": svc["type"],
                        "port": svc["port"],
                        "status": "error",
                    }
                )

        return jsonify(
            {
                "system": {
                    "cpu_percent": cpu_percent,
                    "memory_percent": mem.percent,
                    "memory_used_gb": mem.used / (1024 * 1024 * 1024),
                    "memory_total_gb": mem.total / (1024 * 1024 * 1024),
                    "disk_percent": disk.percent,
                    "disk_used_gb": disk.used / (1024 * 1024 * 1024),
                    "disk_total_gb": disk.total / (1024 * 1024 * 1024),
                },
                "services": services,
                "timestamp": datetime.now().isoformat(),
            }
        )
    except Exception as e:
        logger.error(f"Error fetching current health: {e}")
        return api_error("Failed to fetch current health", 500, str(e))


@app.route("/api/analytics", methods=["GET"])
@require_auth
def get_analytics():
    """Get usage analytics. Query: period (day/week/month/year), days (custom)."""
    period = request.args.get("period", "week")
    days = request.args.get("days", type=int) or {
        "day": 1,
        "week": 7,
        "month": 30,
        "year": 365,
    }.get(period, 7)
    df = f"-{days} days"

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        analytics = {
            "period": period,
            "days": days,
            "generated_at": datetime.now().isoformat(),
        }

        # Activity trends
        act = conn.execute(
            "SELECT DATE(created_at) as date, action, COUNT(*) as cnt FROM activity_log WHERE created_at >= DATE('now', ?) GROUP BY date, action",
            (df,),
        ).fetchall()
        daily = {}
        for r in act:
            if r["date"] not in daily:
                daily[r["date"]] = {"total": 0, "actions": {}}
            daily[r["date"]]["actions"][r["action"]] = r["cnt"]
            daily[r["date"]]["total"] += r["cnt"]
        analytics["activity"] = {
            "by_day": daily,
            "top_actions": [
                dict(r)
                for r in conn.execute(
                    "SELECT action, COUNT(*) as count FROM activity_log WHERE created_at >= DATE('now', ?) GROUP BY action ORDER BY count DESC LIMIT 10",
                    (df,),
                ).fetchall()
            ],
        }

        # Tasks
        ts = conn.execute(
            "SELECT COUNT(*) as t, SUM(CASE WHEN status='completed' THEN 1 ELSE 0 END) as c, SUM(CASE WHEN status='failed' THEN 1 ELSE 0 END) as f, SUM(CASE WHEN status='pending' THEN 1 ELSE 0 END) as p, AVG(CASE WHEN completed_at IS NOT NULL THEN (julianday(completed_at)-julianday(started_at))*86400 END) as d FROM task_queue WHERE created_at >= DATE('now', ?)",
            (df,),
        ).fetchone()
        t, c, f = ts["t"] or 0, ts["c"] or 0, ts["f"] or 0
        analytics["tasks"] = {
            "total": t,
            "completed": c,
            "failed": f,
            "pending": ts["p"] or 0,
            "completion_rate": round(c / max(t, 1) * 100, 1),
            "failure_rate": round(f / max(t, 1) * 100, 1),
            "avg_duration_sec": round(ts["d"] or 0, 2),
        }
        analytics["tasks"]["by_type"] = [
            dict(r)
            for r in conn.execute(
                "SELECT task_type, COUNT(*) as count FROM task_queue WHERE created_at >= DATE('now', ?) GROUP BY task_type ORDER BY count DESC",
                (df,),
            ).fetchall()
        ]

        # Errors
        es = conn.execute(
            "SELECT COUNT(*) as u, SUM(occurrence_count) as o, SUM(CASE WHEN status='new' THEN 1 ELSE 0 END) as n, SUM(CASE WHEN status='resolved' THEN 1 ELSE 0 END) as r FROM errors WHERE first_seen >= DATE('now', ?)",
            (df,),
        ).fetchone()
        u, r = es["u"] or 0, es["r"] or 0
        analytics["errors"] = {
            "unique": u,
            "occurrences": es["o"] or 0,
            "new": es["n"] or 0,
            "resolved": r,
            "resolution_rate": round(r / max(u, 1) * 100, 1),
        }
        analytics["errors"]["by_type"] = [
            dict(r)
            for r in conn.execute(
                "SELECT error_type, COUNT(*) as count, SUM(occurrence_count) as occurrences FROM errors WHERE first_seen >= DATE('now', ?) GROUP BY error_type ORDER BY occurrences DESC LIMIT 10",
                (df,),
            ).fetchall()
        ]

        # Features
        fs = conn.execute(
            "SELECT COUNT(*) as t, SUM(CASE WHEN status='completed' THEN 1 ELSE 0 END) as c, SUM(CASE WHEN status='in_progress' THEN 1 ELSE 0 END) as i FROM features WHERE created_at >= DATE('now', ?)",
            (df,),
        ).fetchone()
        ft, fc = fs["t"] or 0, fs["c"] or 0
        analytics["features"] = {
            "total": ft,
            "completed": fc,
            "in_progress": fs["i"] or 0,
            "completion_rate": round(fc / max(ft, 1) * 100, 1),
        }

        # Bugs
        bs = conn.execute(
            "SELECT COUNT(*) as t, SUM(CASE WHEN status='resolved' THEN 1 ELSE 0 END) as r, SUM(CASE WHEN status='open' THEN 1 ELSE 0 END) as o, SUM(CASE WHEN severity='critical' THEN 1 ELSE 0 END) as cr FROM bugs WHERE created_at >= DATE('now', ?)",
            (df,),
        ).fetchone()
        bt, br = bs["t"] or 0, bs["r"] or 0
        analytics["bugs"] = {
            "total": bt,
            "resolved": br,
            "open": bs["o"] or 0,
            "critical": bs["cr"] or 0,
            "resolution_rate": round(br / max(bt, 1) * 100, 1),
        }

        # Trends
        analytics["trends"] = {
            "peak_hours": [
                dict(r)
                for r in conn.execute(
                    "SELECT strftime('%H', created_at) as hour, COUNT(*) as count FROM activity_log WHERE created_at >= DATE('now', ?) GROUP BY hour ORDER BY count DESC",
                    (df,),
                ).fetchall()
            ],
            "by_day_of_week": [
                {
                    "day": ["Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"][
                        int(r["d"])
                    ],
                    "count": r["c"],
                }
                for r in conn.execute(
                    "SELECT strftime('%w', created_at) as d, COUNT(*) as c FROM activity_log WHERE created_at >= DATE('now', ?) GROUP BY d",
                    (df,),
                ).fetchall()
            ],
        }

        # Growth
        prev = (
            conn.execute(
                "SELECT COUNT(*) FROM task_queue WHERE created_at >= DATE('now', ?) AND created_at < DATE('now', ?)",
                (f"-{days*2} days", df),
            ).fetchone()[0]
            or 0
        )
        analytics["trends"]["growth"] = {
            "current": t,
            "previous": prev,
            "change_pct": (
                round((t - prev) / max(prev, 1) * 100, 1)
                if prev
                else (100 if t else 0)
            ),
        }

        return jsonify(analytics)


# ============================================================================
# SOFT DELETE API
# ============================================================================

SOFT_DELETE_TABLES = [
    "projects",
    "milestones",
    "features",
    "bugs",
    "devops_tasks",
    "nodes",
    "errors",
    "task_queue",
    "workers",
]


@app.route("/api/deleted", methods=["GET"])
@require_auth
def get_deleted_entities():
    """Get all soft-deleted entities across all tables.

    Query Parameters:
        table: Filter to specific table (optional)
        limit: Max records per table (default: 50)
    """
    table_filter = request.args.get("table")
    limit = min(request.args.get("limit", 50, type=int), 200)

    if table_filter and table_filter not in SOFT_DELETE_TABLES:
        return api_error(
            f"Table '{table_filter}' not supported", 400, "invalid_table"
        )

    tables_to_query = [table_filter] if table_filter else SOFT_DELETE_TABLES
    result = {}

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        for table in tables_to_query:
            try:
                rows = conn.execute(
                    """
                    SELECT * FROM {table}
                    WHERE deleted_at IS NOT NULL
                    ORDER BY deleted_at DESC
                    LIMIT ?
                """,
                    (limit,),
                ).fetchall()
                result[table] = [dict(r) for r in rows]
            except sqlite3.OperationalError:
                result[table] = {
                    "error": "Table does not have soft delete columns"
                }

    return jsonify(result)


@app.route("/api/deleted/stats", methods=["GET"])
@require_auth
def get_deletion_stats():
    """Get soft delete statistics for all tables."""
    stats = {}

    with get_db_connection() as conn:
        for table in SOFT_DELETE_TABLES:
            try:
                result = conn.execute(
                    """
                    SELECT
                        COUNT(*) as total,
                        SUM(CASE WHEN deleted_at IS NULL THEN 1 ELSE 0 END) as active,
                        SUM(CASE WHEN deleted_at IS NOT NULL THEN 1 ELSE 0 END) as deleted
                    FROM {table}
                """
                ).fetchone()
                stats[table] = {
                    "total": result[0] or 0,
                    "active": result[1] or 0,
                    "deleted": result[2] or 0,
                }
            except sqlite3.OperationalError:
                stats[table] = {"error": "Table not available"}

    return jsonify(stats)


@app.route("/api/deleted/<table>/<int:record_id>/restore", methods=["POST"])
@require_auth
def restore_deleted_entity(table, record_id):
    """Restore a soft-deleted entity.

    Args:
        table: Table name (must be in SOFT_DELETE_TABLES)
        record_id: ID of the record to restore
    """
    if table not in SOFT_DELETE_TABLES:
        return api_error(
            f"Table '{table}' not supported", 400, "invalid_table"
        )

    with get_db_connection() as conn:
        result = conn.execute(
            """
            UPDATE {table} SET
                deleted_at = NULL,
                deleted_by = NULL
            WHERE id = ? AND deleted_at IS NOT NULL
        """,
            (record_id,),
        )

        if result.rowcount == 0:
            return api_error(
                "Record not found or not deleted", 404, "not_found"
            )

        log_activity("restore", table, record_id)

    return jsonify({"success": True, "table": table, "id": record_id})


@app.route("/api/deleted/<table>/<int:record_id>/purge", methods=["DELETE"])
@require_auth
def purge_deleted_entity(table, record_id):
    """Permanently delete a soft-deleted entity.

    Args:
        table: Table name (must be in SOFT_DELETE_TABLES)
        record_id: ID of the record to permanently delete
    """
    if table not in SOFT_DELETE_TABLES:
        return api_error(
            f"Table '{table}' not supported", 400, "invalid_table"
        )

    with get_db_connection() as conn:
        # Only allow purging already soft-deleted records
        result = conn.execute(
            """
            DELETE FROM {table}
            WHERE id = ? AND deleted_at IS NOT NULL
        """,
            (record_id,),
        )

        if result.rowcount == 0:
            return api_error(
                "Record not found or not soft-deleted", 404, "not_found"
            )

        log_activity("purge", table, record_id)

    return jsonify(
        {"success": True, "table": table, "id": record_id, "permanent": True}
    )


@app.route("/api/deleted/purge-old", methods=["POST"])
@require_auth
def purge_old_deleted():
    """Permanently delete records that have been soft-deleted for a period.

    Body:
        days: Delete records older than this many days (default: 30, min: 7)
        table: Specific table to purge (optional, purges all if not specified)
    """
    data = request.get_json() or {}
    days = max(data.get("days", 30), 7)  # Minimum 7 days
    table_filter = data.get("table")

    if table_filter and table_filter not in SOFT_DELETE_TABLES:
        return api_error(
            f"Table '{table_filter}' not supported", 400, "invalid_table"
        )

    tables_to_purge = [table_filter] if table_filter else SOFT_DELETE_TABLES
    result = {}

    with get_db_connection() as conn:
        for table in tables_to_purge:
            try:
                deleted = conn.execute(
                    """
                    DELETE FROM {table}
                    WHERE deleted_at IS NOT NULL
                      AND deleted_at < datetime('now', '-{days} days')
                """
                ).rowcount
                result[table] = deleted
            except sqlite3.OperationalError:
                result[table] = {"error": "Failed to purge"}

        log_activity(
            "purge_old_deleted",
            "system",
            None,
            f"days={days}, tables={tables_to_purge}",
        )

    return jsonify({"success": True, "days": days, "purged": result})


@app.route("/api/deleted/bulk-restore", methods=["POST"])
@require_auth
def bulk_restore_deleted():
    """Restore multiple soft-deleted entities.

    Body:
        items: List of {table: str, id: int} objects to restore
    """
    data = request.get_json() or {}
    items = data.get("items", [])

    if not items or len(items) > 100:
        return api_error(
            "Provide 1-100 items to restore", 400, "invalid_request"
        )

    restored = []
    errors = []

    with get_db_connection() as conn:
        for item in items:
            table = item.get("table")
            record_id = item.get("id")

            if table not in SOFT_DELETE_TABLES:
                errors.append(
                    {"table": table, "id": record_id, "error": "Invalid table"}
                )
                continue

            try:
                result = conn.execute(
                    """
                    UPDATE {table} SET
                        deleted_at = NULL,
                        deleted_by = NULL
                    WHERE id = ? AND deleted_at IS NOT NULL
                """,
                    (record_id,),
                )

                if result.rowcount > 0:
                    restored.append({"table": table, "id": record_id})
                else:
                    errors.append(
                        {
                            "table": table,
                            "id": record_id,
                            "error": "Not found or not deleted",
                        }
                    )
            except sqlite3.OperationalError as e:
                errors.append(
                    {"table": table, "id": record_id, "error": str(e)}
                )

        if restored:
            log_activity(
                "bulk_restore", "system", None, f"count={len(restored)}"
            )

    return jsonify(
        {
            "success": len(errors) == 0,
            "restored": restored,
            "errors": errors,
            "restored_count": len(restored),
            "error_count": len(errors),
        }
    )


@app.route("/api/workload", methods=["GET"])
@require_auth
def get_workload_distribution():
    """Get team workload distribution across assignees and workers.

    Query params:
        period: day/week/month (default: week)
        include_completed: Include completed tasks in workload (default: false)

    Returns:
        by_assignee: Workload grouped by task assignee
        by_worker: Workload grouped by worker
        by_status: Task counts by status
        balance: Workload balance metrics
        recommendations: Suggestions for rebalancing
    """
    period = request.args.get("period", "week")
    include_completed = (
        request.args.get("include_completed", "false").lower() == "true"
    )
    days = {"day": 1, "week": 7, "month": 30}.get(period, 7)
    df = f"-{days} days"

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        workload = {
            "period": period,
            "days": days,
            "generated_at": datetime.now().isoformat(),
            "by_assignee": [],
            "by_worker": [],
            "by_status": {},
            "by_priority": {},
            "balance": {},
            "recommendations": [],
        }

        # Status filter for active tasks
        status_filter = (
            ""
            if include_completed
            else "AND status NOT IN ('completed', 'failed', 'cancelled')"
        )

        # Workload by assigned worker
        worker_stats = conn.execute(
            """
            SELECT
                COALESCE(assigned_worker, 'unassigned') as worker,
                COUNT(*) as task_count,
                SUM(CASE WHEN status = 'running' THEN 1 ELSE 0 END) as running,
                SUM(CASE WHEN status = 'pending' THEN 1 ELSE 0 END) as pending,
                SUM(COALESCE(story_points, 0)) as total_story_points,
                SUM(COALESCE(estimated_hours, 0)) as total_estimated_hours,
                AVG(priority) as avg_priority,
                MIN(created_at) as oldest_task
            FROM task_queue
            WHERE created_at >= DATE('now', ?) {status_filter}
            GROUP BY assigned_worker
            ORDER BY task_count DESC
        """,
            (df,),
        ).fetchall()

        for w in worker_stats:
            workload["by_worker"].append(
                {
                    "worker": w["worker"],
                    "task_count": w["task_count"],
                    "running": w["running"] or 0,
                    "pending": w["pending"] or 0,
                    "story_points": w["total_story_points"] or 0,
                    "estimated_hours": round(
                        w["total_estimated_hours"] or 0, 1
                    ),
                    "avg_priority": round(w["avg_priority"] or 0, 1),
                    "oldest_task": w["oldest_task"],
                }
            )

        # Feature/Bug workload by assignee
        assignee_features = conn.execute(
            """
            SELECT
                COALESCE(assigned_to, 'unassigned') as assignee,
                COUNT(*) as feature_count,
                SUM(CASE WHEN status = 'in_progress' THEN 1 ELSE 0 END) as in_progress,
                SUM(CASE WHEN status = 'pending' THEN 1 ELSE 0 END) as pending
            FROM features
            WHERE created_at >= DATE('now', ?) AND status NOT IN ('completed', 'cancelled')
            GROUP BY assigned_to
        """,
            (df,),
        ).fetchall()

        assignee_bugs = conn.execute(
            """
            SELECT
                COALESCE(assigned_to, 'unassigned') as assignee,
                COUNT(*) as bug_count,
                SUM(CASE WHEN status = 'in_progress' THEN 1 ELSE 0 END) as in_progress,
                SUM(CASE WHEN severity = 'critical' THEN 1 ELSE 0 END) as critical
            FROM bugs
            WHERE created_at >= DATE('now', ?) AND status NOT IN ('resolved', 'closed')
            GROUP BY assigned_to
        """,
            (df,),
        ).fetchall()

        # Combine assignee data
        assignee_map = {}
        for f in assignee_features:
            assignee_map[f["assignee"]] = {
                "assignee": f["assignee"],
                "features": f["feature_count"],
                "features_in_progress": f["in_progress"] or 0,
                "bugs": 0,
                "bugs_critical": 0,
            }
        for b in assignee_bugs:
            if b["assignee"] in assignee_map:
                assignee_map[b["assignee"]]["bugs"] = b["bug_count"]
                assignee_map[b["assignee"]]["bugs_critical"] = (
                    b["critical"] or 0
                )
            else:
                assignee_map[b["assignee"]] = {
                    "assignee": b["assignee"],
                    "features": 0,
                    "features_in_progress": 0,
                    "bugs": b["bug_count"],
                    "bugs_critical": b["critical"] or 0,
                }

        workload["by_assignee"] = sorted(
            assignee_map.values(),
            key=lambda x: x["features"] + x["bugs"],
            reverse=True,
        )

        # Task counts by status
        status_counts = conn.execute(
            """
            SELECT status, COUNT(*) as count
            FROM task_queue
            GROUP BY status
        """
        ).fetchall()
        workload["by_status"] = {
            s["status"]: s["count"] for s in status_counts
        }

        # Task counts by priority
        priority_counts = conn.execute(
            """
            SELECT
                CASE
                    WHEN priority >= 8 THEN 'critical'
                    WHEN priority >= 5 THEN 'high'
                    WHEN priority >= 2 THEN 'medium'
                    ELSE 'low'
                END as priority_level,
                COUNT(*) as count
            FROM task_queue
            WHERE status IN ('pending', 'running')
            GROUP BY priority_level
        """
        ).fetchall()
        workload["by_priority"] = {
            p["priority_level"]: p["count"] for p in priority_counts
        }

        # Calculate balance metrics
        if workload["by_worker"]:
            task_counts = [
                w["task_count"]
                for w in workload["by_worker"]
                if w["worker"] != "unassigned"
            ]
            if task_counts:
                avg_load = sum(task_counts) / len(task_counts)
                max_load = max(task_counts)
                min_load = min(task_counts)
                variance = sum((x - avg_load) ** 2 for x in task_counts) / len(
                    task_counts
                )
                std_dev = variance**0.5

                workload["balance"] = {
                    "total_workers": len(task_counts),
                    "total_tasks": sum(task_counts),
                    "avg_tasks_per_worker": round(avg_load, 1),
                    "max_tasks": max_load,
                    "min_tasks": min_load,
                    "std_deviation": round(std_dev, 2),
                    "balance_score": (
                        round(100 - (std_dev / max(avg_load, 1) * 100), 1)
                        if avg_load > 0
                        else 100
                    ),
                }

                # Generate recommendations
                unassigned = next(
                    (
                        w
                        for w in workload["by_worker"]
                        if w["worker"] == "unassigned"
                    ),
                    None,
                )
                if unassigned and unassigned["task_count"] > 0:
                    workload["recommendations"].append(
                        {
                            "type": "unassigned_tasks",
                            "message": f"{unassigned['task_count']} tasks are unassigned",
                            "severity": (
                                "warning"
                                if unassigned["task_count"] > 5
                                else "info"
                            ),
                        }
                    )

                overloaded = [
                    w
                    for w in workload["by_worker"]
                    if w["worker"] != "unassigned"
                    and w["task_count"] > avg_load * 1.5
                ]
                for w in overloaded:
                    workload["recommendations"].append(
                        {
                            "type": "overloaded_worker",
                            "message": f"Worker '{
                                w['worker']}' has {
                                w['task_count']} tasks (avg: {
                                round(
                                    avg_load,
                                    1)})",
                            "severity": "warning",
                        }
                    )

                underutilized = [
                    w
                    for w in workload["by_worker"]
                    if w["worker"] != "unassigned"
                    and w["task_count"] < avg_load * 0.5
                    and avg_load > 2
                ]
                for w in underutilized:
                    workload["recommendations"].append(
                        {
                            "type": "underutilized_worker",
                            "message": f"Worker '{
                                w['worker']}' has only {
                                w['task_count']} tasks",
                            "severity": "info",
                        }
                    )

        # Capacity planning
        running_tasks = workload["by_status"].get("running", 0)
        pending_tasks = workload["by_status"].get("pending", 0)
        active_workers = len(
            [
                w
                for w in workload["by_worker"]
                if w["worker"] != "unassigned" and w["running"] > 0
            ]
        )

        workload["capacity"] = {
            "running_tasks": running_tasks,
            "pending_tasks": pending_tasks,
            "active_workers": active_workers,
            "tasks_per_active_worker": round(
                running_tasks / max(active_workers, 1), 1
            ),
            "queue_depth": pending_tasks,
            "estimated_queue_time_hours": round(
                pending_tasks * 0.5, 1
            ),  # Rough estimate
        }

        return jsonify(workload)


@app.route("/api/workload/rebalance", methods=["POST"])
@require_auth
def rebalance_workload():
    """Suggest or apply workload rebalancing.

    Body:
        apply: If true, actually reassign tasks (default: false, dry run)
        strategy: 'even' (distribute evenly) or 'capacity' (based on worker capacity)
        task_ids: Optional list of specific task IDs to rebalance

    Returns:
        suggestions: List of suggested reassignments
        applied: Whether changes were applied
    """
    data = request.get_json() or {}
    apply_changes = data.get("apply", False)
    strategy = data.get("strategy", "even")
    task_ids = data.get("task_ids", [])

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Get active workers
        workers = conn.execute(
            """
            SELECT id, name, status,
                   (SELECT COUNT(*) FROM task_queue WHERE assigned_worker = workers.id AND status IN ('pending', 'running')) as current_load
            FROM workers
            WHERE status != 'offline'
            ORDER BY current_load ASC
        """
        ).fetchall()

        if not workers:
            return (
                jsonify(
                    {
                        "error": "No active workers available",
                        "suggestions": [],
                        "applied": False,
                    }
                ),
                400,
            )

        # Get tasks to rebalance
        if task_ids:
            placeholders = ",".join("?" * len(task_ids))
            tasks = conn.execute(
                """
                SELECT id, task_type, priority, assigned_worker, story_points
                FROM task_queue
                WHERE id IN ({placeholders}) AND status = 'pending'
            """,
                task_ids,
            ).fetchall()
        else:
            # Get unassigned or overloaded tasks
            avg_load = (
                sum(w["current_load"] for w in workers) / len(workers)
                if workers
                else 0
            )
            tasks = conn.execute(
                """
                SELECT id, task_type, priority, assigned_worker, story_points
                FROM task_queue
                WHERE status = 'pending' AND (assigned_worker IS NULL OR assigned_worker = '')
                ORDER BY priority DESC, created_at ASC
                LIMIT 50
            """
            ).fetchall()

        suggestions = []
        worker_loads = {w["id"]: w["current_load"] for w in workers}
        worker_names = {w["id"]: w["name"] or w["id"] for w in workers}

        for task in tasks:
            if strategy == "even":
                # Assign to worker with lowest load
                target_worker = min(
                    worker_loads.keys(), key=lambda w: worker_loads[w]
                )
            else:
                # Capacity-based (could factor in worker skills, etc.)
                target_worker = min(
                    worker_loads.keys(), key=lambda w: worker_loads[w]
                )

            if task["assigned_worker"] != target_worker:
                suggestions.append(
                    {
                        "task_id": task["id"],
                        "task_type": task["task_type"],
                        "priority": task["priority"],
                        "from_worker": task["assigned_worker"] or "unassigned",
                        "to_worker": worker_names[target_worker],
                        "to_worker_id": target_worker,
                    }
                )
                worker_loads[target_worker] += 1

        # Apply changes if requested
        if apply_changes and suggestions:
            for s in suggestions:
                conn.execute(
                    "UPDATE task_queue SET assigned_worker = ? WHERE id = ?",
                    (s["to_worker_id"], s["task_id"]),
                )
            log_activity(
                "rebalance_workload",
                "task_queue",
                None,
                f"Reassigned {len(suggestions)} tasks",
            )

        return jsonify(
            {
                "strategy": strategy,
                "suggestions": suggestions,
                "total_suggestions": len(suggestions),
                "applied": apply_changes,
                "worker_loads_after": {
                    worker_names[w]: load for w, load in worker_loads.items()
                },
            }
        )


@app.route("/api/reports/weekly", methods=["GET"])
@require_auth
def get_weekly_report():
    """Generate weekly summary report.

    Query params:
        weeks_ago: Number of weeks back (default 0 = current week)
        format: Response format - 'json' or 'markdown' (default 'json')

    Returns comprehensive weekly metrics including:
        - Features completed/started
        - Bugs resolved/opened
        - Tasks processed
        - Errors occurred
        - Deployment activity
        - Team activity summary
    """
    weeks_ago = request.args.get("weeks_ago", 0, type=int)
    output_format = request.args.get("format", "json")

    # Calculate week boundaries (Monday to Sunday)
    today = datetime.now().date()
    days_since_monday = today.weekday()
    week_start = today - timedelta(days=days_since_monday + (weeks_ago * 7))
    week_end = week_start + timedelta(days=6)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        report = {
            "period": {
                "week_start": str(week_start),
                "week_end": str(week_end),
                "weeks_ago": weeks_ago,
            },
            "features": {},
            "bugs": {},
            "tasks": {},
            "errors": {},
            "deployments": {},
            "activity": {},
            "highlights": [],
        }

        # Features metrics
        features = conn.execute(
            """
            SELECT
                COUNT(*) as total_active,
                SUM(CASE WHEN status = 'completed' AND date(updated_at) BETWEEN ? AND ? THEN 1 ELSE 0 END) as completed,
                SUM(CASE WHEN date(created_at) BETWEEN ? AND ? THEN 1 ELSE 0 END) as created,
                SUM(CASE WHEN status = 'in_progress' THEN 1 ELSE 0 END) as in_progress
            FROM features
        """,
            (week_start, week_end, week_start, week_end),
        ).fetchone()
        report["features"] = {
            "completed_this_week": features["completed"] or 0,
            "created_this_week": features["created"] or 0,
            "currently_in_progress": features["in_progress"] or 0,
            "total_active": features["total_active"] or 0,
        }

        # Bugs metrics
        bugs = conn.execute(
            """
            SELECT
                COUNT(*) as total,
                SUM(CASE WHEN status = 'resolved' AND date(updated_at) BETWEEN ? AND ? THEN 1 ELSE 0 END) as resolved,
                SUM(CASE WHEN date(created_at) BETWEEN ? AND ? THEN 1 ELSE 0 END) as opened,
                SUM(CASE WHEN status = 'open' THEN 1 ELSE 0 END) as open_count,
                SUM(CASE WHEN status = 'open' AND severity = 'critical' THEN 1 ELSE 0 END) as critical_open
            FROM bugs
        """,
            (week_start, week_end, week_start, week_end),
        ).fetchone()
        report["bugs"] = {
            "resolved_this_week": bugs["resolved"] or 0,
            "opened_this_week": bugs["opened"] or 0,
            "currently_open": bugs["open_count"] or 0,
            "critical_open": bugs["critical_open"] or 0,
            "net_change": (bugs["resolved"] or 0) - (bugs["opened"] or 0),
        }

        # Tasks metrics
        tasks = conn.execute(
            """
            SELECT
                COUNT(*) as total,
                SUM(CASE WHEN status = 'completed' AND date(completed_at) BETWEEN ? AND ? THEN 1 ELSE 0 END) as completed,
                SUM(CASE WHEN status = 'failed' AND date(completed_at) BETWEEN ? AND ? THEN 1 ELSE 0 END) as failed,
                SUM(CASE WHEN date(created_at) BETWEEN ? AND ? THEN 1 ELSE 0 END) as created,
                SUM(CASE WHEN status = 'pending' THEN 1 ELSE 0 END) as pending
            FROM task_queue
        """,
            (week_start, week_end, week_start, week_end, week_start, week_end),
        ).fetchone()
        completed_tasks = tasks["completed"] or 0
        failed_tasks = tasks["failed"] or 0
        total_processed = completed_tasks + failed_tasks
        report["tasks"] = {
            "completed_this_week": completed_tasks,
            "failed_this_week": failed_tasks,
            "created_this_week": tasks["created"] or 0,
            "currently_pending": tasks["pending"] or 0,
            "success_rate": round(
                (
                    (completed_tasks / total_processed * 100)
                    if total_processed > 0
                    else 100
                ),
                1,
            ),
        }

        # Errors metrics
        errors = conn.execute(
            """
            SELECT
                COUNT(*) as total_unique,
                SUM(occurrence_count) as total_occurrences,
                SUM(CASE WHEN date(first_seen) BETWEEN ? AND ? THEN 1 ELSE 0 END) as new_errors,
                SUM(CASE WHEN status = 'resolved' AND date(last_seen) BETWEEN ? AND ? THEN 1 ELSE 0 END) as resolved,
                SUM(CASE WHEN status = 'active' THEN occurrence_count ELSE 0 END) as active_occurrences
            FROM errors
        """,
            (week_start, week_end, week_start, week_end),
        ).fetchone()
        report["errors"] = {
            "new_this_week": errors["new_errors"] or 0,
            "resolved_this_week": errors["resolved"] or 0,
            "total_unique_active": errors["total_unique"] or 0,
            "active_occurrences": errors["active_occurrences"] or 0,
        }

        # Deployments
        deployments = conn.execute(
            """
            SELECT
                COUNT(*) as total,
                SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) as successful,
                SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed
            FROM deployments
            WHERE date(deployed_at) BETWEEN ? AND ?
        """,
            (week_start, week_end),
        ).fetchone()
        deploy_total = deployments["total"] or 0
        deploy_success = deployments["successful"] or 0
        report["deployments"] = {
            "total_this_week": deploy_total,
            "successful": deploy_success,
            "failed": deployments["failed"] or 0,
            "success_rate": round(
                (
                    (deploy_success / deploy_total * 100)
                    if deploy_total > 0
                    else 100
                ),
                1,
            ),
        }

        # Activity summary
        activity = conn.execute(
            """
            SELECT action, COUNT(*) as count
            FROM activity_log
            WHERE date(created_at) BETWEEN ? AND ?
            GROUP BY action
            ORDER BY count DESC
            LIMIT 10
        """,
            (week_start, week_end),
        ).fetchall()
        report["activity"] = {
            "total_actions": sum(a["count"] for a in activity),
            "by_action": {a["action"]: a["count"] for a in activity},
        }

        # Generate highlights
        highlights = []
        if report["features"]["completed_this_week"] > 0:
            highlights.append(
                f"Completed {
                    report['features']['completed_this_week']} features"
            )
        if (
            report["bugs"]["resolved_this_week"]
            > report["bugs"]["opened_this_week"]
        ):
            highlights.append(
                f"Net reduction of {report['bugs']['net_change']} bugs"
            )
        if report["tasks"]["success_rate"] >= 95:
            highlights.append(
                f"Task success rate: {report['tasks']['success_rate']}%"
            )
        if report["bugs"]["critical_open"] > 0:
            highlights.append(
                f"Warning: {
                    report['bugs']['critical_open']} critical bugs open"
            )
        if report["errors"]["new_this_week"] > 10:
            highlights.append(
                f"Alert: {
                    report['errors']['new_this_week']} new errors this week"
            )
        report["highlights"] = highlights

        # Return as markdown if requested
        if output_format == "markdown":
            md = """# Weekly Report: {week_start} to {week_end}

## Features
- Completed: {report['features']['completed_this_week']}
- Created: {report['features']['created_this_week']}
- In Progress: {report['features']['currently_in_progress']}

## Bugs
- Resolved: {report['bugs']['resolved_this_week']}
- Opened: {report['bugs']['opened_this_week']}
- Net Change: {report['bugs']['net_change']:+d}
- Critical Open: {report['bugs']['critical_open']}

## Tasks
- Completed: {report['tasks']['completed_this_week']}
- Failed: {report['tasks']['failed_this_week']}
- Success Rate: {report['tasks']['success_rate']}%

## Errors
- New: {report['errors']['new_this_week']}
- Resolved: {report['errors']['resolved_this_week']}

## Deployments
- Total: {report['deployments']['total_this_week']}
- Success Rate: {report['deployments']['success_rate']}%

## Highlights
""" + "\n".join(
                f"- {h}" for h in highlights
            )
            return md, 200, {"Content-Type": "text/markdown"}

        return jsonify(report)


@app.route("/api/reports/burndown", methods=["GET"])
@require_auth
def get_burndown_report():
    """Generate burndown chart data for sprints/milestones.

    Query parameters:
        milestone_id: Specific milestone to analyze (optional)
        project_id: Filter by project (optional)
        start_date: Start date for analysis (default: 14 days ago)
        end_date: End date for analysis (default: today)
        entity_type: What to track - features, bugs, tasks, or all (default: all)
        include_forecast: Include completion forecast (default: true)
    """
    milestone_id = request.args.get("milestone_id", type=int)
    project_id = request.args.get("project_id", type=int)
    start_date_str = request.args.get("start_date")
    end_date_str = request.args.get("end_date")
    entity_type = request.args.get("entity_type", "all")
    include_forecast = (
        request.args.get("include_forecast", "true").lower() == "true"
    )

    if start_date_str:
        start_date = datetime.strptime(start_date_str, "%Y-%m-%d").date()
    else:
        start_date = (datetime.now() - timedelta(days=14)).date()

    if end_date_str:
        end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
    else:
        end_date = datetime.now().date()

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        milestone = None
        if milestone_id:
            milestone = conn.execute(
                "SELECT id, name, start_date, target_date, status, project_id FROM milestones WHERE id = ?",
                (milestone_id,),
            ).fetchone()
            if milestone:
                if milestone["start_date"]:
                    start_date = datetime.strptime(
                        milestone["start_date"], "%Y-%m-%d"
                    ).date()
                if milestone["target_date"]:
                    end_date = datetime.strptime(
                        milestone["target_date"], "%Y-%m-%d"
                    ).date()
                project_id = project_id or milestone["project_id"]

        total_days = (end_date - start_date).days + 1
        if total_days < 1:
            return jsonify({"error": "Invalid date range"}), 400

        project_filter = "AND project_id = ?" if project_id else ""
        milestone_filter = "AND milestone_id = ?" if milestone_id else ""
        daily_data = []
        current_date = start_date

        while current_date <= end_date:
            date_str = str(current_date)
            day_data = {"date": date_str}

            if entity_type in ["features", "all"]:
                params = [date_str]
                if project_id:
                    params.append(project_id)
                if milestone_id:
                    params.append(milestone_id)
                params.append(date_str)
                feature_query = f"SELECT COUNT(*) as remaining FROM features WHERE date(created_at) <= ? {project_filter} {milestone_filter} AND (status NOT IN ('completed', 'done', 'cancelled') OR (status IN ('completed', 'done') AND date(updated_at) > ?))"
                day_data["features_remaining"] = conn.execute(
                    feature_query, params
                ).fetchone()["remaining"]

            if entity_type in ["bugs", "all"]:
                params = [date_str]
                if project_id:
                    params.append(project_id)
                if milestone_id:
                    params.append(milestone_id)
                params.append(date_str)
                bug_query = f"SELECT COUNT(*) as remaining FROM bugs WHERE date(created_at) <= ? {project_filter} {milestone_filter} AND (status NOT IN ('resolved', 'fixed', 'closed', 'wontfix') OR (status IN ('resolved', 'fixed', 'closed') AND date(updated_at) > ?))"
                day_data["bugs_remaining"] = conn.execute(
                    bug_query, params
                ).fetchone()["remaining"]

            if entity_type in ["tasks", "all"]:
                task_query = "SELECT COUNT(*) as remaining FROM task_queue WHERE date(created_at) <= ? AND (status NOT IN ('completed', 'cancelled') OR (status = 'completed' AND date(completed_at) > ?))"
                day_data["tasks_remaining"] = conn.execute(
                    task_query, [date_str, date_str]
                ).fetchone()["remaining"]

            day_data["total_remaining"] = sum(
                [
                    day_data.get("features_remaining", 0),
                    day_data.get("bugs_remaining", 0),
                    day_data.get("tasks_remaining", 0),
                ]
            )
            daily_data.append(day_data)
            current_date += timedelta(days=1)

        if daily_data:
            initial_total = daily_data[0]["total_remaining"]
            for i, day in enumerate(daily_data):
                day["ideal_remaining"] = round(
                    max(
                        0,
                        initial_total
                        - (initial_total * i / max(total_days - 1, 1)),
                    ),
                    1,
                )

        velocity, forecast = None, None
        if include_forecast and len(daily_data) >= 2:
            completed_per_day = [
                daily_data[i - 1]["total_remaining"]
                - daily_data[i]["total_remaining"]
                for i in range(1, len(daily_data))
            ]
            if completed_per_day:
                avg_velocity = sum(completed_per_day) / len(completed_per_day)
                velocity = {
                    "average_daily": round(avg_velocity, 2),
                    "total_completed": daily_data[0]["total_remaining"]
                    - daily_data[-1]["total_remaining"],
                    "days_elapsed": len(daily_data) - 1,
                }
                current_remaining = daily_data[-1]["total_remaining"]
                if avg_velocity > 0 and current_remaining > 0:
                    days_to_complete = current_remaining / avg_velocity
                    forecast_date = end_date + timedelta(
                        days=int(days_to_complete)
                    )
                    forecast = {
                        "estimated_completion_date": str(forecast_date),
                        "days_remaining": int(days_to_complete),
                        "on_track": forecast_date <= end_date,
                    }
                elif current_remaining == 0:
                    forecast = {
                        "estimated_completion_date": str(end_date),
                        "days_remaining": 0,
                        "on_track": True,
                        "completed": True,
                    }

        summary = {
            "period": {
                "start_date": str(start_date),
                "end_date": str(end_date),
                "total_days": total_days,
            },
            "initial_total": (
                daily_data[0]["total_remaining"] if daily_data else 0
            ),
            "current_total": (
                daily_data[-1]["total_remaining"] if daily_data else 0
            ),
            "total_completed": (
                (
                    daily_data[0]["total_remaining"]
                    - daily_data[-1]["total_remaining"]
                )
                if daily_data
                else 0
            ),
        }

        if milestone_id and milestone:
            summary["milestone"] = {
                "id": milestone["id"],
                "name": milestone["name"],
                "status": milestone["status"],
                "target_date": milestone["target_date"],
            }
        if project_id:
            project = conn.execute(
                "SELECT id, name FROM projects WHERE id = ?", (project_id,)
            ).fetchone()
            if project:
                summary["project"] = {
                    "id": project["id"],
                    "name": project["name"],
                }

        if (
            daily_data
            and len(daily_data) > 1
            and daily_data[0]["total_remaining"] > 0
        ):
            actual, ideal, initial = (
                daily_data[-1]["total_remaining"],
                daily_data[-1]["ideal_remaining"],
                daily_data[0]["total_remaining"],
            )
            actual_pct, ideal_pct = (actual / initial) * 100, (
                ideal / initial
            ) * 100
            deviation = actual_pct - ideal_pct
            health = (
                "on_track"
                if deviation <= 5
                else (
                    "slight_delay"
                    if deviation <= 15
                    else "at_risk" if deviation <= 30 else "critical"
                )
            )
            summary["health"] = {
                "status": health,
                "deviation_percent": round(deviation, 1),
                "actual_remaining_percent": round(actual_pct, 1),
                "ideal_remaining_percent": round(ideal_pct, 1),
            }

        return jsonify(
            {
                "burndown": daily_data,
                "summary": summary,
                "velocity": velocity,
                "forecast": forecast,
                "entity_type": entity_type,
                "filters": {
                    "milestone_id": milestone_id,
                    "project_id": project_id,
                },
            }
        )


@app.route("/api/reports/burndown/milestones", methods=["GET"])
@require_auth
def get_burndown_milestones():
    """Get list of milestones available for burndown analysis."""
    project_id = request.args.get("project_id", type=int)
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        query = "SELECT m.id, m.name, m.status, m.start_date, m.target_date, m.project_id, p.name as project_name, (SELECT COUNT(*) FROM features WHERE milestone_id = m.id) as feature_count, (SELECT COUNT(*) FROM bugs WHERE milestone_id = m.id) as bug_count FROM milestones m LEFT JOIN projects p ON m.project_id = p.id WHERE m.status NOT IN ('completed', 'cancelled')"
        params = []
        if project_id:
            query += " AND m.project_id = ?"
            params.append(project_id)
        query += " ORDER BY m.target_date ASC, m.name"
        milestones = conn.execute(query, params).fetchall()
        result = [
            {
                "id": m["id"],
                "name": m["name"],
                "status": m["status"],
                "start_date": m["start_date"],
                "target_date": m["target_date"],
                "project_id": m["project_id"],
                "project_name": m["project_name"],
                "feature_count": m["feature_count"],
                "bug_count": m["bug_count"],
                "total_items": m["feature_count"] + m["bug_count"],
            }
            for m in milestones
        ]
        return jsonify({"milestones": result, "total": len(result)})


@app.route("/api/reports/velocity", methods=["GET"])
@require_auth
def get_velocity_report():
    """Generate comprehensive sprint velocity report.

    Query params:
        sprints: Number of sprints to analyze (default 6)
        sprint_length: Days per sprint (default 14)
        project_id: Filter by project
        milestone_id: Filter by milestone
        worker_id: Filter by assigned worker
        include_forecast: Include velocity forecasting (default true)
        format: 'json' or 'markdown' (default json)
    """
    num_sprints = min(request.args.get("sprints", 6, type=int), 24)
    sprint_length = request.args.get("sprint_length", 14, type=int)
    project_id = request.args.get("project_id", type=int)
    milestone_id = request.args.get("milestone_id", type=int)
    worker_id = request.args.get("worker_id")
    include_forecast = (
        request.args.get("include_forecast", "true").lower() == "true"
    )
    output_format = request.args.get("format", "json")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Build filters
        filters = []
        params_base = []

        if project_id:
            filters.append("project_id = ?")
            params_base.append(project_id)
        if milestone_id:
            filters.append("milestone_id = ?")
            params_base.append(milestone_id)
        if worker_id:
            filters.append("assigned_worker = ?")
            params_base.append(worker_id)

        filter_clause = f" AND {' AND '.join(filters)}" if filters else ""

        # Gather velocity data for each sprint
        sprints_data = []
        for i in range(num_sprints):
            start_days = (i + 1) * sprint_length
            end_days = i * sprint_length

            params = params_base.copy()
            params.insert(0, f"-{start_days} days")
            params.insert(1, f"-{end_days} days" if end_days > 0 else "+1 day")

            result = conn.execute(
                """
                SELECT
                    COUNT(*) as tasks_completed,
                    COALESCE(SUM(story_points), 0) as points_completed,
                    COALESCE(SUM(estimated_hours), 0) as estimated_hours,
                    COALESCE(SUM(actual_hours), 0) as actual_hours,
                    COALESCE(AVG(story_points), 0) as avg_points,
                    COUNT(DISTINCT assigned_worker) as unique_workers,
                    COUNT(DISTINCT task_type) as task_types_count
                FROM task_queue
                WHERE status = 'completed'
                  AND completed_at >= DATE('now', ?)
                  AND completed_at < DATE('now', ?)
                  {filter_clause}
            """,
                params,
            ).fetchone()

            # Calculate sprint dates
            sprint_end = datetime.now() - timedelta(days=end_days)
            sprint_start = datetime.now() - timedelta(days=start_days)

            sprint_data = {
                "sprint_number": num_sprints - i,
                "sprint_label": f"Sprint {num_sprints - i}",
                "start_date": sprint_start.strftime("%Y-%m-%d"),
                "end_date": sprint_end.strftime("%Y-%m-%d"),
                "tasks_completed": result["tasks_completed"],
                "points_completed": result["points_completed"] or 0,
                "estimated_hours": round(result["estimated_hours"] or 0, 1),
                "actual_hours": round(result["actual_hours"] or 0, 1),
                "avg_points_per_task": round(result["avg_points"] or 0, 1),
                "unique_workers": result["unique_workers"],
                "task_types": result["task_types_count"],
            }

            # Calculate efficiency
            if sprint_data["estimated_hours"] > 0:
                sprint_data["efficiency"] = round(
                    (
                        sprint_data["estimated_hours"]
                        / max(sprint_data["actual_hours"], 0.1)
                    )
                    * 100,
                    1,
                )
            else:
                sprint_data["efficiency"] = None

            sprints_data.append(sprint_data)

        # Reverse to chronological order
        sprints_data.reverse()

        # Calculate summary statistics
        total_points = sum(s["points_completed"] for s in sprints_data)
        total_tasks = sum(s["tasks_completed"] for s in sprints_data)
        points_list = [
            s["points_completed"]
            for s in sprints_data
            if s["points_completed"] > 0
        ]

        avg_velocity = round(total_points / max(num_sprints, 1), 1)
        min_velocity = min(points_list) if points_list else 0
        max_velocity = max(points_list) if points_list else 0

        # Calculate trend (comparing recent vs older sprints)
        mid = len(sprints_data) // 2
        recent_avg = sum(
            s["points_completed"] for s in sprints_data[mid:]
        ) / max(len(sprints_data) - mid, 1)
        older_avg = sum(
            s["points_completed"] for s in sprints_data[:mid]
        ) / max(mid, 1)

        if older_avg > 0:
            trend_pct = round(((recent_avg - older_avg) / older_avg) * 100, 1)
            trend = (
                "improving"
                if trend_pct > 5
                else "declining" if trend_pct < -5 else "stable"
            )
        else:
            trend_pct = 0
            trend = "stable"

        # Velocity by worker (top 10)
        params_workers = params_base.copy()
        params_workers.insert(0, f"-{num_sprints * sprint_length} days")

        by_worker = conn.execute(
            """
            SELECT
                assigned_worker,
                COUNT(*) as tasks_completed,
                COALESCE(SUM(story_points), 0) as points_completed,
                COALESCE(AVG(story_points), 0) as avg_points
            FROM task_queue
            WHERE status = 'completed'
              AND completed_at >= DATE('now', ?)
              AND assigned_worker IS NOT NULL
              {filter_clause}
            GROUP BY assigned_worker
            ORDER BY points_completed DESC
            LIMIT 10
        """,
            params_workers,
        ).fetchall()

        # Velocity by task type
        by_type = conn.execute(
            """
            SELECT
                task_type,
                COUNT(*) as tasks_completed,
                COALESCE(SUM(story_points), 0) as points_completed,
                COALESCE(AVG(
                    CASE WHEN started_at IS NOT NULL
                    THEN (julianday(completed_at) - julianday(started_at)) * 24
                    ELSE NULL END
                ), 0) as avg_hours
            FROM task_queue
            WHERE status = 'completed'
              AND completed_at >= DATE('now', ?)
              {filter_clause}
            GROUP BY task_type
            ORDER BY points_completed DESC
        """,
            params_workers,
        ).fetchall()

        # Forecast (if requested)
        forecast = None
        if include_forecast and points_list:
            # Use weighted moving average for forecast
            weights = list(range(1, len(points_list) + 1))
            weighted_sum = sum(p * w for p, w in zip(points_list, weights))
            weight_total = sum(weights)
            predicted_velocity = round(weighted_sum / weight_total, 1)

            # Calculate confidence based on variance
            variance = sum((p - avg_velocity) ** 2 for p in points_list) / max(
                len(points_list), 1
            )
            std_dev = variance**0.5
            confidence = max(
                0,
                min(
                    100, round(100 - (std_dev / max(avg_velocity, 1)) * 50, 1)
                ),
            )

            forecast = {
                "predicted_next_sprint": predicted_velocity,
                "confidence_percent": confidence,
                "range_low": max(0, round(predicted_velocity - std_dev, 1)),
                "range_high": round(predicted_velocity + std_dev, 1),
                "method": "weighted_moving_average",
            }

        report = {
            "report_type": "sprint_velocity",
            "generated_at": datetime.now().isoformat(),
            "parameters": {
                "sprints_analyzed": num_sprints,
                "sprint_length_days": sprint_length,
                "project_id": project_id,
                "milestone_id": milestone_id,
                "worker_id": worker_id,
            },
            "summary": {
                "average_velocity": avg_velocity,
                "min_velocity": min_velocity,
                "max_velocity": max_velocity,
                "total_points": total_points,
                "total_tasks": total_tasks,
                "trend": trend,
                "trend_percent": trend_pct,
            },
            "sprints": sprints_data,
            "by_worker": [dict(w) for w in by_worker],
            "by_task_type": [dict(t) for t in by_type],
            "forecast": forecast,
        }

        # Markdown format
        if output_format == "markdown":
            md = """# Sprint Velocity Report

Generated: {report['generated_at']}

## Summary
- **Average Velocity**: {avg_velocity} points/sprint
- **Trend**: {trend} ({'+' if trend_pct > 0 else ''}{trend_pct}%)
- **Total Points**: {total_points}
- **Total Tasks**: {total_tasks}

## Sprint History
| Sprint | Dates | Points | Tasks | Efficiency |
|--------|-------|--------|-------|------------|
"""
            for s in sprints_data:
                eff = f"{s['efficiency']}%" if s["efficiency"] else "N/A"
                md += f"| {
                    s['sprint_label']} | {
                    s['start_date']} to {
                    s['end_date']} | {
                    s['points_completed']} | {
                    s['tasks_completed']} | {eff} |\n"

            if forecast:
                md += """
## Forecast
- **Predicted Next Sprint**: {forecast['predicted_next_sprint']} points
- **Confidence**: {forecast['confidence_percent']}%
- **Range**: {forecast['range_low']} - {forecast['range_high']} points
"""
            return md, 200, {"Content-Type": "text/markdown"}

        return jsonify(report)


# ============================================================================
# SLA COMPLIANCE REPORTS
# ============================================================================


@app.route("/api/reports/sla", methods=["GET"])
@require_auth
def get_sla_compliance_report():
    """Generate SLA compliance report for tasks and issues.

    Calculates compliance with SLAs based on response time and resolution time targets.

    Query params:
        period: Time period - 'day', 'week', 'month', 'quarter', 'year' (default 'month')
        project_id: Filter by project
        category: Filter by category - 'tasks', 'bugs', 'errors', 'all' (default 'all')
        severity: Filter by severity for bugs/errors
        include_details: Include individual item details (default false)
        format: 'json' or 'markdown' (default 'json')
    """
    period = request.args.get("period", "month")
    project_id = request.args.get("project_id", type=int)
    category = request.args.get("category", "all")
    severity_filter = request.args.get("severity")
    include_details = (
        request.args.get("include_details", "false").lower() == "true"
    )
    output_format = request.args.get("format", "json")

    # SLA targets (in hours)
    SLA_TARGETS = {
        "tasks": {
            "critical": {"response": 1, "resolution": 8},
            "high": {"response": 2, "resolution": 24},
            "medium": {"response": 4, "resolution": 48},
            "low": {"response": 8, "resolution": 72},
        },
        "bugs": {
            "critical": {"response": 1, "resolution": 4},
            "high": {"response": 2, "resolution": 24},
            "medium": {"response": 8, "resolution": 72},
            "low": {"response": 24, "resolution": 168},
        },
        "errors": {
            "critical": {"response": 0.5, "resolution": 2},
            "high": {"response": 1, "resolution": 8},
            "medium": {"response": 4, "resolution": 48},
            "low": {"response": 24, "resolution": 168},
        },
    }

    period_days = {
        "day": 1,
        "week": 7,
        "month": 30,
        "quarter": 90,
        "year": 365,
    }
    days = period_days.get(period, 30)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        report = {
            "report_type": "sla_compliance",
            "generated_at": datetime.now().isoformat(),
            "period": period,
            "period_days": days,
            "project_id": project_id,
            "sla_targets": SLA_TARGETS,
            "categories": {},
        }

        project_filter = "AND project_id = ?" if project_id else ""
        project_params = [project_id] if project_id else []

        # Task SLA compliance
        if category in ["all", "tasks"]:
            params = [f"-{days} days"] + project_params
            tasks = conn.execute(
                """
                SELECT id, task_type, status, priority, created_at, started_at, completed_at,
                       (julianday(COALESCE(started_at, 'now')) - julianday(created_at)) * 24 as response_hours,
                       CASE WHEN completed_at IS NOT NULL
                            THEN (julianday(completed_at) - julianday(created_at)) * 24
                            ELSE NULL END as resolution_hours
                FROM task_queue WHERE created_at >= DATE('now', ?) {project_filter}
            """,
                params,
            ).fetchall()

            total = len(tasks)
            met_response = met_resolution = 0
            by_priority = {}

            for task in tasks:
                priority = task["priority"] or 5
                priority_level = (
                    "critical"
                    if priority >= 9
                    else (
                        "high"
                        if priority >= 7
                        else "medium" if priority >= 4 else "low"
                    )
                )
                target = SLA_TARGETS["tasks"].get(
                    priority_level, {"response": 4, "resolution": 48}
                )

                response_met = (
                    task["response_hours"] is not None
                    and task["response_hours"] <= target["response"]
                )
                resolution_met = (
                    task["resolution_hours"] is not None
                    and task["resolution_hours"] <= target["resolution"]
                )

                if response_met:
                    met_response += 1
                if resolution_met:
                    met_resolution += 1

                if priority_level not in by_priority:
                    by_priority[priority_level] = {
                        "total": 0,
                        "response_met": 0,
                        "resolution_met": 0,
                    }
                by_priority[priority_level]["total"] += 1
                if response_met:
                    by_priority[priority_level]["response_met"] += 1
                if resolution_met:
                    by_priority[priority_level]["resolution_met"] += 1

            report["categories"]["tasks"] = {
                "total": total,
                "met_sla": met_resolution,
                "response_sla": {
                    "met": met_response,
                    "rate": round((met_response / max(total, 1)) * 100, 1),
                },
                "resolution_sla": {
                    "met": met_resolution,
                    "rate": round((met_resolution / max(total, 1)) * 100, 1),
                },
                "by_priority": {
                    p: {
                        **d,
                        "response_rate": round(
                            (d["response_met"] / max(d["total"], 1)) * 100, 1
                        ),
                        "resolution_rate": round(
                            (d["resolution_met"] / max(d["total"], 1)) * 100, 1
                        ),
                    }
                    for p, d in by_priority.items()
                },
            }

        # Bug SLA compliance
        if category in ["all", "bugs"]:
            params = [f"-{days} days"] + project_params
            sev_clause = "AND severity = ?" if severity_filter else ""
            if severity_filter:
                params.append(severity_filter)

            bugs = conn.execute(
                """
                SELECT id, title, severity, status, created_at, updated_at,
                       (julianday(COALESCE(updated_at, 'now')) - julianday(created_at)) * 24 as age_hours
                FROM bugs WHERE created_at >= DATE('now', ?) {project_filter} {sev_clause}
            """,
                params,
            ).fetchall()

            total = len(bugs)
            met_sla = 0
            by_severity = {}

            for bug in bugs:
                severity = bug["severity"] or "medium"
                target = SLA_TARGETS["bugs"].get(severity, {"resolution": 72})
                resolution_met = (
                    bug["age_hours"] <= target["resolution"]
                    if bug["status"] != "resolved"
                    else True
                )

                if resolution_met:
                    met_sla += 1

                if severity not in by_severity:
                    by_severity[severity] = {"total": 0, "met": 0}
                by_severity[severity]["total"] += 1
                if resolution_met:
                    by_severity[severity]["met"] += 1

            report["categories"]["bugs"] = {
                "total": total,
                "met_sla": met_sla,
                "compliance_rate": round((met_sla / max(total, 1)) * 100, 1),
                "by_severity": {
                    s: {
                        **d,
                        "rate": round(
                            (d["met"] / max(d["total"], 1)) * 100, 1
                        ),
                    }
                    for s, d in by_severity.items()
                },
            }

        # Error SLA compliance
        if category in ["all", "errors"]:
            params = [f"-{days} days"]
            sev_clause = "AND error_type = ?" if severity_filter else ""
            if severity_filter:
                params.append(severity_filter)

            errors = conn.execute(
                """
                SELECT id, error_type, status, first_seen, last_seen,
                       (julianday(COALESCE(last_seen, 'now')) - julianday(first_seen)) * 24 as age_hours
                FROM errors WHERE first_seen >= DATE('now', ?) {sev_clause}
            """,
                params,
            ).fetchall()

            total = len(errors)
            met_sla = 0
            by_type = {}

            for error in errors:
                error_type = error["error_type"] or "error"
                severity = (
                    "critical"
                    if error_type == "critical"
                    else "high" if error_type == "error" else "medium"
                )
                target = SLA_TARGETS["errors"].get(
                    severity, {"resolution": 48}
                )
                resolution_met = error["age_hours"] <= target["resolution"]

                if resolution_met:
                    met_sla += 1

                if error_type not in by_type:
                    by_type[error_type] = {"total": 0, "met": 0}
                by_type[error_type]["total"] += 1
                if resolution_met:
                    by_type[error_type]["met"] += 1

            report["categories"]["errors"] = {
                "total": total,
                "met_sla": met_sla,
                "compliance_rate": round((met_sla / max(total, 1)) * 100, 1),
                "by_type": {
                    t: {
                        **d,
                        "rate": round(
                            (d["met"] / max(d["total"], 1)) * 100, 1
                        ),
                    }
                    for t, d in by_type.items()
                },
            }

        # Overall compliance
        total_met = sum(
            c.get("met_sla", 0) for c in report["categories"].values()
        )
        total_items = sum(
            c.get("total", 0) for c in report["categories"].values()
        )
        compliance_rate = round((total_met / max(total_items, 1)) * 100, 1)

        report["overall"] = {
            "total_items": total_items,
            "met_sla": total_met,
            "missed_sla": total_items - total_met,
            "compliance_rate": compliance_rate,
            "status": (
                "healthy"
                if compliance_rate >= 95
                else "warning" if compliance_rate >= 85 else "critical"
            ),
        }

        # Markdown format
        if output_format == "markdown":
            overall = report["overall"]
            md = """# SLA Compliance Report

**Generated**: {report['generated_at']}
**Period**: {period} ({days} days)
**Status**: {overall['status'].upper()}

## Overall Compliance

| Metric | Value |
|--------|-------|
| Total Items | {overall['total_items']} |
| Met SLA | {overall['met_sla']} |
| Missed SLA | {overall['missed_sla']} |
| **Compliance Rate** | **{overall['compliance_rate']}%** |
"""
            return md, 200, {"Content-Type": "text/markdown"}

        return jsonify(report)


@app.route("/api/reports/sla/summary", methods=["GET"])
@require_auth
def get_sla_summary():
    """Get quick SLA summary for dashboard widgets."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        task_stats = conn.execute(
            """
            SELECT COUNT(*) as total,
                   SUM(CASE WHEN status = 'completed'
                       AND (julianday(completed_at) - julianday(created_at)) * 24 <= 48
                       THEN 1 ELSE 0 END) as met_sla
            FROM task_queue WHERE created_at >= DATE('now', '-30 days')
        """
        ).fetchone()

        bug_stats = conn.execute(
            """
            SELECT COUNT(*) as total,
                   SUM(CASE WHEN status = 'resolved' THEN 1 ELSE 0 END) as met_sla
            FROM bugs WHERE created_at >= DATE('now', '-30 days')
        """
        ).fetchone()

        error_stats = conn.execute(
            """
            SELECT COUNT(*) as total,
                   SUM(CASE WHEN status = 'resolved' THEN 1 ELSE 0 END) as met_sla
            FROM errors WHERE first_seen >= DATE('now', '-30 days')
        """
        ).fetchone()

        total = (
            (task_stats["total"] or 0)
            + (bug_stats["total"] or 0)
            + (error_stats["total"] or 0)
        )
        met = (
            (task_stats["met_sla"] or 0)
            + (bug_stats["met_sla"] or 0)
            + (error_stats["met_sla"] or 0)
        )
        rate = round((met / max(total, 1)) * 100, 1)

        return jsonify(
            {
                "period": "30 days",
                "summary": {
                    "tasks": {
                        "total": task_stats["total"] or 0,
                        "met": task_stats["met_sla"] or 0,
                    },
                    "bugs": {
                        "total": bug_stats["total"] or 0,
                        "met": bug_stats["met_sla"] or 0,
                    },
                    "errors": {
                        "total": error_stats["total"] or 0,
                        "met": error_stats["met_sla"] or 0,
                    },
                },
                "overall": {
                    "total": total,
                    "met": met,
                    "rate": rate,
                    "status": (
                        "healthy"
                        if rate >= 95
                        else "warning" if rate >= 85 else "critical"
                    ),
                },
            }
        )


@app.route("/api/reports/sla/trends", methods=["GET"])
@require_auth
def get_sla_trends():
    """Get SLA compliance trends over time."""
    periods = min(request.args.get("periods", 6, type=int), 12)
    period_length = request.args.get("period_length", "week")

    days_per_period = {"day": 1, "week": 7, "month": 30}
    period_days = days_per_period.get(period_length, 7)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        trends = []

        for i in range(periods):
            start_days = (i + 1) * period_days
            end_days = i * period_days

            task_stats = conn.execute(
                """
                SELECT COUNT(*) as total,
                       SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as met
                FROM task_queue
                WHERE created_at >= DATE('now', ?) AND created_at < DATE('now', ?)
            """,
                (
                    f"-{start_days} days",
                    f"-{end_days} days" if end_days > 0 else "+1 day",
                ),
            ).fetchone()

            bug_stats = conn.execute(
                """
                SELECT COUNT(*) as total,
                       SUM(CASE WHEN status = 'resolved' THEN 1 ELSE 0 END) as met
                FROM bugs
                WHERE created_at >= DATE('now', ?) AND created_at < DATE('now', ?)
            """,
                (
                    f"-{start_days} days",
                    f"-{end_days} days" if end_days > 0 else "+1 day",
                ),
            ).fetchone()

            total = (task_stats["total"] or 0) + (bug_stats["total"] or 0)
            met = (task_stats["met"] or 0) + (bug_stats["met"] or 0)

            end_date = datetime.now() - timedelta(days=end_days)
            start_date = datetime.now() - timedelta(days=start_days)

            trends.append(
                {
                    "period": periods - i,
                    "start_date": start_date.strftime("%Y-%m-%d"),
                    "end_date": end_date.strftime("%Y-%m-%d"),
                    "total": total,
                    "met": met,
                    "rate": round((met / max(total, 1)) * 100, 1),
                }
            )

        trends.reverse()

        trend_direction = "stable"
        if len(trends) >= 2:
            recent, older = trends[-1]["rate"], trends[0]["rate"]
            trend_direction = (
                "improving"
                if recent > older + 2
                else "declining" if recent < older - 2 else "stable"
            )

        return jsonify(
            {
                "period_length": period_length,
                "periods": periods,
                "trends": trends,
                "trend_direction": trend_direction,
            }
        )


# ============================================================================
# BOTTLENECK ANALYSIS REPORTS
# ============================================================================


@app.route("/api/reports/bottleneck", methods=["GET"])
@require_auth
def get_bottleneck_analysis():
    """Get comprehensive bottleneck analysis across projects.

    Query params:
        project_id: Filter to specific project (optional)
        days: Analysis period in days (default: 30)

    Returns: blocked items, stale work, overdue milestones, queue congestion, critical bugs, high WIP.
    """
    project_id = request.args.get("project_id", type=int)
    days = min(request.args.get("days", 30, type=int), 90)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        bottlenecks = []
        pf = f"AND project_id = {project_id}" if project_id else ""

        # 1. Blocked Items
        blocked = conn.execute(
            """
            SELECT 'feature' as item_type, id, title, project_id, updated_at,
                   (SELECT name FROM projects WHERE id = features.project_id) as project_name,
                   julianday('now') - julianday(updated_at) as days_blocked
            FROM features WHERE status = 'blocked' {pf}
            UNION ALL
            SELECT 'bug' as item_type, id, title, project_id, updated_at,
                   (SELECT name FROM projects WHERE id = bugs.project_id) as project_name,
                   julianday('now') - julianday(updated_at) as days_blocked
            FROM bugs WHERE status = 'blocked' {pf}
        """
        ).fetchall()
        for item in blocked:
            bottlenecks.append(
                {
                    "type": "blocked_item",
                    "severity": (
                        "high" if item["days_blocked"] > 7 else "medium"
                    ),
                    "item_type": item["item_type"],
                    "item_id": item["id"],
                    "title": item["title"],
                    "project_id": item["project_id"],
                    "project_name": item["project_name"],
                    "days_blocked": round(item["days_blocked"], 1),
                    "message": f"{
                        item['item_type'].title()} '{
                        item['title']}' blocked for {
                            round(
                                item['days_blocked'],
                                1)} days",
                }
            )

        # 2. Stale In-Progress Items
        stale = conn.execute(
            """
            SELECT 'feature' as item_type, id, title, project_id, updated_at,
                   (SELECT name FROM projects WHERE id = features.project_id) as project_name,
                   julianday('now') - julianday(updated_at) as days_stale
            FROM features WHERE status = 'in_progress' AND updated_at < datetime('now', '-7 days') {pf}
            UNION ALL
            SELECT 'bug' as item_type, id, title, project_id, updated_at,
                   (SELECT name FROM projects WHERE id = bugs.project_id) as project_name,
                   julianday('now') - julianday(updated_at) as days_stale
            FROM bugs WHERE status IN ('open', 'in_progress') AND updated_at < datetime('now', '-7 days') {pf}
        """
        ).fetchall()
        for item in stale:
            bottlenecks.append(
                {
                    "type": "stale_item",
                    "severity": (
                        "high" if item["days_stale"] > 14 else "medium"
                    ),
                    "item_type": item["item_type"],
                    "item_id": item["id"],
                    "title": item["title"],
                    "project_id": item["project_id"],
                    "project_name": item["project_name"],
                    "days_stale": round(item["days_stale"], 1),
                    "message": f"{
                        item['item_type'].title()} '{
                        item['title']}' no updates for {
                            round(
                                item['days_stale'],
                                1)} days",
                }
            )

        # 3. Overdue Milestones
        overdue = conn.execute(
            """
            SELECT m.id, m.name, m.project_id, m.target_date, p.name as project_name,
                   julianday('now') - julianday(m.target_date) as days_overdue,
                   (SELECT COUNT(*) FROM features WHERE milestone_id = m.id AND status != 'completed') as pending
            FROM milestones m JOIN projects p ON m.project_id = p.id
            WHERE m.target_date < date('now') AND m.status != 'completed' AND p.status = 'active'
            {pf.replace('project_id', 'm.project_id') if project_id else ''}
        """
        ).fetchall()
        for m in overdue:
            bottlenecks.append(
                {
                    "type": "overdue_milestone",
                    "severity": (
                        "critical" if m["days_overdue"] > 14 else "high"
                    ),
                    "item_type": "milestone",
                    "item_id": m["id"],
                    "title": m["name"],
                    "project_id": m["project_id"],
                    "project_name": m["project_name"],
                    "days_overdue": round(m["days_overdue"], 1),
                    "pending_features": m["pending"],
                    "message": f"Milestone '{
                        m['name']}' is {
                        round(
                            m['days_overdue'],
                            1)} days overdue with {
                                m['pending']} pending features",
                }
            )

        # 4. Queue Congestion
        queues = conn.execute(
            """
            SELECT task_type, COUNT(*) as count,
                   AVG(julianday('now') - julianday(created_at)) as avg_wait,
                   MAX(julianday('now') - julianday(created_at)) as max_wait
            FROM task_queue WHERE status = 'pending' GROUP BY task_type HAVING count > 5 OR avg_wait > 2
        """
        ).fetchall()
        for q in queues:
            bottlenecks.append(
                {
                    "type": "queue_congestion",
                    "severity": "high" if q["avg_wait"] > 5 else "medium",
                    "queue_type": q["task_type"],
                    "pending_count": q["count"],
                    "avg_wait_days": round(q["avg_wait"], 1),
                    "max_wait_days": round(q["max_wait"], 1),
                    "message": f"Queue '{
                        q['task_type']}' has {
                        q['count']} pending, avg wait: {
                            round(
                                q['avg_wait'],
                                1)} days",
                }
            )

        # 5. Critical Bugs Aging
        critical = conn.execute(
            """
            SELECT b.id, b.title, b.project_id, p.name as project_name,
                   julianday('now') - julianday(b.created_at) as age_days
            FROM bugs b JOIN projects p ON b.project_id = p.id
            WHERE b.severity = 'critical' AND b.status = 'open'
            AND julianday('now') - julianday(b.created_at) > 2 {pf.replace('project_id', 'b.project_id') if project_id else ''}
        """
        ).fetchall()
        for b in critical:
            bottlenecks.append(
                {
                    "type": "critical_bug_aging",
                    "severity": "critical",
                    "item_type": "bug",
                    "item_id": b["id"],
                    "title": b["title"],
                    "project_id": b["project_id"],
                    "project_name": b["project_name"],
                    "age_days": round(b["age_days"], 1),
                    "message": f"Critical bug '{
                        b['title']}' unresolved for {
                        round(
                            b['age_days'],
                            1)} days",
                }
            )

        # 6. High WIP
        wip = conn.execute(
            """
            SELECT p.id as project_id, p.name as project_name,
                (SELECT COUNT(*) FROM features WHERE project_id = p.id AND status = 'in_progress') as features_wip,
                (SELECT COUNT(*) FROM bugs WHERE project_id = p.id AND status = 'in_progress') as bugs_wip
            FROM projects p WHERE p.status = 'active' {pf.replace('project_id', 'p.id') if project_id else ''}
        """
        ).fetchall()
        for w in wip:
            total_wip = (w["features_wip"] or 0) + (w["bugs_wip"] or 0)
            if total_wip > 10:
                bottlenecks.append(
                    {
                        "type": "high_wip",
                        "severity": "medium" if total_wip < 20 else "high",
                        "project_id": w["project_id"],
                        "project_name": w["project_name"],
                        "features_in_progress": w["features_wip"] or 0,
                        "bugs_in_progress": w["bugs_wip"] or 0,
                        "total_wip": total_wip,
                        "message": f"Project '{
                            w['project_name']}' has high WIP: {total_wip} items",
                    }
                )

        # Sort by severity
        sev_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
        bottlenecks.sort(
            key=lambda x: (
                sev_order.get(x["severity"], 4),
                -x.get(
                    "days_blocked", x.get("days_stale", x.get("age_days", 0))
                ),
            )
        )

        summary = {
            "total_bottlenecks": len(bottlenecks),
            "by_severity": {
                s: len([b for b in bottlenecks if b["severity"] == s])
                for s in ["critical", "high", "medium", "low"]
            },
            "by_type": {},
        }
        for b in bottlenecks:
            summary["by_type"][b["type"]] = (
                summary["by_type"].get(b["type"], 0) + 1
            )

        project_counts = {}
        for b in bottlenecks:
            pid = b.get("project_id")
            if pid:
                if pid not in project_counts:
                    project_counts[pid] = {
                        "id": pid,
                        "name": b.get("project_name", "Unknown"),
                        "count": 0,
                    }
                project_counts[pid]["count"] += 1
        summary["top_affected_projects"] = sorted(
            project_counts.values(), key=lambda x: -x["count"]
        )[:5]

        return jsonify(
            {
                "bottlenecks": bottlenecks,
                "summary": summary,
                "analysis_period_days": days,
                "project_filter": project_id,
            }
        )


@app.route("/api/reports/bottleneck/trends", methods=["GET"])
@require_auth
def get_bottleneck_trends():
    """Get bottleneck trends over time."""
    periods = min(request.args.get("periods", 4, type=int), 12)
    period_length = request.args.get("period_length", "week")
    project_id = request.args.get("project_id", type=int)
    days_per_period = {"day": 1, "week": 7, "month": 30}.get(period_length, 7)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        trends = []
        pf = f"AND project_id = {project_id}" if project_id else ""
        mf = f"AND project_id = {project_id}" if project_id else ""

        for i in range(periods):
            start_days = (i + 1) * days_per_period
            end_days = i * days_per_period
            end_clause = f"'-{end_days} days'" if end_days > 0 else "'+1 day'"

            blocked = conn.execute(
                """
                SELECT COUNT(*) FROM (
                    SELECT 1 FROM features WHERE status = 'blocked' AND updated_at >= DATE('now', '-{start_days} days') AND updated_at < DATE('now', {end_clause}) {pf}
                    UNION ALL SELECT 1 FROM bugs WHERE status = 'blocked' AND updated_at >= DATE('now', '-{start_days} days') AND updated_at < DATE('now', {end_clause}) {pf}
                )
            """
            ).fetchone()[0]

            stale = conn.execute(
                """
                SELECT COUNT(*) FROM (
                    SELECT 1 FROM features WHERE status = 'in_progress' AND updated_at < DATE('now', '-{start_days + 7} days') AND created_at < DATE('now', '-{end_days} days') {pf}
                    UNION ALL SELECT 1 FROM bugs WHERE status IN ('open', 'in_progress') AND updated_at < DATE('now', '-{start_days + 7} days') AND created_at < DATE('now', '-{end_days} days') {pf}
                )
            """
            ).fetchone()[0]

            overdue_ms = conn.execute(
                """
                SELECT COUNT(*) FROM milestones WHERE target_date < DATE('now', '-{end_days} days')
                AND target_date >= DATE('now', '-{start_days} days') AND status != 'completed' {mf}
            """
            ).fetchone()[0]

            end_date = datetime.now() - timedelta(days=end_days)
            start_date = datetime.now() - timedelta(days=start_days)
            trends.append(
                {
                    "period": periods - i,
                    "start_date": start_date.strftime("%Y-%m-%d"),
                    "end_date": end_date.strftime("%Y-%m-%d"),
                    "blocked_items": blocked,
                    "stale_items": stale,
                    "overdue_milestones": overdue_ms,
                    "total": blocked + stale + overdue_ms,
                }
            )

        trends.reverse()
        trend_direction = "stable"
        if len(trends) >= 2:
            recent, older = trends[-1]["total"], trends[0]["total"]
            trend_direction = (
                "improving"
                if recent < older - 1
                else "worsening" if recent > older + 1 else "stable"
            )

        return jsonify(
            {
                "period_length": period_length,
                "periods": periods,
                "trends": trends,
                "trend_direction": trend_direction,
                "project_filter": project_id,
            }
        )


@app.route("/api/reports/bottleneck/by-project", methods=["GET"])
@require_auth
def get_bottleneck_by_project():
    """Get bottleneck summary grouped by project."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        projects = conn.execute(
            """
            SELECT p.id, p.name, p.status, p.priority,
                (SELECT COUNT(*) FROM features WHERE project_id = p.id AND status = 'blocked') as blocked_features,
                (SELECT COUNT(*) FROM bugs WHERE project_id = p.id AND status = 'blocked') as blocked_bugs,
                (SELECT COUNT(*) FROM features WHERE project_id = p.id AND status = 'in_progress' AND updated_at < datetime('now', '-7 days')) as stale_features,
                (SELECT COUNT(*) FROM bugs WHERE project_id = p.id AND status IN ('open', 'in_progress') AND updated_at < datetime('now', '-7 days')) as stale_bugs,
                (SELECT COUNT(*) FROM milestones WHERE project_id = p.id AND target_date < date('now') AND status != 'completed') as overdue_milestones,
                (SELECT COUNT(*) FROM bugs WHERE project_id = p.id AND severity = 'critical' AND status = 'open') as critical_bugs
            FROM projects p WHERE p.status = 'active' ORDER BY p.priority DESC, p.name
        """
        ).fetchall()

        results = []
        for p in projects:
            total = (
                (p["blocked_features"] or 0)
                + (p["blocked_bugs"] or 0)
                + (p["stale_features"] or 0)
                + (p["stale_bugs"] or 0)
                + (p["overdue_milestones"] or 0)
                + (p["critical_bugs"] or 0)
            )
            severity = (
                "critical"
                if p["critical_bugs"] > 0 or p["overdue_milestones"] > 2
                else (
                    "high"
                    if (p["blocked_features"] or 0) + (p["blocked_bugs"] or 0)
                    > 3
                    else "medium" if total > 5 else "healthy"
                )
            )
            results.append(
                {
                    "project_id": p["id"],
                    "project_name": p["name"],
                    "priority": p["priority"],
                    "severity": severity,
                    "bottlenecks": {
                        "blocked_features": p["blocked_features"] or 0,
                        "blocked_bugs": p["blocked_bugs"] or 0,
                        "stale_features": p["stale_features"] or 0,
                        "stale_bugs": p["stale_bugs"] or 0,
                        "overdue_milestones": p["overdue_milestones"] or 0,
                        "critical_bugs": p["critical_bugs"] or 0,
                    },
                    "total_bottlenecks": total,
                }
            )

        results.sort(key=lambda x: (-x["total_bottlenecks"], -x["priority"]))
        summary = {
            "total_projects": len(results),
            "projects_with_bottlenecks": len(
                [r for r in results if r["total_bottlenecks"] > 0]
            ),
            "by_severity": {
                s: len([r for r in results if r["severity"] == s])
                for s in ["critical", "high", "medium", "healthy"]
            },
        }
        return jsonify({"projects": results, "summary": summary})


@app.route("/api/reports/bottleneck/recommendations", methods=["GET"])
@require_auth
def get_bottleneck_recommendations():
    """Get actionable recommendations to resolve bottlenecks."""
    project_id = request.args.get("project_id", type=int)
    limit = min(request.args.get("limit", 10, type=int), 25)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        recommendations = []
        pf = f"AND project_id = {project_id}" if project_id else ""
        mf = f"AND m.project_id = {project_id}" if project_id else ""

        # Blocked items to unblock
        blocked = conn.execute(
            """
            SELECT 'feature' as item_type, id, title, project_id,
                   (SELECT name FROM projects WHERE id = features.project_id) as project_name,
                   julianday('now') - julianday(updated_at) as days_blocked
            FROM features WHERE status = 'blocked' {pf}
            UNION ALL SELECT 'bug' as item_type, id, title, project_id,
                   (SELECT name FROM projects WHERE id = bugs.project_id) as project_name,
                   julianday('now') - julianday(updated_at) as days_blocked
            FROM bugs WHERE status = 'blocked' AND severity IN ('critical', 'high') {pf}
            ORDER BY days_blocked DESC LIMIT 5
        """
        ).fetchall()
        for item in blocked:
            recommendations.append(
                {
                    "priority": (
                        "critical" if item["days_blocked"] > 7 else "high"
                    ),
                    "action": "unblock",
                    "item_type": item["item_type"],
                    "item_id": item["id"],
                    "title": item["title"],
                    "project_id": item["project_id"],
                    "project_name": item["project_name"],
                    "reason": f"Blocked for {
                        round(
                            item['days_blocked'],
                            1)} days",
                    "recommendation": f"Investigate and unblock {
                        item['item_type']} '{
                            item['title']}'",
                }
            )

        # Critical bugs to address
        aging = conn.execute(
            """
            SELECT id, title, project_id, (SELECT name FROM projects WHERE id = bugs.project_id) as project_name,
                   julianday('now') - julianday(created_at) as age_days
            FROM bugs WHERE severity = 'critical' AND status = 'open' AND julianday('now') - julianday(created_at) > 3 {pf}
            ORDER BY age_days DESC LIMIT 3
        """
        ).fetchall()
        for bug in aging:
            recommendations.append(
                {
                    "priority": "critical",
                    "action": "prioritize",
                    "item_type": "bug",
                    "item_id": bug["id"],
                    "title": bug["title"],
                    "project_id": bug["project_id"],
                    "project_name": bug["project_name"],
                    "reason": f"Critical bug open for {
                        round(
                            bug['age_days'],
                            1)} days",
                    "recommendation": f"Immediately prioritize critical bug '{
                        bug['title']}'",
                }
            )

        # Stale work to review
        stale = conn.execute(
            """
            SELECT 'feature' as item_type, id, title, project_id,
                   (SELECT name FROM projects WHERE id = features.project_id) as project_name,
                   julianday('now') - julianday(updated_at) as days_stale
            FROM features WHERE status = 'in_progress' AND updated_at < datetime('now', '-14 days') {pf}
            ORDER BY days_stale DESC LIMIT 3
        """
        ).fetchall()
        for item in stale:
            recommendations.append(
                {
                    "priority": "medium",
                    "action": "review",
                    "item_type": item["item_type"],
                    "item_id": item["id"],
                    "title": item["title"],
                    "project_id": item["project_id"],
                    "project_name": item["project_name"],
                    "reason": f"No updates for {
                        round(
                            item['days_stale'],
                            1)} days",
                    "recommendation": f"Review stale {
                        item['item_type']} '{
                        item['title']}' - reassign or close",
                }
            )

        # Overdue milestones
        overdue = conn.execute(
            """
            SELECT m.id, m.name, m.project_id, p.name as project_name,
                   julianday('now') - julianday(m.target_date) as days_overdue,
                   (SELECT COUNT(*) FROM features WHERE milestone_id = m.id AND status != 'completed') as pending
            FROM milestones m JOIN projects p ON m.project_id = p.id
            WHERE m.target_date < date('now') AND m.status != 'completed' {mf}
            ORDER BY days_overdue DESC LIMIT 3
        """
        ).fetchall()
        for m in overdue:
            recommendations.append(
                {
                    "priority": "high",
                    "action": "reschedule",
                    "item_type": "milestone",
                    "item_id": m["id"],
                    "title": m["name"],
                    "project_id": m["project_id"],
                    "project_name": m["project_name"],
                    "reason": f"Overdue by {
                        round(
                            m['days_overdue'],
                            1)} days with {
                        m['pending']} pending features",
                    "recommendation": f"Create recovery plan for milestone '{
                        m['name']}'",
                }
            )

        priority_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
        recommendations.sort(
            key=lambda x: priority_order.get(x["priority"], 4)
        )
        return jsonify(
            {
                "recommendations": recommendations[:limit],
                "total_count": len(recommendations[:limit]),
                "project_filter": project_id,
            }
        )


@app.route("/api/service-status", methods=["GET"])
@require_auth
def get_service_status():
    """Get status of all monitored services.

    Returns comprehensive service status including:
    - All configured services grouped by project
    - Online/offline status via health checks
    - Response times for online services
    - Summary statistics
    """
    import subprocess
    import time

    import requests

    # Service definitions (same as system_overview)
    MACMINI_HOST = "100.112.58.92"
    SERVICES = {
        "edu_apps": {
            "dev": {"port": 5051, "protocol": "https", "host": MACMINI_HOST},
            "qa": {"port": 5052, "protocol": "https", "host": MACMINI_HOST},
            "prod": {"port": 5063, "protocol": "https", "host": MACMINI_HOST},
            "env_1": {"port": 5054, "protocol": "https", "host": MACMINI_HOST},
            "env_2": {"port": 5055, "protocol": "https", "host": MACMINI_HOST},
            "env_3": {"port": 5056, "protocol": "https", "host": MACMINI_HOST},
            "env_4": {"port": 5057, "protocol": "https", "host": MACMINI_HOST},
            "env_5": {"port": 5058, "protocol": "https", "host": MACMINI_HOST},
        },
        "kanbanflow": {
            "dev": {"port": 6051, "protocol": "https", "host": MACMINI_HOST},
            "feature1": {
                "port": 6052,
                "protocol": "https",
                "host": MACMINI_HOST,
            },
            "feature2": {
                "port": 6053,
                "protocol": "https",
                "host": MACMINI_HOST,
            },
            "prod": {"port": 6054, "protocol": "https", "host": MACMINI_HOST},
        },
        "architect": {
            "prod": {"port": 8080, "protocol": "http"},
            "qa": {"port": 8081, "protocol": "http"},
            "dev": {"port": 8082, "protocol": "http"},
            "https": {"port": 8085, "protocol": "https"},
            "main": {"port": 8086, "protocol": "http"},
        },
    }

    def check_local_port(port):
        """Check if a port is listening locally."""
        try:
            result = subprocess.run(
                ["lso", "-i", f":{port}", "-t"],
                capture_output=True,
                text=True,
                timeout=2,
            )
            return bool(result.stdout.strip())
        except Exception:
            return False

    def check_remote_health(host, port, protocol):
        """Check remote service via HTTP health endpoint."""
        try:
            url = f"{protocol}://{host}:{port}/health"
            start = time.time()
            response = requests.get(url, timeout=3, verify=False)
            elapsed = round((time.time() - start) * 1000, 1)
            return response.status_code == 200, elapsed
        except Exception:
            return False, None

    try:
        services = []
        total_online = 0
        total_offline = 0
        by_project = {}

        for project, envs in SERVICES.items():
            if project not in by_project:
                by_project[project] = {"online": 0, "offline": 0}

            for env_name, config in envs.items():
                port = config["port"]
                host = config.get("host")
                protocol = config.get("protocol", "http")

                if host:
                    is_online, response_ms = check_remote_health(
                        host, port, protocol
                    )
                else:
                    is_online = check_local_port(port)
                    response_ms = None

                status = "online" if is_online else "offline"

                service_info = {
                    "project": project,
                    "environment": env_name,
                    "host": host or "localhost",
                    "port": port,
                    "protocol": protocol,
                    "status": status,
                    "url": f"{protocol}://{host or 'localhost'}:{port}",
                }

                if response_ms is not None:
                    service_info["response_ms"] = response_ms

                services.append(service_info)

                if is_online:
                    total_online += 1
                    by_project[project]["online"] += 1
                else:
                    total_offline += 1
                    by_project[project]["offline"] += 1

        return jsonify(
            {
                "timestamp": datetime.now().isoformat(),
                "summary": {
                    "total": total_online + total_offline,
                    "online": total_online,
                    "offline": total_offline,
                    "by_project": by_project,
                },
                "services": services,
            }
        )

    except Exception as e:
        logger.error(f"Error checking service status: {e}")
        return api_error(
            "Failed to check service status", 500, "service_check_error"
        )


def _check_worker_by_pid(pid_file_path, log_file_path=None, log_lines=5):
    """Helper to check worker status by PID file."""
    result = {"status": "stopped", "pid": None}
    try:
        pid_file = Path(pid_file_path)
        if pid_file.exists():
            pid = int(pid_file.read_text().strip())
            try:
                os.kill(pid, 0)
                result = {"status": "running", "pid": pid}
                # Get memory usage if psutil available
                try:
                    import psutil

                    proc = psutil.Process(pid)
                    result["memory_mb"] = round(
                        proc.memory_info().rss / 1024 / 1024, 1
                    )
                    result["cpu_percent"] = proc.cpu_percent(interval=0.1)
                except Exception:
                    pass
                # Get recent log entries
                if log_file_path:
                    log_file = Path(log_file_path)
                    if log_file.exists():
                        lines = (
                            log_file.read_text()
                            .strip()
                            .split("\n")[-log_lines:]
                        )
                        result["recent_logs"] = lines
            except OSError:
                result = {"status": "stopped", "pid": pid, "note": "stale PID"}
    except Exception as e:
        result["error"] = str(e)
    return result


def _check_worker_by_pgrep(pattern, log_file_path=None, count_pattern=None):
    """Helper to check worker status by pgrep."""
    result = {"status": "stopped"}
    try:
        proc = subprocess.run(
            ["pgrep", "-f", pattern], capture_output=True, text=True, timeout=2
        )
        if proc.returncode == 0:
            pids = [int(p) for p in proc.stdout.strip().split("\n") if p]
            result = {"status": "running", "pids": pids}
            # Get stats from log
            if log_file_path:
                log_file = Path(log_file_path)
                if log_file.exists():
                    content = log_file.read_text()
                    if count_pattern:
                        result["count"] = content.count(count_pattern)
                    lines = content.strip().split("\n")[-3:]
                    result["recent_logs"] = lines
    except Exception as e:
        result["error"] = str(e)
    return result


@app.route("/api/system-status", methods=["GET"])
@require_auth
def get_system_status():
    """Get status of autonomous system components (orchestrator, auto-confirm, workers)."""
    status = {"timestamp": datetime.now().isoformat(), "components": {}}

    # Check orchestrator
    status["components"]["orchestrator"] = _check_worker_by_pid(
        "/tmp/project_orchestrator.pid", "/tmp/project_orchestrator.log"
    )

    # Check auto-confirm worker
    autoconfirm = _check_worker_by_pgrep(
        "auto_confirm_worker", "/tmp/auto_confirm.log", "Confirmed #"
    )
    if "count" in autoconfirm:
        autoconfirm["total_confirmations"] = autoconfirm.pop("count")
    status["components"]["auto_confirm"] = autoconfirm

    # Check assigner worker
    status["components"]["assigner"] = _check_worker_by_pid(
        "/tmp/assigner_worker.pid", "/tmp/assigner_worker.log"
    )

    # Check milestone worker
    status["components"]["milestone_worker"] = _check_worker_by_pid(
        "/tmp/milestone_worker.pid", "/tmp/milestone_worker.log"
    )

    # Check task worker
    status["components"]["task_worker"] = _check_worker_by_pid(
        "/tmp/architect_worker.pid", "/tmp/architect_worker.log"
    )

    # Check Perplexity/Comet connection
    comet_status = {"status": "not running"}
    try:
        import requests as req

        resp = req.get("http://localhost:9222/json", timeout=2)
        if resp.status_code == 200:
            targets = resp.json()
            perplexity_targets = [
                t for t in targets if "perplexity" in t.get("url", "").lower()
            ]
            comet_status = {
                "status": "running",
                "total_targets": len(targets),
                "perplexity_targets": len(perplexity_targets),
            }
    except Exception:
        pass
    status["components"]["comet"] = comet_status

    # Check tmux worker sessions
    workers_status = {"sessions": [], "active_count": 0}
    try:
        result = subprocess.run(
            [
                "tmux",
                "list-sessions",
                "-F",
                "#{session_name}:#{session_attached}",
            ],
            capture_output=True,
            text=True,
            timeout=5,
        )
        if result.returncode == 0:
            worker_sessions = [
                "architect",
                "autoconfirm",
                "task_worker1",
                "task_worker2",
                "task_worker3",
                "task_worker4",
                "task_worker5",
            ]
            for line in result.stdout.strip().split("\n"):
                if ":" in line:
                    name, attached = line.rsplit(":", 1)
                    if name in worker_sessions:
                        workers_status["sessions"].append(
                            {"name": name, "attached": attached == "1"}
                        )
            workers_status["active_count"] = len(workers_status["sessions"])
    except Exception as e:
        workers_status["error"] = str(e)
    status["components"]["tmux_sessions"] = workers_status

    # Get task queue stats from database
    queue_stats = {"pending": 0, "in_progress": 0, "completed_24h": 0}
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            # Pending and in-progress counts
            rows = conn.execute(
                """
                SELECT status, COUNT(*) as cnt FROM task_queue
                WHERE status IN ('pending', 'in_progress')
                GROUP BY status
            """
            ).fetchall()
            for row in rows:
                if row["status"] == "pending":
                    queue_stats["pending"] = row["cnt"]
                elif row["status"] == "in_progress":
                    queue_stats["in_progress"] = row["cnt"]
            # Completed in last 24 hours
            completed = conn.execute(
                """
                SELECT COUNT(*) FROM task_queue
                WHERE status = 'completed'
                AND completed_at > datetime('now', '-24 hours')
            """
            ).fetchone()[0]
            queue_stats["completed_24h"] = completed
    except Exception as e:
        queue_stats["error"] = str(e)
    status["task_queue"] = queue_stats

    # Get autopilot status if table exists
    autopilot_stats = {"active_runs": 0, "pending_milestones": 0}
    try:
        with get_db_connection() as conn:
            # Check if autopilot tables exist
            tables = conn.execute(
                """
                SELECT name FROM sqlite_master
                WHERE type='table' AND name IN ('autopilot_runs', 'autopilot_milestones')
            """
            ).fetchall()
            table_names = [t[0] for t in tables]

            if "autopilot_runs" in table_names:
                active = conn.execute(
                    """
                    SELECT COUNT(*) FROM autopilot_runs
                    WHERE status IN ('running', 'planning', 'implementing')
                """
                ).fetchone()[0]
                autopilot_stats["active_runs"] = active

            if "autopilot_milestones" in table_names:
                pending = conn.execute(
                    """
                    SELECT COUNT(*) FROM autopilot_milestones
                    WHERE status = 'pending'
                """
                ).fetchone()[0]
                autopilot_stats["pending_milestones"] = pending
    except Exception:
        pass
    status["autopilot"] = autopilot_stats

    # Overall status calculation
    core_components = ["orchestrator", "auto_confirm", "task_worker"]
    running_core = sum(
        1
        for c in core_components
        if status["components"].get(c, {}).get("status") == "running"
    )
    total_running = sum(
        1
        for c in status["components"].values()
        if c.get("status") == "running"
    )

    if running_core >= 2:
        status["overall"] = "healthy"
    elif total_running >= 2:
        status["overall"] = "degraded"
    elif total_running >= 1:
        status["overall"] = "minimal"
    else:
        status["overall"] = "stopped"

    status["summary"] = {
        "running_components": total_running,
        "total_components": len(status["components"]),
    }

    return jsonify(status)


@app.route("/api/system/cleanup", methods=["POST"])
@require_auth
def system_cleanup():
    """Run storage cleanup operations.

    Request body (all optional):
    {
        "dry_run": true,              # Show what would be deleted
        "backup_retention": 10,       # Number of backups to keep per environment
        "log_age_days": 30,           # Delete logs older than N days
        "task_age_days": 7,           # Delete completed tasks older than N days
        "error_age_days": 30,         # Delete resolved errors older than N days
        "prompt_age_days": 14         # Archive prompts older than N days
    }

    Returns cleanup statistics.
    """
    try:
        data = request.get_json() or {}
        dry_run = data.get("dry_run", True)  # Default to dry-run for safety

        # Import cleanup worker
        sys.path.insert(0, str(BASE_DIR / "workers"))
        from cleanup_worker import DEFAULT_BACKUP_RETENTION, CleanupWorker

        # Build configuration
        backup_retention = DEFAULT_BACKUP_RETENTION.copy()
        if "backup_retention" in data:
            for env in backup_retention:
                backup_retention[env] = data["backup_retention"]

        log_age = data.get("log_age_days", 30)
        task_age = data.get("task_age_days", 7)
        error_age = data.get("error_age_days", 30)
        prompt_age = data.get("prompt_age_days", 14)

        # Create worker and run cleanup
        worker = CleanupWorker(
            dry_run=dry_run,
            backup_retention=backup_retention,
            log_age_days=log_age,
            task_age_days=task_age,
            error_age_days=error_age,
            prompt_age_days=prompt_age,
        )

        stats = worker.run_cleanup()

        # Log activity
        log_activity(
            "system_cleanup",
            "system",
            None,
            {"dry_run": dry_run, "stats": stats.to_dict()},
        )

        return jsonify(
            {
                "success": True,
                "dry_run": dry_run,
                "stats": stats.to_dict(),
                "message": (
                    "Cleanup completed"
                    if not dry_run
                    else "Dry run completed (no files deleted)"
                ),
            }
        )

    except Exception as e:
        logger.error(f"System cleanup failed: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/system/cleanup/status", methods=["GET"])
@require_auth
def system_cleanup_status():
    """Get cleanup daemon status and last run statistics."""
    try:
        sys.path.insert(0, str(BASE_DIR / "workers"))
        from cleanup_worker import PID_FILE, load_state

        # Check if daemon is running
        daemon_running = False
        daemon_pid = None
        if PID_FILE.exists():
            with open(PID_FILE, "r") as f:
                daemon_pid = int(f.read().strip())
            try:
                os.kill(daemon_pid, 0)  # Check if process exists
                daemon_running = True
            except ProcessLookupError:
                daemon_running = False

        # Load last run state
        state = load_state()

        return jsonify(
            {
                "success": True,
                "daemon_running": daemon_running,
                "daemon_pid": daemon_pid,
                "last_run": state.get("last_run") if state else None,
                "last_stats": state.get("stats") if state else None,
            }
        )

    except Exception as e:
        logger.error(f"Failed to get cleanup status: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/orchestrator/tasks", methods=["GET"])
@require_auth
def get_orchestrator_tasks():
    """Get orchestrator task progress from Google Sheets."""
    try:
        # Try to import gspread and get sheets client
        import gspread
        from google.oauth2.service_account import Credentials

        SPREADSHEET_ID = "12i2uO6-41uZdHl_a9BbhBHhR1qbNlAqOgH-CWQBz7rA"
        CREDENTIALS_PATH = (
            Path.home() / ".config" / "gspread" / "service_account.json"
        )

        if not CREDENTIALS_PATH.exists():
            return jsonify(
                {
                    "error": "Google Sheets credentials not configured",
                    "tasks": [],
                    "summary": {
                        "pending": 0,
                        "implementing": 0,
                        "completed": 0,
                        "total": 0,
                    },
                }
            )

        scopes = [
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive",
        ]
        creds = Credentials.from_service_account_file(
            str(CREDENTIALS_PATH), scopes=scopes
        )
        client = gspread.authorize(creds)

        spreadsheet = client.open_by_key(SPREADSHEET_ID)
        ws = spreadsheet.worksheet("DevTasks")
        rows = ws.get_all_values()

        tasks = []
        summary = {
            "pending": 0,
            "implementing": 0,
            "completed": 0,
            "failed": 0,
            "blocked": 0,
            "total": 0,
        }

        if len(rows) > 1:
            # Header: ID, Description, Type, Priority, Status, Assigned,
            # Session, Project, Started, Completed, Result
            for row in rows[1:]:
                if not row or not row[0]:
                    continue

                task_id = row[0]
                description = row[1] if len(row) > 1 else ""
                task_type = row[2] if len(row) > 2 else "dev"
                priority = row[3] if len(row) > 3 else "5"
                status = (row[4] if len(row) > 4 else "pending").lower()
                assigned = row[5] if len(row) > 5 else ""
                session = row[6] if len(row) > 6 else ""
                project = row[7] if len(row) > 7 else ""
                started = row[8] if len(row) > 8 else ""
                completed_at = row[9] if len(row) > 9 else ""

                # Normalize status for counting
                if status in (
                    "in_progress",
                    "implementing",
                    "running",
                    "assigned",
                ):
                    count_status = "implementing"
                elif status in ("done", "complete", "completed"):
                    count_status = "completed"
                elif status in ("failed", "error"):
                    count_status = "failed"
                elif status == "blocked":
                    count_status = "blocked"
                else:
                    count_status = "pending"

                summary[count_status] = summary.get(count_status, 0) + 1
                summary["total"] += 1

                tasks.append(
                    {
                        "id": task_id,
                        "description": description[:100]
                        + ("..." if len(description) > 100 else ""),
                        "type": task_type,
                        "priority": priority,
                        "status": status,
                        "status_category": count_status,
                        "assigned": assigned,
                        "session": session,
                        "project": project,
                        "started": started,
                        "completed": completed_at,
                    }
                )

        return jsonify(
            {
                "tasks": tasks,
                "summary": summary,
                "timestamp": datetime.now().isoformat(),
            }
        )

    except Exception as e:
        logger.error(f"Error fetching orchestrator tasks: {e}")
        return jsonify(
            {
                "error": str(e),
                "tasks": [],
                "summary": {
                    "pending": 0,
                    "implementing": 0,
                    "completed": 0,
                    "total": 0,
                },
            }
        )


@app.route("/api/services/config", methods=["GET"])
def get_services_config():
    """Get the centralized services configuration file. No auth - shared config."""
    config_path = DATA_DIR / "services.json"
    if config_path.exists():
        with open(config_path) as f:
            return jsonify(json.load(f))
    return jsonify({"error": "Config not found"}), 404


@app.route("/api/services", methods=["GET"])
@require_auth
def get_running_services():
    """Get list of running services/apps with their ports."""
    import subprocess

    services = []

    # Load known services from config file
    config_path = DATA_DIR / "services.json"
    known_services = {}
    tailscale_host = "100.112.58.92"  # Default tailscale host

    print(
        f"[Services API] Looking for config at: {config_path}, exists: {
            config_path.exists()}"
    )

    if config_path.exists():
        with open(config_path) as f:
            config = json.load(f)
            tailscale_host = config.get("hosts", {}).get(
                "tailscale", tailscale_host
            )
            apps = config.get("apps", {})

            for svc_id, svc_info in config.get("services", {}).items():
                port = svc_info.get("port")
                app_id = svc_info.get("app")
                app_config = apps.get(app_id, {})
                protocol = svc_info.get("protocol", "http")

                known_services[port] = {
                    "id": svc_id,
                    "name": app_config.get("name", app_id),
                    "type": app_config.get("type", "unknown"),
                    "env": svc_info.get("env", "unknown").upper(),
                    "protocol": protocol,
                    "url": f"{protocol}://{tailscale_host}:{port}",
                }

    # Fallback if no config - all architect envs use HTTPS
    if not known_services:
        known_services = {
            8080: {
                "id": "architect-prod",
                "name": "Architect",
                "type": "dashboard",
                "env": "PROD",
                "protocol": "https",
                "url": f"https://{tailscale_host}:8080",
            },
            8085: {
                "id": "architect-feature1",
                "name": "Architect",
                "type": "dashboard",
                "env": "Feature1",
                "protocol": "https",
                "url": f"https://{tailscale_host}:8085",
            },
            8086: {
                "id": "architect-feature2",
                "name": "Architect",
                "type": "dashboard",
                "env": "Feature2",
                "protocol": "https",
                "url": f"https://{tailscale_host}:8086",
            },
            8087: {
                "id": "architect-feature3",
                "name": "Architect",
                "type": "dashboard",
                "env": "Feature3",
                "protocol": "https",
                "url": f"https://{tailscale_host}:8087",
            },
            5051: {
                "id": "edu-dev",
                "name": "Basic Edu Apps",
                "type": "webapp",
                "env": "DEV",
                "protocol": "https",
                "url": f"https://{tailscale_host}:5051",
            },
            5052: {
                "id": "edu-qa",
                "name": "Basic Edu Apps",
                "type": "webapp",
                "env": "QA",
                "protocol": "https",
                "url": f"https://{tailscale_host}:5052",
            },
            5063: {
                "id": "edu-prod",
                "name": "Basic Edu Apps",
                "type": "webapp",
                "env": "PROD",
                "protocol": "https",
                "url": f"https://{tailscale_host}:5063",
            },
        }

    try:
        # Get listening ports using lsof
        result = subprocess.run(
            ["lso", "-iTCP", "-sTCP:LISTEN", "-P", "-n"],
            capture_output=True,
            text=True,
            timeout=5,
        )

        for line in result.stdout.split("\n")[1:]:  # Skip header
            parts = line.split()
            if len(parts) >= 9 and "Python" in parts[0]:
                pid = parts[1]
                port_info = parts[8]
                # Extract port from *:PORT or IP:PORT
                if ":" in port_info:
                    port_str = port_info.split(":")[-1]
                    try:
                        port = int(port_str)
                        service_info = known_services.get(
                            port,
                            {
                                "id": f"unknown-{port}",
                                "name": "Python App",
                                "type": "unknown",
                                "env": "Unknown",
                                "protocol": "http",
                                "url": f"http://{tailscale_host}:{port}",
                            },
                        )

                        # Check if service is healthy (use service protocol)
                        health_status = "unknown"
                        try:
                            import ssl
                            import urllib.request

                            protocol = service_info.get("protocol", "https")
                            health_url = (
                                f"{protocol}://localhost:{port}/health"
                            )
                            req = urllib.request.Request(
                                health_url, method="GET"
                            )
                            # Create SSL context that doesn't verify certs (for
                            # self-signed)
                            ctx = ssl.create_default_context()
                            ctx.check_hostname = False
                            ctx.verify_mode = ssl.CERT_NONE
                            with urllib.request.urlopen(
                                req, timeout=2, context=ctx
                            ) as resp:
                                if resp.status == 200:
                                    health_status = "healthy"
                        except Exception:
                            health_status = "no-health-endpoint"

                        services.append(
                            {
                                "id": service_info.get(
                                    "id", f"unknown-{port}"
                                ),
                                "port": port,
                                "pid": pid,
                                "name": service_info["name"],
                                "type": service_info["type"],
                                "env": service_info["env"],
                                "health": health_status,
                                "url": service_info.get(
                                    "url", f"http://{tailscale_host}:{port}"
                                ),
                            }
                        )
                    except ValueError:
                        pass

        # Sort by port
        services.sort(key=lambda x: x["port"])

    except Exception as e:
        return jsonify({"error": str(e), "services": []}), 500

    return jsonify({"services": services})


# ============================================================================
# CODE SOURCE SCANNING
# ============================================================================


@app.route("/api/scan-sources", methods=["POST"])
@require_auth
def scan_code_sources():
    """Scan code source directories for projects with detailed extraction."""
    discovered = []

    for source_dir in CODE_SOURCES:
        if source_dir.exists():
            for item in source_dir.iterdir():
                if item.is_dir() and not item.name.startswith("."):
                    # Check for common project indicators
                    is_project = any(
                        [
                            (item / "setup.py").exists(),
                            (item / "pyproject.toml").exists(),
                            (item / "package.json").exists(),
                            (item / "Cargo.toml").exists(),
                            (item / "go.mod").exists(),
                            (item / ".git").exists(),
                            (item / "requirements.txt").exists(),
                        ]
                    )

                    if is_project:
                        project_info = extract_project_details(item)
                        project_info["source"] = str(source_dir)
                        discovered.append(project_info)

    return jsonify(
        {"discovered": discovered, "sources_scanned": len(CODE_SOURCES)}
    )


def extract_project_details(project_path: Path) -> dict:
    """Extract detailed information from a project directory."""
    info = {
        "name": project_path.name,
        "path": str(project_path),
        "description": None,
        "objectives": [],
        "features": [],
        "status": "active",
        "language": None,
        "framework": None,
    }

    # Try to get description from README
    for readme_name in ["README.md", "readme.md", "README.txt", "README"]:
        readme_path = project_path / readme_name
        if readme_path.exists():
            try:
                content = readme_path.read_text()[:2000]  # First 2000 chars
                # Extract first paragraph as description
                lines = content.split("\n")
                desc_lines = []
                in_desc = False
                for line in lines:
                    if line.startswith("#"):
                        if in_desc:
                            break
                        in_desc = True
                        continue
                    if in_desc and line.strip():
                        desc_lines.append(line.strip())
                    elif in_desc and not line.strip() and desc_lines:
                        break
                if desc_lines:
                    info["description"] = " ".join(desc_lines)[:500]
                break
            except OSError:
                pass

    # Try to get objectives from CLAUDE.md
    claude_md = project_path / "CLAUDE.md"
    if claude_md.exists():
        try:
            content = claude_md.read_text()
            # Look for objectives/goals section
            import re

            obj_match = re.search(
                r"(?:##?\s*(?:Objectives?|Goals?|Purpose|Overview))\s*\n(.*?)(?=\n##?\s|\Z)",
                content,
                re.IGNORECASE | re.DOTALL,
            )
            if obj_match:
                objectives_text = obj_match.group(1).strip()
                # Extract bullet points
                for line in objectives_text.split("\n"):
                    line = line.strip()
                    if line.startswith(("- ", "* ", " ")):
                        info["objectives"].append(line[2:].strip()[:200])
                    elif (
                        line
                        and not line.startswith("#")
                        and len(info["objectives"]) < 5
                    ):
                        info["objectives"].append(line[:200])
        except (OSError, re.error):
            pass

    # Detect language/framework
    if (project_path / "package.json").exists():
        info["language"] = "JavaScript/TypeScript"
        try:
            pkg = json.loads((project_path / "package.json").read_text())
            deps = {
                **pkg.get("dependencies", {}),
                **pkg.get("devDependencies", {}),
            }
            if "react" in deps:
                info["framework"] = "React"
            elif "vue" in deps:
                info["framework"] = "Vue"
            elif "express" in deps:
                info["framework"] = "Express"
        except (OSError, json.JSONDecodeError, KeyError):
            pass
    elif (project_path / "requirements.txt").exists() or (
        project_path / "setup.py"
    ).exists():
        info["language"] = "Python"
        req_file = project_path / "requirements.txt"
        if req_file.exists():
            try:
                reqs = req_file.read_text().lower()
                if "flask" in reqs:
                    info["framework"] = "Flask"
                elif "django" in reqs:
                    info["framework"] = "Django"
                elif "fastapi" in reqs:
                    info["framework"] = "FastAPI"
            except OSError:
                pass
    elif (project_path / "Cargo.toml").exists():
        info["language"] = "Rust"
    elif (project_path / "go.mod").exists():
        info["language"] = "Go"

    # Look for feature files
    features_dir = project_path / "features"
    if features_dir.exists():
        for f in features_dir.glob("*.json"):
            try:
                feat = json.loads(f.read_text())
                if "features" in feat:
                    for name, data in feat["features"].items():
                        info["features"].append(
                            {
                                "name": name,
                                "status": data.get("status", "unknown"),
                            }
                        )
            except (OSError, json.JSONDecodeError, KeyError, TypeError):
                pass

    return info


@app.route("/api/extract-tmux-info", methods=["POST"])
@require_auth
def extract_tmux_info():
    """Extract information from tmux sessions about what they're working on."""
    sessions_info = []

    try:
        # Get all sessions
        result = subprocess.run(
            [
                "tmux",
                "list-sessions",
                "-F",
                "#{session_name}:#{session_path}:#{pane_current_path}",
            ],
            capture_output=True,
            text=True,
            timeout=5,
        )

        if result.returncode != 0:
            return jsonify({"sessions": [], "error": "No tmux sessions"})

        for line in result.stdout.strip().split("\n"):
            if not line:
                continue

            parts = line.split(":")
            session_name = parts[0]

            # Get recent output from each session
            capture_result = subprocess.run(
                [
                    "tmux",
                    "capture-pane",
                    "-t",
                    session_name,
                    "-p",
                    "-S",
                    "-50",
                ],
                capture_output=True,
                text=True,
                timeout=5,
            )

            output = (
                capture_result.stdout if capture_result.returncode == 0 else ""
            )

            # Try to identify what the session is doing
            purpose = "Unknown"
            current_task = None

            output_lower = output.lower()
            if "claude" in session_name.lower() or "claude" in output_lower:
                purpose = "Claude Agent"
                # Look for task context in output
                import re

                task_match = re.search(
                    r"\[(?:FEATURE|BUG|ERROR) #\d+\].*", output
                )
                if task_match:
                    current_task = task_match.group(0)
            elif "python" in output_lower or "flask" in output_lower:
                purpose = "Python Development"
            elif "npm" in output_lower or "node" in output_lower:
                purpose = "Node.js Development"
            elif "git" in output_lower:
                purpose = "Git Operations"
            elif "test" in output_lower or "pytest" in output_lower:
                purpose = "Testing"

            sessions_info.append(
                {
                    "name": session_name,
                    "path": parts[2] if len(parts) > 2 else None,
                    "purpose": purpose,
                    "current_task": current_task,
                    "last_lines": output.split("\n")[-10:] if output else [],
                }
            )

        # Update tmux_sessions in database with purposes
        with get_db_connection() as conn:
            for tmux_session in sessions_info:
                conn.execute(
                    """
                    UPDATE tmux_sessions SET purpose = ?, last_activity = CURRENT_TIMESTAMP
                    WHERE session_name = ?
                """,
                    (tmux_session["purpose"], tmux_session["name"]),
                )

        return jsonify({"sessions": sessions_info})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/import-project", methods=["POST"])
@require_auth
def import_project():
    """Import a discovered project with extracted details."""
    data = request.get_json()
    path = data.get("path")
    name = data.get("name") or Path(path).name
    description = data.get("description")
    objectives = data.get("objectives", [])
    features = data.get("features", [])
    language = data.get("language")
    framework = data.get("framework")

    # If no description provided, try to extract
    if not description:
        project_details = extract_project_details(Path(path))
        description = (
            project_details.get("description") or f"Imported from {path}"
        )
        objectives = objectives or project_details.get("objectives", [])
        features = features or project_details.get("features", [])
        language = language or project_details.get("language")
        framework = framework or project_details.get("framework")

    with get_db_connection() as conn:
        # Check if already imported
        existing = conn.execute(
            "SELECT id FROM projects WHERE source_path = ?", (path,)
        ).fetchone()

        if existing:
            return (
                jsonify(
                    {"error": "Project already imported", "id": existing[0]}
                ),
                400,
            )

        # Check if project with same name already exists
        existing_name = conn.execute(
            "SELECT id FROM projects WHERE name = ?", (name,)
        ).fetchone()

        if existing_name:
            return (
                jsonify(
                    {
                        "error": f'Project with name "{name}" already exists',
                        "id": existing_name[0],
                    }
                ),
                400,
            )

        # Build description with language/framework info
        full_desc = description
        if language or framework:
            full_desc += (
                f"\n\nTech: {language or ''} {framework or ''}".strip()
            )
        if objectives:
            full_desc += "\n\nObjectives:\n" + "\n".join(
                f"- {o}" for o in objectives[:5]
            )

        try:
            cursor = conn.execute(
                """
                INSERT INTO projects (name, source_path, description, status)
                VALUES (?, ?, ?, 'active')
            """,
                (name, path, full_desc),
            )
        except sqlite3.IntegrityError as e:
            if "projects.name" in str(e):
                return (
                    jsonify(
                        {"error": f'Project with name "{name}" already exists'}
                    ),
                    400,
                )
            elif "projects.source_path" in str(e):
                return (
                    jsonify(
                        {"error": "Project already imported from this path"}
                    ),
                    400,
                )
            raise

        project_id = cursor.lastrowid

        # Import discovered features
        for feat in features[:20]:  # Limit to 20 features
            conn.execute(
                """
                INSERT INTO features (project_id, name, status, description)
                VALUES (?, ?, ?, 'Imported from feature file')
            """,
                (
                    project_id,
                    feat.get("name", "Unknown"),
                    feat.get("status", "draft"),
                ),
            )

        # Try to import errors.md if exists
        errors_file = Path(path) / "errors.md"
        if errors_file.exists():
            import_errors_from_file(project_id, errors_file)

        log_activity("import_project", "project", project_id, name)

        return jsonify(
            {
                "id": project_id,
                "success": True,
                "features_imported": len(features),
            }
        )


def import_errors_from_file(project_id: int, errors_file: Path):
    """Import errors from an errors.md file."""
    try:
        content = errors_file.read_text()

        # Parse errors (format: ### [ ] or ### [X])
        import re

        error_pattern = r"### \[([X ])\] (.+?)(?=\n###|\n## |\Z)"

        with get_db_connection() as conn:
            for match in re.finditer(error_pattern, content, re.DOTALL):
                resolved = match.group(1) == "X"
                error_text = match.group(2).strip()

                # Extract first line as title, rest as description
                lines = error_text.split("\n")
                title = lines[0].strip()
                description = (
                    "\n".join(lines[1:]).strip() if len(lines) > 1 else ""
                )

                conn.execute(
                    """
                    INSERT INTO errors (project_id, error_type, message, source, status)
                    VALUES (?, 'imported', ?, 'errors.md', ?)
                """,
                    (project_id, title, "resolved" if resolved else "open"),
                )

    except Exception as e:
        logger.error(f"Failed to import errors from {errors_file}: {e}")


# ============================================================================
# ASSIGN TO TMUX SESSION
# ============================================================================


@app.route("/api/assign-to-tmux", methods=["POST"])
@require_auth
def assign_to_tmux():
    """Assign a task (feature, bug, error) to a tmux session.

    Uses database connection with retry logic to handle concurrent access (fixes #36).
    """
    data = request.get_json()

    entity_type = data.get("entity_type")  # feature, bug, error, devops
    entity_id = data.get("entity_id")
    session_name = data.get("session")
    message = data.get("message")
    use_queue = data.get("use_queue", False)  # Queue instead of direct send

    # Session only required for direct tmux send
    if not all([entity_type, entity_id]):
        return jsonify({"error": "entity_type and entity_id required"}), 400
    if not use_queue and not session_name:
        return (
            jsonify(
                {
                    "error": "session required for direct tmux send (or use use_queue=true)"
                }
            ),
            400,
        )

    # Get entity details - use connection with timeout/retry for concurrency
    # (fixes #36)
    try:
        conn = get_db_connection()
    except sqlite3.OperationalError as e:
        if "database is locked" in str(e):
            return jsonify({"error": "Database busy, please retry"}), 503
        raise

    if entity_type == "feature":
        entity = conn.execute(
            "SELECT * FROM features WHERE id = ?", (entity_id,)
        ).fetchone()
        table = "features"
    elif entity_type == "bug":
        entity = conn.execute(
            "SELECT * FROM bugs WHERE id = ?", (entity_id,)
        ).fetchone()
        table = "bugs"
    elif entity_type == "error":
        entity = conn.execute(
            "SELECT * FROM errors WHERE id = ?", (entity_id,)
        ).fetchone()
        table = "errors"
    elif entity_type == "devops":
        entity = conn.execute(
            "SELECT * FROM devops_tasks WHERE id = ?", (entity_id,)
        ).fetchone()
        table = "devops_tasks"
    else:
        conn.close()
        return jsonify({"error": "Invalid entity type"}), 400

    if not entity:
        conn.close()
        return jsonify({"error": f"{entity_type} not found"}), 404

    # Build message with full workflow if not provided
    if not message:
        # Convert Row to dict for easier access
        entity_dict = dict(entity)

        # Get dashboard URL from request
        dashboard_url = request.host_url.rstrip("/")

        # Get project path if available
        project_path = ""
        if entity_type in ("feature", "bug") and entity_dict.get("project_id"):
            project = conn.execute(
                "SELECT source_path FROM projects WHERE id = ?",
                (entity_dict["project_id"],),
            ).fetchone()
            if project:
                project_path = f"\nProject Path: {project['source_path']}"

        if entity_type == "feature":
            message = """Implement this feature and complete the full workflow:

FEATURE ID: {entity_id}
Name: {entity_dict['name']}
Description: {entity_dict.get('description', 'N/A')}
Spec: {entity_dict.get('spec') or 'N/A'}{project_path}

WORKFLOW - Complete ALL steps:
1. Implement the feature in the codebase
2. Add tests for the new functionality
3. Run tests to verify everything works
4. Commit with a descriptive message
5. Restart/deploy the server
6. Mark feature as completed:
   curl -X PUT "{dashboard_url}/api/features/{entity_id}" -H "Content-Type: application/json" -H "Cookie: $(cat /tmp/arch_cookies.txt | grep session | cut -f7)" -d '{{"status": "completed"}}'

Start by analyzing what needs to be implemented."""

        elif entity_type == "bug":
            message = """Fix this bug and complete the full workflow:

BUG ID: {entity_id}
Title: {entity_dict['title']}
Description: {entity_dict.get('description', 'N/A')}
Severity: {entity_dict.get('severity', 'N/A')}
Stack trace: {entity_dict.get('stack_trace') or 'N/A'}{project_path}

WORKFLOW - Complete ALL steps:
1. Analyze and fix the bug in the codebase
2. Add test to prevent regression
3. Run tests to verify the fix
4. Commit with a descriptive message
5. Restart/deploy the server
6. Mark bug as resolved:
   curl -X PUT "{dashboard_url}/api/bugs/{entity_id}" -H "Content-Type: application/json" -H "Cookie: $(cat /tmp/arch_cookies.txt | grep session | cut -f7)" -d '{{"status": "resolved"}}'

Start by analyzing the bug."""

        elif entity_type == "error":
            message = """Fix this error and complete the full workflow:

ERROR ID: {entity_id}
Type: {entity_dict['error_type']}
Message: {entity_dict['message']}
Source: {entity_dict.get('source', 'N/A')}
Occurrences: {entity_dict.get('occurrence_count', 1)}
Stack trace: {entity_dict.get('stack_trace') or 'N/A'}

WORKFLOW - Complete ALL steps:
1. Analyze and fix the error in the codebase
2. Run tests to verify the fix
3. Commit with a descriptive message
4. Restart/deploy the server
5. Mark error as resolved:
   curl -X POST "{dashboard_url}/api/errors/{entity_id}/resolve" -H "Cookie: $(cat /tmp/arch_cookies.txt | grep session | cut -f7)"

Start by analyzing the error."""

        elif entity_type == "devops":
            message = """Complete this DevOps task and the full workflow:

TASK ID: {entity_id}
Name: {entity_dict['name']}
Description: {entity_dict.get('description', 'N/A')}
Type: {entity_dict.get('task_type', 'N/A')}

WORKFLOW - Complete ALL steps:
1. Complete the DevOps task
2. Verify it works correctly
3. Commit any code changes
4. Mark task as completed:
   curl -X PUT "{dashboard_url}/api/devops-tasks/{entity_id}" -H "Content-Type: application/json" -H "Cookie: $(cat /tmp/arch_cookies.txt | grep session | cut -f7)" -d '{{"status": "completed"}}'

Start by analyzing what needs to be done."""

    # Queue mode: Add to task queue for background processing
    if use_queue:
        import json as json_module

        task_data = json_module.dumps(
            {
                "entity_type": entity_type,
                "entity_id": entity_id,
                "session": session_name or "arch_env3",  # Default session
                "message": message,
                "dashboard_url": dashboard_url,
            }
        )

        cursor = conn.execute(
            """
            INSERT INTO task_queue (task_type, task_data, priority, max_retries)
            VALUES ('claude_task', ?, 2, 3)
        """,
            (task_data,),
        )
        task_id = cursor.lastrowid

        # Update entity status to 'queued'
        if table != "errors":
            conn.execute(
                """
                UPDATE {table} SET
                    status = 'queued',
                    updated_at = CURRENT_TIMESTAMP
                WHERE id = ?
            """,
                (entity_id,),
            )
        else:
            conn.execute(
                """
                UPDATE errors SET status = 'queued', last_seen = CURRENT_TIMESTAMP WHERE id = ?
            """,
                (entity_id,),
            )

        conn.commit()
        conn.close()
        log_activity(
            f"queue_{entity_type}",
            entity_type,
            entity_id,
            f"task_id:{task_id}",
        )

        return jsonify(
            {
                "success": True,
                "queued": True,
                "task_id": task_id,
                "message": f"{entity_type.title()} added to queue",
            }
        )

    # Direct mode: Send to tmux (text and Enter separately for Claude Code)
    try:
        subprocess.run(
            ["tmux", "send-keys", "-t", session_name, message],
            check=True,
            timeout=5,
        )
        time.sleep(1.0)
        # Send Enter using paste-buffer (works with Claude Code)
        subprocess.run(
            ["tmux", "load-buffer", "-"], input=b"\r", check=True, timeout=5
        )
        subprocess.run(
            ["tmux", "paste-buffer", "-t", session_name], check=True, timeout=5
        )

        # Update entity with assignment
        if table != "errors":
            conn.execute(
                """
                UPDATE {table} SET
                    tmux_session = ?,
                    status = CASE WHEN status = 'open' OR status = 'draft' THEN 'in_progress' ELSE status END,
                    updated_at = CURRENT_TIMESTAMP
                WHERE id = ?
            """,
                (session_name, entity_id),
            )

        conn.commit()
        log_activity(
            f"assign_{entity_type}_to_tmux",
            entity_type,
            entity_id,
            session_name,
        )

        return jsonify({"success": True})

    except sqlite3.OperationalError as e:
        if "database is locked" in str(e):
            return jsonify({"error": "Database busy, please retry"}), 503
        return jsonify({"error": str(e)}), 500
    except Exception as e:
        return jsonify({"error": str(e)}), 500
    finally:
        conn.close()


# ============================================================================
# DATABASE CONNECTION POOL API
# ============================================================================


@app.route("/api/db/pool/stats", methods=["GET"])
@require_auth
def get_db_pool_stats():
    """Get database connection pool statistics."""
    try:
        from db import POOL_CONFIG

        db_type = request.args.get("db_type")
        stats = get_pool_stats(db_type)
        stats["config"] = POOL_CONFIG.copy()

        return jsonify(stats)
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/db/pool/health", methods=["POST"])
@require_auth
def run_db_pool_health_check():
    """Run health check on database connection pools."""
    try:
        from db import pool_health_check

        db_type = request.json.get("db_type") if request.json else None
        result = pool_health_check(db_type)

        log_activity(
            "db_pool_health_check", f'Health check: {db_type or "all"}'
        )
        return jsonify({"success": True, "result": result})
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/db/pool/config", methods=["GET", "PUT"])
@require_auth
def manage_db_pool_config():
    """Get or update database pool configuration."""
    try:
        from db import POOL_CONFIG, reset_pools, set_pool_config

        if request.method == "GET":
            return jsonify(POOL_CONFIG.copy())

        # PUT - update config
        data = request.json
        if not data:
            return api_error("No configuration provided", 400)

        # Validate config keys
        valid_keys = {
            "min_connections",
            "max_connections",
            "max_overflow",
            "pool_timeout",
            "recycle_time",
            "health_check_interval",
            "enabled",
        }
        invalid_keys = set(data.keys()) - valid_keys
        if invalid_keys:
            return api_error(f"Invalid config keys: {invalid_keys}", 400)

        set_pool_config(data)

        # Reset pools if requested
        if data.get("reset_pools"):
            reset_pools()

        log_activity(
            "db_pool_config", f"Updated pool config: {list(data.keys())}"
        )
        return jsonify({"success": True, "config": POOL_CONFIG.copy()})
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/db/pool/warmup", methods=["POST"])
@require_auth
def warmup_db_pools():
    """Pre-create connection pools for specified databases."""
    try:
        from db import warmup_pools

        data = request.json or {}
        db_types = data.get("db_types")

        warmup_pools(db_types)
        stats = get_pool_stats()

        log_activity("db_pool_warmup", f'Warmed up pools: {db_types or "all"}')
        return jsonify({"success": True, "stats": stats})
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/db/pool/reset", methods=["POST"])
@require_auth
def reset_db_pools():
    """Close and reset all connection pools."""
    try:
        from db import reset_pools

        reset_pools()

        log_activity("db_pool_reset", "Reset all connection pools")
        return jsonify({"success": True, "message": "All pools reset"})
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/db/pool/metrics", methods=["GET"])
@require_auth
def get_db_pool_metrics():
    """Get comprehensive connection pool metrics."""
    try:
        from db import get_pool_metrics

        return jsonify(get_pool_metrics())
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/db/pool/summary", methods=["GET"])
@require_auth
def get_db_pool_summary():
    """Get a simple summary of pool status."""
    try:
        from db import get_pool_summary

        return jsonify(get_pool_summary())
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/db/pool/health-checker", methods=["GET", "POST", "DELETE"])
@require_auth
def manage_pool_health_checker():
    """Manage the background pool health checker."""
    try:
        from db import (
            _health_checker,
            get_health_check_results,
            start_health_checker,
            stop_health_checker,
        )

        if request.method == "GET":
            return jsonify(
                {
                    "running": (
                        _health_checker.is_running()
                        if _health_checker
                        else False
                    ),
                    "last_results": get_health_check_results(),
                }
            )

        if request.method == "POST":
            data = request.json or {}
            interval = data.get("check_interval")
            start_health_checker(interval)
            log_activity("pool_health_checker", "Started pool health checker")
            return jsonify(
                {"success": True, "message": "Health checker started"}
            )

        if request.method == "DELETE":
            stop_health_checker()
            log_activity("pool_health_checker", "Stopped pool health checker")
            return jsonify(
                {"success": True, "message": "Health checker stopped"}
            )

    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/db/pool/initialize", methods=["POST"])
@require_auth
def initialize_db_pools():
    """Initialize connection pools with optional warmup and health checker."""
    try:
        from db import get_pool_metrics, initialize_pools

        data = request.json or {}
        warmup = data.get("warmup", True)
        health_checker = data.get("health_checker", True)
        db_types = data.get("db_types")

        initialize_pools(
            warmup=warmup, health_checker=health_checker, db_types=db_types
        )

        log_activity(
            "pool_initialize",
            f"Initialized pools: warmup={warmup}, health_checker={health_checker}",
        )
        return jsonify(
            {
                "success": True,
                "message": "Pools initialized",
                "metrics": get_pool_metrics(),
            }
        )
    except Exception as e:
        return api_error(str(e), 500)


# ============================================================================
# CIRCUIT BREAKER API
# ============================================================================


@app.route("/api/circuits", methods=["GET"])
@require_auth
def get_all_circuits():
    """Get status of all circuit breakers."""
    try:
        from services.circuit_breaker import get_all_circuit_status

        return jsonify(get_all_circuit_status())
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/circuits/<name>", methods=["GET"])
@require_auth
def get_circuit_status(name):
    """Get status of a specific circuit breaker."""
    try:
        from services.circuit_breaker import CircuitBreaker

        circuits = CircuitBreaker.get_all()
        if name not in circuits:
            return api_error(f"Circuit '{name}' not found", 404)

        return jsonify(circuits[name].get_status())
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/circuits/<name>/reset", methods=["POST"])
@require_auth
def reset_circuit_endpoint(name):
    """Reset a circuit breaker to closed state."""
    try:
        from services.circuit_breaker import reset_circuit

        if reset_circuit(name):
            log_activity("circuit_reset", f"Reset circuit: {name}")
            return jsonify(
                {"success": True, "message": f"Circuit '{name}' reset"}
            )
        return api_error(f"Circuit '{name}' not found", 404)
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/circuits/<name>/open", methods=["POST"])
@require_auth
def force_open_circuit_endpoint(name):
    """Force a circuit breaker open (for testing/maintenance)."""
    try:
        from services.circuit_breaker import force_open_circuit

        data = request.json or {}
        duration = data.get("duration", 60.0)

        if force_open_circuit(name, duration):
            log_activity(
                "circuit_force_open",
                f"Forced circuit open: {name} for {duration}s",
            )
            return jsonify(
                {
                    "success": True,
                    "message": f"Circuit '{name}' forced open for {duration}s",
                }
            )
        return api_error(f"Circuit '{name}' not found", 404)
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/circuits/reset-all", methods=["POST"])
@require_auth
def reset_all_circuits():
    """Reset all circuit breakers."""
    try:
        from services.circuit_breaker import CircuitBreaker

        CircuitBreaker.reset_all()
        log_activity("circuits_reset_all", "Reset all circuit breakers")
        return jsonify({"success": True, "message": "All circuits reset"})
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/circuits/health", methods=["GET"])
@require_auth
def get_circuits_health():
    """Get health summary of all circuit breakers."""
    try:
        from services.circuit_breaker import CircuitBreaker, CircuitState

        circuits = CircuitBreaker.get_all()

        summary = {
            "total": len(circuits),
            "closed": 0,
            "open": 0,
            "half_open": 0,
            "circuits": {},
        }

        for name, cb in circuits.items():
            state = cb.state
            if state == CircuitState.CLOSED:
                summary["closed"] += 1
            elif state == CircuitState.OPEN:
                summary["open"] += 1
            elif state == CircuitState.HALF_OPEN:
                summary["half_open"] += 1

            summary["circuits"][name] = {
                "state": state.value,
                "success_rate": cb.metrics.success_rate,
                "total_calls": cb.metrics.total_calls,
            }

        summary["healthy"] = summary["open"] == 0
        return jsonify(summary)
    except Exception as e:
        return api_error(str(e), 500)


# ============================================================================
# JIRA INTEGRATION API
# ============================================================================


@app.route("/api/integrations/jira/config", methods=["GET"])
@require_auth
def get_jira_config():
    """Get current Jira integration configuration (secrets masked)."""
    try:
        from services.jira_integration import JiraService

        jira = JiraService()
        config = jira.get_config()

        if not config:
            return jsonify({"configured": False})

        # Mask sensitive fields
        masked_config = config.copy()
        if "api_token" in masked_config and masked_config["api_token"]:
            masked_config["api_token"] = "***masked***"

        return jsonify({"configured": True, "config": masked_config})
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/integrations/jira/config", methods=["POST"])
@require_auth
def configure_jira():
    """Configure Jira integration."""
    try:
        from services.jira_integration import JiraService

        data = request.get_json()
        if not data:
            return api_error("Configuration data required", 400)

        # Required fields
        required = ["base_url", "email", "api_token", "project_key"]
        missing = [f for f in required if not data.get(f)]
        if missing:
            return api_error(
                f"Missing required fields: {', '.join(missing)}", 400
            )

        jira = JiraService()
        jira.configure(
            base_url=data["base_url"].rstrip("/"),
            email=data["email"],
            api_token=data["api_token"],
            project_key=data["project_key"],
            sync_direction=data.get("sync_direction", "bidirectional"),
            conflict_resolution=data.get("conflict_resolution", "newer_wins"),
            sync_interval_minutes=data.get("sync_interval_minutes", 15),
            issue_types=data.get(
                "issue_types", ["Bug", "Task", "Story", "Epic"]
            ),
            status_mapping=data.get("status_mapping", {}),
            priority_mapping=data.get("priority_mapping", {}),
            custom_field_mapping=data.get("custom_field_mapping", {}),
        )

        log_activity(
            "jira_configured",
            f"Configured Jira integration for project {data['project_key']}",
        )
        return jsonify(
            {"success": True, "message": "Jira integration configured"}
        )
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/integrations/jira/config", methods=["DELETE"])
@require_auth
def delete_jira_config():
    """Remove Jira integration configuration."""
    try:
        from services.jira_integration import JiraService

        jira = JiraService()
        jira.delete_config()

        log_activity(
            "jira_config_deleted", "Removed Jira integration configuration"
        )
        return jsonify(
            {"success": True, "message": "Jira configuration removed"}
        )
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/integrations/jira/test", methods=["POST"])
@require_auth
def test_jira_connection():
    """Test Jira connection with current configuration."""
    try:
        from services.jira_integration import JiraService

        jira = JiraService()

        if not jira.is_configured():
            return api_error("Jira is not configured", 400)

        result = jira.test_connection()

        if result["success"]:
            log_activity(
                "jira_connection_test", "Jira connection test successful"
            )

        return jsonify(result)
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/integrations/jira/sync", methods=["POST"])
@require_auth
def sync_from_jira():
    """Sync issues from Jira to local database."""
    try:
        from datetime import datetime

        from services.jira_integration import JiraService

        jira = JiraService()

        if not jira.is_configured():
            return api_error("Jira is not configured", 400)

        data = request.get_json() or {}

        # Parse options
        since = None
        if data.get("since"):
            since = datetime.fromisoformat(
                data["since"].replace("Z", "+00:00")
            )

        issue_types = data.get("issue_types")  # Optional filter
        dry_run = data.get("dry_run", False)

        result = jira.sync_from_jira(
            since=since, issue_types=issue_types, dry_run=dry_run
        )

        log_activity(
            "jira_sync",
            f"Synced from Jira: {
                result.created} created, {
                result.updated} updated, "
            f"{result.skipped} skipped, {len(result.errors)} errors"
            + (" (dry run)" if dry_run else ""),
        )

        return jsonify(
            {
                "success": result.success,
                "created": result.created,
                "updated": result.updated,
                "skipped": result.skipped,
                "errors": result.errors,
                "details": result.details,
                "dry_run": dry_run,
            }
        )
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/integrations/jira/push", methods=["POST"])
@require_auth
def push_to_jira():
    """Push a local item (feature/bug/milestone) to Jira."""
    try:
        from services.jira_integration import JiraService

        jira = JiraService()

        if not jira.is_configured():
            return api_error("Jira is not configured", 400)

        data = request.get_json()
        if not data:
            return api_error("Request data required", 400)

        item_type = data.get("item_type")  # 'feature', 'bug', 'milestone'
        item_id = data.get("item_id")

        if not item_type or not item_id:
            return api_error("item_type and item_id are required", 400)

        if item_type not in ["feature", "bug", "milestone"]:
            return api_error(
                "item_type must be feature, bug, or milestone", 400
            )

        result = jira.push_to_jira(item_type, item_id)

        if result.get("success"):
            log_activity(
                "jira_push",
                f"Pushed {item_type} #{item_id} to Jira as {
                    result.get('jira_key')}",
            )

        return jsonify(result)
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/integrations/jira/status", methods=["GET"])
@require_auth
def get_jira_sync_status():
    """Get Jira sync status and statistics."""
    try:
        from services.jira_integration import JiraService

        jira = JiraService()

        if not jira.is_configured():
            return jsonify({"configured": False})

        status = jira.get_sync_status()
        status["configured"] = True

        return jsonify(status)
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/integrations/jira/mappings", methods=["GET"])
@require_auth
def get_jira_mappings():
    """Get all Jira-local item mappings."""
    try:
        from services.jira_integration import JiraService

        jira = JiraService()

        # Optional filter by type
        item_type = request.args.get("type")

        mappings = jira.get_mappings(item_type=item_type)

        return jsonify({"mappings": mappings, "count": len(mappings)})
    except Exception as e:
        return api_error(str(e), 500)


@app.route(
    "/api/integrations/jira/mappings/<int:mapping_id>", methods=["DELETE"]
)
@require_auth
def unlink_jira_mapping(mapping_id):
    """Remove a Jira mapping (unlink local item from Jira issue)."""
    try:
        from services.jira_integration import JiraService

        jira = JiraService()

        if jira.unlink(mapping_id):
            log_activity("jira_unlink", f"Removed Jira mapping #{mapping_id}")
            return jsonify({"success": True, "message": "Mapping removed"})

        return api_error("Mapping not found", 404)
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/integrations/jira/issue/<jira_key>", methods=["GET"])
@require_auth
def get_jira_issue(jira_key):
    """Get a specific Jira issue by key."""
    try:
        from services.jira_integration import JiraService

        jira = JiraService()

        if not jira.is_configured():
            return api_error("Jira is not configured", 400)

        issue = jira.get_issue(jira_key)

        if not issue:
            return api_error(f"Issue {jira_key} not found", 404)

        return jsonify(issue)
    except Exception as e:
        return api_error(str(e), 500)


@app.route("/api/integrations/jira/search", methods=["POST"])
@require_auth
def search_jira_issues():
    """Search Jira issues with JQL."""
    try:
        from services.jira_integration import JiraService

        jira = JiraService()

        if not jira.is_configured():
            return api_error("Jira is not configured", 400)

        data = request.get_json() or {}
        jql = data.get("jql", "")
        max_results = min(data.get("max_results", 50), 100)  # Cap at 100

        if not jql:
            # Default to project issues
            config = jira.get_config()
            jql = f"project = {config.get('project_key', 'UNKNOWN')}"

        issues = jira.search_issues(jql, max_results=max_results)

        return jsonify({"issues": issues, "count": len(issues), "jql": jql})
    except Exception as e:
        return api_error(str(e), 500)


# ============================================================================
# FILE MODIFICATION API
# ============================================================================


@app.route("/api/files/read", methods=["POST"])
@require_auth
def read_file():
    """Read a file from a project."""
    try:
        data = request.get_json()
        file_path = data.get("path", "")

        if not file_path:
            return jsonify({"error": "File path required"}), 400

        # Security: only allow reading from known project paths
        path = Path(file_path).resolve()
        allowed = False
        for source in CODE_SOURCES:
            if str(path).startswith(str(source.resolve())):
                allowed = True
                break

        if not allowed:
            return (
                jsonify(
                    {"error": "Access denied - path not in allowed sources"}
                ),
                403,
            )

        if not path.exists():
            return jsonify({"error": "File not found"}), 404

        if path.is_dir():
            # Return directory listing
            files = []
            for item in path.iterdir():
                files.append(
                    {
                        "name": item.name,
                        "is_dir": item.is_dir(),
                        "size": item.stat().st_size if item.is_file() else 0,
                        "modified": datetime.fromtimestamp(
                            item.stat().st_mtime
                        ).isoformat(),
                    }
                )
            return jsonify(
                {
                    "type": "directory",
                    "files": sorted(
                        files, key=lambda x: (not x["is_dir"], x["name"])
                    ),
                }
            )

        # Read file content
        try:
            content = path.read_text()
            return jsonify(
                {
                    "type": "file",
                    "content": content,
                    "path": str(path),
                    "size": len(content),
                }
            )
        except UnicodeDecodeError:
            return (
                jsonify({"error": "Binary file cannot be read as text"}),
                400,
            )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/files/write", methods=["POST"])
@require_auth
def write_file():
    """Write content to a file."""
    try:
        data = request.get_json()
        file_path = data.get("path", "")
        content = data.get("content", "")

        if not file_path:
            return jsonify({"error": "File path required"}), 400

        path = Path(file_path).resolve()

        # Security check
        allowed = False
        for source in CODE_SOURCES:
            if str(path).startswith(str(source.resolve())):
                allowed = True
                break

        if not allowed:
            return jsonify({"error": "Access denied"}), 403

        # Create backup
        if path.exists():
            backup_path = path.with_suffix(path.suffix + ".bak")
            backup_path.write_text(path.read_text())

        # Write new content
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(content)

        log_activity("file_write", "file", None, str(path))

        return jsonify({"success": True, "path": str(path)})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/files/feature/<int:feature_id>", methods=["GET"])
@require_auth
def get_feature_files(feature_id):
    """Get files associated with a feature."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            feature = conn.execute(
                "SELECT f.*, p.source_path FROM features f JOIN projects p ON f.project_id = p.id WHERE f.id = ?",
                (feature_id,),
            ).fetchone()

            if not feature:
                return jsonify({"error": "Feature not found"}), 404

            source_path = feature["source_path"]
            if not source_path:
                return jsonify(
                    {"files": [], "message": "No source path for project"}
                )

            # Look for feature-related files
            path = Path(source_path)
            feature_name = (
                feature["name"].lower().replace(" ", "_").replace("-", "_")
            )

            related_files = []
            if path.exists():
                for f in path.rglob("*"):
                    if f.is_file() and feature_name in f.name.lower():
                        related_files.append(
                            {
                                "path": str(f),
                                "name": f.name,
                                "relative": str(f.relative_to(path)),
                            }
                        )

            return jsonify(
                {"files": related_files, "project_path": source_path}
            )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ============================================================================
# REPORTS AND DOCUMENTATION API
# ============================================================================


@app.route("/api/reports", methods=["GET"])
@require_auth
def list_reports():
    """List all documentation and report files."""
    try:
        base_path = Path(__file__).parent
        docs_path = base_path / "docs"

        reports = []

        # Scan docs directory
        if docs_path.exists():
            for f in docs_path.glob("*.md"):
                stats = f.stat()
                reports.append(
                    {
                        "name": f.name,
                        "path": str(f.relative_to(base_path)),
                        "size": stats.st_size,
                        "modified": datetime.fromtimestamp(
                            stats.st_mtime
                        ).isoformat(),
                        "type": "documentation",
                    }
                )

        # Scan root for report files
        for pattern in ["*REPORT*.md", "*AUDIT*.md", "WORKER_PROGRESS*.md"]:
            for f in base_path.glob(pattern):
                if f.is_file():
                    stats = f.stat()
                    reports.append(
                        {
                            "name": f.name,
                            "path": str(f.relative_to(base_path)),
                            "size": stats.st_size,
                            "modified": datetime.fromtimestamp(
                                stats.st_mtime
                            ).isoformat(),
                            "type": "report",
                        }
                    )

        # Sort by modified time (newest first)
        reports.sort(key=lambda x: x["modified"], reverse=True)

        return jsonify({"reports": reports, "count": len(reports)})

    except Exception as e:
        logger.error(f"Error listing reports: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/reports/<path:filename>", methods=["GET"])
@require_auth
def view_report(filename):
    """View contents of a report or documentation file."""
    try:
        base_path = Path(__file__).parent

        # Security: only allow files in docs/ or root-level report files
        file_path = base_path / filename

        # Prevent directory traversal
        try:
            file_path = file_path.resolve()
            base_path = base_path.resolve()
            if not str(file_path).startswith(str(base_path)):
                return jsonify({"error": "Access denied"}), 403
        except Exception:
            return jsonify({"error": "Invalid path"}), 400

        # Only allow markdown files
        if not file_path.suffix == ".md":
            return jsonify({"error": "Only markdown files allowed"}), 400

        if not file_path.exists():
            return jsonify({"error": "File not found"}), 404

        # Read file content
        content = file_path.read_text(encoding="utf-8")
        stats = file_path.stat()

        return jsonify(
            {
                "filename": file_path.name,
                "path": str(file_path.relative_to(base_path)),
                "content": content,
                "size": stats.st_size,
                "modified": datetime.fromtimestamp(stats.st_mtime).isoformat(),
                "lines": len(content.split("\n")),
            }
        )

    except Exception as e:
        logger.error(f"Error viewing report {filename}: {e}")
        return jsonify({"error": str(e)}), 500


# ============================================================================
# AGENTS API
# ============================================================================


@app.route("/api/agents", methods=["GET"])
@require_auth
def list_agents():
    """List all registered agents (Claude sessions in tmux)."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            # Get tmux sessions that are likely Claude agents
            # Optimized: Use LEFT JOIN with pre-aggregated counts instead of
            # correlated subqueries
            sessions = conn.execute(
                """
                SELECT ts.*, n.hostname as node_hostname,
                    COALESCE(fc.feature_count, 0) as active_features,
                    COALESCE(bc.bug_count, 0) as active_bugs
                FROM tmux_sessions ts
                LEFT JOIN nodes n ON ts.node_id = n.id
                LEFT JOIN (
                    SELECT tmux_session, COUNT(*) as feature_count
                    FROM features WHERE status = 'in_progress'
                    GROUP BY tmux_session
                ) fc ON fc.tmux_session = ts.session_name
                LEFT JOIN (
                    SELECT tmux_session, COUNT(*) as bug_count
                    FROM bugs WHERE status = 'in_progress'
                    GROUP BY tmux_session
                ) bc ON bc.tmux_session = ts.session_name
                WHERE ts.purpose LIKE '%claude%' OR ts.purpose LIKE '%agent%' OR ts.session_name LIKE '%claude%'
                ORDER BY ts.last_activity DESC
            """
            ).fetchall()

            agents = [dict(s) for s in sessions]
            return jsonify(agents)

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/agents/send", methods=["POST"])
@require_auth
def send_to_agent():
    """Send a task/message to an agent."""
    try:
        data = request.get_json()
        session_name = data.get("session")
        message = data.get("message", "")
        task_type = data.get(
            "task_type", "general"
        )  # general, feature, bug, error
        entity_id = data.get("entity_id")

        if not session_name or not message:
            return jsonify({"error": "Session and message required"}), 400

        # Format message based on task type
        formatted_message = message
        if task_type == "feature" and entity_id:
            with get_db_connection() as conn:
                conn.row_factory = sqlite3.Row
                feature = conn.execute(
                    "SELECT * FROM features WHERE id = ?", (entity_id,)
                ).fetchone()
                if feature:
                    formatted_message = f"[FEATURE #{entity_id}] {
                        feature['name']}\n{
                        feature['description'] or ''}\n\n{message}"
        elif task_type == "bug" and entity_id:
            with get_db_connection() as conn:
                conn.row_factory = sqlite3.Row
                bug = conn.execute(
                    "SELECT * FROM bugs WHERE id = ?", (entity_id,)
                ).fetchone()
                if bug:
                    formatted_message = f"[BUG #{entity_id}] {
                        bug['title']}\nSeverity: {
                        bug['severity']}\n{
                        bug['description'] or ''}\n\n{message}"

        # Send to tmux (text and C-m separately for Claude Code)
        subprocess.run(
            ["tmux", "send-keys", "-t", session_name, formatted_message],
            capture_output=True,
            text=True,
        )
        result = subprocess.run(
            ["tmux", "send-keys", "-t", session_name, "Enter"],
            capture_output=True,
            text=True,
        )

        if result.returncode != 0:
            return jsonify({"error": f"Failed to send: {result.stderr}"}), 500

        log_activity("agent_task_sent", task_type, entity_id, session_name)

        return jsonify({"success": True})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/agents/broadcast", methods=["POST"])
@require_auth
def broadcast_to_agents():
    """Send a message to all agents."""
    try:
        data = request.get_json()
        message = data.get("message", "")

        if not message:
            return jsonify({"error": "Message required"}), 400

        # Get all agent sessions
        result = subprocess.run(
            ["tmux", "list-sessions", "-F", "#{session_name}"],
            capture_output=True,
            text=True,
        )

        sessions = (
            result.stdout.strip().split("\n") if result.stdout.strip() else []
        )

        # Filter for Claude/agent sessions
        agent_sessions = [
            s
            for s in sessions
            if "claude" in s.lower() or "agent" in s.lower()
        ]

        sent_to = []
        for sess_name in agent_sessions:
            try:
                subprocess.run(
                    [
                        "tmux",
                        "send-keys",
                        "-t",
                        sess_name,
                        f"[BROADCAST] {message}",
                    ],
                    check=True,
                )
                subprocess.run(
                    ["tmux", "send-keys", "-t", sess_name, "Enter"], check=True
                )
                sent_to.append(sess_name)
            except (subprocess.SubprocessError, OSError):
                pass

        return jsonify({"success": True, "sent_to": sent_to})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ============================================================================
# WORKERS MANAGEMENT API
# ============================================================================


@app.route("/api/workers/status", methods=["GET"])
@require_auth
def worker_status():
    """Get status of all worker types."""
    try:
        import psutil

        workers = {
            "task_worker": {"running": False, "pid": None},
            "error_daemon": {"running": False, "pid": None},
            "deploy_worker": {"running": False, "pid": None},
            "confirm_worker": {"running": False, "pid": None},
        }

        # Check PID files
        pid_files = {
            "task_worker": Path("/tmp/architect_worker.pid"),
            "error_daemon": Path("/tmp/architect_error_daemon.pid"),
            "deploy_worker": Path("/tmp/architect_deploy_worker.pid"),
            "confirm_worker": Path("/tmp/architect_confirm_worker.pid"),
        }

        for worker_type, pid_file in pid_files.items():
            if pid_file.exists():
                try:
                    pid = int(pid_file.read_text().strip())
                    if psutil.pid_exists(pid):
                        workers[worker_type] = {"running": True, "pid": pid}
                except (OSError, ValueError):
                    pass

        # Get registered workers from DB
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            registered = conn.execute(
                """
                SELECT * FROM workers
                WHERE last_heartbeat > datetime('now', '-5 minutes')
                ORDER BY last_heartbeat DESC
            """
            ).fetchall()

            return jsonify(
                {
                    "system_workers": workers,
                    "registered_workers": [dict(w) for w in registered],
                    "stats": {
                        "total": len(registered),
                        "active": len(
                            [w for w in registered if w["status"] == "active"]
                        ),
                        "idle": len(
                            [w for w in registered if w["status"] == "idle"]
                        ),
                        "offline": len(
                            [w for w in registered if w["status"] == "offline"]
                        ),
                    },
                }
            )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/workers/start/<worker_type>", methods=["POST"])
@require_auth
def start_worker(worker_type):
    """Start a worker process."""
    try:
        workers_dir = BASE_DIR / "workers"

        worker_scripts = {
            "task": workers_dir / "task_worker.py",
            "error_daemon": workers_dir / "error_task_daemon.py",
            "deploy": workers_dir / "deploy_worker.py",
            "confirm": workers_dir / "confirm_worker.py",
        }

        if worker_type not in worker_scripts:
            return (
                jsonify({"error": f"Unknown worker type: {worker_type}"}),
                400,
            )

        script = worker_scripts[worker_type]
        if not script.exists():
            return (
                jsonify({"error": f"Worker script not found: {script}"}),
                404,
            )

        # Determine the correct daemon argument for each worker
        daemon_args = {
            "task": "--daemon",
            "error_daemon": "start",
            "deploy": "--daemon",
            "confirm": "--daemon",
        }

        # Start worker as daemon
        subprocess.Popen(
            ["python3", str(script), daemon_args.get(worker_type, "--daemon")],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            start_new_session=True,
        )

        log_activity("worker_start", "worker", None, worker_type)

        return jsonify({"success": True, "worker": worker_type})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/workers/stop/<worker_type>", methods=["POST"])
@require_auth
def stop_worker(worker_type):
    """Stop a worker process."""
    try:
        pid_files = {
            "task": Path("/tmp/architect_worker.pid"),
            "error_daemon": Path("/tmp/architect_error_daemon.pid"),
            "deploy": Path("/tmp/architect_deploy_worker.pid"),
            "confirm": Path("/tmp/architect_confirm_worker.pid"),
        }

        if worker_type not in pid_files:
            return (
                jsonify({"error": f"Unknown worker type: {worker_type}"}),
                400,
            )

        pid_file = pid_files[worker_type]
        if pid_file.exists():
            try:
                pid = int(pid_file.read_text().strip())
                os.kill(pid, 15)  # SIGTERM
                pid_file.unlink()
                log_activity("worker_stop", "worker", None, worker_type)
                return jsonify({"success": True, "worker": worker_type})
            except ProcessLookupError:
                pid_file.unlink()
                return jsonify(
                    {"success": True, "message": "Worker was not running"}
                )

        return jsonify({"success": True, "message": "Worker not running"})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/workers/autoconfirm/kill-switch", methods=["GET", "POST"])
@require_auth
def autoconfirm_kill_switch():
    """Get or set auto-confirm kill switch status.

    GET: Returns current status
    POST: Sets kill switch (body: {'enabled': true/false})
    """
    kill_switch_file = Path("/tmp/auto_confirm_kill_switch")

    if request.method == "GET":
        # Check if kill switch is active
        enabled = False
        if kill_switch_file.exists():
            try:
                with open(kill_switch_file, "r") as f:
                    content = f.read().strip().lower()
                    enabled = content in ["1", "true", "enabled", "stop"]
            except Exception:
                pass

        return jsonify(
            {
                "success": True,
                "kill_switch_enabled": enabled,
                "status": "stopped" if enabled else "running",
            }
        )

    elif request.method == "POST":
        data = request.get_json() or {}
        enabled = data.get("enabled", False)

        try:
            if enabled:
                # Enable kill switch (stop auto-confirm)
                with open(kill_switch_file, "w") as f:
                    f.write("stop")
                message = "Auto-confirm disabled (kill switch activated)"
                log_activity(
                    "autoconfirm_kill_switch", "worker", None, "enabled"
                )
            else:
                # Disable kill switch (allow auto-confirm to run)
                if kill_switch_file.exists():
                    kill_switch_file.unlink()
                message = "Auto-confirm enabled (kill switch deactivated)"
                log_activity(
                    "autoconfirm_kill_switch", "worker", None, "disabled"
                )

            return jsonify(
                {
                    "success": True,
                    "kill_switch_enabled": enabled,
                    "message": message,
                }
            )
        except Exception as e:
            return jsonify({"success": False, "error": str(e)}), 500


# ============================================================================
# SESSION ASSIGNER API
# ============================================================================


@app.route("/api/assigner/status", methods=["GET"])
@require_auth
def get_assigner_status():
    """Get session assigner status - assignments, environments, and task history."""
    try:
        state_file = BASE_DIR / "data" / "session_state.json"
        if not state_file.exists():
            return jsonify(
                {
                    "sessions": {},
                    "env_assignments": {},
                    "scope_locks": {},
                    "task_history": [],
                }
            )

        state = json.loads(state_file.read_text())

        # Add summary stats
        sessions = state.get("sessions", {})
        summary = {
            "total_sessions": len(sessions),
            "by_project": {},
            "by_status": {},
        }

        for sess_name, info in sessions.items():
            project = info.get("project", "unknown")
            status = info.get("status", "unknown")
            summary["by_project"][project] = (
                summary["by_project"].get(project, 0) + 1
            )
            summary["by_status"][status] = (
                summary["by_status"].get(status, 0) + 1
            )

        return jsonify(
            {
                "sessions": sessions,
                "env_assignments": state.get("env_assignments", {}),
                "scope_locks": state.get("scope_locks", {}),
                "task_history": state.get("task_history", [])[
                    -50:
                ],  # Last 50 entries
                "summary": summary,
            }
        )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/assigner/history", methods=["GET"])
@require_auth
def get_assigner_history():
    """Get task assignment history."""
    try:
        limit = request.args.get("limit", 50, type=int)
        state_file = BASE_DIR / "data" / "session_state.json"

        if not state_file.exists():
            return jsonify({"history": []})

        state = json.loads(state_file.read_text())
        history = state.get("task_history", [])

        # Sort by timestamp descending and limit
        history = sorted(
            history, key=lambda x: x.get("timestamp", ""), reverse=True
        )[:limit]

        return jsonify({"history": history})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/assigner/prompts/<int:prompt_id>/archive", methods=["POST"])
@require_auth
def archive_prompt(prompt_id):
    """Archive a prompt (hide from main view without deleting)."""
    try:
        assigner_db = BASE_DIR / "data" / "assigner" / "assigner.db"
        conn = sqlite3.connect(str(assigner_db))
        cursor = conn.cursor()

        # Update archived status
        cursor.execute(
            """
            UPDATE prompts
            SET archived = 1, archived_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """,
            (prompt_id,),
        )

        if cursor.rowcount == 0:
            conn.close()
            return (
                jsonify({"success": False, "error": "Prompt not found"}),
                404,
            )

        conn.commit()
        conn.close()

        log_activity("archive_prompt", "assigner", prompt_id, "archived")
        return jsonify({"success": True, "message": "Prompt archived"})

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/assigner/prompts/<int:prompt_id>/unarchive", methods=["POST"])
@require_auth
def unarchive_prompt(prompt_id):
    """Unarchive a prompt (restore to main view)."""
    try:
        assigner_db = BASE_DIR / "data" / "assigner" / "assigner.db"
        conn = sqlite3.connect(str(assigner_db))
        cursor = conn.cursor()

        # Update archived status
        cursor.execute(
            """
            UPDATE prompts
            SET archived = 0, archived_at = NULL
            WHERE id = ?
        """,
            (prompt_id,),
        )

        if cursor.rowcount == 0:
            conn.close()
            return (
                jsonify({"success": False, "error": "Prompt not found"}),
                404,
            )

        conn.commit()
        conn.close()

        log_activity("unarchive_prompt", "assigner", prompt_id, "unarchived")
        return jsonify({"success": True, "message": "Prompt unarchived"})

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/assigner/prompts/bulk-archive", methods=["POST"])
@require_auth
def bulk_archive_prompts():
    """Archive multiple prompts at once.

    Body: {
        "ids": [1, 2, 3],  # Optional: specific IDs
        "status": "completed",  # Optional: archive all with status
        "before_date": "2025-01-01"  # Optional: archive before date
    }
    """
    try:
        data = request.get_json() or {}
        ids = data.get("ids", [])
        status = data.get("status")
        before_date = data.get("before_date")

        assigner_db = BASE_DIR / "data" / "assigner" / "assigner.db"
        conn = sqlite3.connect(str(assigner_db))
        cursor = conn.cursor()

        # Build query based on parameters
        if ids:
            # Archive specific IDs
            placeholders = ",".join("?" * len(ids))
            cursor.execute(
                """
                UPDATE prompts
                SET archived = 1, archived_at = CURRENT_TIMESTAMP
                WHERE id IN ({placeholders}) AND archived = 0
            """,
                ids,
            )
        elif status:
            # Archive all with specific status
            cursor.execute(
                """
                UPDATE prompts
                SET archived = 1, archived_at = CURRENT_TIMESTAMP
                WHERE status = ? AND archived = 0
            """,
                (status,),
            )
        elif before_date:
            # Archive all before date
            cursor.execute(
                """
                UPDATE prompts
                SET archived = 1, archived_at = CURRENT_TIMESTAMP
                WHERE created_at < ? AND archived = 0
            """,
                (before_date,),
            )
        else:
            conn.close()
            return (
                jsonify(
                    {
                        "success": False,
                        "error": "Must provide ids, status, or before_date",
                    }
                ),
                400,
            )

        count = cursor.rowcount
        conn.commit()
        conn.close()

        log_activity(
            "bulk_archive_prompts",
            "assigner",
            None,
            f"archived {count} prompts",
        )
        return jsonify(
            {
                "success": True,
                "message": f"Archived {count} prompt(s)",
                "count": count,
            }
        )

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/assigner/tasks/health", methods=["GET"])
def get_tasks_health():
    """Get health status of all active tasks.

    Returns:
        - Task count by status
        - Tasks with issues (blocked, stuck, etc.)
        - Session health

    Public endpoint for monitor.html to check task health.
    """
    try:
        import os
        import sys

        sys.path.insert(0, os.path.join(os.path.dirname(__file__), "workers"))

        from task_monitor import TaskMonitor

        monitor = TaskMonitor()
        report = monitor.get_status_report()

        response = jsonify(report)
        response.headers["Access-Control-Allow-Origin"] = "*"
        return response

    except Exception as e:
        logger.error(f"Error getting task health: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/assigner/tasks/<int:task_id>/health", methods=["GET"])
def get_task_health(task_id):
    """Get health status of a specific task.

    Public endpoint for checking individual task health.
    """
    try:
        import os
        import sys

        sys.path.insert(0, os.path.join(os.path.dirname(__file__), "workers"))

        from task_monitor import TaskMonitor

        monitor = TaskMonitor()
        tasks = monitor.get_all_tasks()
        task = next((t for t in tasks if t["id"] == task_id), None)

        if not task:
            return jsonify({"error": "Task not found or not active"}), 404

        health = monitor.check_task_health(task)
        result = (
            health
            if health
            else {"task_id": task_id, "status": "healthy", "alerts": []}
        )

        response = jsonify(result)
        response.headers["Access-Control-Allow-Origin"] = "*"
        return response

    except Exception as e:
        logger.error(f"Error checking task {task_id} health: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/assigner/tasks/<int:task_id>/unblock", methods=["POST"])
def unblock_task(task_id):
    """Auto-handle common approval prompts for a blocked task.

    Body: {
        "action": "accept" | "reject" | "yes" | "no" | "enter"
    }

    Public endpoint for monitor to unblock tasks.
    """
    try:
        import os
        import sys

        sys.path.insert(0, os.path.join(os.path.dirname(__file__), "workers"))

        import subprocess

        from task_monitor import TaskMonitor

        data = request.get_json() or {}
        action = data.get("action", "accept").lower()

        monitor = TaskMonitor()
        tasks = monitor.get_all_tasks()
        task = next((t for t in tasks if t["id"] == task_id), None)

        if not task:
            return jsonify({"error": "Task not found or not active"}), 404

        session = task.get("session")
        if not session:
            return jsonify({"error": "Task not assigned to a session"}), 400

        # Map action to tmux command
        action_map = {
            "accept": "Enter",
            "yes": "y",
            "no": "n",
            "reject": "3",  # Option 3 in edit prompts
            "enter": "Enter",
        }

        key = action_map.get(action, "Enter")

        # Send command to session
        result = subprocess.run(
            ["tmux", "send-keys", "-t", session, key, "Enter"],
            capture_output=True,
            text=True,
            timeout=5,
        )

        if result.returncode == 0:
            response = jsonify(
                {
                    "success": True,
                    "task_id": task_id,
                    "session": session,
                    "action": action,
                    "message": f"Sent {key} to {session}",
                }
            )
            response.headers["Access-Control-Allow-Origin"] = "*"
            return response
        else:
            return (
                jsonify({"error": f"Failed to send command: {result.stderr}"}),
                500,
            )

    except Exception as e:
        logger.error(f"Error unblocking task {task_id}: {e}")
        return jsonify({"error": str(e)}), 500


# ============================================================================
# CLAUDE WRAPPER API
# ============================================================================


@app.route("/api/wrapper/status", methods=["GET"])
@require_auth
def get_wrapper_status():
    """Get Claude wrapper status for all sessions."""
    try:
        state_file = Path("/tmp/claude_wrapper_state.json")
        wrapper_state = None

        if state_file.exists():
            try:
                wrapper_state = json.loads(state_file.read_text())
            except json.JSONDecodeError:
                wrapper_state = None

        # Get list of tmux sessions running with wrapper
        result = subprocess.run(
            ["tmux", "list-sessions", "-F", "#{session_name}"],
            capture_output=True,
            text=True,
        )

        sessions = []
        if result.returncode == 0:
            for session_name in result.stdout.strip().split("\n"):
                if not session_name:
                    continue

                # Check if session has wrapper running
                capture = subprocess.run(
                    [
                        "tmux",
                        "capture-pane",
                        "-t",
                        session_name,
                        "-p",
                        "-S",
                        "-50",
                    ],
                    capture_output=True,
                    text=True,
                )

                output = capture.stdout if capture.returncode == 0 else ""
                has_wrapper = (
                    "[Wrapper]" in output or "[AutoResponse]" in output
                )
                has_claude = (
                    "bypass permissions" in output or "accept edits" in output
                )

                sessions.append(
                    {
                        "name": session_name,
                        "has_wrapper": has_wrapper,
                        "has_claude": has_claude,
                        "output_preview": output[-200:] if output else "",
                    }
                )

        return jsonify(
            {
                "wrapper_state": wrapper_state,
                "sessions": sessions,
                "wrapper_available": (
                    BASE_DIR / "claude_wrapper" / "claude_wrapper.py"
                ).exists(),
            }
        )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/wrapper/start", methods=["POST"])
@require_auth
def start_wrapped_session():
    """Start a new tmux session with Claude wrapper."""
    try:
        data = request.get_json(silent=True) or {}
        session_name = data.get("session_name", f"claude_{int(time.time())}")
        auto_respond = data.get("auto_respond", False)
        approve_all = data.get("approve_all", False)

        # Check if session already exists
        result = subprocess.run(
            ["tmux", "has-session", "-t", session_name], capture_output=True
        )
        if result.returncode == 0:
            return (
                jsonify({"error": f"Session '{session_name}' already exists"}),
                400,
            )

        # Build wrapper command
        wrapper_path = BASE_DIR / "claude_wrapper" / "claude_wrapper.py"
        if not wrapper_path.exists():
            return jsonify({"error": "Wrapper not found"}), 404

        wrapper_args = ""
        if approve_all:
            wrapper_args = " --approve-all"
        elif auto_respond:
            wrapper_args = " --auto-respond"

        # Create tmux session with bash (so it stays alive)
        result = subprocess.run(
            ["tmux", "new-session", "-d", "-s", session_name, "bash"],
            capture_output=True,
            text=True,
        )

        if result.returncode != 0:
            return (
                jsonify(
                    {"error": f"Failed to create session: {result.stderr}"}
                ),
                500,
            )

        # Send the wrapper command to the session
        time.sleep(0.5)
        subprocess.run(
            [
                "tmux",
                "send-keys",
                "-t",
                session_name,
                f"python3 {wrapper_path}{wrapper_args}",
                "Enter",
            ],
            capture_output=True,
        )

        log_activity(
            "wrapper_start",
            "wrapper",
            None,
            f"Started wrapped session: {session_name}",
        )

        return jsonify(
            {
                "success": True,
                "session_name": session_name,
                "auto_respond": auto_respond or approve_all,
                "approve_all": approve_all,
            }
        )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ============================================================================
# GO WRAPPER API
# ============================================================================


@app.route("/api/go-wrapper/metrics", methods=["GET"])
@require_auth
def get_go_wrapper_metrics():
    """Proxy Go Wrapper metrics API."""
    import requests

    try:
        go_wrapper_url = "http://100.112.58.92:8151/api/metrics"
        response = requests.get(go_wrapper_url, timeout=5)

        if response.ok:
            return jsonify(response.json())
        else:
            return (
                jsonify(
                    {
                        "error": "Go Wrapper API unavailable",
                        "status_code": response.status_code,
                    }
                ),
                response.status_code,
            )

    except requests.exceptions.Timeout:
        return jsonify({"error": "Go Wrapper API timeout"}), 504
    except requests.exceptions.ConnectionError:
        return jsonify({"error": "Cannot connect to Go Wrapper API"}), 503
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/go-wrapper/agents", methods=["GET"])
@require_auth
def get_go_wrapper_agents():
    """Proxy Go Wrapper agents list API."""
    import requests

    try:
        go_wrapper_url = "http://100.112.58.92:8151/api/agents"
        response = requests.get(go_wrapper_url, timeout=5)

        if response.ok:
            return jsonify(response.json())
        else:
            return (
                jsonify({"error": "Go Wrapper API unavailable"}),
                response.status_code,
            )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/go-wrapper/health", methods=["GET"])
@require_auth
def get_go_wrapper_health():
    """Proxy Go Wrapper health API."""
    import requests

    try:
        go_wrapper_url = "http://100.112.58.92:8151/api/health"
        response = requests.get(go_wrapper_url, timeout=5)

        if response.ok:
            return jsonify(response.json())
        else:
            return (
                jsonify({"error": "Go Wrapper API unavailable"}),
                response.status_code,
            )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ============================================================================
# ACTIVITY API
# ============================================================================


@app.route("/api/activity", methods=["GET"])
@require_auth
def get_activity():
    """Get activity log entries."""
    project_id = request.args.get("project_id")
    action = request.args.get("action")
    limit = request.args.get("limit", 50, type=int)

    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            query = "SELECT * FROM activity_log WHERE 1=1"
            params = []

            if project_id:
                query += " AND (entity_id = ? AND entity_type = 'project')"
                params.append(project_id)

            if action:
                query += " AND action = ?"
                params.append(action)

            query += " ORDER BY created_at DESC LIMIT ?"
            params.append(limit)

            rows = conn.execute(query, params).fetchall()
            return jsonify([dict(r) for r in rows])

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/activity", methods=["POST"])
@require_auth
def create_activity():
    """Create activity log entry (for notes/feedback)."""
    data = request.get_json(silent=True) or {}

    action = data.get("action", "note")
    entity_type = data.get("entity_type")
    entity_id = data.get("entity_id")
    details = data.get("details", "")

    try:
        log_activity(action, entity_type, entity_id, details)
        return jsonify({"success": True})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ============================================================================
# KUDOS API
# ============================================================================


@app.route("/api/kudos", methods=["GET"])
@require_auth
@api_error_handler
def list_kudos():
    """List kudos entries with optional filters."""
    sender_user_id = request.args.get("sender_user_id", type=int)
    recipient_user_id = request.args.get("recipient_user_id", type=int)
    project_id = request.args.get("project_id", type=int)
    category = request.args.get("category")
    if category:
        category = category.lower()
    limit = min(request.args.get("limit", 50, type=int), 200)
    offset = request.args.get("offset", 0, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = kudos.list_kudos(
            conn,
            sender_user_id=sender_user_id,
            recipient_user_id=recipient_user_id,
            project_id=project_id,
            category=category,
            limit=limit,
            offset=offset,
        )
        return jsonify(result)


@app.route("/api/kudos", methods=["POST"])
@require_auth
@api_error_handler
def create_kudos():
    """Create a kudos entry for team recognition."""
    data = request.get_json() or {}
    sender_user_id = session.get("user_id")

    recipient_user_id = data.get("recipient_user_id")
    if recipient_user_id is None:
        return api_error(
            "recipient_user_id is required", 400, "validation_error"
        )

    try:
        recipient_user_id = int(recipient_user_id)
    except (TypeError, ValueError):
        return api_error(
            "recipient_user_id must be an integer", 400, "validation_error"
        )

    if sender_user_id == recipient_user_id:
        return api_error(
            "recipient_user_id must be different from sender",
            400,
            "validation_error",
        )

    message = data.get("message")
    if not message or not str(message).strip():
        return api_error("message is required", 400, "validation_error")

    project_id = data.get("project_id")
    if project_id is not None:
        try:
            project_id = int(project_id)
        except (TypeError, ValueError):
            return api_error(
                "project_id must be an integer", 400, "validation_error"
            )

    points = data.get("points", 1)
    try:
        points = int(points)
    except (TypeError, ValueError):
        return api_error("points must be an integer", 400, "validation_error")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        result = kudos.create_kudos(
            conn,
            sender_user_id=sender_user_id,
            recipient_user_id=recipient_user_id,
            message=message,
            project_id=project_id,
            category=data.get("category"),
            points=points,
            metadata=data.get("metadata"),
        )
        if "error" in result:
            return api_error(result["error"], 400, "validation_error")

        log_activity(
            "send_kudos",
            "kudos",
            result.get("id"),
            json.dumps(
                {
                    "recipient_user_id": recipient_user_id,
                    "project_id": project_id,
                    "category": result.get("category"),
                    "points": result.get("points"),
                }
            ),
        )
        return jsonify(result), 201


@app.route("/api/kudos/<int:kudos_id>", methods=["GET"])
@require_auth
@api_error_handler
def get_kudos_entry(kudos_id):
    """Get a single kudos entry."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        entry = kudos.get_kudos(conn, kudos_id)
        if not entry:
            return api_error("Kudos not found", 404, "not_found")
        return jsonify(entry)


@app.route("/api/kudos/<int:kudos_id>", methods=["DELETE"])
@require_auth
@api_error_handler
def delete_kudos_entry(kudos_id):
    """Delete a kudos entry (sender or admin only)."""
    user_id = session.get("user_id")
    user_role = session.get("role")

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        entry = kudos.get_kudos(conn, kudos_id)
        if not entry:
            return api_error("Kudos not found", 404, "not_found")

        if user_role != "admin" and entry.get("sender_user_id") != user_id:
            return api_error("Access denied", 403, "permission_denied")

        deleted = kudos.delete_kudos(conn, kudos_id)
        if deleted:
            log_activity("delete_kudos", "kudos", kudos_id)
        return jsonify({"success": deleted})


# ============================================================================
# ACCOUNTS/CROSS-NODE API
# ============================================================================


@app.route("/api/accounts", methods=["GET"])
@require_auth
def list_accounts():
    """List all accounts across nodes."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            # Get nodes with their accounts
            nodes = conn.execute(
                """
                SELECT n.*,
                    (SELECT COUNT(*) FROM tmux_sessions WHERE node_id = n.id) as session_count,
                    (SELECT COUNT(*) FROM workers WHERE node_id = n.id) as worker_count
                FROM nodes n
                ORDER BY n.hostname
            """
            ).fetchall()

            accounts = []
            for node in nodes:
                accounts.append(
                    {
                        "node_id": node["id"],
                        "hostname": node["hostname"],
                        "ip": node["ip_address"],
                        "user": node["ssh_user"],
                        "status": node["status"],
                        "sessions": node["session_count"],
                        "workers": node["worker_count"],
                        "last_heartbeat": node["last_heartbeat"],
                    }
                )

            return jsonify(accounts)

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/accounts/aggregate", methods=["GET"])
@require_auth
def aggregate_data():
    """Get aggregated data across all nodes."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            # Errors by node
            errors_by_node = conn.execute(
                """
                SELECT n.hostname, COUNT(e.id) as error_count,
                       SUM(e.occurrence_count) as total_occurrences
                FROM nodes n
                LEFT JOIN errors e ON e.node_id = n.id AND e.status = 'open'
                GROUP BY n.id
            """
            ).fetchall()

            # Features by node
            features_by_node = conn.execute(
                """
                SELECT assigned_node as node, status, COUNT(*) as count
                FROM features
                WHERE assigned_node IS NOT NULL
                GROUP BY assigned_node, status
            """
            ).fetchall()

            # Sessions by node
            sessions_by_node = conn.execute(
                """
                SELECT n.hostname, COUNT(ts.id) as session_count,
                       SUM(CASE WHEN ts.attached = 1 THEN 1 ELSE 0 END) as attached_count
                FROM nodes n
                LEFT JOIN tmux_sessions ts ON ts.node_id = n.id
                GROUP BY n.id
            """
            ).fetchall()

            return jsonify(
                {
                    "errors_by_node": [dict(r) for r in errors_by_node],
                    "features_by_node": [dict(r) for r in features_by_node],
                    "sessions_by_node": [dict(r) for r in sessions_by_node],
                    "totals": {
                        "nodes": len(errors_by_node),
                        "total_errors": sum(
                            r["error_count"] or 0 for r in errors_by_node
                        ),
                        "total_sessions": sum(
                            r["session_count"] or 0 for r in sessions_by_node
                        ),
                    },
                }
            )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ============================================================================
# CONTINUOUS IMPROVEMENT API
# ============================================================================


@app.route("/api/ci/health", methods=["GET"])
@require_auth
def get_ci_health():
    """Get continuous improvement system health."""
    try:
        import sqlite3

        ci_db = BASE_DIR / "data" / "continuous_improvement" / "ci.db"

        if not ci_db.exists():
            return (
                jsonify(
                    {
                        "success": False,
                        "error": "CI database not found - worker may not have started yet",
                    }
                ),
                404,
            )

        conn = sqlite3.connect(str(ci_db))
        cursor = conn.cursor()

        # Get latest health metrics
        cursor.execute(
            """
            SELECT * FROM health_metrics
            ORDER BY timestamp DESC
            LIMIT 1
        """
        )
        health_row = cursor.fetchone()

        health = None
        if health_row:
            import json

            health = {
                "timestamp": health_row[1],
                "workers_running": health_row[2],
                "workers_total": health_row[3],
                "database_size_mb": health_row[4],
                "disk_usage_percent": health_row[5],
                "memory_usage_percent": health_row[6],
                "cpu_usage_percent": health_row[7],
                "session_count": health_row[8],
                "error_count_24h": health_row[9],
                "issues": json.loads(health_row[10]) if health_row[10] else [],
            }

        # Count goals
        cursor.execute("SELECT COUNT(*) FROM goals WHERE status = 'active'")
        active_goals = cursor.fetchone()[0]

        # Count pending suggestions
        cursor.execute(
            "SELECT COUNT(*) FROM suggestions WHERE status = 'pending'"
        )
        pending_suggestions = cursor.fetchone()[0]

        conn.close()

        return jsonify(
            {
                "success": True,
                "health": health,
                "active_goals": active_goals,
                "pending_suggestions": pending_suggestions,
            }
        )

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/ci/goals", methods=["GET"])
@require_auth
def get_ci_goals():
    """Get continuous improvement goals."""
    try:
        import sqlite3

        ci_db = BASE_DIR / "data" / "continuous_improvement" / "ci.db"

        if not ci_db.exists():
            return jsonify({"success": False, "goals": []}), 404

        conn = sqlite3.connect(str(ci_db))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        status_filter = request.args.get("status")

        if status_filter:
            cursor.execute(
                """
                SELECT * FROM goals
                WHERE status = ?
                ORDER BY priority DESC, created_at DESC
            """,
                (status_filter,),
            )
        else:
            cursor.execute(
                """
                SELECT * FROM goals
                ORDER BY priority DESC, created_at DESC
            """
            )

        goals = []
        for row in cursor.fetchall():
            goals.append(
                {
                    "id": row["id"],
                    "title": row["title"],
                    "description": row["description"],
                    "status": row["status"],
                    "priority": row["priority"],
                    "progress": row["progress"],
                    "target_date": row["target_date"],
                    "dependencies": row["dependencies"],
                    "created_at": row["created_at"],
                    "updated_at": row["updated_at"],
                    "completed_at": row["completed_at"],
                }
            )

        conn.close()

        return jsonify({"success": True, "goals": goals})

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/ci/goals", methods=["POST"])
@require_auth
def create_ci_goal():
    """Create a new continuous improvement goal."""
    try:
        data = request.get_json() or {}

        title = data.get("title")
        if not title:
            return (
                jsonify({"success": False, "error": "title is required"}),
                400,
            )

        description = data.get("description", "")
        priority = data.get("priority", 5)
        target_date = data.get("target_date")

        import sqlite3
        from datetime import datetime

        ci_db = BASE_DIR / "data" / "continuous_improvement" / "ci.db"

        conn = sqlite3.connect(str(ci_db))
        cursor = conn.cursor()

        now = datetime.now().isoformat()

        cursor.execute(
            """
            INSERT INTO goals
            (title, description, status, priority, progress, target_date, created_at, updated_at)
            VALUES (?, ?, 'active', ?, 0, ?, ?, ?)
        """,
            (title, description, priority, target_date, now, now),
        )

        goal_id = cursor.lastrowid
        conn.commit()
        conn.close()

        log_activity("ci_goal_created", "goal", goal_id, title)

        return jsonify({"success": True, "goal_id": goal_id})

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/ci/goals/<int:goal_id>", methods=["PUT"])
@require_auth
def update_ci_goal(goal_id):
    """Update a continuous improvement goal."""
    try:
        data = request.get_json() or {}

        import sqlite3
        from datetime import datetime

        ci_db = BASE_DIR / "data" / "continuous_improvement" / "ci.db"

        conn = sqlite3.connect(str(ci_db))
        cursor = conn.cursor()

        # Build update query dynamically
        updates = []
        params = []

        if "title" in data:
            updates.append("title = ?")
            params.append(data["title"])
        if "description" in data:
            updates.append("description = ?")
            params.append(data["description"])
        if "status" in data:
            updates.append("status = ?")
            params.append(data["status"])
        if "priority" in data:
            updates.append("priority = ?")
            params.append(data["priority"])
        if "progress" in data:
            updates.append("progress = ?")
            params.append(data["progress"])
        if "target_date" in data:
            updates.append("target_date = ?")
            params.append(data["target_date"])

        updates.append("updated_at = ?")
        params.append(datetime.now().isoformat())

        # Add completed_at if status is completed
        if data.get("status") == "completed":
            updates.append("completed_at = ?")
            params.append(datetime.now().isoformat())

        params.append(goal_id)

        cursor.execute(
            """
            UPDATE goals
            SET {', '.join(updates)}
            WHERE id = ?
        """,
            params,
        )

        conn.commit()
        conn.close()

        log_activity(
            "ci_goal_updated",
            "goal",
            goal_id,
            data.get("title", f"Goal #{goal_id}"),
        )

        return jsonify({"success": True})

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/ci/suggestions", methods=["GET"])
@require_auth
def get_ci_suggestions():
    """Get continuous improvement suggestions."""
    try:
        import sqlite3

        ci_db = BASE_DIR / "data" / "continuous_improvement" / "ci.db"

        if not ci_db.exists():
            return jsonify({"success": False, "suggestions": []}), 404

        conn = sqlite3.connect(str(ci_db))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        status_filter = request.args.get("status")
        limit = int(request.args.get("limit", 20))

        if status_filter:
            cursor.execute(
                """
                SELECT * FROM suggestions
                WHERE status = ?
                ORDER BY priority DESC, created_at DESC
                LIMIT ?
            """,
                (status_filter, limit),
            )
        else:
            cursor.execute(
                """
                SELECT * FROM suggestions
                ORDER BY priority DESC, created_at DESC
                LIMIT ?
            """,
                (limit,),
            )

        suggestions = []
        for row in cursor.fetchall():
            suggestions.append(
                {
                    "id": row["id"],
                    "goal_id": row["goal_id"],
                    "title": row["title"],
                    "description": row["description"],
                    "rationale": row["rationale"],
                    "implementation_plan": row["implementation_plan"],
                    "status": row["status"],
                    "priority": row["priority"],
                    "estimated_impact": row["estimated_impact"],
                    "risk_level": row["risk_level"],
                    "created_at": row["created_at"],
                    "updated_at": row["updated_at"],
                    "approved_at": row["approved_at"],
                    "implemented_at": row["implemented_at"],
                }
            )

        conn.close()

        return jsonify({"success": True, "suggestions": suggestions})

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/ci/suggestions/<int:suggestion_id>/approve", methods=["POST"])
@require_auth
def approve_ci_suggestion(suggestion_id):
    """Approve a continuous improvement suggestion."""
    try:
        import sqlite3
        from datetime import datetime

        ci_db = BASE_DIR / "data" / "continuous_improvement" / "ci.db"

        conn = sqlite3.connect(str(ci_db))
        cursor = conn.cursor()

        now = datetime.now().isoformat()

        cursor.execute(
            """
            UPDATE suggestions
            SET status = 'approved', approved_at = ?, updated_at = ?
            WHERE id = ?
        """,
            (now, now, suggestion_id),
        )

        conn.commit()
        conn.close()

        log_activity(
            "ci_suggestion_approved",
            "suggestion",
            suggestion_id,
            f"Suggestion #{suggestion_id}",
        )

        return jsonify({"success": True})

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/ci/suggestions/<int:suggestion_id>/reject", methods=["POST"])
@require_auth
def reject_ci_suggestion(suggestion_id):
    """Reject a continuous improvement suggestion."""
    try:
        import sqlite3
        from datetime import datetime

        ci_db = BASE_DIR / "data" / "continuous_improvement" / "ci.db"

        conn = sqlite3.connect(str(ci_db))
        cursor = conn.cursor()

        now = datetime.now().isoformat()

        cursor.execute(
            """
            UPDATE suggestions
            SET status = 'rejected', updated_at = ?
            WHERE id = ?
        """,
            (now, suggestion_id),
        )

        conn.commit()
        conn.close()

        log_activity(
            "ci_suggestion_rejected",
            "suggestion",
            suggestion_id,
            f"Suggestion #{suggestion_id}",
        )

        return jsonify({"success": True})

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


# ============================================================================
# TERMIUS EXPORT
# ============================================================================


@app.route("/api/termius/export", methods=["GET"])
@require_auth
def export_termius():
    """
    Export hosts and accounts for Termius import.

    Query params:
        - mode: 'basic' (hosts only) or 'full' (hosts + tmux snippets)
        - format: 'json' (Termius format) or 'download' (file download)
    """
    mode = request.args.get("mode", "basic")
    output_format = request.args.get("format", "json")

    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            # Get all nodes with their tmux sessions
            nodes = conn.execute(
                """
                SELECT n.*,
                    GROUP_CONCAT(ts.session_name) as sessions
                FROM nodes n
                LEFT JOIN tmux_sessions ts ON n.id = ts.node_id
                GROUP BY n.id
                ORDER BY n.hostname
            """
            ).fetchall()

            # Build Termius-compatible export
            termius_export = {
                "version": 2,
                "timestamp": datetime.now().isoformat(),
                "groups": [],
                "hosts": [],
                "snippets": [],
                "keys": [],
            }

            # Create main group
            main_group_id = str(uuid.uuid4())
            termius_export["groups"].append(
                {
                    "id": main_group_id,
                    "label": "Architect Cluster",
                    "parent_group": None,
                    "color": "#58a6ff",
                }
            )

            # Process each node
            for node in nodes:
                node_dict = dict(node)
                host_id = str(uuid.uuid4())

                # Basic host entry
                host_entry = {
                    "id": host_id,
                    "label": node_dict.get("hostname") or node_dict.get("id"),
                    "group": main_group_id,
                    "address": node_dict.get("ip_address", ""),
                    "port": node_dict.get("ssh_port", 22),
                    "username": node_dict.get("ssh_user", ""),
                    "use_ssh_key": False,
                    "ssh_key": None,
                    "tags": [
                        node_dict.get("role", "worker"),
                        node_dict.get("status", "offline"),
                    ],
                }
                termius_export["hosts"].append(host_entry)

                # Add tmux session snippets if mode is 'full'
                if mode == "full" and node_dict.get("sessions"):
                    sessions = node_dict["sessions"].split(",")

                    for session_name in sessions:
                        if session_name:
                            # Create snippet for direct tmux attach
                            snippet_id = str(uuid.uuid4())
                            termius_export["snippets"].append(
                                {
                                    "id": snippet_id,
                                    "label": f"{
                                        node_dict.get(
                                            'hostname',
                                            'host')}  {session_name}",
                                    "content": f"tmux attach-session -t {session_name}",
                                    "host": host_id,
                                    "tags": ["tmux", session_name],
                                }
                            )

            # Add common utility snippets
            termius_export["snippets"].extend(
                [
                    {
                        "id": str(uuid.uuid4()),
                        "label": "List tmux sessions",
                        "content": "tmux list-sessions",
                        "host": None,
                        "tags": ["tmux", "utility"],
                    },
                    {
                        "id": str(uuid.uuid4()),
                        "label": "New tmux session",
                        "content": "tmux new-session -s ${SESSION_NAME}",
                        "host": None,
                        "tags": ["tmux", "utility"],
                    },
                    {
                        "id": str(uuid.uuid4()),
                        "label": "Check server status",
                        "content": "systemctl status || launchctl list | head -20",
                        "host": None,
                        "tags": ["admin", "utility"],
                    },
                ]
            )

            if output_format == "download":
                from flask import Response

                filename = f"termius_architect_{mode}_{
                    datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                return Response(
                    json.dumps(termius_export, indent=2),
                    mimetype="application/json",
                    headers={
                        "Content-Disposition": f"attachment; filename={filename}"
                    },
                )

            return jsonify(termius_export)

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/termius/preview", methods=["GET"])
@require_auth
def preview_termius_export():
    """Preview what will be exported to Termius."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            nodes = conn.execute(
                """
                SELECT n.id, n.hostname, n.ip_address, n.ssh_port, n.ssh_user, n.status, n.role,
                    COUNT(ts.id) as session_count,
                    GROUP_CONCAT(ts.session_name) as sessions
                FROM nodes n
                LEFT JOIN tmux_sessions ts ON n.id = ts.node_id
                GROUP BY n.id
                ORDER BY n.hostname
            """
            ).fetchall()

            preview = {
                "hosts": [],
                "total_hosts": len(nodes),
                "total_sessions": 0,
            }

            for node in nodes:
                node_dict = dict(node)
                sessions = (
                    node_dict["sessions"].split(",")
                    if node_dict.get("sessions")
                    else []
                )
                preview["total_sessions"] += len(sessions)

                preview["hosts"].append(
                    {
                        "hostname": node_dict.get("hostname"),
                        "ip": node_dict.get("ip_address"),
                        "port": node_dict.get("ssh_port", 22),
                        "user": node_dict.get("ssh_user"),
                        "status": node_dict.get("status"),
                        "role": node_dict.get("role"),
                        "sessions": sessions,
                    }
                )

            return jsonify(preview)

    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ============================================================================
# DOCUMENTATION
# ============================================================================


@app.route("/api/documentation", methods=["GET"])
@require_auth
def get_documentation():
    """Get system documentation with current state."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            # Get current stats
            stats = {}

            # Projects count
            cursor.execute(
                "SELECT COUNT(*) FROM projects WHERE status != 'archived'"
            )
            stats["projects"] = cursor.fetchone()[0]

            # Features count
            cursor.execute(
                "SELECT COUNT(*) as total, SUM(CASE WHEN status = 'in_progress' THEN 1 ELSE 0 END) as in_progress FROM features"
            )
            row = cursor.fetchone()
            stats["features_total"] = row["total"] or 0
            stats["features_in_progress"] = row["in_progress"] or 0

            # Bugs count
            cursor.execute("SELECT COUNT(*) FROM bugs WHERE status = 'open'")
            stats["bugs_open"] = cursor.fetchone()[0]

            # Errors count
            cursor.execute("SELECT COUNT(*) FROM errors WHERE status = 'open'")
            stats["errors_open"] = cursor.fetchone()[0]

            # Apps count
            cursor.execute("SELECT COUNT(*) FROM apps")
            stats["apps_total"] = cursor.fetchone()[0]

            # tmux sessions (count all sessions, they're active if they exist)
            cursor.execute("SELECT COUNT(*) FROM tmux_sessions")
            stats["tmux_active"] = cursor.fetchone()[0]

            # Check for stored documentation (table may not exist)
            doc_row = None
            try:
                cursor.execute(
                    """
                    SELECT content, version, updated_at FROM documentation
                    ORDER BY updated_at DESC LIMIT 1
                """
                )
                doc_row = cursor.fetchone()
            except sqlite3.OperationalError:
                pass  # Table doesn't exist, use generated docs

            if doc_row:
                return jsonify(
                    {
                        "markdown": doc_row["content"],
                        "html": markdown_to_html(doc_row["content"]),
                        "version": doc_row["version"],
                        "updated_at": doc_row["updated_at"],
                        "stats": stats,
                    }
                )

            # Generate dynamic documentation
            markdown = generate_documentation(stats)
            return jsonify(
                {
                    "markdown": markdown,
                    "html": markdown_to_html(markdown),
                    "version": "1.0.0",
                    "updated_at": datetime.now().isoformat(),
                    "stats": stats,
                }
            )

    except Exception as e:
        logger.error(f"Documentation error: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/documentation/files", methods=["GET"])
@require_auth
def get_documentation_files():
    """List documentation files from docs/ directory."""
    try:
        docs_dir = Path(__file__).parent / "docs"
        if not docs_dir.exists():
            return jsonify({"files": []})

        files = []
        for file_path in docs_dir.glob("*.md"):
            try:
                stat = file_path.stat()
                files.append(
                    {
                        "name": file_path.name,
                        "title": file_path.stem.replace("_", " ").title(),
                        "size": stat.st_size,
                        "modified": datetime.fromtimestamp(
                            stat.st_mtime
                        ).isoformat(),
                    }
                )
            except Exception as e:
                logger.warning(f"Could not read file {file_path}: {e}")

        return jsonify({"files": sorted(files, key=lambda x: x["name"])})

    except Exception as e:
        logger.error(f"Error listing documentation files: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/documentation/files/<filename>", methods=["GET"])
@require_auth
def get_documentation_file(filename):
    """Get a specific documentation file from docs/ directory."""
    try:
        # Sanitize filename to prevent directory traversal
        safe_filename = Path(filename).name
        if not safe_filename.endswith(".md"):
            safe_filename += ".md"

        docs_dir = Path(__file__).parent / "docs"
        file_path = docs_dir / safe_filename

        if not file_path.exists() or not file_path.is_file():
            return jsonify({"error": "File not found"}), 404

        # Security check: ensure file is within docs directory
        if not str(file_path.resolve()).startswith(str(docs_dir.resolve())):
            return jsonify({"error": "Invalid file path"}), 403

        content = file_path.read_text(encoding="utf-8")
        stat = file_path.stat()

        return jsonify(
            {
                "filename": safe_filename,
                "title": file_path.stem.replace("_", " ").title(),
                "markdown": content,
                "html": markdown_to_html(content),
                "size": stat.st_size,
                "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
            }
        )

    except Exception as e:
        logger.error(f"Error reading documentation file {filename}: {e}")
        return jsonify({"error": str(e)}), 500


def generate_documentation(stats):
    """Generate documentation markdown with current stats."""
    return """# Architect Dashboard Documentation

## Overview
The Architect Dashboard is a distributed project management and automation platform that enables:
- **Autonomous Development**: AI-assisted coding through tmux/Claude integration
- **Project Management**: Track projects, features, bugs, and milestones
- **Error Aggregation**: Collect and manage errors across distributed nodes
- **Deployment Orchestration**: Multi-environment deployment with testing gates
- **Real-time Monitoring**: Live dashboards with auto-refresh

## Current System State
- **Projects**: {stats.get('projects', 0)} total
- **Features**: {stats.get('features_total', 0)} total ({stats.get('features_in_progress', 0)} in progress)
- **Bugs**: {stats.get('bugs_open', 0)} open
- **Errors**: {stats.get('errors_open', 0)} unresolved
- **Apps**: {stats.get('apps_total', 0)} registered
- **tmux Sessions**: {stats.get('tmux_active', 0)} active

## Features

### 1. Queue Management
The Queue panel is the command center for task orchestration:
- **Priority Queue**: Tasks ordered by priority and creation time
- **Auto-assignment**: Automatically assign tasks to available Claude sessions
- **Status Tracking**: pending  assigned  in_progress  completed/failed

### 2. Apps Management
Track and manage applications across environments:
- **App Registry**: Central catalog of all applications
- **Environment Status**: dev/qa/prod deployment states
- **Health Monitoring**: Real-time health checks

### 3. Projects & Milestones
Full project lifecycle management:
- **Project Creation**: Define projects with source paths
- **Milestone Tracking**: Group features by release targets
- **Progress Visualization**: Track completion percentages

### 4. Features & Bugs
Issue tracking with AI integration:
- **Feature Specifications**: Detailed feature definitions
- **Bug Tracking**: Severity levels (critical/high/medium/low)
- **tmux Assignment**: Send to Claude for autonomous fixing
- **Status Workflow**: open  in_progress  testing  completed

### 5. Error Aggregation
Centralized error management:
- **Auto-deduplication**: Group similar errors
- **Occurrence Tracking**: Count and timestamp tracking
- **Bug Conversion**: Create bugs from errors
- **Batch Operations**: Resolve multiple errors at once

### 6. tmux Sessions
AI-powered development integration:
- **Session Management**: Create, monitor, kill sessions
- **Claude Code Integration**: Send tasks directly to Claude
- **Live Capture**: View session output in real-time
- **Command Execution**: Send commands to sessions

### 7. Testing Framework
Quality assurance integration:
- **Test Runs**: Track test execution history
- **Pass/Fail Metrics**: Monitor test health
- **Deployment Gates**: Block deploys on test failures

### 8. Deployments
Multi-environment deployment management:
- **Tag-based Deploys**: v*.*.*  QA, release-*.*.*  PROD
- **Test Prerequisites**: Require passing tests before deploy
- **Rollback Support**: Track deployment history

### 9. Cluster Management
Distributed node orchestration:
- **Node Registry**: Track all cluster nodes
- **Health Monitoring**: CPU, memory, disk metrics
- **Task Distribution**: Route tasks to appropriate nodes

### 10. Settings & Accounts
Configuration and access management:
- **Cross-node Accounts**: Manage distributed access
- **Environment Configuration**: Multi-env settings

## API Reference

### Authentication
All API endpoints require session authentication except `/api/errors` (POST) and `/health`.

### Core Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/projects` | GET, POST | List/create projects |
| `/api/projects/<id>` | PUT, DELETE | Update/archive project |
| `/api/features` | GET, POST | List/create features |
| `/api/features/<id>` | PUT | Update feature |
| `/api/bugs` | GET, POST | List/create bugs |
| `/api/bugs/<id>` | PUT | Update bug |
| `/api/kudos` | GET, POST | List/create kudos |
| `/api/errors` | GET, POST | List/log errors |
| `/api/errors/<id>/resolve` | POST | Resolve error |
| `/api/errors/<id>/create-bug` | POST | Convert to bug |

### tmux Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/tmux/sessions` | GET | List sessions |
| `/api/tmux/sessions/refresh` | POST | Refresh from tmux |
| `/api/tmux/send` | POST | Send command |
| `/api/tmux/capture` | POST | Capture output |
| `/api/tmux/create` | POST | Create session |
| `/api/tmux/kill` | POST | Kill session |

### Queue Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/queue` | GET, POST | List/add queue items |
| `/api/queue/<id>` | PUT, DELETE | Update/remove item |
| `/api/assign-to-tmux` | POST | Assign task to session |

### Utility Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/stats` | GET | Dashboard statistics |
| `/api/documentation` | GET | This documentation |
| `/health` | GET | Health check |

## Environment Configuration

### Multi-Environment Setup
| Environment | Port | Purpose |
|-------------|------|---------|
| PROD | 8080 | Production |
| QA | 8081 | Quality Assurance |
| DEV | 8082 | Development |
| ENV3 | 8085 | Feature Development |

### Environment Variables
| Variable | Default | Description |
|----------|---------|-------------|
| `PORT` | 8080 | Server port |
| `HOST` | 0.0.0.0 | Server host |
| `SECRET_KEY` | (auto) | Flask secret |
| `ARCHITECT_USER` | architect | Admin username |
| `ARCHITECT_PASSWORD` | peace5 | Admin password |

## Keyboard Shortcuts

| Shortcut | Action |
|----------|--------|
| `Cmd/Ctrl + K` | Global search |
| `Cmd/Ctrl + R` | Refresh current panel |
| `Cmd/Ctrl + N` | New item (context-aware) |
| `Esc` | Close modals |

## Recent Activity Integration
Activity items in the Overview panel are clickable:
- **tmux actions**  Navigate to Sessions panel
- **Error actions**  Navigate to Errors panel
- **Feature/Bug actions**  Navigate to respective panels

## Mobile Support
The dashboard is fully responsive with:
- **Bottom Navigation**: Mobile-optimized nav bar
- **Touch Gestures**: Swipe-friendly interactions
- **Safe Area**: iOS notch/home indicator support
- **Fullscreen Modals**: Optimized for small screens

---
*Generated: {datetime.now().isoformat()}*
*Dashboard Version: 1.0.0*
"""


def markdown_to_html(md):
    """Convert markdown to HTML (simple implementation)."""
    import re

    html = md
    # Escape HTML
    html = html.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

    # Headers
    html = re.sub(r"^### (.*)$", r"<h3>\1</h3>", html, flags=re.MULTILINE)
    html = re.sub(r"^## (.*)$", r"<h2>\1</h2>", html, flags=re.MULTILINE)
    html = re.sub(r"^# (.*)$", r"<h1>\1</h1>", html, flags=re.MULTILINE)

    # Bold and italic
    html = re.sub(r"\*\*\*(.*?)\*\*\*", r"<strong><em>\1</em></strong>", html)
    html = re.sub(r"\*\*(.*?)\*\*", r"<strong>\1</strong>", html)
    html = re.sub(r"\*(.*?)\*", r"<em>\1</em>", html)

    # Inline code
    html = re.sub(r"`([^`]+)`", r"<code>\1</code>", html)

    # Tables
    def process_table_row(match):
        content = match.group(1)
        cells = [c.strip() for c in content.split("|")]
        if all(re.match(r"^-+$", c) for c in cells):
            return "<!-- table separator -->"
        tag = "th" if "---" not in content else "td"
        return "<tr>" + "".join(f"<{tag}>{c}</{tag}>" for c in cells) + "</tr>"

    html = re.sub(r"^\|(.+)\|$", process_table_row, html, flags=re.MULTILINE)

    # Wrap tables
    html = re.sub(r"((?:<tr>.*</tr>\s*)+)", r"<table>\1</table>", html)
    html = html.replace("<!-- table separator -->", "")

    # Horizontal rule
    html = re.sub(r"^---$", r"<hr>", html, flags=re.MULTILINE)

    # Paragraphs
    html = re.sub(r"\n\n", r"</p><p>", html)
    html = re.sub(r"\n", r"<br>", html)
    html = "<p>" + html + "</p>"

    # Clean up
    html = re.sub(r"<p><h", r"<h", html)
    html = re.sub(r"</h(\d)></p>", r"</h\1>", html)
    html = re.sub(r"<p><table>", r"<table>", html)
    html = re.sub(r"</table></p>", r"</table>", html)
    html = re.sub(r"<p><hr></p>", r"<hr>", html)
    html = re.sub(r"<p></p>", r"", html)

    return html


@app.route("/api/documentation/update", methods=["POST"])
@require_auth
def update_documentation():
    """Regenerate and store documentation. Called before deployments."""
    try:
        data = request.get_json(silent=True) or {}
        trigger_type = data.get("trigger_type", "manual")
        environment = data.get(
            "environment", os.environ.get("APP_ENV", "prod")
        )
        triggered_by = session.get("username", "system")

        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            # Ensure tables exist
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS documentation (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    version TEXT NOT NULL,
                    content TEXT NOT NULL,
                    html_content TEXT,
                    generated_by TEXT DEFAULT 'system',
                    environment TEXT,
                    stats_snapshot TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """
            )
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS documentation_updates (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    trigger_type TEXT NOT NULL,
                    triggered_by TEXT,
                    environment TEXT,
                    old_version TEXT,
                    new_version TEXT,
                    changes_summary TEXT,
                    status TEXT DEFAULT 'pending',
                    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    completed_at TIMESTAMP,
                    error_message TEXT
                )
            """
            )

            # Get current version
            cursor.execute(
                "SELECT version FROM documentation ORDER BY updated_at DESC LIMIT 1"
            )
            row = cursor.fetchone()
            old_version = row["version"] if row else "0.0.0"

            # Calculate new version (increment patch)
            version_parts = old_version.split(".")
            new_version = f"{
                version_parts[0]}.{
                version_parts[1]}.{
                int(
                    version_parts[2]) +
                1}"

            # Log the update start
            cursor.execute(
                """
                INSERT INTO documentation_updates (trigger_type, triggered_by, environment, old_version, new_version, status)
                VALUES (?, ?, ?, ?, ?, 'running')
            """,
                (
                    trigger_type,
                    triggered_by,
                    environment,
                    old_version,
                    new_version,
                ),
            )
            update_id = cursor.lastrowid

            # Get current stats
            stats = {}
            cursor.execute(
                "SELECT COUNT(*) FROM projects WHERE status != 'archived'"
            )
            stats["projects"] = cursor.fetchone()[0]

            cursor.execute(
                "SELECT COUNT(*) as total, SUM(CASE WHEN status = 'in_progress' THEN 1 ELSE 0 END) as in_progress FROM features"
            )
            row = cursor.fetchone()
            stats["features_total"] = row["total"] or 0
            stats["features_in_progress"] = row["in_progress"] or 0

            cursor.execute("SELECT COUNT(*) FROM bugs WHERE status = 'open'")
            stats["bugs_open"] = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM errors WHERE status = 'open'")
            stats["errors_open"] = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM apps")
            stats["apps_total"] = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM tmux_sessions")
            stats["tmux_active"] = cursor.fetchone()[0]

            # Generate documentation
            markdown = generate_documentation(stats)
            html = markdown_to_html(markdown)

            # Store new documentation
            cursor.execute(
                """
                INSERT INTO documentation (version, content, html_content, generated_by, environment, stats_snapshot)
                VALUES (?, ?, ?, ?, ?, ?)
            """,
                (
                    new_version,
                    markdown,
                    html,
                    triggered_by,
                    environment,
                    json.dumps(stats),
                ),
            )

            # Update the log
            cursor.execute(
                """
                UPDATE documentation_updates
                SET status = 'completed', completed_at = CURRENT_TIMESTAMP,
                    changes_summary = ?
                WHERE id = ?
            """,
                (
                    f"Updated stats: {
                        stats['projects']} projects, {
                        stats['features_total']} features, {
                        stats['bugs_open']} bugs",
                    update_id,
                ),
            )

            conn.commit()

            logger.info(
                f"Documentation updated: {old_version} -> {new_version} by {triggered_by}"
            )

            return jsonify(
                {
                    "success": True,
                    "old_version": old_version,
                    "new_version": new_version,
                    "triggered_by": triggered_by,
                    "trigger_type": trigger_type,
                    "stats": stats,
                }
            )

    except Exception as e:
        logger.error(f"Documentation update error: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/documentation/history", methods=["GET"])
@require_auth
def get_documentation_history():
    """Get documentation update history."""
    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            # Check if table exists
            cursor.execute(
                "SELECT name FROM sqlite_master WHERE type='table' AND name='documentation_updates'"
            )
            if not cursor.fetchone():
                return jsonify({"updates": [], "versions": []})

            cursor.execute(
                """
                SELECT * FROM documentation_updates
                ORDER BY started_at DESC
                LIMIT 20
            """
            )
            updates = [dict(row) for row in cursor.fetchall()]

            cursor.execute(
                """
                SELECT version, generated_by, environment, created_at
                FROM documentation
                ORDER BY created_at DESC
                LIMIT 10
            """
            )
            versions = [dict(row) for row in cursor.fetchall()]

            return jsonify({"updates": updates, "versions": versions})

    except Exception as e:
        logger.error(f"Documentation history error: {e}")
        return jsonify({"error": str(e)}), 500


# ============================================================================
# SSL CERTIFICATE MANAGEMENT
# ============================================================================


def get_tailscale_ip():
    """Get the Tailscale IP address if available."""
    try:
        result = subprocess.run(
            ["tailscale", "ip", "-4"],
            capture_output=True,
            text=True,
            timeout=5,
        )
        if result.returncode == 0:
            return result.stdout.strip()
    except Exception:
        pass
    return None


def get_local_ips():
    """Get local IP addresses."""
    ips = ["127.0.0.1"]
    try:
        import socket

        hostname = socket.gethostname()
        ips.append(socket.gethostbyname(hostname))
    except Exception:
        pass
    return ips


def generate_ssl_certificate(
    cert_path: str, key_path: str, hostnames: list = None, days: int = 365
) -> dict:
    """Generate a self-signed SSL certificate with SANs for Tailscale access."""
    import socket
    import tempfile

    sans = ["localhost", socket.gethostname()]
    sans.extend(get_local_ips())
    ts_ip = get_tailscale_ip()
    if ts_ip:
        sans.append(ts_ip)
    if hostnames:
        sans.extend(hostnames)
    sans = list(dict.fromkeys(sans))
    san_entries = [
        f"IP:{s}" if s.replace(".", "").isdigit() else f"DNS:{s}" for s in sans
    ]
    cfg_content = f"[req]\ndefault_bits=4096\nprompt=no\ndefault_md=sha256\ndistinguished_name=dn\nx509_extensions=v3\n[dn]\nCN={
        socket.gethostname()}\nO=Architect Dashboard\n[v3]\nbasicConstraints=CA:FALSE\nkeyUsage=digitalSignature,keyEncipherment\nextendedKeyUsage=serverAuth\nsubjectAltName={
        ','.join(san_entries)}\n"
    with tempfile.NamedTemporaryFile(
        mode="w", suffix=".cnf", delete=False
    ) as f:
        f.write(cfg_content)
        cfg_path = f.name
    try:
        r = subprocess.run(
            [
                "openssl",
                "req",
                "-x509",
                "-newkey",
                "rsa:4096",
                "-keyout",
                key_path,
                "-out",
                cert_path,
                "-days",
                str(days),
                "-nodes",
                "-config",
                cfg_path,
            ],
            capture_output=True,
            text=True,
            timeout=30,
        )
        os.unlink(cfg_path)
        if r.returncode != 0:
            return {"success": False, "error": r.stderr}
        print(f"[SSL] Certificate generated with SANs: {', '.join(sans)}")
        return {"success": True, "sans": sans}
    except Exception as e:
        try:
            os.unlink(cfg_path)
        except Exception:
            pass
        return {"success": False, "error": str(e)}


@app.route("/api/ssl/status", methods=["GET"])
@require_auth
def get_ssl_status():
    """Get SSL certificate status and details."""
    cert_file, key_file = DATA_DIR / "server.crt", DATA_DIR / "server.key"
    status = {
        "enabled": cert_file.exists() and key_file.exists(),
        "cert_path": str(cert_file),
        "tailscale_ip": get_tailscale_ip(),
        "local_ips": get_local_ips(),
    }
    if cert_file.exists():
        try:
            r = subprocess.run(
                [
                    "openssl",
                    "x509",
                    "-in",
                    str(cert_file),
                    "-noout",
                    "-enddate",
                    "-subject",
                ],
                capture_output=True,
                text=True,
                timeout=10,
            )
            if r.returncode == 0:
                for line in r.stdout.strip().split("\n"):
                    if "notAfter" in line:
                        status["expiry"] = line.replace("notAfter=", "")
                    if "subject" in line:
                        status["subject"] = line.replace("subject=", "")
            status["created_at"] = datetime.fromtimestamp(
                cert_file.stat().st_mtime
            ).isoformat()
            r2 = subprocess.run(
                ["openssl", "x509", "-in", str(cert_file), "-noout", "-text"],
                capture_output=True,
                text=True,
                timeout=10,
            )
            if "DNS:" in r2.stdout or "IP Address:" in r2.stdout:
                for line in r2.stdout.split("\n"):
                    if "DNS:" in line or "IP Address:" in line:
                        status["sans"] = line.strip()
                        break
        except Exception as e:
            status["error"] = str(e)
    return jsonify(status)


@app.route("/api/ssl/generate", methods=["POST"])
@require_auth
def regenerate_ssl_certificate():
    """Regenerate SSL certificate with custom hostnames."""
    data = request.get_json() or {}
    hostnames, days, force = (
        data.get("hostnames", []),
        data.get("days", 365),
        data.get("force", False),
    )
    cert_file, key_file = DATA_DIR / "server.crt", DATA_DIR / "server.key"
    if cert_file.exists() and not force:
        return (
            jsonify(
                {"error": "Certificate exists. Use force=true to regenerate."}
            ),
            409,
        )
    if cert_file.exists():
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        cert_file.rename(DATA_DIR / f"server.crt.backup.{ts}")
        key_file.rename(DATA_DIR / f"server.key.backup.{ts}")
    result = generate_ssl_certificate(
        str(cert_file), str(key_file), hostnames, days
    )
    if result["success"]:
        log_activity(
            "generate_ssl_cert", "ssl", None, f"SANs: {result.get('sans')}"
        )
        return jsonify(
            {
                "success": True,
                "message": "Certificate generated. Restart server to apply.",
                "sans": result.get("sans"),
                "restart_required": True,
            }
        )
    return jsonify({"error": result.get("error")}), 500


@app.route("/api/ssl/download", methods=["GET"])
@require_auth
def download_ssl_certificate():
    """Download the SSL certificate for importing into browsers."""
    cert_file = DATA_DIR / "server.crt"
    if not cert_file.exists():
        return jsonify({"error": "No certificate found"}), 404
    response = make_response(open(cert_file).read())
    response.headers["Content-Type"] = "application/x-pem-file"
    response.headers["Content-Disposition"] = (
        "attachment; filename=architect_dashboard.crt"
    )
    return response


# ============================================================================
# HEALTH CHECK
# ============================================================================


@app.route("/api/config/compression", methods=["GET"])
@require_auth
def get_compression_config():
    """Get API response compression configuration.

    Returns:
        enabled: Whether compression is enabled
        algorithm: Compression algorithm (gzip/brotli)
        level: Compression level (1-9)
        min_size: Minimum response size to compress
        mimetypes: List of compressed content types
    """
    return jsonify(
        {
            "enabled": True,
            "algorithm": app.config.get("COMPRESS_ALGORITHM", "gzip"),
            "level": app.config.get("COMPRESS_LEVEL", 6),
            "min_size": app.config.get("COMPRESS_MIN_SIZE", 500),
            "mimetypes": app.config.get("COMPRESS_MIMETYPES", []),
            "description": "Responses larger than min_size bytes are automatically compressed",
        }
    )


@app.route("/health", methods=["GET", "OPTIONS"])
def health_check():
    """Health check endpoint for monitoring.

    Returns system health status including database connectivity, CPU, memory.
    No authentication required. Supports CORS for cross-environment monitoring.

    Returns:
        200: System healthy with metrics
        503: System unhealthy

    Example Response:
        {"status": "healthy", "database": {"status": "connected"}}

    cURL Example:
        curl "http://localhost:8080/health"
    """
    if request.method == "OPTIONS":
        response = make_response()
        response.headers["Access-Control-Allow-Origin"] = "*"
        response.headers["Access-Control-Allow-Methods"] = "GET, OPTIONS"
        response.headers["Access-Control-Allow-Headers"] = "Content-Type"
        return response

    try:
        # Check database connection
        db_start = time.time()
        with get_db_connection() as conn:
            conn.execute("SELECT 1")
        db_response_ms = round((time.time() - db_start) * 1000, 1)

        # Get CPU and memory stats
        cpu = None
        memory = None
        try:
            import psutil

            cpu = round(psutil.cpu_percent(interval=0.1), 1)
            memory = round(psutil.virtual_memory().percent, 1)
        except ImportError:
            # psutil not available, try shell commands on macOS/Linux
            try:
                # Get CPU from top command
                top_output = subprocess.run(
                    ["top", "-l", "1", "-n", "0"],
                    capture_output=True,
                    text=True,
                    timeout=2,
                )
                for line in top_output.stdout.split("\n"):
                    if "CPU usage" in line:
                        # Parse "CPU usage: 5.26% user, 10.52% sys, 84.21%
                        # idle"
                        parts = line.split(",")
                        user = float(
                            parts[0]
                            .split(":")[1]
                            .strip()
                            .replace("%", "")
                            .split()[0]
                        )
                        sys_cpu = float(
                            parts[1].strip().replace("%", "").split()[0]
                        )
                        cpu = round(user + sys_cpu, 1)
                        break
                # Get memory from vm_stat
                vm_output = subprocess.run(
                    ["vm_stat"], capture_output=True, text=True, timeout=2
                )
                pages_free = pages_active = pages_inactive = pages_wired = 0
                for line in vm_output.stdout.split("\n"):
                    if "Pages free" in line:
                        pages_free = int(
                            line.split(":")[1].strip().rstrip(".")
                        )
                    elif "Pages active" in line:
                        pages_active = int(
                            line.split(":")[1].strip().rstrip(".")
                        )
                    elif "Pages inactive" in line:
                        pages_inactive = int(
                            line.split(":")[1].strip().rstrip(".")
                        )
                    elif "Pages wired" in line:
                        pages_wired = int(
                            line.split(":")[1].strip().rstrip(".")
                        )
                total = (
                    pages_free + pages_active + pages_inactive + pages_wired
                )
                if total > 0:
                    used = pages_active + pages_wired
                    memory = round((used / total) * 100, 1)
            except Exception:
                pass

        # Determine status based on db response time
        db_status = "connected"
        status = "healthy"
        if db_response_ms > 1000:
            db_status = "slow"
            status = "degraded"
        elif db_response_ms > 5000:
            db_status = "critical"
            status = "unhealthy"

        health_data = {
            "status": status,
            "environment": APP_ENV,
            "timestamp": datetime.now().isoformat(),
            "database": db_status,
            "db_response_ms": db_response_ms,
        }
        if cpu is not None:
            health_data["cpu"] = cpu
        if memory is not None:
            health_data["memory"] = memory

        # Add system component status
        try:
            # Check tmux sessions
            tmux_result = subprocess.run(
                ["tmux", "list-sessions", "-F", "#{session_name}"],
                capture_output=True,
                text=True,
                timeout=2,
            )
            if tmux_result.returncode == 0:
                sessions = [
                    s.strip()
                    for s in tmux_result.stdout.strip().split("\n")
                    if s.strip()
                ]
                health_data["tmux_sessions"] = len(sessions)

            # Check orchestrator
            orchestrator_pid_file = Path("/tmp/project_orchestrator.pid")
            if orchestrator_pid_file.exists():
                pid = int(orchestrator_pid_file.read_text().strip())
                try:
                    os.kill(pid, 0)
                    health_data["orchestrator"] = "running"
                except OSError:
                    health_data["orchestrator"] = "stopped"
            else:
                health_data["orchestrator"] = "stopped"

            # Check auto-confirm worker
            auto_confirm_result = subprocess.run(
                ["pgrep", "-", "auto_confirm_worker"],
                capture_output=True,
                text=True,
                timeout=2,
            )
            health_data["auto_confirm"] = (
                "running" if auto_confirm_result.returncode == 0 else "stopped"
            )

        except Exception:
            pass  # Non-critical, continue without component status

        # Send notification if status is degraded or unhealthy
        if status in ("degraded", "unhealthy"):
            try:
                from db import DB_PATHS
                from services.notifications import notify_health_alert

                notify_health_alert(
                    component="dashboard",
                    status=status,
                    details=(
                        f"Database response: {db_response_ms}ms"
                        if db_status != "connected"
                        else None
                    ),
                    metrics={
                        "cpu": cpu,
                        "memory": memory,
                        "db_ms": db_response_ms,
                    },
                    db_path=str(DB_PATHS["main"]),
                )
            except Exception:
                pass  # Don't fail health check due to notification error

        response = jsonify(health_data)
        response.headers["Access-Control-Allow-Origin"] = "*"
        return response
    except Exception as e:
        # Send critical notification on health check failure
        try:
            from db import DB_PATHS
            from services.notifications import notify_health_alert

            notify_health_alert(
                component="dashboard",
                status="unhealthy",
                details=str(e),
                db_path=str(DB_PATHS["main"]),
            )
        except Exception:
            pass

        response = jsonify({"status": "unhealthy", "error": str(e)})
        response.headers["Access-Control-Allow-Origin"] = "*"
        return response, 500


# ============================================================================
# AUTOMATED ROLLBACK API
# ============================================================================


@app.route("/api/rollback/snapshots", methods=["GET"])
@require_auth
def list_rollback_snapshots():
    """List available rollback snapshots.

    Query params:
        limit: Maximum snapshots to return (default 20)

    Returns:
        snapshots: List of snapshot metadata
    """
    limit = request.args.get("limit", 20, type=int)
    manager = rollback_manager.get_rollback_manager()
    snapshots = manager.list_snapshots(limit)
    return jsonify({"snapshots": snapshots, "count": len(snapshots)})


@app.route("/api/rollback/snapshots", methods=["POST"])
@require_auth
def create_rollback_snapshot():
    """Create a new rollback snapshot.

    Request body:
        description: Optional description for the snapshot

    Returns:
        snapshot_id: ID of created snapshot
    """
    data = request.get_json() or {}
    description = data.get("description")

    manager = rollback_manager.get_rollback_manager()
    snapshot_id = manager.create_snapshot(description)

    log_activity("create_rollback_snapshot", "rollback", None, snapshot_id)

    return jsonify(
        {
            "success": True,
            "snapshot_id": snapshot_id,
            "message": f"Snapshot {snapshot_id} created successfully",
        }
    )


@app.route("/api/rollback/snapshots/<snapshot_id>", methods=["GET"])
@require_auth
def get_rollback_snapshot(snapshot_id):
    """Get details of a specific snapshot.

    Returns:
        Snapshot metadata including git info and database backup path
    """
    manager = rollback_manager.get_rollback_manager()
    snapshot = manager.get_snapshot(snapshot_id)

    if not snapshot:
        return api_error(
            f"Snapshot not found: {snapshot_id}", 404, "not_found"
        )

    return jsonify(snapshot)


@app.route("/api/rollback/execute", methods=["POST"])
@require_auth
def execute_rollback():
    """Execute a rollback to a specific snapshot.

    Request body:
        snapshot_id: ID of snapshot to rollback to
        restart_service: Whether to restart the service (default true)

    Returns:
        Rollback result with operations performed
    """
    data = request.get_json()
    if not data or not data.get("snapshot_id"):
        return api_error("snapshot_id is required", 400, "validation_error")

    snapshot_id = data["snapshot_id"]
    restart_service = data.get("restart_service", True)

    manager = rollback_manager.get_rollback_manager()

    # Verify snapshot exists
    if not manager.get_snapshot(snapshot_id):
        return api_error(
            f"Snapshot not found: {snapshot_id}", 404, "not_found"
        )

    # Execute rollback
    result = manager.rollback(snapshot_id, restart_service)

    log_activity(
        "execute_rollback",
        "rollback",
        None,
        f"{'Success' if result['success'] else 'Failed'}: {snapshot_id}",
    )

    if result.get("success"):
        return jsonify(result)
    else:
        return jsonify(result), 500


@app.route("/api/rollback/monitor/start", methods=["POST"])
@require_auth
def start_rollback_monitoring():
    """Start health monitoring with automatic rollback.

    Request body:
        snapshot_id: Snapshot ID to rollback to if health fails
        duration: Monitoring duration in seconds (default 300)
        check_interval: Health check interval in seconds (default 10)
        failure_threshold: Consecutive failures before rollback (default 3)

    Returns:
        Status of monitoring start
    """
    data = request.get_json()
    if not data or not data.get("snapshot_id"):
        return api_error("snapshot_id is required", 400, "validation_error")

    manager = rollback_manager.get_rollback_manager()

    # Check if already monitoring
    status = manager.get_monitoring_status()
    if status["active"]:
        return api_error(
            "Health monitoring already active", 409, "already_active"
        )

    # Start monitoring
    manager.start_health_monitoring(
        snapshot_id=data["snapshot_id"],
        duration=data.get("duration", 300),
        check_interval=data.get("check_interval", 10),
        failure_threshold=data.get("failure_threshold", 3),
    )

    log_activity(
        "start_rollback_monitoring", "rollback", None, data["snapshot_id"]
    )

    return jsonify(
        {
            "success": True,
            "message": "Health monitoring started",
            "snapshot_id": data["snapshot_id"],
            "duration": data.get("duration", 300),
        }
    )


@app.route("/api/rollback/monitor/stop", methods=["POST"])
@require_auth
def stop_rollback_monitoring():
    """Stop health monitoring."""
    manager = rollback_manager.get_rollback_manager()
    manager.stop_health_monitoring()

    log_activity("stop_rollback_monitoring", "rollback", None)

    return jsonify({"success": True, "message": "Health monitoring stopped"})


@app.route("/api/rollback/monitor/status", methods=["GET"])
@require_auth
def get_rollback_monitor_status():
    """Get current rollback monitoring status.

    Returns:
        active: Whether monitoring is active
        current_snapshot_id: Snapshot ID being watched
        failure_count: Current consecutive failure count
        failure_threshold: Failures needed to trigger rollback
    """
    manager = rollback_manager.get_rollback_manager()
    status = manager.get_monitoring_status()
    return jsonify(status)


@app.route("/api/rollback/history", methods=["GET"])
@require_auth
def get_rollback_history():
    """Get rollback history.

    Query params:
        limit: Maximum entries to return (default 50)

    Returns:
        history: List of rollback events
    """
    limit = request.args.get("limit", 50, type=int)
    manager = rollback_manager.get_rollback_manager()
    history = manager.get_history(limit)

    return jsonify({"history": history, "count": len(history)})


@app.route("/api/rollback/health-check", methods=["GET"])
@require_auth
def manual_health_check():
    """Perform a manual health check (same as automated monitoring uses).

    Returns:
        Health check result
    """
    manager = rollback_manager.get_rollback_manager()
    health = manager._check_health()
    return jsonify(health)


@app.route("/api/rollback/cleanup", methods=["POST"])
@require_auth
def cleanup_rollback_snapshots():
    """Clean up old snapshots.

    Request body:
        keep_count: Number of recent snapshots to keep (default 10)

    Returns:
        Success status
    """
    data = request.get_json() or {}
    keep_count = data.get("keep_count", 10)

    manager = rollback_manager.get_rollback_manager()
    manager.cleanup_old_snapshots(keep_count)

    log_activity(
        "cleanup_rollback_snapshots", "rollback", None, f"kept {keep_count}"
    )

    return jsonify(
        {
            "success": True,
            "message": f"Cleanup complete, kept {keep_count} most recent snapshots",
        }
    )


@app.route("/api/config", methods=["GET"])
@require_auth
def get_config():
    """Get current configuration (sensitive values masked)."""
    try:
        from config import ENV_VARS, Config

        config_dict = Config.to_dict(include_sensitive=False)

        # Add metadata about each variable
        config_info = {}
        for name, value in config_dict.items():
            env_var = ENV_VARS.get(name)
            if env_var:
                config_info[name] = {
                    "value": value,
                    "type": env_var.var_type,
                    "default": "***" if env_var.sensitive else env_var.default,
                    "description": env_var.description,
                    "required": env_var.required,
                    "source": "env" if os.environ.get(name) else "default",
                }

        return jsonify(
            {
                "config": config_info,
                "environment": os.environ.get("APP_ENV", "prod"),
                "count": len(config_info),
            }
        )
    except ImportError:
        # Fallback to legacy CONFIG
        return jsonify(
            {
                "config": {
                    k: (
                        "***"
                        if "PASSWORD" in k or "KEY" in k or "SECRET" in k
                        else v
                    )
                    for k, v in CONFIG.items()
                },
                "environment": os.environ.get("APP_ENV", "prod"),
                "note": "Using legacy config (env_config module not available)",
            }
        )
    except Exception as e:
        logger.error(f"Config error: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/metrics", methods=["GET"])
def get_metrics():
    """Prometheus-style metrics endpoint.

    Returns metrics in Prometheus text format for scraping.
    No authentication required for monitoring systems.
    """
    lines = []

    def add_metric(
        name, value, help_text=None, metric_type="gauge", labels=None
    ):
        """Add a metric in Prometheus format."""
        if help_text:
            lines.append(f"# HELP {name} {help_text}")
        if metric_type:
            lines.append(f"# TYPE {name} {metric_type}")
        if labels:
            label_str = ",".join(f'{k}="{v}"' for k, v in labels.items())
            lines.append(f"{name}{{{label_str}}} {value}")
        else:
            lines.append(f"{name} {value}")

    try:
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            # Project metrics
            project_count = conn.execute(
                "SELECT COUNT(*) as count FROM projects WHERE status = 'active'"
            ).fetchone()["count"]
            add_metric(
                "architect_projects_total",
                project_count,
                "Total number of active projects",
                "gauge",
            )

            # Feature metrics by status
            feature_stats = conn.execute(
                """
                SELECT status, COUNT(*) as count FROM features GROUP BY status
            """
            ).fetchall()
            lines.append(
                "# HELP architect_features_total Number of features by status"
            )
            lines.append("# TYPE architect_features_total gauge")
            for row in feature_stats:
                lines.append(
                    f'architect_features_total{{status="{
                        row["status"]}"}} {
                        row["count"]}'
                )

            # Bug metrics by status and severity
            bug_stats = conn.execute(
                """
                SELECT status, COUNT(*) as count FROM bugs GROUP BY status
            """
            ).fetchall()
            lines.append(
                "# HELP architect_bugs_total Number of bugs by status"
            )
            lines.append("# TYPE architect_bugs_total gauge")
            for row in bug_stats:
                lines.append(
                    f'architect_bugs_total{{status="{
                        row["status"]}"}} {
                        row["count"]}'
                )

            bug_severity = conn.execute(
                """
                SELECT severity, COUNT(*) as count FROM bugs
                WHERE status NOT IN ('fixed', 'closed', 'wontfix')
                GROUP BY severity
            """
            ).fetchall()
            lines.append(
                "# HELP architect_bugs_open_by_severity Open bugs by severity"
            )
            lines.append("# TYPE architect_bugs_open_by_severity gauge")
            for row in bug_severity:
                sev = row["severity"] or "unknown"
                lines.append(
                    f'architect_bugs_open_by_severity{{severity="{sev}"}} {
                        row["count"]}'
                )

            # Error metrics
            error_stats = conn.execute(
                """
                SELECT status, COUNT(*) as count, COALESCE(SUM(occurrence_count), 0) as occurrences
                FROM errors GROUP BY status
            """
            ).fetchall()
            lines.append(
                "# HELP architect_errors_total Number of unique errors by status"
            )
            lines.append("# TYPE architect_errors_total gauge")
            for row in error_stats:
                lines.append(
                    f'architect_errors_total{{status="{
                        row["status"]}"}} {
                        row["count"]}'
                )
            lines.append(
                "# HELP architect_error_occurrences_total Total error occurrences by status"
            )
            lines.append("# TYPE architect_error_occurrences_total counter")
            for row in error_stats:
                lines.append(
                    f'architect_error_occurrences_total{{status="{
                        row["status"]}"}} {
                        row["occurrences"]}'
                )

            # Task queue metrics
            task_stats = conn.execute(
                """
                SELECT status, COUNT(*) as count FROM task_queue GROUP BY status
            """
            ).fetchall()
            lines.append(
                "# HELP architect_tasks_total Number of tasks by status"
            )
            lines.append("# TYPE architect_tasks_total gauge")
            for row in task_stats:
                lines.append(
                    f'architect_tasks_total{{status="{
                        row["status"]}"}} {
                        row["count"]}'
                )

            # Worker metrics
            worker_stats = conn.execute(
                """
                SELECT status, COUNT(*) as count FROM workers GROUP BY status
            """
            ).fetchall()
            lines.append(
                "# HELP architect_workers_total Number of workers by status"
            )
            lines.append("# TYPE architect_workers_total gauge")
            for row in worker_stats:
                lines.append(
                    f'architect_workers_total{{status="{
                        row["status"]}"}} {
                        row["count"]}'
                )

            # Node metrics
            node_stats = conn.execute(
                """
                SELECT status, COUNT(*) as count FROM nodes GROUP BY status
            """
            ).fetchall()
            lines.append(
                "# HELP architect_nodes_total Number of cluster nodes by status"
            )
            lines.append("# TYPE architect_nodes_total gauge")
            for row in node_stats:
                lines.append(
                    f'architect_nodes_total{{status="{
                        row["status"]}"}} {
                        row["count"]}'
                )

            # tmux session count
            tmux_count = conn.execute(
                "SELECT COUNT(*) as count FROM tmux_sessions"
            ).fetchone()["count"]
            add_metric(
                "architect_tmux_sessions_total",
                tmux_count,
                "Total number of tracked tmux sessions",
                "gauge",
            )

            # Autopilot run metrics
            try:
                autopilot_stats = conn.execute(
                    """
                    SELECT status, COUNT(*) as count FROM autopilot_runs GROUP BY status
                """
                ).fetchall()
                lines.append(
                    "# HELP architect_autopilot_runs_total Number of autopilot runs by status"
                )
                lines.append("# TYPE architect_autopilot_runs_total gauge")
                for row in autopilot_stats:
                    lines.append(
                        f'architect_autopilot_runs_total{{status="{
                            row["status"]}"}} {
                            row["count"]}'
                    )
            except sqlite3.OperationalError:
                pass  # Table may not exist

            # Apps metrics
            try:
                app_count = conn.execute(
                    "SELECT COUNT(*) as count FROM apps"
                ).fetchone()["count"]
                add_metric(
                    "architect_apps_total",
                    app_count,
                    "Total number of managed apps",
                    "gauge",
                )
            except sqlite3.OperationalError:
                pass  # Table may not exist

            # Milestone metrics
            try:
                milestone_stats = conn.execute(
                    """
                    SELECT status, COUNT(*) as count FROM milestones GROUP BY status
                """
                ).fetchall()
                lines.append(
                    "# HELP architect_milestones_total Number of milestones by status"
                )
                lines.append("# TYPE architect_milestones_total gauge")
                for row in milestone_stats:
                    lines.append(
                        f'architect_milestones_total{{status="{
                            row["status"]}"}} {
                            row["count"]}'
                    )
            except sqlite3.OperationalError:
                pass  # Table may not exist

            # Secrets count (no values exposed)
            try:
                secret_count = conn.execute(
                    "SELECT COUNT(*) as count FROM secrets"
                ).fetchone()["count"]
                add_metric(
                    "architect_secrets_total",
                    secret_count,
                    "Total number of stored secrets",
                    "gauge",
                )
            except sqlite3.OperationalError:
                pass  # Table may not exist

        # System metrics
        try:
            import psutil

            add_metric(
                "architect_cpu_usage_percent",
                round(psutil.cpu_percent(interval=0.1), 2),
                "Current CPU usage percentage",
                "gauge",
            )
            mem = psutil.virtual_memory()
            add_metric(
                "architect_memory_usage_percent",
                round(mem.percent, 2),
                "Current memory usage percentage",
                "gauge",
            )
            add_metric(
                "architect_memory_used_bytes",
                mem.used,
                "Memory used in bytes",
                "gauge",
            )
            add_metric(
                "architect_memory_available_bytes",
                mem.available,
                "Memory available in bytes",
                "gauge",
            )
            disk = psutil.disk_usage("/")
            add_metric(
                "architect_disk_usage_percent",
                round(disk.percent, 2),
                "Disk usage percentage",
                "gauge",
            )
        except ImportError:
            pass  # psutil not available

        # Process uptime
        try:
            import psutil

            process = psutil.Process()
            uptime_seconds = time.time() - process.create_time()
            add_metric(
                "architect_uptime_seconds",
                round(uptime_seconds, 0),
                "Process uptime in seconds",
                "counter",
            )
        except Exception:
            pass

        # Build info metric
        lines.append("# HELP architect_build_info Build information")
        lines.append("# TYPE architect_build_info gauge")
        lines.append(
            'architect_build_info{version="1.0.0",python_version="'
            + f'{
                sys.version_info.major}.{
                sys.version_info.minor}.{
                sys.version_info.micro}"'
            + "} 1"
        )

        response = make_response("\n".join(lines) + "\n")
        response.headers["Content-Type"] = "text/plain; charset=utf-8"
        response.headers["Access-Control-Allow-Origin"] = "*"
        return response

    except Exception as e:
        logger.error(f"Error generating metrics: {e}")
        response = make_response(f"# Error generating metrics: {e}\n")
        response.headers["Content-Type"] = "text/plain; charset=utf-8"
        return response, 500


@app.route("/api/env-health", methods=["GET"])
@require_auth
def get_env_health():
    """Proxy health checks to other environment ports.

    This avoids CORS and mixed-content issues when the dashboard
    checks health of other running instances.

    Query params:
        port: The port to check (8080, 8081, 8082, 8085)
        all: If 'true', check all known environment ports
    """
    import ssl
    import urllib.request

    # Define known environment ports
    ENV_PORTS = {
        8080: {"name": "PROD", "icon": "", "protocol": "https"},
        8081: {"name": "QA", "icon": "", "protocol": "http"},
        8082: {"name": "DEV", "icon": "", "protocol": "http"},
        8085: {"name": "ENV3", "icon": "", "protocol": "http"},
    }

    check_all = request.args.get("all", "false").lower() == "true"

    if check_all:
        results = []
        for port, info in ENV_PORTS.items():
            result = {"port": port, **info}
            try:
                protocol = info["protocol"]
                url = f"{protocol}://127.0.0.1:{port}/health"
                ctx = ssl.create_default_context()
                ctx.check_hostname = False
                ctx.verify_mode = ssl.CERT_NONE

                req = urllib.request.Request(url, method="GET")
                with urllib.request.urlopen(
                    req, timeout=2, context=ctx
                ) as resp:
                    if resp.status == 200:
                        data = json.loads(resp.read().decode())
                        result["status"] = data.get("status", "healthy")
                        result["cpu"] = data.get("cpu")
                        result["memory"] = data.get("memory")
                    else:
                        result["status"] = "unhealthy"
            except Exception as e:
                result["status"] = "offline"
                result["error"] = str(e)
            results.append(result)
        return jsonify(results)

    # Single port check
    port = request.args.get("port", type=int)
    if not port or port not in ENV_PORTS:
        return (
            jsonify(
                {
                    "error": f"Invalid port. Use one of: {list(ENV_PORTS.keys())}"
                }
            ),
            400,
        )

    info = ENV_PORTS[port]
    try:
        protocol = info["protocol"]
        url = f"{protocol}://127.0.0.1:{port}/health"
        ctx = ssl.create_default_context()
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE

        req = urllib.request.Request(url, method="GET")
        with urllib.request.urlopen(req, timeout=2, context=ctx) as resp:
            if resp.status == 200:
                data = json.loads(resp.read().decode())
                return jsonify(
                    {
                        "port": port,
                        "name": info["name"],
                        "status": data.get("status", "healthy"),
                        "cpu": data.get("cpu"),
                        "memory": data.get("memory"),
                    }
                )
            return (
                jsonify(
                    {"port": port, "name": info["name"], "status": "unhealthy"}
                ),
                503,
            )
    except Exception as e:
        return (
            jsonify(
                {
                    "port": port,
                    "name": info["name"],
                    "status": "offline",
                    "error": str(e),
                }
            ),
            503,
        )


# ============================================================================
# TESTING AND CI/CD
# ============================================================================


@app.route("/api/tests/run", methods=["POST"])
@require_auth
def run_tests():
    """Run test suite and record results.

    Supports running specific test categories:
    - ui: Frontend component tests
    - api: API endpoint tests
    - db: Database tests
    - auth: Authentication tests
    - a11y: Accessibility tests
    """
    import subprocess
    import uuid

    data = request.get_json(silent=True) or {}
    categories = data.get("categories", ["all"])
    category = data.get("category", "all")  # Legacy support
    environment = data.get("environment", APP_ENV)

    # If legacy category param used, convert to categories list
    if category != "all" and categories == ["all"]:
        categories = [category]

    run_id = str(uuid.uuid4())[:8]
    category_str = (
        ",".join(categories)
        if isinstance(categories, list)
        else str(categories)
    )

    with get_db_connection() as conn:
        conn.execute(
            """
            INSERT INTO test_runs (run_id, environment, triggered_by, trigger_type, status, category)
            VALUES (?, ?, ?, ?, ?, ?)
        """,
            (
                run_id,
                environment,
                session.get("username", "api"),
                "manual",
                "running",
                category_str,
            ),
        )
        conn.commit()

    # Run tests in background
    def run_tests_async():
        try:
            # Build test command based on categories
            test_files = []
            category_map = {
                "ui": "tests/verify_dashboard.py",  # UI/frontend tests
                "api": "tests/test_api.py",
                "db": "tests/test_database.py",
                "database": "tests/test_database.py",
                "auth": "tests/test_api.py",  # Auth tests are in test_api.py
                "a11y": "tests/verify_dashboard.py",  # Accessibility in dashboard tests
                "utils": "tests/test_utils.py",
                "workflow": "tests/test_workflow.py",
                "tmux": "tests/test_tmux_claude.py",
                "e2e": "tests/test_e2e_claude_workflow.py",
            }

            if "all" in categories:
                test_files = ["tests/"]
            else:
                for cat in categories:
                    if cat in category_map:
                        test_files.append(category_map[cat])
                # Also run utils tests if api or db
                if any(c in categories for c in ["api", "db"]):
                    if "tests/test_utils.py" not in test_files:
                        test_files.append("tests/test_utils.py")

            # Build command
            cmd = (
                ["python3", "-m", "pytest"]
                + test_files
                + [
                    "-v",
                    "--tb=short",
                    f"--junit-xml=/tmp/test_results_{run_id}.xml",
                ]
            )

            env = os.environ.copy()
            env["APP_ENV"] = "test"

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300,
                env=env,
                cwd=str(BASE_DIR),
            )

            # Parse results
            passed = result.stdout.count(" PASSED")
            failed = result.stdout.count(" FAILED")
            skipped = result.stdout.count(" SKIPPED")
            errors = result.stdout.count(" ERROR")

            # Parse coverage if available
            coverage = 0
            if "TOTAL" in result.stdout and "%" in result.stdout:
                import re

                cov_match = re.search(
                    r"TOTAL\s+\d+\s+\d+\s+(\d+)%", result.stdout
                )
                if cov_match:
                    coverage = int(cov_match.group(1))

            status = "passed" if failed == 0 and errors == 0 else "failed"

            with get_db_connection() as conn:
                conn.execute(
                    """
                    UPDATE test_runs SET
                        status = ?, total_tests = ?, passed = ?, failed = ?,
                        skipped = ?, errors = ?, output = ?, coverage = ?,
                        completed_at = CURRENT_TIMESTAMP
                    WHERE run_id = ?
                """,
                    (
                        status,
                        passed + failed + skipped + errors,
                        passed,
                        failed,
                        skipped,
                        errors,
                        result.stdout + result.stderr,
                        coverage,
                        run_id,
                    ),
                )
                conn.commit()

            # Log test failures to errors table for tracking
            if failed > 0 or errors > 0:
                # Extract failure details from output
                failure_summary = (
                    f"Test run {run_id}: {failed} failed, {errors} errors"
                )
                if categories != ["all"]:
                    failure_summary = f"[{category_str}] {failure_summary}"

                # Get the relevant error output (last 2000 chars of stderr or
                # failure lines)
                error_output = (
                    result.stderr if result.stderr else result.stdout
                )
                # Extract FAILED lines for context
                failed_lines = [
                    line
                    for line in result.stdout.split("\n")
                    if "FAILED" in line or "ERROR" in line
                ]
                stack_trace = (
                    "\n".join(failed_lines[-20:])
                    if failed_lines
                    else error_output[-2000:]
                )

                log_error_to_db(
                    error_type="test_failure",
                    message=failure_summary,
                    source=f"test_run:{run_id}",
                    stack_trace=stack_trace,
                    auto_queue=False,  # Don't auto-queue test failures for tmux assignment
                )

        except Exception as e:
            # Log the exception to errors table
            log_error_to_db(
                error_type="test_failure",
                message=f"Test run {run_id} crashed: {str(e)}",
                source=f"test_run:{run_id}",
                stack_trace=str(e),
                auto_queue=False,
            )

            with get_db_connection() as conn:
                conn.execute(
                    """
                    UPDATE test_runs SET status = ?, output = ?, completed_at = CURRENT_TIMESTAMP
                    WHERE run_id = ?
                """,
                    ("error", str(e), run_id),
                )
                conn.commit()

    import threading

    thread = threading.Thread(target=run_tests_async)
    thread.start()

    return jsonify(
        {"run_id": run_id, "status": "started", "categories": categories}
    )


@app.route("/api/tests/runs", methods=["GET"])
@require_auth
def get_test_runs():
    """Get test run history."""
    limit = request.args.get("limit", 20, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(
            """
            SELECT * FROM test_runs ORDER BY started_at DESC LIMIT ?
        """,
            (limit,),
        )
        runs = [dict(row) for row in cursor.fetchall()]

    return jsonify(runs)


@app.route("/api/tests/runs/<run_id>", methods=["GET"])
@require_auth
def get_test_run(run_id):
    """Get details of a specific test run."""
    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(
            "SELECT * FROM test_runs WHERE run_id = ?", (run_id,)
        )
        run = cursor.fetchone()

        if not run:
            return jsonify({"error": "Test run not found"}), 404

        # Get individual results if available
        results_cursor = conn.execute(
            "SELECT * FROM test_results WHERE run_id = ?", (run_id,)
        )
        results = [dict(row) for row in results_cursor.fetchall()]

    return jsonify({"run": dict(run), "results": results})


@app.route("/api/deployments", methods=["GET"])
@require_auth
def get_deployments():
    """Get deployment history."""
    environment = request.args.get("environment")
    limit = request.args.get("limit", 20, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        if environment:
            cursor = conn.execute(
                """
                SELECT * FROM deployments WHERE target_environment = ?
                ORDER BY started_at DESC LIMIT ?
            """,
                (environment, limit),
            )
        else:
            cursor = conn.execute(
                """
                SELECT * FROM deployments ORDER BY started_at DESC LIMIT ?
            """,
                (limit,),
            )

        deployments = [dict(row) for row in cursor.fetchall()]

    return jsonify(deployments)


@app.route("/api/deployments", methods=["POST"])
@require_auth
def create_deployment():
    """Create a new deployment."""
    import uuid

    data = request.get_json()
    if not data:
        return jsonify({"error": "No data provided"}), 400

    target_env = data.get("environment", "dev")
    tag = data.get("tag")
    run_tests = data.get("run_tests", True)

    deployment_id = str(uuid.uuid4())[:8]

    with get_db_connection() as conn:
        # Check deployment gate requirements
        cursor = conn.execute(
            "SELECT * FROM deployment_gates WHERE environment = ?",
            (target_env,),
        )
        gate = cursor.fetchone()

        requires_tests = gate[1] if gate else (target_env != "dev")

        # Create deployment record
        conn.execute(
            """
            INSERT INTO deployments (deployment_id, tag, target_environment, status, deployed_by, tests_required)
            VALUES (?, ?, ?, ?, ?, ?)
        """,
            (
                deployment_id,
                tag,
                target_env,
                "pending",
                session.get("username"),
                1 if requires_tests else 0,
            ),
        )
        conn.commit()

    log_activity(
        "create", "deployment", deployment_id, f"Deployment to {target_env}"
    )

    return jsonify(
        {
            "deployment_id": deployment_id,
            "status": "pending",
            "requires_tests": requires_tests,
        }
    )


@app.route("/api/deployments/<deployment_id>/execute", methods=["POST"])
@require_auth
def execute_deployment(deployment_id):
    """Execute a pending deployment."""
    import subprocess

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(
            "SELECT * FROM deployments WHERE deployment_id = ?",
            (deployment_id,),
        )
        deployment = cursor.fetchone()

        if not deployment:
            return jsonify({"error": "Deployment not found"}), 404

        if deployment["status"] != "pending":
            return (
                jsonify(
                    {"error": f'Deployment already {deployment["status"]}'}
                ),
                400,
            )

        # Update to running
        conn.execute(
            "UPDATE deployments SET status = 'running' WHERE deployment_id = ?",
            (deployment_id,),
        )
        conn.commit()

    # Execute deployment in background
    def deploy_async():
        target_env = deployment["target_environment"]
        try:
            # Run deployment script
            script_path = BASE_DIR / "scripts" / "deploy_by_tag.sh"
            if script_path.exists():
                result = subprocess.run(
                    [
                        str(script_path),
                        deployment["tag"] or "HEAD",
                        target_env,
                    ],
                    capture_output=True,
                    text=True,
                    timeout=600,
                    cwd=str(BASE_DIR),
                )
                status = "completed" if result.returncode == 0 else "failed"
                error = result.stderr if result.returncode != 0 else None
            else:
                # Fallback: just restart the service
                port = {"dev": 8082, "qa": 8081, "prod": 8080}.get(
                    target_env, 8080
                )
                status = "completed"
                error = None

            with get_db_connection() as conn:
                conn.execute(
                    """
                    UPDATE deployments SET status = ?, error_message = ?, completed_at = CURRENT_TIMESTAMP
                    WHERE deployment_id = ?
                """,
                    (status, error, deployment_id),
                )
                conn.commit()

        except Exception as e:
            with get_db_connection() as conn:
                conn.execute(
                    """
                    UPDATE deployments SET status = 'failed', error_message = ?, completed_at = CURRENT_TIMESTAMP
                    WHERE deployment_id = ?
                """,
                    (str(e), deployment_id),
                )
                conn.commit()

    import threading

    thread = threading.Thread(target=deploy_async)
    thread.start()

    return jsonify({"status": "executing", "deployment_id": deployment_id})


@app.route("/api/deployments/thresholds", methods=["GET"])
@require_auth
def get_deployment_thresholds():
    """Get deployment threshold status - how many commits/features until next deploy."""
    import subprocess

    # Thresholds (configurable)
    DEV_TO_QA_COMMITS = 3
    DEV_TO_QA_FEATURES = 2
    QA_TO_PROD_COMMITS = 5
    QA_TO_PROD_FEATURES = 3

    result = {
        "dev_to_qa": {
            "commits": 0,
            "commit_threshold": DEV_TO_QA_COMMITS,
            "features": 0,
            "feature_threshold": DEV_TO_QA_FEATURES,
            "ready": False,
        },
        "qa_to_prod": {
            "commits": 0,
            "commit_threshold": QA_TO_PROD_COMMITS,
            "features": 0,
            "feature_threshold": QA_TO_PROD_FEATURES,
            "ready": False,
        },
    }

    try:
        # Check if we're in a git repo
        git_check = subprocess.run(
            ["git", "rev-parse", "--git-dir"],
            capture_output=True,
            text=True,
            cwd=str(BASE_DIR),
        )
        if git_check.returncode != 0:
            return jsonify(result)

        # Get last QA deployment state
        state_file = Path("/tmp/architect_deploy_state.json")
        last_qa_commit = None
        last_prod_commit = None

        if state_file.exists():
            import json

            with open(state_file) as f:
                state = json.load(f)
                last_qa_commit = state.get("qa_commit")
                last_prod_commit = state.get("prod_commit")

        # Count commits since last QA deploy (dev -> qa)
        if last_qa_commit:
            count_result = subprocess.run(
                ["git", "rev-list", "--count", f"{last_qa_commit}..HEAD"],
                capture_output=True,
                text=True,
                cwd=str(BASE_DIR),
            )
            if count_result.returncode == 0:
                result["dev_to_qa"]["commits"] = int(
                    count_result.stdout.strip() or 0
                )
        else:
            # No prior deploy, count from beginning
            count_result = subprocess.run(
                ["git", "rev-list", "--count", "HEAD"],
                capture_output=True,
                text=True,
                cwd=str(BASE_DIR),
            )
            if count_result.returncode == 0:
                result["dev_to_qa"]["commits"] = min(
                    int(count_result.stdout.strip() or 0), 10
                )

        # Count completed features since last deploy
        with get_db_connection() as conn:
            conn.row_factory = sqlite3.Row

            # Features completed recently (last 7 days as proxy)
            features = conn.execute(
                """
                SELECT COUNT(*) as count FROM features
                WHERE status = 'completed'
                AND updated_at > datetime('now', '-7 days')
            """
            ).fetchone()
            result["dev_to_qa"]["features"] = (
                features["count"] if features else 0
            )

            # Same for QA -> PROD
            result["qa_to_prod"]["features"] = result["dev_to_qa"]["features"]

        # Count commits for QA -> PROD
        if last_prod_commit and last_qa_commit:
            count_result = subprocess.run(
                [
                    "git",
                    "rev-list",
                    "--count",
                    f"{last_prod_commit}..{last_qa_commit}",
                ],
                capture_output=True,
                text=True,
                cwd=str(BASE_DIR),
            )
            if count_result.returncode == 0:
                result["qa_to_prod"]["commits"] = int(
                    count_result.stdout.strip() or 0
                )

        # Check if ready for deployment
        result["dev_to_qa"]["ready"] = (
            result["dev_to_qa"]["commits"] >= DEV_TO_QA_COMMITS
            or result["dev_to_qa"]["features"] >= DEV_TO_QA_FEATURES
        )
        result["qa_to_prod"]["ready"] = (
            result["qa_to_prod"]["commits"] >= QA_TO_PROD_COMMITS
            or result["qa_to_prod"]["features"] >= QA_TO_PROD_FEATURES
        )

    except Exception as e:
        app.logger.error(f"Error checking deployment thresholds: {e}")

    return jsonify(result)


@app.route("/api/deployments/auto-check", methods=["POST"])
@require_auth
def auto_deploy_check():
    """Check thresholds and trigger auto-deployment if met."""
    import subprocess

    try:
        script_path = BASE_DIR / "scripts" / "auto_deploy.sh"
        if not script_path.exists():
            return jsonify(
                {"message": "Auto-deploy script not found", "deployed": False}
            )

        result = subprocess.run(
            [str(script_path), "deploy"],
            capture_output=True,
            text=True,
            cwd=str(BASE_DIR),
            timeout=120,
        )

        if "Deployed" in result.stdout:
            # Parse which environment and tag
            lines = result.stdout.split("\n")
            for line in lines:
                if "Deployed" in line:
                    return jsonify(
                        {
                            "deployed": True,
                            "message": line,
                            "environment": (
                                "qa" if "QA" in result.stdout else "prod"
                            ),
                            "output": result.stdout,
                        }
                    )

        return jsonify(
            {
                "deployed": False,
                "message": "No deployment thresholds met",
                "output": result.stdout,
            }
        )

    except Exception as e:
        return jsonify({"error": str(e), "deployed": False}), 500


@app.route("/api/migrations/status", methods=["GET"])
@require_auth
def get_migration_status():
    """Get database migration status with schema info."""
    try:
        from migrations.alembic_manager import AlembicMigrationManager

        manager = AlembicMigrationManager(str(DB_PATH))
        status = manager.get_status()
        return jsonify(status)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/migrations/run", methods=["POST"])
@require_auth
def run_migrations():
    """Run pending database migrations."""
    try:
        from migrations.alembic_manager import AlembicMigrationManager

        data = request.get_json() or {}
        dry_run = data.get("dry_run", False)
        backup = data.get("backup", True)

        manager = AlembicMigrationManager(str(DB_PATH))
        result = manager.run_migrations(backup=backup, dry_run=dry_run)

        if not dry_run and result["applied"]:
            log_activity(
                "migration",
                "database",
                None,
                f'Applied {len(result["applied"])} migrations',
            )

        return jsonify(result)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/migrations/pending", methods=["GET"])
@require_auth
def get_pending_migrations():
    """Get list of pending migrations."""
    try:
        from migrations.alembic_manager import AlembicMigrationManager

        manager = AlembicMigrationManager(str(DB_PATH))
        pending = manager.get_pending_migrations()
        return jsonify(
            {
                "pending": [
                    {"version": v, "path": p, "type": t} for v, p, t in pending
                ],
                "count": len(pending),
            }
        )
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/migrations/history", methods=["GET"])
@require_auth
def get_migration_history():
    """Get migration execution history."""
    try:
        from migrations.alembic_manager import AlembicMigrationManager

        limit = request.args.get("limit", 50, type=int)
        manager = AlembicMigrationManager(str(DB_PATH))
        history = manager.get_migration_history(limit=limit)
        return jsonify({"history": history, "count": len(history)})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/migrations/rollback", methods=["POST"])
@require_auth
def rollback_migration():
    """Rollback the last migration (Python migrations only)."""
    try:
        from migrations.alembic_manager import AlembicMigrationManager

        data = request.get_json() or {}
        version = data.get("version")  # Optional: specific version
        dry_run = data.get("dry_run", False)

        manager = AlembicMigrationManager(str(DB_PATH))
        result = manager.rollback_migration(version=version, dry_run=dry_run)

        if result:
            if not dry_run:
                log_activity(
                    "migration_rollback",
                    "database",
                    None,
                    f"Rolled back migration {result.version}",
                )
            return jsonify(
                {
                    "success": True,
                    "version": result.version,
                    "dry_run": dry_run,
                }
            )
        return jsonify(
            {"success": False, "message": "No migrations to rollback"}
        )
    except ValueError as e:
        return jsonify({"error": str(e)}), 400
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/migrations/generate", methods=["POST"])
@require_auth
def generate_migration():
    """Generate a new migration file."""
    try:
        from migrations.alembic_manager import AlembicMigrationManager

        data = request.get_json() or {}
        name = data.get("name")
        migration_type = data.get("type", "sql")

        if not name:
            return jsonify({"error": "Migration name required"}), 400

        manager = AlembicMigrationManager(str(DB_PATH))
        filepath = manager.generate_migration(name, migration_type)

        log_activity(
            "migration_created", "database", None, f"Created migration: {name}"
        )
        return jsonify(
            {
                "success": True,
                "filepath": filepath,
                "filename": Path(filepath).name,
            }
        )
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/migrations/stamp", methods=["POST"])
@require_auth
def stamp_migration():
    """Mark a migration as applied without executing it."""
    try:
        from migrations.alembic_manager import AlembicMigrationManager

        data = request.get_json() or {}
        version = data.get("version")
        stamp_all = data.get("all", False)

        manager = AlembicMigrationManager(str(DB_PATH))

        if stamp_all:
            stamped = manager.stamp_all()
            log_activity(
                "migration_stamp",
                "database",
                None,
                f"Stamped all pending migrations ({len(stamped)})",
            )
            return jsonify({"success": True, "stamped": stamped})

        if not version:
            return jsonify({"error": "Version required"}), 400

        if manager.stamp(version):
            log_activity(
                "migration_stamp",
                "database",
                None,
                f"Stamped migration {version}",
            )
            return jsonify({"success": True, "version": version})

        return jsonify(
            {"success": False, "message": "Version already applied"}
        )
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/migrations/schema", methods=["GET"])
@require_auth
def get_database_schema():
    """Get current database schema information."""
    try:
        from migrations.alembic_manager import AlembicMigrationManager

        manager = AlembicMigrationManager(str(DB_PATH))
        schema = manager.get_schema_info()

        # Convert TableInfo objects to dicts
        schema_dict = {}
        for table_name, table_info in schema.items():
            schema_dict[table_name] = {
                "name": table_info.name,
                "columns": table_info.columns,
                "indexes": table_info.indexes,
                "foreign_keys": table_info.foreign_keys,
            }

        return jsonify(
            {"tables": schema_dict, "table_count": len(schema_dict)}
        )
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/migrations/schema/export", methods=["GET"])
@require_auth
def export_database_schema():
    """Export current database schema as SQL."""
    try:
        from migrations.alembic_manager import AlembicMigrationManager

        manager = AlembicMigrationManager(str(DB_PATH))
        schema_sql = manager.export_schema()
        return jsonify({"schema": schema_sql, "database": str(DB_PATH)})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/migrations/backups", methods=["GET"])
@require_auth
def get_migration_backups():
    """Get list of database backups."""
    try:
        from migrations.alembic_manager import AlembicMigrationManager

        manager = AlembicMigrationManager(str(DB_PATH))
        backups = manager.list_backups()
        return jsonify({"backups": backups, "count": len(backups)})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/migrations/backups/create", methods=["POST"])
@require_auth
def create_migration_backup():
    """Create a manual database backup."""
    try:
        from migrations.alembic_manager import AlembicMigrationManager

        data = request.get_json() or {}
        suffix = data.get("suffix", "manual")

        manager = AlembicMigrationManager(str(DB_PATH))
        backup_path = manager.backup_database(suffix=suffix)

        log_activity(
            "backup_created",
            "database",
            None,
            f"Created backup: {Path(backup_path).name}",
        )
        return jsonify(
            {
                "success": True,
                "backup_path": backup_path,
                "filename": Path(backup_path).name,
            }
        )
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ============================================================================
# ORCHESTRATION API - Autopilot Control Tower
# ============================================================================


def get_orchestrator_db():
    """Get orchestrator database connection."""
    return get_db_connection()


def run_migration(migration_file: str):
    """Run a single migration file."""
    migration_path = BASE_DIR / "migrations" / migration_file
    if migration_path.exists():
        with get_db_connection() as conn:
            conn.executescript(migration_path.read_text())
        logger.info(f"Applied migration: {migration_file}")
        return True
    return False


# NOTE: Migrations are now handled by migrations/manager.py (auto_migrate function)
# The hardcoded migration calls below have been disabled to prevent conflicts
# All migrations should go through the proper migration manager which
# tracks them in schema_versions

# Initialize roles tables and seed default roles
try:
    with get_db_connection() as conn:
        user_roles.init_roles_tables(conn)
        user_roles.seed_default_roles(conn)
except Exception as e:
    logger.debug(f"Roles initialization: {e}")

# Initialize project permissions tables
try:
    with get_db_connection() as conn:
        project_permissions.init_project_permissions_tables(conn)
except Exception as e:
    logger.debug(f"Project permissions initialization: {e}")

# Initialize Slack integration tables
try:
    with get_db_connection() as conn:
        slack_integration.init_slack_tables(conn)
except Exception as e:
    logger.debug(f"Slack integration initialization: {e}")

# Add working_directory column to tmux_sessions for session hierarchy
try:
    with get_db_connection() as conn:
        conn.execute(
            "ALTER TABLE tmux_sessions ADD COLUMN working_directory TEXT"
        )
except Exception as e:
    # This will fail if column already exists, which is fine
    pass


@app.route("/api/queue", methods=["GET"])
@require_auth
def get_review_queue():
    """Get review queue summary and items."""
    try:
        from orchestrator.review_queue import ReviewQueueManager

        queue = ReviewQueueManager(str(DB_PATH))
        item_type = request.args.get("type")
        app_id = request.args.get("app_id", type=int)
        return jsonify(
            {
                "summary": queue.get_queue_summary(),
                "items": queue.get_queue_items(
                    item_type=item_type, app_id=app_id
                ),
            }
        )
    except Exception as e:
        return jsonify({"summary": {"total": 0}, "items": [], "error": str(e)})


@app.route("/api/apps", methods=["GET"])
@require_auth
def list_managed_apps():
    """List all managed applications for autopilot.

    Returns apps from the apps table, and optionally includes projects
    that haven't been added as apps yet (include_projects=true).
    """
    try:
        from orchestrator.app_manager import AppManager

        manager = AppManager(str(DB_PATH))
        autopilot_enabled = request.args.get("autopilot_enabled")
        include_projects = (
            request.args.get("include_projects", "true").lower() == "true"
        )

        if autopilot_enabled is not None:
            autopilot_enabled = autopilot_enabled.lower() == "true"

        apps = manager.list_apps(autopilot_enabled=autopilot_enabled)

        # Include projects that aren't already registered as apps
        if include_projects:
            with get_db_connection() as conn:
                conn.row_factory = sqlite3.Row
                # Get all active projects (status != 'archived')
                projects = conn.execute(
                    """
                    SELECT id, name, description, source_path, status, created_at, updated_at
                    FROM projects WHERE status != 'archived'
                """
                ).fetchall()

                # Get existing app source paths to avoid duplicates
                existing_paths = {
                    a.get("source_path", "").lower()
                    for a in apps
                    if a.get("source_path")
                }
                existing_names = {a.get("name", "").lower() for a in apps}

                # Add projects as apps if not already present
                for p in projects:
                    p_dict = dict(p)
                    # Skip if already exists as an app (by path or name)
                    if (
                        p_dict.get("source_path", "").lower() in existing_paths
                        or p_dict.get("name", "").lower() in existing_names
                    ):
                        continue

                    apps.append(
                        {
                            "id": f"project-{p_dict['id']}",
                            "name": p_dict["name"],
                            "description": p_dict.get("description"),
                            "source_path": p_dict.get("source_path", ""),
                            "autopilot_enabled": False,
                            "autopilot_mode": "observe",
                            "current_phase": "idle",
                            "is_project": True,  # Flag to indicate this is from projects table
                            "project_id": p_dict["id"],
                            "created_at": p_dict.get("created_at"),
                            "updated_at": p_dict.get("updated_at"),
                        }
                    )

        return jsonify(apps)
    except Exception as e:
        return jsonify({"error": str(e), "apps": []})


@app.route("/api/apps", methods=["POST"])
@require_auth
def create_managed_app():
    """Create a new managed application."""
    try:
        from orchestrator.app_manager import AppManager

        manager = AppManager(str(DB_PATH))
        data = request.get_json()
        if not data or not data.get("name"):
            return jsonify({"error": "Name is required"}), 400
        app_id = manager.create_app(
            name=data["name"],
            source_path=data.get("source_path", ""),
            description=data.get("description"),
            goal=data.get("goal"),
            autopilot_mode=data.get("autopilot_mode", "observe"),
        )
        return jsonify({"id": app_id, "success": True})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/apps/<int:app_id>", methods=["GET"])
@require_auth
def get_managed_app_status(app_id):
    """Get detailed status for an application."""
    try:
        from orchestrator.app_manager import AppManager

        manager = AppManager(str(DB_PATH))
        status = manager.get_app_status(app_id)
        if not status:
            return jsonify({"error": "App not found"}), 404
        return jsonify(status)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/apps/<int:app_id>/start", methods=["POST"])
@require_auth
def start_app_autopilot(app_id):
    """Start autopilot for an application."""
    try:
        from orchestrator.app_manager import AppManager
        from orchestrator.run_executor import RunExecutor

        manager = AppManager(str(DB_PATH))
        executor = RunExecutor(str(DB_PATH))
        data = request.get_json(silent=True) or {}
        mode = data.get("mode", "observe")
        goal = data.get("goal")
        manager.enable_autopilot(app_id, mode=mode)
        if goal:
            manager.set_goal(app_id, goal)
        run_id = executor.create_run(app_id, trigger_type="manual", goal=goal)
        return jsonify({"success": True, "run_id": run_id})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/apps/<int:app_id>/stop", methods=["POST"])
@require_auth
def stop_app_autopilot(app_id):
    """Stop autopilot for an application."""
    try:
        from orchestrator.app_manager import AppManager

        manager = AppManager(str(DB_PATH))
        manager.disable_autopilot(app_id)
        return jsonify({"success": True})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# String-based app routes for project-X IDs (fixes #34)
@app.route("/api/apps/<app_id>/start", methods=["POST"])
@require_auth
def start_app_autopilot_str(app_id):
    """Start autopilot for an application (supports project-X IDs)."""
    try:
        # Handle project-X format
        if isinstance(app_id, str) and app_id.startswith("project-"):
            project_id = int(app_id.replace("project-", ""))
            # For projects, we create an app entry first or just return success
            with get_db_connection() as conn:
                conn.row_factory = sqlite3.Row
                project = conn.execute(
                    "SELECT * FROM projects WHERE id = ?", (project_id,)
                ).fetchone()
                if not project:
                    return jsonify({"error": "Project not found"}), 404
                # Return success - project doesn't need full autopilot
                return jsonify(
                    {
                        "success": True,
                        "message": f"Project '{
                            project['name']}' ready for development",
                        "project_id": project_id,
                    }
                )
        else:
            # Regular int app_id - delegate to existing function
            return start_app_autopilot(int(app_id))
    except ValueError:
        return jsonify({"error": "Invalid app ID format"}), 400
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/apps/<app_id>/stop", methods=["POST"])
@require_auth
def stop_app_autopilot_str(app_id):
    """Stop autopilot for an application (supports project-X IDs)."""
    try:
        if isinstance(app_id, str) and app_id.startswith("project-"):
            # Projects don't have autopilot state
            return jsonify({"success": True, "message": "Project stopped"})
        else:
            return stop_app_autopilot(int(app_id))
    except ValueError:
        return jsonify({"error": "Invalid app ID format"}), 400
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/runs", methods=["GET"])
@require_auth
def list_runs():
    """List runs, optionally filtered by app."""
    try:
        app_id = request.args.get("app_id", type=int)
        conn = get_orchestrator_db()
        query = "SELECT r.*, a.name as app_name FROM runs r JOIN apps a ON r.app_id = a.id WHERE 1=1"
        params = []
        if app_id:
            query += " AND r.app_id = ?"
            params.append(app_id)
        query += " ORDER BY r.started_at DESC LIMIT 50"
        rows = conn.execute(query, params).fetchall()
        conn.close()
        return jsonify([dict(row) for row in rows])
    except Exception as e:
        return jsonify({"error": str(e), "runs": []})


@app.route("/api/runs/<int:run_id>", methods=["GET"])
@require_auth
def get_run_details(run_id):
    """Get detailed run information with steps and artifacts."""
    try:
        from orchestrator.run_executor import RunExecutor

        executor = RunExecutor(str(DB_PATH))
        run = executor.get_run_with_details(run_id)
        if not run:
            return jsonify({"error": "Run not found"}), 404
        return jsonify(run)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/milestones/<int:milestone_id>/evidence", methods=["GET"])
@require_auth
def get_milestone_evidence(milestone_id):
    """Get evidence packet for a milestone."""
    try:
        from orchestrator.milestone_tracker import MilestoneTracker

        tracker = MilestoneTracker(str(DB_PATH))
        evidence = tracker.get_evidence_packet(milestone_id)
        if not evidence:
            return jsonify({"error": "Milestone not found"}), 404
        return jsonify(evidence)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/milestones/<int:milestone_id>/approve", methods=["POST"])
@require_auth
def approve_orchestration_milestone(milestone_id):
    """Approve a milestone."""
    try:
        from orchestrator.milestone_tracker import MilestoneTracker

        tracker = MilestoneTracker(str(DB_PATH))
        data = request.get_json(silent=True) or {}
        reviewer = session.get("username", "unknown")
        if tracker.approve_milestone(
            milestone_id, reviewer=reviewer, notes=data.get("notes")
        ):
            return jsonify({"success": True})
        return jsonify({"error": "Approval failed"}), 400
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/milestones/<int:milestone_id>/export", methods=["GET"])
@require_auth
def export_milestone_document(milestone_id):
    """Export milestone as markdown document."""
    try:
        from orchestrator.milestone_tracker import MilestoneTracker

        tracker = MilestoneTracker(str(DB_PATH))
        evidence = tracker.get_evidence_packet(milestone_id)
        if not evidence:
            return jsonify({"error": "Milestone not found"}), 404

        # Generate markdown document
        md = f"# Milestone: {evidence['name']}\n\n"
        md += f"**App:** {evidence.get('app', {}).get('name', 'Unknown')}\n"
        md += f"**Status:** {evidence['status']}\n"
        md += f"**Risk Score:** {evidence.get('risk_score', 0)}/100\n\n"

        if evidence.get("description"):
            md += f"## Description\n{evidence['description']}\n\n"

        ev = evidence.get("evidence", {})

        # What changed
        what = ev.get("what_changed", {})
        if what.get("commits") or what.get("prs") or what.get("diff_summary"):
            md += "## What Changed\n\n"
            if what.get("diff_summary"):
                md += f"```\n{what['diff_summary']}\n```\n\n"
            if what.get("commits"):
                md += "### Commits\n"
                for c in what["commits"]:
                    md += (
                        f"- `{c.get('sha', '')[:8]}` {c.get('message', '')}\n"
                    )
                md += "\n"
            if what.get("files_changed"):
                md += f"### Files Changed ({len(what['files_changed'])})\n"
                for f in what["files_changed"][:20]:
                    md += f"- {f}\n"
                md += "\n"

        # Why changed
        why = ev.get("why_changed", {})
        if why.get("plan") or why.get("decision_trail"):
            md += "## Why It Changed\n\n"
            if why.get("plan"):
                md += f"### Plan\n{why['plan']}\n\n"
            if why.get("decision_trail"):
                md += "### Decision Trail\n"
                for d in why["decision_trail"]:
                    md += f"- **{d.get('title')}**: {d.get('content',
                                                           '')[:200]}\n"
                md += "\n"

        # Proof
        proof = ev.get("proof", {})
        if proof.get("test_reports"):
            md += "## Test Results\n\n"
            for t in proof["test_reports"]:
                md += f"### {t.get('title', 'Tests')}\n"
                md += f"- Passed: {t.get('passed', 0)}\n"
                md += f"- Failed: {t.get('failed', 0)}\n"
                md += f"- Skipped: {t.get('skipped', 0)}\n\n"

        # Risk
        risk = ev.get("risk", {})
        md += "## Risk Assessment\n\n"
        md += f"**Risk Score:** {risk.get('score', 0)}/100\n\n"
        if risk.get("factors"):
            md += "**Risk Factors:**\n"
            for f in risk["factors"]:
                md += f"- {f}\n"
            md += "\n"
        if risk.get("rollback_steps"):
            md += "**Rollback Steps:**\n"
            for i, s in enumerate(risk["rollback_steps"], 1):
                md += f"{i}. {s}\n"
            md += "\n"

        md += "---\n"
        md += f"*Generated: {datetime.utcnow().isoformat()}*\n"

        # Return as markdown file or JSON
        format_type = request.args.get("format", "json")
        if format_type == "markdown":
            response = make_response(md)
            response.headers["Content-Type"] = "text/markdown"
            response.headers["Content-Disposition"] = (
                f"attachment; filename=milestone_{milestone_id}.md"
            )
            return response

        return jsonify({"markdown": md, "evidence": evidence})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ============================================================================
# SESSION ASSIGNER & SOP INTEGRATION
# ============================================================================


@app.route("/api/assigner/sessions", methods=["GET"])
@require_auth
def get_assigner_sessions():
    """Get all sessions from the session assigner state file."""
    # Session state is always in the root data dir, not env-specific
    state_file = BASE_DIR / "data" / "session_state.json"
    if not state_file.exists():
        return jsonify(
            {
                "sessions": {},
                "message": "No session state file found",
                "path": str(state_file),
            }
        )

    try:
        with open(state_file, "r") as f:
            state = json.load(f)
        return jsonify(state)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/assigner/sync", methods=["POST"])
@require_auth
def sync_assigner_to_queue():
    """Sync session assigner tasks to the dashboard task queue."""
    # Session state is always in the root data dir, not env-specific
    state_file = BASE_DIR / "data" / "session_state.json"
    if not state_file.exists():
        return jsonify({"synced": 0, "message": "No session state file found"})

    try:
        with open(state_file, "r") as f:
            state = json.load(f)

        sessions = state.get("sessions", {})
        synced = 0

        with get_db_connection() as conn:
            for session_name, session_data in sessions.items():
                if session_data.get("status") != "assigned":
                    continue

                task_desc = session_data.get("task", "")
                project = session_data.get("project", "unknown")
                env_info = session_data.get("env_info", {})

                # Check if task already exists (by session name in task_data)
                existing = conn.execute(
                    """
                    SELECT id FROM task_queue
                    WHERE task_data LIKE ? AND status IN ('pending', 'in_progress')
                """,
                    (f'%"session": "{session_name}"%',),
                ).fetchone()

                if existing:
                    continue

                # Create task in queue
                task_data = {
                    "session": session_name,
                    "project": project,
                    "env_info": env_info,
                    "description": task_desc,
                    "source": "session_assigner",
                }

                conn.execute(
                    """
                    INSERT INTO task_queue (task_type, task_data, priority, status)
                    VALUES (?, ?, ?, 'in_progress')
                """,
                    ("session_task", json.dumps(task_data), 5),
                )
                synced += 1

        return jsonify({"synced": synced, "total_sessions": len(sessions)})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/sop", methods=["GET"])
@require_auth
def get_sop():
    """Get all SOP files."""
    sop_dir = Path(__file__).parent / "scripts"
    sops = []

    # Find all SOP-related markdown files
    for pattern in ["SOP*.md", "*_sop.md", "sop_*.md"]:
        for sop_file in sop_dir.glob(pattern):
            try:
                content = sop_file.read_text()
                sops.append(
                    {
                        "name": sop_file.name,
                        "path": str(sop_file),
                        "content": content,
                        "size": len(content),
                        "modified": datetime.fromtimestamp(
                            sop_file.stat().st_mtime
                        ).isoformat(),
                    }
                )
            except Exception as e:
                sops.append({"name": sop_file.name, "error": str(e)})

    # Also check for CLAUDE.md files
    for claude_md in [Path(__file__).parent / "CLAUDE.md"]:
        if claude_md.exists():
            try:
                content = claude_md.read_text()
                sops.append(
                    {
                        "name": claude_md.name,
                        "path": str(claude_md),
                        "content": content,
                        "size": len(content),
                        "modified": datetime.fromtimestamp(
                            claude_md.stat().st_mtime
                        ).isoformat(),
                    }
                )
            except Exception as e:
                pass

    return jsonify({"sops": sops})


@app.route("/api/sop/<path:filename>", methods=["GET"])
@require_auth
def get_sop_file(filename):
    """Get a specific SOP file."""
    # Security: only allow specific directories
    allowed_dirs = [
        Path(__file__).parent / "scripts",
        Path(__file__).parent,
    ]

    for base_dir in allowed_dirs:
        sop_file = base_dir / filename
        if sop_file.exists() and sop_file.suffix == ".md":
            try:
                content = sop_file.read_text()
                return jsonify(
                    {
                        "name": filename,
                        "path": str(sop_file),
                        "content": content,
                        "modified": datetime.fromtimestamp(
                            sop_file.stat().st_mtime
                        ).isoformat(),
                    }
                )
            except Exception as e:
                return jsonify({"error": str(e)}), 500

    return jsonify({"error": "File not found"}), 404


@app.route("/api/sop/<path:filename>", methods=["PUT"])
@require_auth
def update_sop_file(filename):
    """Update a specific SOP file."""
    data = request.get_json()
    content = data.get("content")

    if content is None:
        return jsonify({"error": "content is required"}), 400

    # Security: only allow specific directories
    allowed_dirs = [
        Path(__file__).parent / "scripts",
        Path(__file__).parent,
    ]

    for base_dir in allowed_dirs:
        sop_file = base_dir / filename
        if sop_file.exists() and sop_file.suffix == ".md":
            try:
                # Backup before writing
                backup_file = sop_file.with_suffix(".md.bak")
                backup_file.write_text(sop_file.read_text())

                # Write new content
                sop_file.write_text(content)

                log_activity(
                    "update_sop", "sop", filename, f"Updated {filename}"
                )

                return jsonify(
                    {
                        "success": True,
                        "name": filename,
                        "size": len(content),
                        "backup": str(backup_file),
                    }
                )
            except Exception as e:
                return jsonify({"error": str(e)}), 500

    return jsonify({"error": "File not found or not allowed"}), 404


# ============================================================================
# SYSTEM EVENTS (24h retention)
# ============================================================================


@app.route("/api/events", methods=["GET"])
@require_auth
def get_system_events():
    """Get system events (restarts, errors, fixes) from last 24 hours."""
    hours = request.args.get("hours", 24, type=int)
    event_type = request.args.get("type")  # restart, error_fix, health_check
    app_name = request.args.get("app")  # edu_apps, kanbanflow, architect

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        # Ensure events table exists
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS system_events (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                event_type TEXT NOT NULL,
                app_name TEXT,
                service_port INTEGER,
                description TEXT,
                details TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """
        )

        # Clean up events older than 24 hours
        conn.execute(
            """
            DELETE FROM system_events
            WHERE created_at < datetime('now', '-24 hours')
        """
        )

        query = """
            SELECT * FROM system_events
            WHERE created_at > datetime('now', ? || ' hours')
        """
        params = [f"-{hours}"]

        if event_type:
            query += " AND event_type = ?"
            params.append(event_type)
        if app_name:
            query += " AND app_name = ?"
            params.append(app_name)

        query += " ORDER BY created_at DESC"

        events = conn.execute(query, params).fetchall()
        return jsonify({"events": [dict(e) for e in events]})


@app.route("/api/events", methods=["POST"])
def log_system_event():
    """Log a system event (no auth required for internal use)."""
    data = request.get_json() or {}

    event_type = data.get(
        "event_type"
    )  # restart, error_fix, health_check, deployment
    if not event_type:
        return jsonify({"error": "event_type is required"}), 400

    with get_db_connection() as conn:
        # Ensure events table exists
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS system_events (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                event_type TEXT NOT NULL,
                app_name TEXT,
                service_port INTEGER,
                description TEXT,
                details TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """
        )

        cursor = conn.execute(
            """
            INSERT INTO system_events (event_type, app_name, service_port, description, details)
            VALUES (?, ?, ?, ?, ?)
        """,
            (
                event_type,
                data.get("app_name"),
                data.get("service_port"),
                data.get("description"),
                json.dumps(data.get("details", {})),
            ),
        )

        # Also add to task queue for visibility
        task_data = {
            "event_type": event_type,
            "app_name": data.get("app_name"),
            "service_port": data.get("service_port"),
            "description": data.get("description"),
            "source": "system_event",
        }

        conn.execute(
            """
            INSERT INTO task_queue (task_type, task_data, priority, status)
            VALUES (?, ?, ?, 'completed')
        """,
            (event_type, json.dumps(task_data), 3),
        )

        return jsonify({"id": cursor.lastrowid, "success": True})


@app.route("/api/queue/filtered", methods=["GET"])
@require_auth
def get_filtered_queue():
    """Get queue items with filters for app and type."""
    app_filter = request.args.get("app", "all")
    type_filter = request.args.get("type", "all")
    status_filter = request.args.get("status", "all")
    hours = request.args.get("hours", 24, type=int)

    with get_db_connection() as conn:
        conn.row_factory = sqlite3.Row

        query = """
            SELECT * FROM task_queue
            WHERE created_at > datetime('now', ? || ' hours')
        """
        params = [f"-{hours}"]

        if status_filter != "all":
            query += " AND status = ?"
            params.append(status_filter)

        if type_filter != "all":
            query += " AND task_type = ?"
            params.append(type_filter)

        if app_filter != "all":
            query += " AND task_data LIKE ?"
            params.append(f'%"{app_filter}"%')

        query += " ORDER BY created_at DESC"

        tasks = conn.execute(query, params).fetchall()
        return jsonify({"tasks": [dict(t) for t in tasks]})


# ============================================================================
# CUSTOM REPORTS API
# ============================================================================


@app.route("/api/reports/custom", methods=["GET"])
@require_auth
@api_error_handler
def get_custom_reports():
    """Get all custom reports for the current user."""
    user_id = session.get("user_id")
    include_shared = (
        request.args.get("include_shared", "true").lower() == "true"
    )

    with get_db_connection() as conn:
        reports = custom_reports.get_reports(conn, user_id, include_shared)
        return jsonify({"reports": reports, "count": len(reports)})


@app.route("/api/reports/custom/sources", methods=["GET"])
@require_auth
@api_error_handler
def get_report_data_sources():
    """Get available data sources for reports."""
    sources = custom_reports.get_data_sources()
    return jsonify(
        {
            "sources": sources,
            "operators": custom_reports.FILTER_OPERATORS,
            "aggregations": custom_reports.AGGREGATION_FUNCTIONS,
            "time_ranges": custom_reports.TIME_RANGES,
        }
    )


@app.route("/api/reports/custom", methods=["POST"])
@require_auth
@api_error_handler
def create_custom_report():
    """Create a new custom report."""
    data = request.get_json()
    validate_required_fields(data, ["name", "data_source", "columns"])

    user_id = session.get("user_id")

    with get_db_connection() as conn:
        report_id = custom_reports.create_report(
            conn,
            name=sanitize_string(data["name"]),
            data_source=data["data_source"],
            columns=data["columns"],
            owner_id=user_id,
            description=sanitize_string(data.get("description", "")),
            filters=data.get("filters", []),
            group_by=data.get("group_by", []),
            order_by=data.get("order_by", []),
            limit=data.get("limit"),
            is_public=data.get("is_public", False),
            config=data.get("config", {}),
        )
        conn.commit()

        log_activity(
            conn,
            user_id,
            "create_report",
            "report",
            report_id,
            {"name": data["name"], "data_source": data["data_source"]},
        )

        report = custom_reports.get_report(conn, report_id)
        return (
            jsonify(
                {
                    "success": True,
                    "report": report,
                    "message": "Report created successfully",
                }
            ),
            201,
        )


@app.route("/api/reports/custom/<int:report_id>", methods=["GET"])
@require_auth
@api_error_handler
def get_custom_report(report_id):
    """Get a specific custom report."""
    with get_db_connection() as conn:
        report = custom_reports.get_report(conn, report_id)
        if not report:
            raise APIError("Report not found", 404)

        return jsonify({"report": report})


@app.route("/api/reports/custom/<int:report_id>", methods=["PUT"])
@require_auth
@api_error_handler
def update_custom_report(report_id):
    """Update a custom report."""
    data = request.get_json()
    user_id = session.get("user_id")

    with get_db_connection() as conn:
        report = custom_reports.get_report(conn, report_id)
        if not report:
            raise APIError("Report not found", 404)

        # Only owner can update
        if report.get("owner_id") != user_id:
            raise APIError("Not authorized to update this report", 403)

        updates = {}
        if "name" in data:
            updates["name"] = sanitize_string(data["name"])
        if "description" in data:
            updates["description"] = sanitize_string(data["description"])
        if "data_source" in data:
            updates["data_source"] = data["data_source"]
        if "columns" in data:
            updates["columns"] = data["columns"]
        if "filters" in data:
            updates["filters"] = data["filters"]
        if "config" in data:
            updates["config"] = data["config"]
        if "is_public" in data:
            updates["is_public"] = data["is_public"]
        if "schedule" in data:
            updates["schedule"] = data["schedule"]

        success = custom_reports.update_report(conn, report_id, **updates)
        conn.commit()

        if success:
            log_activity(
                conn, user_id, "update_report", "report", report_id, updates
            )
            updated_report = custom_reports.get_report(conn, report_id)
            return jsonify(
                {
                    "success": True,
                    "report": updated_report,
                    "message": "Report updated successfully",
                }
            )
        else:
            return jsonify(
                {"success": False, "message": "No changes to apply"}
            )


@app.route("/api/reports/custom/<int:report_id>", methods=["DELETE"])
@require_auth
@api_error_handler
def delete_custom_report(report_id):
    """Delete a custom report."""
    user_id = session.get("user_id")

    with get_db_connection() as conn:
        report = custom_reports.get_report(conn, report_id)
        if not report:
            raise APIError("Report not found", 404)

        # Only owner can delete
        if report.get("owner_id") != user_id:
            raise APIError("Not authorized to delete this report", 403)

        success = custom_reports.delete_report(conn, report_id)
        conn.commit()

        if success:
            log_activity(
                conn,
                user_id,
                "delete_report",
                "report",
                report_id,
                {"name": report.get("name")},
            )
            return jsonify(
                {"success": True, "message": "Report deleted successfully"}
            )
        else:
            raise APIError("Failed to delete report", 500)


@app.route("/api/reports/custom/<int:report_id>/run", methods=["POST"])
@require_auth
@api_error_handler
def run_custom_report(report_id):
    """Run a custom report and return results."""
    data = request.get_json() or {}
    runtime_filters = data.get("filters", {})

    with get_db_connection() as conn:
        report = custom_reports.get_report(conn, report_id)
        if not report:
            raise APIError("Report not found", 404)

        # Check access
        user_id = session.get("user_id")
        if not report.get("is_public") and report.get("owner_id") != user_id:
            raise APIError("Not authorized to run this report", 403)

        result = custom_reports.run_report(conn, report_id, runtime_filters)
        conn.commit()

        if result.get("error"):
            return (
                jsonify(
                    {"success": False, "error": result["error"], "results": []}
                ),
                400,
            )

        return jsonify({"success": True, **result})


@app.route("/api/reports/custom/preview", methods=["POST"])
@require_auth
@api_error_handler
def preview_custom_report():
    """Preview a report without saving it."""
    data = request.get_json()
    validate_required_fields(data, ["data_source", "columns"])

    limit = min(data.get("limit", 10), 100)

    with get_db_connection() as conn:
        result = custom_reports.preview_report(conn, data, limit)

        if result.get("error"):
            return (
                jsonify(
                    {"success": False, "error": result["error"], "results": []}
                ),
                400,
            )

        return jsonify({"success": True, **result})


@app.route("/api/reports/custom/<int:report_id>/history", methods=["GET"])
@require_auth
@api_error_handler
def get_report_history(report_id):
    """Get run history for a report."""
    limit = request.args.get("limit", 20, type=int)

    with get_db_connection() as conn:
        report = custom_reports.get_report(conn, report_id)
        if not report:
            raise APIError("Report not found", 404)

        history = custom_reports.get_report_history(conn, report_id, limit)
        return jsonify(
            {
                "report_id": report_id,
                "report_name": report.get("name"),
                "history": history,
                "count": len(history),
            }
        )


@app.route("/api/reports/custom/<int:report_id>/duplicate", methods=["POST"])
@require_auth
@api_error_handler
def duplicate_custom_report(report_id):
    """Duplicate an existing report."""
    data = request.get_json() or {}
    new_name = sanitize_string(data.get("name", ""))
    user_id = session.get("user_id")

    with get_db_connection() as conn:
        original = custom_reports.get_report(conn, report_id)
        if not original:
            raise APIError("Report not found", 404)

        if not new_name:
            new_name = f"Copy of {original['name']}"

        new_id = custom_reports.duplicate_report(
            conn, report_id, new_name, user_id
        )
        conn.commit()

        log_activity(
            conn,
            user_id,
            "duplicate_report",
            "report",
            new_id,
            {"original_id": report_id, "name": new_name},
        )

        new_report = custom_reports.get_report(conn, new_id)
        return (
            jsonify(
                {
                    "success": True,
                    "report": new_report,
                    "message": "Report duplicated successfully",
                }
            ),
            201,
        )


@app.route("/api/reports/custom/<int:report_id>/schedule", methods=["PUT"])
@require_auth
@api_error_handler
def schedule_custom_report(report_id):
    """Schedule a report for automatic execution."""
    data = request.get_json()
    validate_required_fields(data, ["frequency"])

    user_id = session.get("user_id")

    with get_db_connection() as conn:
        report = custom_reports.get_report(conn, report_id)
        if not report:
            raise APIError("Report not found", 404)

        if report.get("owner_id") != user_id:
            raise APIError("Not authorized to schedule this report", 403)

        success = custom_reports.schedule_report(
            conn,
            report_id,
            frequency=data["frequency"],
            time_of_day=data.get("time_of_day", "08:00"),
            day_of_week=data.get("day_of_week"),
            day_of_month=data.get("day_of_month"),
            recipients=data.get("recipients", []),
            enabled=data.get("enabled", True),
        )
        conn.commit()

        if success:
            log_activity(
                conn, user_id, "schedule_report", "report", report_id, data
            )
            updated = custom_reports.get_report(conn, report_id)
            return jsonify(
                {
                    "success": True,
                    "report": updated,
                    "message": "Schedule updated successfully",
                }
            )
        else:
            return jsonify(
                {"success": False, "message": "Failed to update schedule"}
            )


@app.route("/api/reports/custom/<int:report_id>/export", methods=["GET"])
@require_auth
@api_error_handler
def export_custom_report(report_id):
    """Export report configuration for backup or sharing."""
    with get_db_connection() as conn:
        config = custom_reports.export_report_config(conn, report_id)
        if not config:
            raise APIError("Report not found", 404)

        return jsonify({"success": True, "config": config})


@app.route("/api/reports/custom/import", methods=["POST"])
@require_auth
@api_error_handler
def import_custom_report():
    """Import a report configuration."""
    data = request.get_json()
    validate_required_fields(data, ["config"])

    user_id = session.get("user_id")
    config = data["config"]

    with get_db_connection() as conn:
        report_id = custom_reports.import_report_config(conn, config, user_id)
        conn.commit()

        log_activity(
            conn,
            user_id,
            "import_report",
            "report",
            report_id,
            {"name": config.get("name")},
        )

        report = custom_reports.get_report(conn, report_id)
        return (
            jsonify(
                {
                    "success": True,
                    "report": report,
                    "message": "Report imported successfully",
                }
            ),
            201,
        )


@app.route("/api/reports/custom/templates", methods=["GET"])
@require_auth
@api_error_handler
def get_report_templates():
    """Get available report templates.

    Returns predefined report templates that can be used
    as starting points for custom reports.
    """
    templates = custom_reports.get_report_templates()
    return jsonify(
        {"success": True, "templates": templates, "count": len(templates)}
    )


@app.route("/api/reports/custom/templates/<template_id>", methods=["GET"])
@require_auth
@api_error_handler
def get_report_template(template_id):
    """Get full details of a specific template."""
    templates = custom_reports.REPORT_TEMPLATES
    if template_id not in templates:
        raise APIError("Template not found", 404)

    template = templates[template_id]
    return jsonify(
        {"success": True, "template": {"id": template_id, **template}}
    )


@app.route(
    "/api/reports/custom/templates/<template_id>/create", methods=["POST"]
)
@require_auth
@api_error_handler
def create_report_from_template(template_id):
    """Create a new report from a template.

    Request body (all optional):
        - name: Custom name for the report
        - filters: Additional filters to add
        - limit: Override result limit
        - is_public: Whether report should be public
    """
    data = request.get_json() or {}
    user_id = session.get("user_id")

    with get_db_connection() as conn:
        try:
            report_id = custom_reports.create_report_from_template(
                conn,
                template_id,
                name=sanitize_string(data.get("name", "")),
                owner_id=user_id,
                filters=data.get("filters"),
                limit=data.get("limit"),
                is_public=data.get("is_public", False),
            )
            conn.commit()

            log_activity(
                conn,
                user_id,
                "create_report_from_template",
                "report",
                report_id,
                {"template": template_id},
            )

            report = custom_reports.get_report(conn, report_id)
            return (
                jsonify(
                    {
                        "success": True,
                        "report": report,
                        "message": f"Report created from template: {template_id}",
                    }
                ),
                201,
            )

        except ValueError as e:
            raise APIError(str(e), 400)


@app.route("/api/reports/custom/<int:report_id>/csv", methods=["GET"])
@require_auth
@api_error_handler
def export_report_csv(report_id):
    """Export report results as CSV.

    Query parameters:
        - filters: JSON-encoded runtime filters (optional)
    """
    runtime_filters_str = request.args.get("filters", "{}")
    try:
        runtime_filters = json.loads(runtime_filters_str)
    except json.JSONDecodeError:
        runtime_filters = {}

    with get_db_connection() as conn:
        report = custom_reports.get_report(conn, report_id)
        if not report:
            raise APIError("Report not found", 404)

        # Check access
        user_id = session.get("user_id")
        if not report.get("is_public") and report.get("owner_id") != user_id:
            raise APIError("Not authorized to export this report", 403)

        csv_data = custom_reports.run_report_csv(
            conn, report_id, runtime_filters
        )

        response = make_response(csv_data)
        response.headers["Content-Type"] = "text/csv"
        response.headers["Content-Disposition"] = (
            f'attachment; filename="{report["name"]}.csv"'
        )
        return response


@app.route("/api/reports/custom/computed-fields", methods=["GET"])
@require_auth
@api_error_handler
def get_computed_fields():
    """Get available computed fields for reports.

    Computed fields are virtual fields calculated from existing data,
    such as age_days, resolution_time, etc.
    """
    fields = custom_reports.get_computed_fields()
    return jsonify({"success": True, "computed_fields": fields})


@app.route("/api/reports/custom/time-ranges", methods=["GET"])
@require_auth
@api_error_handler
def get_time_ranges():
    """Get available time range presets for filtering."""
    return jsonify(
        {
            "success": True,
            "time_ranges": custom_reports.TIME_RANGES,
            "descriptions": {
                "today": "Current day",
                "yesterday": "Previous day",
                "last_7_days": "Past 7 days",
                "last_30_days": "Past 30 days",
                "last_90_days": "Past 90 days",
                "this_week": "Current week (Mon-Sun)",
                "this_month": "Current calendar month",
                "this_quarter": "Current quarter",
                "this_year": "Current calendar year",
            },
        }
    )


@app.route("/api/reports/custom/quick-count", methods=["POST"])
@require_auth
@api_error_handler
def quick_count_report():
    """Quick count query without creating a report.

    Request body:
        - data_source: Data source to query
        - filters: Optional filters
    """
    data = request.get_json()
    validate_required_fields(data, ["data_source"])

    data_source = data["data_source"]
    filters = data.get("filters", [])

    if data_source not in custom_reports.REPORT_DATA_SOURCES:
        raise APIError(f"Invalid data source: {data_source}", 400)

    with get_db_connection() as conn:
        count = custom_reports.quick_count(conn, data_source, filters)
        return jsonify(
            {"success": True, "data_source": data_source, "count": count}
        )


@app.route("/api/reports/custom/quick-aggregate", methods=["POST"])
@require_auth
@api_error_handler
def quick_aggregate_report():
    """Quick aggregation query without creating a report.

    Request body:
        - data_source: Data source to query
        - field: Field to aggregate
        - aggregate: Aggregation function (COUNT, SUM, AVG, MIN, MAX)
        - filters: Optional filters
        - group_by: Optional grouping field
    """
    data = request.get_json()
    validate_required_fields(data, ["data_source", "field"])

    data_source = data["data_source"]
    field = data["field"]
    aggregate = data.get("aggregate", "COUNT")
    filters = data.get("filters", [])
    group_by = data.get("group_by")

    if data_source not in custom_reports.REPORT_DATA_SOURCES:
        raise APIError(f"Invalid data source: {data_source}", 400)

    if aggregate.upper() not in custom_reports.AGGREGATION_FUNCTIONS:
        raise APIError(f"Invalid aggregate function: {aggregate}", 400)

    with get_db_connection() as conn:
        results = custom_reports.quick_aggregate(
            conn, data_source, field, aggregate, filters, group_by
        )
        return jsonify(
            {
                "success": True,
                "data_source": data_source,
                "field": field,
                "aggregate": aggregate,
                "results": results,
                "count": len(results),
            }
        )


# ============================================================================
# APP SETTINGS API
# ============================================================================


@app.route("/api/settings", methods=["GET"])
@require_auth
@api_error_handler
def get_all_settings():
    """Get all application settings."""
    category = request.args.get("category")
    include_metadata = request.args.get("metadata", "false").lower() == "true"

    with get_db_connection() as conn:
        settings = app_settings.get_settings(conn, category, include_metadata)

        response = {
            "settings": settings,
            "categories": app_settings.get_categories(),
        }

        if category:
            response["category"] = category

        return jsonify(response)


@app.route("/api/settings/categories", methods=["GET"])
@require_auth
@api_error_handler
def get_setting_categories():
    """Get all setting categories."""
    return jsonify({"categories": app_settings.get_categories()})


@app.route("/api/settings/<path:key>", methods=["GET"])
@require_auth
@api_error_handler
def get_single_setting(key):
    """Get a single setting value."""
    with get_db_connection() as conn:
        value = app_settings.get_setting(conn, key)

        if value is None and key not in app_settings.DEFAULT_SETTINGS:
            raise APIError(f"Setting not found: {key}", 404)

        metadata = app_settings.get_setting_metadata(key)

        return jsonify({"key": key, "value": value, "metadata": metadata})


@app.route("/api/settings/<path:key>", methods=["PUT"])
@require_auth
@api_error_handler
def update_single_setting(key):
    """Update a single setting."""
    data = request.get_json()
    if "value" not in data:
        raise APIError("Value is required", 400)

    user_id = session.get("user_id")
    username = session.get("username", "unknown")

    with get_db_connection() as conn:
        try:
            app_settings.set_setting(conn, key, data["value"], username)
            conn.commit()

            log_activity(
                conn,
                user_id,
                "update_setting",
                "setting",
                None,
                {"key": key, "value": str(data["value"])[:100]},
            )

            new_value = app_settings.get_setting(conn, key)
            return jsonify(
                {
                    "success": True,
                    "key": key,
                    "value": new_value,
                    "message": "Setting updated successfully",
                }
            )
        except ValueError as e:
            raise APIError(str(e), 400)


@app.route("/api/settings", methods=["PUT"])
@require_auth
@api_error_handler
def update_multiple_settings():
    """Update multiple settings at once."""
    data = request.get_json()
    if not data or "settings" not in data:
        raise APIError("Settings object is required", 400)

    user_id = session.get("user_id")
    username = session.get("username", "unknown")

    with get_db_connection() as conn:
        result = app_settings.set_settings(conn, data["settings"], username)
        conn.commit()

        if result["success"]:
            log_activity(
                conn,
                user_id,
                "update_settings",
                "setting",
                None,
                {
                    "count": len(result["success"]),
                    "keys": result["success"][:10],
                },
            )

        return jsonify(
            {
                "success": len(result["errors"]) == 0,
                "updated": result["success"],
                "errors": result["errors"],
                "message": f"Updated {len(result['success'])} settings",
            }
        )


@app.route("/api/settings/<path:key>", methods=["DELETE"])
@require_auth
@api_error_handler
def reset_single_setting(key):
    """Reset a setting to its default value."""
    user_id = session.get("user_id")
    username = session.get("username", "unknown")

    with get_db_connection() as conn:
        try:
            was_reset = app_settings.reset_setting(conn, key, username)
            conn.commit()

            if was_reset:
                log_activity(
                    conn,
                    user_id,
                    "reset_setting",
                    "setting",
                    None,
                    {"key": key},
                )

            default_value = app_settings.DEFAULT_SETTINGS.get(key, {}).get(
                "value"
            )

            return jsonify(
                {
                    "success": True,
                    "key": key,
                    "value": default_value,
                    "message": "Setting reset to default",
                }
            )
        except ValueError as e:
            raise APIError(str(e), 400)


@app.route("/api/settings/category/<category>", methods=["DELETE"])
@require_auth
@api_error_handler
def reset_category_settings(category):
    """Reset all settings in a category to defaults."""
    user_id = session.get("user_id")
    username = session.get("username", "unknown")

    with get_db_connection() as conn:
        try:
            count = app_settings.reset_category(conn, category, username)
            conn.commit()

            log_activity(
                conn,
                user_id,
                "reset_category",
                "setting",
                None,
                {"category": category, "count": count},
            )

            return jsonify(
                {
                    "success": True,
                    "category": category,
                    "reset_count": count,
                    "message": f"Reset {count} settings in {category}",
                }
            )
        except ValueError as e:
            raise APIError(str(e), 400)


@app.route("/api/settings/export", methods=["GET"])
@require_auth
@api_error_handler
def export_settings():
    """Export all non-default settings."""
    with get_db_connection() as conn:
        export_data = app_settings.export_settings(conn)
        return jsonify({"success": True, "data": export_data})


@app.route("/api/settings/import", methods=["POST"])
@require_auth
@api_error_handler
def import_settings():
    """Import settings from export."""
    data = request.get_json()
    if not data or "data" not in data:
        raise APIError("Export data is required", 400)

    user_id = session.get("user_id")
    username = session.get("username", "unknown")

    with get_db_connection() as conn:
        result = app_settings.import_settings(conn, data["data"], username)
        conn.commit()

        log_activity(
            conn,
            user_id,
            "import_settings",
            "setting",
            None,
            {"count": len(result["success"])},
        )

        return jsonify(
            {
                "success": len(result["errors"]) == 0,
                "imported": result["success"],
                "errors": result["errors"],
                "message": f"Imported {len(result['success'])} settings",
            }
        )


@app.route("/api/settings/history", methods=["GET"])
@require_auth
@api_error_handler
def get_settings_history():
    """Get settings change history."""
    key = request.args.get("key")
    limit = request.args.get("limit", 50, type=int)

    with get_db_connection() as conn:
        history = app_settings.get_settings_history(conn, key, limit)
        return jsonify({"history": history, "count": len(history)})


@app.route("/api/settings/defaults", methods=["GET"])
@require_auth
@api_error_handler
def get_default_settings():
    """Get all default setting values and metadata."""
    category = request.args.get("category")

    defaults = {}
    for key, config in app_settings.DEFAULT_SETTINGS.items():
        if category and config.get("category") != category:
            continue

        defaults[key] = {
            "default_value": config["value"],
            "type": config["type"],
            "category": config.get("category", "general"),
            "description": config.get("description", ""),
            "editable": config.get("editable", True),
        }

        if "min" in config:
            defaults[key]["min"] = config["min"]
        if "max" in config:
            defaults[key]["max"] = config["max"]
        if "options" in config:
            defaults[key]["options"] = config["options"]

    return jsonify(
        {"defaults": defaults, "categories": app_settings.get_categories()}
    )


# ============================================================================
# TYPING INDICATORS API
# ============================================================================


@app.route("/api/collaboration/typing", methods=["POST"])
@require_auth
@api_error_handler
def set_typing_status():
    """Set typing status for current user."""
    data = request.get_json()
    validate_required_fields(data, ["entity_type", "entity_id"])

    user_id = str(session.get("user_id", session.get("username")))
    username = session.get("username", "Unknown")

    manager = get_typing_manager()

    action = data.get("action", "start")
    if action == "stop":
        result = manager.clear_typing(
            user_id,
            data["entity_type"],
            str(data["entity_id"]),
            data.get("field"),
        )
    else:
        result = manager.set_typing(
            user_id=user_id,
            username=username,
            entity_type=data["entity_type"],
            entity_id=str(data["entity_id"]),
            field=data.get("field"),
        )

    return jsonify({"success": True, **result})


@app.route(
    "/api/collaboration/typing/<entity_type>/<entity_id>", methods=["GET"]
)
@require_auth
@api_error_handler
def get_typing_status(entity_type, entity_id):
    """Get users currently typing in an entity."""
    user_id = str(session.get("user_id", session.get("username")))
    field = request.args.get("field")

    manager = get_typing_manager()
    users = manager.get_typing_users(
        entity_type, entity_id, field, exclude_user=user_id
    )

    return jsonify(
        {
            "entity_type": entity_type,
            "entity_id": entity_id,
            "field": field,
            "typing_users": users,
            "count": len(users),
        }
    )


@app.route(
    "/api/collaboration/activity/<entity_type>/<entity_id>", methods=["GET"]
)
@require_auth
@api_error_handler
def get_entity_collaboration_activity(entity_type, entity_id):
    """Get all collaboration activity for an entity."""
    manager = get_typing_manager()
    activity = manager.get_entity_activity(entity_type, entity_id)

    return jsonify({"success": True, **activity})


@app.route("/api/collaboration/viewing", methods=["POST"])
@require_auth
@api_error_handler
def set_viewing_status():
    """Set viewing status for current user."""
    data = request.get_json()
    validate_required_fields(data, ["entity_type", "entity_id"])

    user_id = str(session.get("user_id", session.get("username")))
    username = session.get("username", "Unknown")

    manager = get_typing_manager()

    action = data.get("action", "start")
    if action == "stop":
        result = manager.clear_viewing(
            user_id, data["entity_type"], str(data["entity_id"])
        )
    else:
        result = manager.set_viewing(
            user_id=user_id,
            username=username,
            entity_type=data["entity_type"],
            entity_id=str(data["entity_id"]),
        )

    return jsonify({"success": True, **result})


@app.route("/api/collaboration/presence", methods=["GET"])
@require_auth
@api_error_handler
def get_online_users():
    """Get all currently online users."""
    manager = get_typing_manager()
    users = manager.get_online_users()

    return jsonify({"online_users": users, "count": len(users)})


@app.route("/api/collaboration/presence/heartbeat", methods=["POST"])
@require_auth
@api_error_handler
def presence_heartbeat():
    """Send presence heartbeat."""
    user_id = str(session.get("user_id", session.get("username")))
    username = session.get("username", "Unknown")

    manager = get_typing_manager()
    manager.heartbeat(user_id, username)

    return jsonify({"success": True, "status": "online"})


@app.route("/api/collaboration/user/<user_id>/activity", methods=["GET"])
@require_auth
@api_error_handler
def get_user_collaboration_activity(user_id):
    """Get collaboration activity for a specific user."""
    manager = get_typing_manager()
    activity = manager.get_user_activity(user_id)

    return jsonify({"success": True, **activity})


@app.route("/api/collaboration/stats", methods=["GET"])
@require_auth
@api_error_handler
def get_collaboration_stats():
    """Get collaboration statistics."""
    manager = get_typing_manager()
    stats = manager.get_stats()

    return jsonify({"success": True, "stats": stats})


# ============================================================================
# DASHBOARD CACHE REFRESH API
# ============================================================================


def _register_cache_callbacks():
    """Register broadcast functions as cache refresh callbacks."""
    cache_mgr = get_cache_manager()
    cache_mgr.register_refresh_callback("stats", broadcast_stats)
    cache_mgr.register_refresh_callback("errors", broadcast_errors)
    cache_mgr.register_refresh_callback("tmux", broadcast_tmux)
    cache_mgr.register_refresh_callback("queue", broadcast_queue)
    cache_mgr.register_refresh_callback("nodes", broadcast_nodes)


@app.route("/api/dashboard/refresh", methods=["POST"])
@require_auth
@api_error_handler
def refresh_dashboard():
    """Refresh dashboard cache and trigger updates.

    Request body (optional):
        components: List of components to refresh (default: all)
        priority: Refresh by priority level ('high', 'medium', 'low')
        broadcast: Whether to broadcast updates via WebSocket (default: true)

    Returns:
        Refresh results including refreshed components
    """
    data = request.get_json() or {}
    user_id = session.get("user_id")

    cache_mgr = get_cache_manager()
    components = data.get("components")
    priority = data.get("priority")
    should_broadcast = data.get("broadcast", True)

    results = {"refreshed": [], "failed": [], "broadcast": should_broadcast}

    if priority:
        # Refresh by priority
        result = cache_mgr.refresh_by_priority(priority)
        results["refreshed"] = result.get("refreshed", [])
        results["failed"] = result.get("failed", [])
        results["priority"] = priority
    elif components:
        # Refresh specific components
        for component in components:
            result = cache_mgr.refresh(component)
            if result.get("refreshed"):
                results["refreshed"].append(component)
            else:
                results["failed"].append(
                    {
                        "component": component,
                        "reason": result.get("error") or result.get("reason"),
                    }
                )
    else:
        # Refresh all
        result = cache_mgr.refresh_all()
        results["refreshed"] = result.get("refreshed", [])
        results["failed"] = result.get("failed", [])

    # Broadcast updates if requested
    if should_broadcast:
        for component in results["refreshed"]:
            try:
                if component == "stats":
                    broadcast_stats()
                elif component == "errors":
                    broadcast_errors()
                elif component == "tmux":
                    broadcast_tmux()
                elif component == "queue":
                    broadcast_queue()
                elif component == "nodes":
                    broadcast_nodes()
            except Exception as e:
                logger.warning(f"Failed to broadcast {component}: {e}")

    results["timestamp"] = datetime.now().isoformat()
    results["success"] = len(results["failed"]) == 0

    log_activity(
        get_db_connection(),
        user_id,
        "refresh_dashboard",
        "dashboard",
        None,
        {"components": results["refreshed"]},
    )

    return jsonify(results)


@app.route("/api/dashboard/refresh/<component>", methods=["POST"])
@require_auth
@api_error_handler
def refresh_dashboard_component(component):
    """Refresh a specific dashboard component.

    Args:
        component: Component name to refresh

    Returns:
        Refresh result for the component
    """
    cache_mgr = get_cache_manager()
    components = get_component_info()

    if component not in components:
        raise APIError(
            f"Unknown component: {component}. Valid: {
                list(
                    components.keys())}",
            400,
        )

    # Check If-None-Match header for conditional refresh
    if_none_match = request.headers.get("If-None-Match")
    if if_none_match and not cache_mgr.should_refresh(
        component, if_none_match
    ):
        return "", 304  # Not Modified

    result = cache_mgr.refresh(component)

    # Broadcast if refresh was successful
    if result.get("refreshed"):
        try:
            if component == "stats":
                broadcast_stats()
            elif component == "errors":
                broadcast_errors()
            elif component == "tmux":
                broadcast_tmux()
            elif component == "queue":
                broadcast_queue()
            elif component == "nodes":
                broadcast_nodes()
        except Exception as e:
            logger.warning(f"Failed to broadcast {component}: {e}")

    response = jsonify({"success": result.get("refreshed", False), **result})

    # Add cache headers
    headers = cache_mgr.get_cache_headers(component)
    for key, value in headers.items():
        response.headers[key] = value

    return response


@app.route("/api/dashboard/cache/status", methods=["GET"])
@require_auth
@api_error_handler
def get_cache_status():
    """Get cache status for all dashboard components."""
    cache_mgr = get_cache_manager()
    status = cache_mgr.get_status()

    return jsonify({"success": True, **status})


@app.route("/api/dashboard/cache/invalidate", methods=["POST"])
@require_auth
@api_error_handler
def invalidate_cache():
    """Invalidate dashboard cache without refreshing.

    Request body (optional):
        components: List of components to invalidate (default: all)

    Returns:
        Invalidation results
    """
    data = request.get_json() or {}
    cache_mgr = get_cache_manager()
    components = data.get("components")

    if components:
        results = []
        for component in components:
            result = cache_mgr.invalidate(component)
            results.append(result)
        return jsonify(
            {
                "success": True,
                "invalidated": [
                    r["component"] for r in results if r["invalidated"]
                ],
                "timestamp": datetime.now().isoformat(),
            }
        )
    else:
        result = cache_mgr.invalidate_all()
        return jsonify(
            {
                "success": True,
                **result,
                "timestamp": datetime.now().isoformat(),
            }
        )


@app.route("/api/dashboard/components", methods=["GET"])
@require_auth
@api_error_handler
def get_dashboard_components():
    """Get information about all dashboard components."""
    components = get_component_info()
    cache_mgr = get_cache_manager()
    status = cache_mgr.get_status()

    # Merge component info with cache status
    result = {}
    for name, info in components.items():
        result[name] = {**info, **status.get("components", {}).get(name, {})}

    return jsonify(
        {
            "components": result,
            "count": len(result),
            "cache_version": status.get("version", 1),
        }
    )


# ============================================================================
# STRATEGIC DASHBOARD API ENDPOINTS
# ============================================================================


@app.route("/api/strategic/dashboard", methods=["GET"])
@require_auth
@api_error_handler
def get_strategic_dashboard():
    """Get strategic dashboard data including vision, roadmap, resources, revenue."""
    try:
        # Get orchestrator state
        orchestrator_state_file = Path(
            "/tmp/architect_persistent_orchestrator_state.json"
        )
        orchestrator_data = {}
        if orchestrator_state_file.exists():
            with open(orchestrator_state_file, "r") as f:
                orchestrator_data = json.load(f)

        # Get roadmap progress from orchestrator database
        db_path = Path("data/orchestrator.db")
        roadmap_progress = {}
        if db_path.exists():
            conn = sqlite3.connect(str(db_path))
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            cursor.execute("SELECT * FROM roadmap_progress ORDER BY phase")
            phases = [dict(row) for row in cursor.fetchall()]

            cursor.execute(
                'SELECT COUNT(*) as count FROM activity_log WHERE DATE(timestamp) = DATE("now")'
            )
            today_activity = cursor.fetchone()["count"]

            conn.close()

            roadmap_progress = {
                "phases": phases,
                "tasks_today": today_activity,
            }

        # Vision and mission
        vision = {
            "statement": "Build autonomous development capacity to create revenue streams through useful applications that serve real needs",
            "mission": "Develop and deploy educational tools (Basic EDU) and other high-value applications with minimal human intervention",
            "alignment_score": 95,
            "conflicts": 0,
            "last_review": datetime.now().isoformat(),
            "next_review": (datetime.now() + timedelta(days=7)).isoformat(),
        }

        # Resources
        resources = {
            "llms": {"count": 3, "active": ["Claude", "Gemini", "Ollama"]},
            "services": {
                "count": 17,
                "categories": [
                    "Infrastructure",
                    "APIs",
                    "Databases",
                    "Authentication",
                ],
            },
            "monthly_cost": 60,
            "breakdown": {
                "Claude API": 50,
                "Gemini API": 10,
                "Ollama": 0,
                "Infrastructure": 0,
            },
            "pending_requests": [
                {
                    "id": "google_voice_web",
                    "name": "Google Voice Web Automation",
                    "cost": "Free (browser automation)",
                    "purpose": "Automated SMS via AnythingLLM/Comet",
                    "priority": "high",
                },
                {
                    "id": "stripe_api",
                    "name": "Stripe API",
                    "cost": "Free",
                    "purpose": "Basic EDU monetization",
                    "priority": "high",
                },
            ],
        }

        # Revenue tracking
        revenue = {
            "basic_edu": {
                "name": "Basic EDU Apps",
                "priority": 1,
                "progress": 80,
                "potential": "HIGH",
                "alignment": 100,
                "next_milestone": "Payment integration (Week 3)",
                "projected_monthly": {
                    "month_3": 999,
                    "month_6": 1498,
                    "month_12": 1896,
                },
            },
            "selam_pharmacy": {
                "name": "Selam Pharmacy",
                "priority": 2,
                "progress": 40,
                "potential": "MEDIUM",
                "alignment": 60,
                "recommendation": "PAUSE - Focus on Basic EDU first",
            },
        }

        # Orchestrator status
        orchestrator_status = {
            "running": orchestrator_data.get("running", False),
            "uptime": orchestrator_data.get("uptime", "0h 0m"),
            "current_phase": orchestrator_data.get("current_phase", 1),
            "tasks_completed_today": orchestrator_data.get(
                "tasks_completed_today", 0
            ),
            "last_cycle": orchestrator_data.get(
                "last_cycle", datetime.now().isoformat()
            ),
            "idle_detection": "Active",
        }

        # Get ALL active Claude sessions
        active_sessions = []
        try:
            assigner_db = Path("data/assigner/assigner.db")
            if assigner_db.exists():
                conn = sqlite3.connect(str(assigner_db))
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()

                # Get all Claude sessions with their tasks
                cursor.execute(
                    """
                    SELECT s.name, s.status, s.provider, s.current_task_id,
                           p.content, p.priority, p.assigned_at
                    FROM sessions s
                    LEFT JOIN prompts p ON s.current_task_id = p.id
                    WHERE s.is_claude = 1
                    ORDER BY
                        CASE s.status
                            WHEN 'busy' THEN 1
                            WHEN 'waiting_input' THEN 2
                            ELSE 3
                        END,
                        s.name
                """
                )

                for row in cursor.fetchall():
                    task_info = None
                    if row["content"]:
                        task_info = {
                            "content": row["content"][:150],
                            "priority": row["priority"],
                            "started_at": row["assigned_at"],
                        }

                    active_sessions.append(
                        {
                            "name": row["name"],
                            "status": row["status"],
                            "provider": row["provider"] or "claude",
                            "task": task_info,
                        }
                    )

                conn.close()
        except Exception as e:
            logger.error(f"Error getting active sessions: {e}")

        # Get recent decisions/activity
        recent_decisions = []
        try:
            if db_path.exists():
                conn = sqlite3.connect(str(db_path))
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()

                cursor.execute(
                    """
                    SELECT timestamp, activity_type, description, phase
                    FROM activity_log
                    ORDER BY id DESC
                    LIMIT 10
                """
                )

                for row in cursor.fetchall():
                    recent_decisions.append(
                        {
                            "timestamp": row["timestamp"],
                            "type": row["activity_type"],
                            "description": row["description"],
                            "phase": row["phase"],
                        }
                    )

                conn.close()
        except Exception as e:
            logger.error(f"Error getting recent decisions: {e}")

        # Next milestone
        next_milestone = {
            "name": "Phase 1 Completion",
            "target_date": "2026-02-13",
            "progress": orchestrator_data.get("phase_progress", {}).get(
                "1", 0
            ),
            "tasks_remaining": 5,
            "blockers": [],
        }

        return jsonify(
            {
                "success": True,
                "vision": vision,
                "roadmap": roadmap_progress,
                "resources": resources,
                "revenue": revenue,
                "orchestrator": orchestrator_status,
                "active_sessions": active_sessions,
                "recent_decisions": recent_decisions,
                "next_milestone": next_milestone,
                "updated_at": datetime.now().isoformat(),
            }
        )

    except Exception as e:
        logger.error(f"Error getting strategic dashboard: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/strategic/resources/<resource_id>/approve", methods=["POST"])
@require_auth
@api_error_handler
def approve_resource(resource_id):
    """Approve a resource request."""
    try:
        # Log the approval
        logger.info(f"Resource approved: {resource_id}")

        # Here you would add logic to actually provision the resource
        # For now, just log it

        return jsonify(
            {
                "success": True,
                "message": f"Resource {resource_id} approved",
                "resource_id": resource_id,
            }
        )

    except Exception as e:
        logger.error(f"Error approving resource: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/strategic/resources/<resource_id>/deny", methods=["POST"])
@require_auth
@api_error_handler
def deny_resource(resource_id):
    """Deny a resource request."""
    try:
        reason = request.json.get("reason", "No reason provided")

        logger.info(f"Resource denied: {resource_id} - Reason: {reason}")

        return jsonify(
            {
                "success": True,
                "message": f"Resource {resource_id} denied",
                "resource_id": resource_id,
                "reason": reason,
            }
        )

    except Exception as e:
        logger.error(f"Error denying resource: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/strategic/roadmap", methods=["GET"])
@require_auth
@api_error_handler
def get_roadmap_progress():
    """Get detailed roadmap progress."""
    try:
        db_path = Path("data/orchestrator.db")
        if not db_path.exists():
            return (
                jsonify(
                    {
                        "success": False,
                        "error": "Orchestrator database not found",
                    }
                ),
                404,
            )

        conn = sqlite3.connect(str(db_path))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        cursor.execute("SELECT * FROM roadmap_progress ORDER BY phase")
        phases = [dict(row) for row in cursor.fetchall()]

        cursor.execute(
            """
            SELECT activity_type, COUNT(*) as count, MAX(timestamp) as last_activity
            FROM activity_log
            GROUP BY activity_type
        """
        )
        activities = [dict(row) for row in cursor.fetchall()]

        conn.close()

        return jsonify(
            {"success": True, "phases": phases, "activities": activities}
        )

    except Exception as e:
        logger.error(f"Error getting roadmap progress: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/strategic/orchestrator/status", methods=["GET"])
@require_auth
@api_error_handler
def get_orchestrator_status():
    """Get persistent orchestrator status."""
    try:
        state_file = Path("/tmp/architect_persistent_orchestrator_state.json")
        pid_file = Path("/tmp/architect_persistent_orchestrator.pid")

        if not state_file.exists():
            return jsonify(
                {
                    "success": True,
                    "running": False,
                    "message": "Orchestrator not running",
                }
            )

        with open(state_file, "r") as f:
            state = json.load(f)

        # Check if process is actually running
        is_running = False
        if pid_file.exists():
            try:
                pid = int(pid_file.read_text().strip())
                os.kill(pid, 0)  # Check if process exists
                is_running = True
            except (ProcessLookupError, ValueError):
                is_running = False

        return jsonify(
            {"success": True, "running": is_running, "state": state}
        )

    except Exception as e:
        logger.error(f"Error getting orchestrator status: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


# ============================================================================
# CONFIGURATION VALIDATION
# ============================================================================


class ConfigValidationError(Exception):
    """Raised when configuration validation fails."""

    pass


def validate_config(skip_db=False, skip_port=False, port=None):
    """Validate application configuration on startup."""
    import socket

    results = {"valid": True, "errors": [], "warnings": [], "checks": {}}

    def add_error(name, msg):
        results["valid"] = False
        results["errors"].append(msg)
        results["checks"][name] = {"status": "error", "message": msg}

    def add_warning(name, msg):
        results["warnings"].append(msg)
        results["checks"][name] = {"status": "warning", "message": msg}

    def add_ok(name, msg="OK"):
        results["checks"][name] = {"status": "ok", "message": msg}

    # Check DATA_DIR
    try:
        if not DATA_DIR.exists():
            DATA_DIR.mkdir(parents=True, exist_ok=True)
            add_ok("data_dir", f"Created {DATA_DIR}")
        else:
            test_file = DATA_DIR / ".write_test"
            test_file.write_text("test")
            test_file.unlink()
            add_ok("data_dir", f"{DATA_DIR} writable")
    except Exception as e:
        add_error("data_dir", f"DATA_DIR error: {e}")

    # Check database
    if not skip_db:
        try:
            with get_db_connection() as conn:
                conn.execute("SELECT 1")
                add_ok("database", f"Connected to {DB_PATH}")
        except Exception as e:
            add_error("database", f"Database error: {e}")

    # Check secret key
    if app.secret_key == "architect-dashboard-secret-key-change-in-production":
        add_warning("secret_key", "Using default SECRET_KEY")
    else:
        add_ok("secret_key", "Custom key configured")

    # Check port
    if not skip_port and port:
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(1)
            if sock.connect_ex(("127.0.0.1", port)) == 0:
                add_warning("port", f"Port {port} in use")
            else:
                add_ok("port", f"Port {port} available")
            sock.close()
        except Exception:
            pass

    # Check admin creds
    if len(CONFIG.get("ADMIN_PASSWORD", "")) < 8:
        add_warning("admin_password", "Password < 8 chars")
    else:
        add_ok("admin_password", "Password OK")

    add_ok("environment", os.environ.get("APP_ENV", "prod"))
    return results


def print_validation_results(results):
    """Print validation results."""
    print("\n[Configuration Validation]")
    print("-" * 40)
    for name, check in results["checks"].items():
        icon = (
            ""
            if check["status"] == "ok"
            else ("" if check["status"] == "warning" else "")
        )
        print(f'  {icon} {name}: {check["message"]}')
    if results["warnings"]:
        print(f'\n  Warnings: {len(results["warnings"])}')
    if results["errors"]:
        print(f'  Errors: {len(results["errors"])}')
    print("-" * 40)
    print(f'  Status: {"VALID" if results["valid"] else "INVALID"}\n')
    return results["valid"]


# ============================================================================
# INITIALIZATION
# ============================================================================

# Register cache refresh callbacks
_register_cache_callbacks()


# ============================================================================
# MAIN ENTRY POINT
# ============================================================================


# Register milestone summaries routes
register_milestone_summaries_routes(app, require_auth)


# ============================================================================
# WEB DASHBOARD ROUTES - Consolidated from web_dashboard.py
# ============================================================================


# Status API endpoint
@app.route("/api/status")
def api_status():
    """Get current system status as JSON."""
    status = {}

    if dashboard:
        try:
            status = dashboard.get_status()
        except Exception as e:
            logger.warning(f"Error getting dashboard status: {e}")

    # Add routing stats if available
    if router:
        try:
            routing_stats = router.get_stats()
            status["routing"] = routing_stats
        except Exception as e:
            logger.debug(f"Error getting routing stats: {e}")

    # Add auto-confirm stats
    if ac_monitor:
        try:
            ac_stats = ac_monitor.get_stats()
            status["auto_confirm_activity"] = ac_stats
        except Exception as e:
            logger.debug(f"Error getting auto-confirm stats: {e}")

    # Add quality stats
    if scorer:
        try:
            quality_stats = scorer.get_stats()
            status["quality"] = quality_stats
        except Exception as e:
            logger.debug(f"Error getting quality stats: {e}")

    # Add scraper stats
    if scraper:
        try:
            scraper_stats = scraper.get_stats()
            status["scraper"] = scraper_stats
        except Exception as e:
            logger.debug(f"Error getting scraper stats: {e}")

    status["timestamp"] = datetime.now().isoformat()
    return jsonify(status)


# Auto-Confirm Activity API Endpoints
@app.route("/api/auto-confirm/activity")
def api_auto_confirm_activity():
    """Get recent auto-confirm activity."""
    if not ac_monitor:
        return jsonify({"error": "Auto-confirm monitor not available"}), 503

    try:
        limit = int(request.args.get("limit", 20))
        minutes = request.args.get("minutes")
        if minutes:
            minutes = int(minutes)

        activity = ac_monitor.get_recent_activity(limit=limit, minutes=minutes)
        return jsonify(activity)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/auto-confirm/stats")
def api_auto_confirm_stats():
    """Get auto-confirm statistics."""
    if not ac_monitor:
        return jsonify({"error": "Auto-confirm monitor not available"}), 503

    try:
        stats = ac_monitor.get_stats()
        return jsonify(stats)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# Quality Scoring API Endpoints
@app.route("/api/quality/stats")
def api_quality_stats():
    """Get quality scoring statistics."""
    if not scorer:
        return jsonify({"error": "Quality scorer not available"}), 503

    try:
        stats = scorer.get_stats()
        return jsonify(stats)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/quality/comparison")
def api_quality_comparison():
    """Get source quality comparison."""
    if not scorer:
        return jsonify({"error": "Quality scorer not available"}), 503

    try:
        comparison = scorer.compare_sources()
        return jsonify(comparison)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# Scraper API Endpoints
@app.route("/api/scraper/stats")
def api_scraper_stats():
    """Get scraper statistics."""
    if not scraper:
        return jsonify({"error": "Scraper not available"}), 503

    try:
        stats = scraper.get_stats()
        return jsonify(stats)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/scraper/recent")
def api_scraper_recent():
    """Get recent scraped results."""
    if not scraper:
        return jsonify({"error": "Scraper not available"}), 503

    try:
        limit = int(request.args.get("limit", 10))
        results = scraper.get_recent_results(limit=limit)
        return jsonify(results)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# Session Monitoring API Endpoints
@app.route("/api/monitor/status")
def api_monitor_status():
    """Get foundation session monitor status."""
    if not session_monitor:
        return (
            jsonify(
                {"error": "Session monitor not available", "available": False}
            ),
            503,
        )

    try:
        status = session_monitor.get_status_summary()
        status["available"] = True
        return jsonify(status)
    except Exception as e:
        return jsonify({"error": str(e), "available": False}), 500


@app.route("/api/monitor/assign-task", methods=["POST"])
def api_monitor_assign_task():
    """Manually assign a task to the foundation session."""
    if not session_monitor:
        return jsonify({"error": "Session monitor not available"}), 503

    try:
        data = request.get_json()
        if not data or "task" not in data:
            return jsonify({"error": "Missing 'task' field"}), 400

        task = {
            "id": "manual",
            "content": data["task"],
            "priority": data.get("priority", 100),
            "category": data.get("category", "manual"),
        }

        success = session_monitor.send_task_to_session(task)

        if success:
            return jsonify(
                {
                    "success": True,
                    "task": task,
                    "message": "Task assigned successfully",
                }
            )
        else:
            return (
                jsonify(
                    {
                        "success": False,
                        "error": "Failed to send task to session",
                    }
                ),
                500,
            )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/monitor/work-log")
def api_monitor_work_log():
    """Get work log entries."""
    try:
        log_file = Path(__file__).parent / "logs" / "foundation_work.log"

        if not log_file.exists():
            return jsonify({"entries": [], "count": 0})

        limit = int(request.args.get("limit", 50))

        with open(log_file, "r") as f:
            lines = f.readlines()

        # Get last N lines
        recent_lines = lines[-limit:] if len(lines) > limit else lines

        entries = []
        for line in recent_lines:
            if line.strip():
                entries.append(line.strip())

        return jsonify(
            {
                "entries": entries,
                "count": len(entries),
                "total_lines": len(lines),
            }
        )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/monitor/check-and-assign")
def api_monitor_check_and_assign():
    """Trigger a check and assignment cycle."""
    if not session_monitor:
        return jsonify({"error": "Session monitor not available"}), 503

    try:
        session_monitor.check_and_assign_work()
        return jsonify(
            {"success": True, "message": "Check and assign cycle completed"}
        )
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# Claude Integration API Endpoints
@app.route("/api/claude/status")
def api_claude_status():
    """Check if Claude session is ready."""
    if not claude:
        return jsonify({"error": "Claude integration not available"}), 503

    try:
        is_ready, msg = claude.check_session_ready()
        return jsonify(
            {"ready": is_ready, "status": msg, "session": claude.session_name}
        )
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/claude/execute", methods=["POST"])
def api_claude_execute():
    """Execute a task on Claude."""
    if not claude:
        return jsonify({"error": "Claude integration not available"}), 503

    try:
        data = request.get_json()
        if not data or "task" not in data:
            return jsonify({"error": "Missing 'task' field"}), 400

        task = data["task"]
        timeout = data.get("timeout", 120)

        result = claude.execute_task(task, timeout=timeout)

        if result:
            return jsonify(result)
        else:
            return jsonify({"error": "Failed to execute task"}), 500

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/claude/stats")
def api_claude_stats():
    """Get Claude execution statistics."""
    if not claude:
        return jsonify({"error": "Claude integration not available"}), 503

    try:
        stats = claude.get_stats()
        return jsonify(stats)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/claude/recent")
def api_claude_recent():
    """Get recent Claude results."""
    if not claude:
        return jsonify({"error": "Claude integration not available"}), 503

    try:
        limit = int(request.args.get("limit", 10))
        results = claude.get_recent_results(limit=limit)
        return jsonify({"results": results, "count": len(results)})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# Result Comparison API Endpoints
@app.route("/api/compare/execute", methods=["POST"])
def api_compare_execute():
    """Execute and compare results from all sources."""
    if not comparator:
        return jsonify({"error": "Comparator not available"}), 503

    try:
        data = request.get_json()
        if not data or "query" not in data:
            return jsonify({"error": "Missing 'query' field"}), 400

        query = data["query"]
        timeout = data.get("timeout", 120)

        comparison = comparator.compare_all(query, timeout=timeout)
        return jsonify(comparison)

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/compare/stats")
def api_compare_stats():
    """Get comparison statistics."""
    if not comparator:
        return jsonify({"error": "Comparator not available"}), 503

    try:
        stats = comparator.get_comparison_stats()
        return jsonify(stats)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/compare/recent")
def api_compare_recent():
    """Get recent comparisons."""
    if not comparator:
        return jsonify({"error": "Comparator not available"}), 503

    try:
        limit = int(request.args.get("limit", 10))
        comparisons = comparator.get_recent_comparisons(limit=limit)
        return jsonify({"comparisons": comparisons, "count": len(comparisons)})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# Comet Integration API Endpoints
@app.route("/api/comet/status")
def api_comet_status():
    """Check if Comet browser is running."""
    if not comet:
        return jsonify({"error": "Comet integration not available"}), 503

    try:
        running = comet.check_browser_running()
        return jsonify({"running": running, "browser": comet.browser_name})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/comet/launch", methods=["POST"])
def api_comet_launch():
    """Launch Comet browser."""
    if not comet:
        return jsonify({"error": "Comet integration not available"}), 503

    try:
        success = comet.launch_browser()
        return jsonify(
            {"success": success, "running": comet.check_browser_running()}
        )
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/comet/execute", methods=["POST"])
def api_comet_execute():
    """Execute a query on Comet."""
    if not comet:
        return jsonify({"error": "Comet integration not available"}), 503

    try:
        data = request.get_json()
        if not data or "query" not in data:
            return jsonify({"error": "Missing 'query' field"}), 400

        query = data["query"]
        timeout = data.get("timeout", 60)

        result = comet.execute_task(query, timeout=timeout)

        if result:
            return jsonify(result)
        else:
            return jsonify({"error": "Failed to execute query"}), 500

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/comet/stats")
def api_comet_stats():
    """Get Comet execution statistics."""
    if not comet:
        return jsonify({"error": "Comet integration not available"}), 503

    try:
        stats = comet.get_stats()
        return jsonify(stats)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/comet/recent")
def api_comet_recent():
    """Get recent Comet results."""
    if not comet:
        return jsonify({"error": "Comet integration not available"}), 503

    try:
        limit = int(request.args.get("limit", 10))
        results = comet.get_recent_results(limit=limit)
        return jsonify({"results": results, "count": len(results)})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ========== CONSOLIDATED MONITORING ROUTES (Port 8081  8080) ==========


@app.route("/monitor")
@require_auth
def monitor_dashboard():
    """System monitoring dashboard (consolidated from web_dashboard.py port 8081)."""
    return render_template("monitor_dashboard.html")


@app.route("/analytics")
@require_auth
def analytics_dashboard():
    """Quality and analytics dashboard (consolidated from web_dashboard.py port 8081)."""
    return render_template("analytics_dashboard.html")


# ============================================================================
# Phase 5: Multi-Environment Status Reporting API
# ============================================================================


@app.route("/api/multi-env/status", methods=["GET"])
@require_auth
def multi_env_status():
    """
    GET /api/multi-env/status - Complete multi-environment system status.

    Returns:
        JSON with:
        - directories: List of dev directories with git status
        - environments: List of sub-environments (dev/qa/staging)
        - pr_groups: PR agent group status and assignments
        - summary: System-wide metrics
        - health: Overall system health
    """
    try:
        from services.multi_env_status import MultiEnvStatusManager

        manager = MultiEnvStatusManager()
        status = manager.get_system_summary()

        return jsonify({
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "status": status
        }), 200

    except Exception as e:
        logger.error(f"Error fetching multi-env status: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/multi-env/directories", methods=["GET"])
@require_auth
def multi_env_directories():
    """
    GET /api/multi-env/directories - List all development directories with git status.

    Returns:
        JSON array with:
        - name: Directory name (dev1-dev5)
        - path: Full path to directory
        - git_branch: Current git branch
        - branch_ahead: Commits ahead of main
        - branch_behind: Commits behind main
        - is_dirty: Whether working directory has changes
        - last_sync: Last time git status was checked
    """
    try:
        from services.multi_env_status import MultiEnvStatusManager

        manager = MultiEnvStatusManager()
        summary = manager.get_system_summary()
        directories = summary.get("directories", {})

        return jsonify({
            "success": True,
            "count": len(directories),
            "directories": list(directories.values())
        }), 200

    except Exception as e:
        logger.error(f"Error fetching directories: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/multi-env/environments", methods=["GET"])
@require_auth
def multi_env_environments():
    """
    GET /api/multi-env/environments - List all sub-environments (dev/qa/staging).

    Returns:
        JSON array with:
        - directory: Parent directory (dev1-dev5)
        - env_type: Type (dev, qa, or staging)
        - status: Running or stopped
        - port: Port number
        - pid: Process ID (if running)
        - url: Access URL (if running)
        - last_activity: Last update timestamp
    """
    try:
        from services.multi_env_status import MultiEnvStatusManager

        manager = MultiEnvStatusManager()
        summary = manager.get_system_summary()
        environments = summary.get("environments", {})

        return jsonify({
            "success": True,
            "count": len(environments),
            "environments": list(environments.values())
        }), 200

    except Exception as e:
        logger.error(f"Error fetching environments: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/multi-env/pr-groups", methods=["GET"])
@require_auth
def multi_env_pr_groups():
    """
    GET /api/multi-env/pr-groups - Get PR agent group status and assignments.

    Returns:
        JSON with PR groups:
        - pr_review_group (PRR): 3 review sessions
        - pr_implementation_group (PRI): 4 implementation sessions
        - pr_integration_group (PRIG): 3 integration sessions

        Each session includes:
        - name: Session name
        - provider: Provider type (claude, codex, ollama)
        - status: Idle or busy
        - current_pr: Currently assigned PR (if any)
        - assignment_strategy: How tasks are assigned
    """
    try:
        from services.multi_env_status import MultiEnvStatusManager

        manager = MultiEnvStatusManager()
        summary = manager.get_system_summary()
        pr_groups = summary.get("pr_groups", {})

        return jsonify({
            "success": True,
            "groups": pr_groups,
            "total_sessions": sum(len(g.get("sessions", [])) for g in pr_groups.values())
        }), 200

    except Exception as e:
        logger.error(f"Error fetching PR groups: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/multi-env/health", methods=["GET"])
@require_auth
def multi_env_health():
    """
    GET /api/multi-env/health - System health check for multi-environment.

    Returns:
        JSON with health status:
        - status: healthy, degraded, or critical
        - directories_running: Count of running directories
        - environments_running: Count of running sub-environments
        - sessions_available: Total available sessions
        - sessions_busy: Count of busy sessions
        - cost_estimate: Monthly cost estimate
        - recommendations: List of issues or recommendations
    """
    try:
        from services.multi_env_status import MultiEnvStatusManager

        manager = MultiEnvStatusManager()
        summary = manager.get_system_summary()

        # Analyze health
        dirs_total = len(summary.get("directories", {}))
        envs_running = sum(1 for e in summary.get("environments", {}).values()
                          if e.get("status") == "running")
        envs_total = len(summary.get("environments", {}))

        sessions_available = summary.get("summary", {}).get("sessions_idle", 0)
        sessions_busy = summary.get("summary", {}).get("sessions_busy", 0)

        recommendations = []
        if envs_running < dirs_total:
            recommendations.append(
                f"Only {envs_running}/{envs_total} environments running"
            )
        if sessions_busy > 5:
            recommendations.append(f"{sessions_busy} sessions are busy")

        # Determine status
        if dirs_total == 0:
            health_status = "critical"
        elif envs_running < dirs_total:
            health_status = "degraded"
        else:
            health_status = "healthy"

        return jsonify({
            "success": True,
            "status": health_status,
            "directories": {
                "total": dirs_total,
                "running": dirs_total  # All running in this setup
            },
            "environments": {
                "total": envs_total,
                "running": envs_running
            },
            "sessions": {
                "available": sessions_available,
                "busy": sessions_busy,
                "total": sessions_available + sessions_busy
            },
            "cost_estimate": {
                "monthly": 376,
                "baseline": 1200,
                "savings_percent": 67
            },
            "recommendations": recommendations,
            "timestamp": datetime.now().isoformat()
        }), 200

    except Exception as e:
        logger.error(f"Error checking multi-env health: {e}")
        return jsonify({"error": str(e)}), 500


# ========== TICKTICK ENDPOINTS ==========


@app.route("/api/ticktick/focus", methods=["GET"])
def get_ticktick_focus():
    """Get Focus list tasks from TickTick cache"""
    try:
        from services.ticktick_query import TickTickCache

        cache = TickTickCache()
        tasks = cache.get_focus_tasks()
        return jsonify({"success": True, "tasks": tasks, "count": len(tasks)}), 200
    except Exception as e:
        logger.error(f"Error getting focus tasks: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/ticktick/stats", methods=["GET"])
def get_ticktick_stats():
    """Get global TickTick statistics"""
    try:
        from services.ticktick_query import TickTickCache

        cache = TickTickCache()
        stats = cache.get_global_stats()
        return jsonify({"success": True, "stats": stats}), 200
    except Exception as e:
        logger.error(f"Error getting stats: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/ticktick/projects", methods=["GET"])
def get_ticktick_projects():
    """Get all TickTick projects with task counts"""
    try:
        from services.ticktick_query import TickTickCache

        cache = TickTickCache()
        projects = cache.get_project_list()
        return jsonify({"success": True, "projects": projects, "count": len(projects)}), 200
    except Exception as e:
        logger.error(f"Error getting projects: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/ticktick/search", methods=["GET"])
def search_ticktick_tasks():
    """Search TickTick tasks by keyword"""
    try:
        query = request.args.get("q", "").strip()
        if not query:
            return jsonify({"error": "Search query required"}), 400

        from services.ticktick_query import TickTickCache

        cache = TickTickCache()
        results = cache.search_tasks(query)
        return jsonify({"success": True, "results": results, "count": len(results)}), 200
    except Exception as e:
        logger.error(f"Error searching tasks: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/ticktick/tasks", methods=["GET"])
def get_ticktick_tasks():
    """Get filtered TickTick tasks"""
    try:
        from services.ticktick_query import TickTickCache

        project_id = request.args.get("project_id")
        status = request.args.get("status", type=int)
        priority_min = request.args.get("priority_min", type=int)
        due_before = request.args.get("due_before")
        limit = request.args.get("limit", 100, type=int)

        cache = TickTickCache()
        tasks = cache.get_tasks(
            project_id=project_id,
            status=status,
            priority_min=priority_min,
            due_before=due_before,
            limit=limit,
        )
        return jsonify({"success": True, "tasks": tasks, "count": len(tasks)}), 200
    except Exception as e:
        logger.error(f"Error getting tasks: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/ticktick/overdue", methods=["GET"])
def get_ticktick_overdue():
    """Get overdue tasks from TickTick"""
    try:
        from services.ticktick_query import TickTickCache

        cache = TickTickCache()
        tasks = cache.get_overdue_tasks()
        return jsonify({"success": True, "tasks": tasks, "count": len(tasks)}), 200
    except Exception as e:
        logger.error(f"Error getting overdue tasks: {e}")
        return jsonify({"error": str(e)}), 500


def main():
    """Main entry point."""
    import argparse

    parser = argparse.ArgumentParser(
        description="Distributed Project Management Dashboard"
    )
    parser.add_argument(
        "--port",
        type=int,
        default=int(os.environ.get("PORT", CONFIG["DEFAULT_PORT"])),
        help="Port to run on",
    )
    parser.add_argument("--host", default="0.0.0.0", help="Host to bind to")
    parser.add_argument(
        "--debug", action="store_true", help="Enable debug mode"
    )
    parser.add_argument("--ssl", action="store_true", help="Enable HTTPS")
    parser.add_argument(
        "--skip-migrations",
        action="store_true",
        help="Skip auto-migrations on startup",
    )

    args = parser.parse_args()

    # Environment safety: prevent debug mode in production
    if args.debug and APP_ENV == "prod":
        print("[SAFETY] Debug mode disabled in production environment")
        args.debug = False

    # Validate environment variables
    try:
        from config import ConfigError, validate_config

        validate_config()
        print("[Config] Environment variables validated")
    except ConfigError as e:
        print(f"[Config] Error: {e}")
        sys.exit(1)
    except ImportError:
        pass  # Config module not available, continue with legacy config

    # Set secure cookie only when running with SSL
    if args.ssl:
        app.config["SESSION_COOKIE_SECURE"] = True

    # Run database migrations automatically on startup
    if not args.skip_migrations:
        try:
            from migrations.manager import auto_migrate

            migration_result = auto_migrate(str(DB_PATH))
            if migration_result["status"] == "migrated":
                print(
                    f"[Migrations] Applied {
                        migration_result['count']} migrations: {
                        migration_result['applied']}"
                )
            elif migration_result["status"] == "error":
                print(
                    f"[Migrations] Warning: {
                        migration_result.get(
                            'error', 'Unknown error')}"
                )
        except Exception as e:
            print(f"[Migrations] Could not run auto-migrations: {e}")
    else:
        print("[Migrations] Skipped (--skip-migrations flag)")

    print(
        """

       Distributed Project Management Dashboard             

  Environment: {APP_ENV.upper()}
  URL: http{'s' if args.ssl else ''}://{args.host}:{args.port}/
  Login: {CONFIG['ADMIN_USER']} / {'***' if APP_ENV == 'prod' else CONFIG['ADMIN_PASSWORD']}
  Database: {DB_PATH}
  Debug: {'ON' if args.debug else 'OFF'}

"""
    )

    ssl_context = None
    if args.ssl:
        # Generate self-signed cert if needed
        cert_file = DATA_DIR / "server.crt"
        key_file = DATA_DIR / "server.key"

        if not cert_file.exists():
            print(
                "Generating self-signed SSL certificate for Tailscale access..."
            )
            generate_ssl_certificate(str(cert_file), str(key_file))

        ssl_context = (str(cert_file), str(key_file))

        # Start HTTP redirect server on port-1 (e.g., 8084 -> 8085)
        def run_http_redirect():
            from http.server import BaseHTTPRequestHandler, HTTPServer

            https_port = args.port
            http_port = args.port - 1

            class RedirectHandler(BaseHTTPRequestHandler):
                def do_GET(self):
                    # Get the host from request, default to tailscale IP
                    host = self.headers.get("Host", "100.112.58.92").split(
                        ":"
                    )[0]
                    self.send_response(301)
                    self.send_header(
                        "Location", f"https://{host}:{https_port}{self.path}"
                    )
                    self.end_headers()

                def do_POST(self):
                    self.do_GET()

                def log_message(self, format, *args):
                    pass  # Suppress logging

            try:
                server = HTTPServer(("0.0.0.0", http_port), RedirectHandler)
                print(f"[Redirect] HTTP:{http_port} -> HTTPS:{https_port}")
                server.serve_forever()
            except Exception as e:
                print(f"[Redirect] Failed to start HTTP redirect: {e}")

        redirect_thread = threading.Thread(
            target=run_http_redirect, daemon=True
        )
        redirect_thread.start()

    # Auto-processor: automatically processes queued tasks every 15 seconds
    import json as json_module

    # Track session health for loop detection
    session_health = (
        {}
    )  # session_name -> {last_output_hash, same_count, last_check}

    # Track recent interactions to avoid duplicates
    recent_interactions = {}  # hash -> timestamp

    def analyze_claude_interactions(
        session_name, output, run_id=None, project_id=None
    ):
        """Analyze Claude output for confirmation requests, questions, errors, and new patterns."""
        import re

        interactions = []

        try:
            with get_db_connection() as conn:
                conn.row_factory = sqlite3.Row

                # Get enabled patterns ordered by priority
                patterns = conn.execute(
                    """
                    SELECT * FROM claude_patterns
                    WHERE enabled = 1
                    ORDER BY priority DESC
                """
                ).fetchall()

                # Common indicators of Claude asking something
                question_indicators = [
                    r"Would you like me to[^?]*\?",
                    r"Do you want (?:me )?to[^?]*\?",
                    r"Should I[^?]*\?",
                    r"Can I[^?]*\?",
                    r"Is it okay[^?]*\?",
                    r"Which option[^?]*\?",
                    r"What (?:would you|do you)[^?]*\?",
                    r"(?:Yes|No|Y|N)\s*[\(\/]\s*(?:Yes|No|Y|N)",  # Y/N prompts
                    r"\[Y/n\]|\[y/N\]",
                    r"Press Enter to continue",
                    r"Continue\?\s*(?:\(|$)",
                ]

                # Error indicators
                error_indicators = [
                    r"rate limit",
                    r"context.*exceeded",
                    r"permission denied",
                    r"api.*error",
                    r"failed to",
                    r"error:",
                ]

                # Check for matches
                lines = output.split("\n")
                for line in lines[-50:]:  # Check last 50 lines
                    line = line.strip()
                    if not line or len(line) < 10:
                        continue

                    # Check against known patterns first
                    matched = False
                    for pattern in patterns:
                        try:
                            if re.search(
                                pattern["pattern_regex"], line, re.IGNORECASE
                            ):
                                interaction_hash = hash(
                                    f"{session_name}:{pattern['pattern_type']}:{line[:100]}"
                                )

                                # Skip if seen recently (within 60 seconds)
                                if interaction_hash in recent_interactions:
                                    if (
                                        time.time()
                                        - recent_interactions[interaction_hash]
                                        < 60
                                    ):
                                        matched = True
                                        continue

                                recent_interactions[interaction_hash] = (
                                    time.time()
                                )

                                interactions.append(
                                    {
                                        "type": pattern["pattern_type"],
                                        "pattern": pattern["pattern_regex"],
                                        "content": line[:500],
                                        "auto_response": pattern[
                                            "auto_response"
                                        ],
                                        "action": pattern["action"],
                                        "matched_pattern_id": pattern["id"],
                                    }
                                )
                                matched = True
                                break
                        except re.error:
                            continue

                    if matched:
                        continue

                    # Check for new question patterns (not in known patterns)
                    for indicator in question_indicators:
                        if re.search(indicator, line, re.IGNORECASE):
                            interaction_hash = hash(
                                f"{session_name}:question:{line[:100]}"
                            )
                            if interaction_hash in recent_interactions:
                                if (
                                    time.time()
                                    - recent_interactions[interaction_hash]
                                    < 60
                                ):
                                    break

                            recent_interactions[interaction_hash] = time.time()
                            interactions.append(
                                {
                                    "type": "question",
                                    "pattern": indicator,
                                    "content": line[:500],
                                    "auto_response": None,
                                    "action": "escalate",
                                    "requires_review": True,
                                }
                            )
                            break

                    # Check for errors
                    for indicator in error_indicators:
                        if re.search(indicator, line, re.IGNORECASE):
                            interaction_hash = hash(
                                f"{session_name}:error:{line[:100]}"
                            )
                            if interaction_hash in recent_interactions:
                                if (
                                    time.time()
                                    - recent_interactions[interaction_hash]
                                    < 60
                                ):
                                    break

                            recent_interactions[interaction_hash] = time.time()
                            interactions.append(
                                {
                                    "type": "error",
                                    "pattern": indicator,
                                    "content": line[:500],
                                    "auto_response": None,
                                    "action": "escalate",
                                    "requires_review": True,
                                }
                            )
                            break

                # Log interactions to database
                for interaction in interactions:
                    handled = 1 if interaction.get("auto_response") else 0
                    requires_review = (
                        1 if interaction.get("requires_review") else 0
                    )

                    # Get surrounding context (3 lines before/after)
                    context = output[-1000:] if len(output) > 1000 else output

                    conn.execute(
                        """
                        INSERT INTO claude_interactions
                        (session_name, interaction_type, pattern, content, context,
                         run_id, project_id, handled, handler_action, requires_review)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                        (
                            session_name,
                            interaction["type"],
                            interaction.get("pattern"),
                            interaction["content"],
                            context,
                            run_id,
                            project_id,
                            handled,
                            interaction.get("action"),
                            requires_review,
                        ),
                    )

                # Clean up old recent_interactions entries (older than 5
                # minutes)
                cutoff = time.time() - 300
                for key in list(recent_interactions.keys()):
                    if recent_interactions[key] < cutoff:
                        del recent_interactions[key]

        except Exception as e:
            print(f"[Interactions] Error analyzing: {e}")

        return interactions

    def check_session_health(session_name):
        """Check if a tmux session is stuck in a loop or unresponsive."""
        try:
            result = subprocess.run(
                ["tmux", "capture-pane", "-t", session_name, "-p"],
                capture_output=True,
                text=True,
                timeout=5,
            )
            if result.returncode != 0:
                return {"status": "error", "message": "Session not found"}

            output = (
                result.stdout[-2000:]
                if len(result.stdout) > 2000
                else result.stdout
            )
            output_hash = hash(output)

            health = session_health.get(
                session_name,
                {"last_hash": None, "same_count": 0, "last_check": 0},
            )

            if health["last_hash"] == output_hash:
                health["same_count"] += 1
            else:
                health["same_count"] = 0
                health["last_hash"] = output_hash

            health["last_check"] = time.time()
            session_health[session_name] = health

            # Detect issues
            issues = []

            # Check if session is waiting for user input (confirmation prompts)
            waiting_patterns = [
                "Do you want to",
                " 1. Yes",
                "Esc to cancel",
                "Yes, allow all edits",
                "Yes, and don't ask again",
                "Type here to tell Claude",
                "> ",  # Input prompt
            ]
            is_waiting_for_input = any(
                pattern in output for pattern in waiting_patterns
            )

            # Only mark as stuck if NOT waiting for input
            if health["same_count"] >= 4 and not is_waiting_for_input:
                issues.append("stuck")
            elif health["same_count"] >= 4 and is_waiting_for_input:
                issues.append(
                    "waiting_input"
                )  # Different status - not stuck, just needs response

            if "error" in output.lower() and "rate limit" in output.lower():
                issues.append("rate_limited")
            if "context" in output.lower() and "exceeded" in output.lower():
                issues.append("context_exceeded")
            if output.count("thinking") > 3 or output.count("Planning") > 5:
                issues.append("possible_loop")

            # Analyze for Claude interactions (confirmations, questions, errors)
            # Get run_id if session is linked to a run
            run_id = None
            project_id = None
            try:
                with get_db_connection() as conn:
                    session_info = conn.execute(
                        """
                        SELECT t.autopilot_run_id, r.project_id
                        FROM tmux_sessions t
                        LEFT JOIN autopilot_runs r ON t.autopilot_run_id = r.id
                        WHERE t.session_name = ?
                    """,
                        (session_name,),
                    ).fetchone()
                    if session_info:
                        run_id = session_info[0]
                        project_id = session_info[1]
            except sqlite3.Error:
                pass

            interactions = analyze_claude_interactions(
                session_name, output, run_id, project_id
            )

            return {
                "status": "issues" if issues else "healthy",
                "issues": issues,
                "same_count": health["same_count"],
                "output_preview": output[-500:],
                "interactions": interactions,
            }
        except Exception as e:
            return {"status": "error", "message": str(e)}

    def task_processor():
        """Background worker to auto-process queued tasks."""
        check_counter = 0
        while True:
            try:
                time.sleep(15)  # Check every 15 seconds
                check_counter += 1

                with get_db_connection() as conn:
                    conn.row_factory = sqlite3.Row

                    # Get next pending task (claude_task or error_fix)
                    task = conn.execute(
                        """
                        SELECT * FROM task_queue
                        WHERE status = 'pending' AND task_type IN ('claude_task', 'error_fix')
                        ORDER BY priority DESC, created_at ASC
                        LIMIT 1
                    """
                    ).fetchone()

                    if task:
                        task_id = task["id"]
                        task_type = task["task_type"]
                        task_data = json_module.loads(task["task_data"])
                        entity_type = task_data.get("entity_type")
                        entity_id = task_data.get("entity_id")

                        # Determine session and message based on task type
                        if task_type == "error_fix":
                            # Find available session for error fix
                            session = task_data.get("session", "arch_env3")
                            error_type = task_data.get("error_type", "unknown")
                            error_msg = task_data.get("message", "")
                            source = task_data.get("source", "unknown")

                            message = """Fix this error automatically:

ERROR TYPE: {error_type}
SOURCE: {source}
MESSAGE: {error_msg}

Please investigate and fix this error. If you need more context, check the source file.
When fixed, update the error status via API:
curl -X POST "http://localhost:{args.port}/api/errors/{entity_id}/resolve" -H "Content-Type: application/json" -d '{{"resolution": "Fixed by autopilot"}}'
"""
                        else:
                            session = task_data.get("session", "arch_env3")
                            message = task_data.get("message", "")

                        conn.execute(
                            "UPDATE task_queue SET status = 'running', started_at = CURRENT_TIMESTAMP WHERE id = ?",
                            (task_id,),
                        )
                        conn.commit()

                        try:
                            # Send to tmux
                            subprocess.run(
                                ["tmux", "send-keys", "-t", session, message],
                                check=True,
                                timeout=10,
                            )
                            time.sleep(0.5)
                            subprocess.run(
                                ["tmux", "load-buffer", "-"],
                                input=b"\r",
                                check=True,
                                timeout=5,
                            )
                            subprocess.run(
                                ["tmux", "paste-buffer", "-t", session],
                                check=True,
                                timeout=5,
                            )

                            conn.execute(
                                "UPDATE task_queue SET status = 'completed', completed_at = CURRENT_TIMESTAMP WHERE id = ?",
                                (task_id,),
                            )

                            # Update entity status
                            if entity_type == "error":
                                conn.execute(
                                    "UPDATE errors SET status = 'in_progress' WHERE id = ?",
                                    (entity_id,),
                                )
                            elif entity_type == "feature":
                                conn.execute(
                                    "UPDATE features SET status = 'in_progress' WHERE id = ?",
                                    (entity_id,),
                                )
                            elif entity_type == "autopilot_run":
                                pass  # Run status managed separately

                            conn.commit()
                            print(
                                f"[AutoFix] Sent {task_type} task {task_id} to {session}"
                            )

                            # Broadcast updates via WebSocket
                            broadcast_queue()
                            broadcast_stats()

                        except subprocess.TimeoutExpired:
                            conn.execute(
                                "UPDATE task_queue SET status = 'failed', error_message = 'tmux timeout' WHERE id = ?",
                                (task_id,),
                            )
                            conn.commit()
                            print(f"[AutoFix] Task {task_id} timed out")
                        except Exception as e:
                            conn.execute(
                                "UPDATE task_queue SET status = 'failed', error_message = ? WHERE id = ?",
                                (str(e)[:500], task_id),
                            )
                            conn.commit()
                            print(f"[AutoFix] Task {task_id} failed: {e}")

                    # Check session health every 4 cycles (60 seconds)
                    if check_counter % 4 == 0:
                        active_runs = conn.execute(
                            """
                            SELECT r.id, r.tmux_session, r.status, p.name as project_name
                            FROM autopilot_runs r
                            JOIN projects p ON r.project_id = p.id
                            WHERE r.status IN ('planning', 'executing', 'testing', 'fixing')
                            AND r.tmux_session IS NOT NULL
                        """
                        ).fetchall()

                        for run in active_runs:
                            if run["tmux_session"]:
                                health = check_session_health(
                                    run["tmux_session"]
                                )
                                if health["status"] == "issues":
                                    issues = ", ".join(health["issues"])
                                    print(
                                        f"[HealthCheck] Session {
                                            run['tmux_session']} has issues: {issues}"
                                    )

                                    # Log as alert (only if not already exists)
                                    alert_title = (
                                        f"Session Issue: {run['tmux_session']}"
                                    )
                                    existing = conn.execute(
                                        "SELECT 1 FROM autopilot_alerts WHERE title = ? AND status = 'open' LIMIT 1",
                                        (alert_title,),
                                    ).fetchone()
                                    if not existing:
                                        conn.execute(
                                            """
                                            INSERT OR IGNORE INTO autopilot_alerts
                                            (project_id, alert_type, severity, title, description, status)
                                            SELECT r.project_id, 'session_issue', 'medium',
                                                   ?, 'Session ' || ? || ' issues: ' || ?, 'open'
                                            FROM autopilot_runs r WHERE r.id = ?
                                        """,
                                            (
                                                alert_title,
                                                run["tmux_session"],
                                                issues,
                                                run["id"],
                                            ),
                                        )
                                        conn.commit()

                                    # Auto-recover stuck sessions
                                    if "stuck" in health["issues"]:
                                        print(
                                            f"[HealthCheck] Attempting to unstick {
                                                run['tmux_session']}"
                                        )
                                        try:
                                            # Send escape and a gentle nudge
                                            subprocess.run(
                                                [
                                                    "tmux",
                                                    "send-keys",
                                                    "-t",
                                                    run["tmux_session"],
                                                    "Escape",
                                                ],
                                                timeout=5,
                                            )
                                            time.sleep(0.5)
                                            subprocess.run(
                                                [
                                                    "tmux",
                                                    "send-keys",
                                                    "-t",
                                                    run["tmux_session"],
                                                    "continue",
                                                ],
                                                timeout=5,
                                            )
                                            subprocess.run(
                                                [
                                                    "tmux",
                                                    "send-keys",
                                                    "-t",
                                                    run["tmux_session"],
                                                    "Enter",
                                                ],
                                                timeout=5,
                                            )
                                        except (
                                            subprocess.SubprocessError,
                                            OSError,
                                        ):
                                            pass

            except Exception as e:
                print(f"[AutoFix] Error in task processor: {e}")

    def session_monitor():
        """Monitor autopilot sessions for issues and auto-recover."""
        while True:
            try:
                time.sleep(60)  # Check every minute
                with get_db_connection() as conn:
                    conn.row_factory = sqlite3.Row

                    # Check for stale runs (no update in 10+ minutes while
                    # supposedly active)
                    stale_runs = conn.execute(
                        """
                        SELECT r.id, r.tmux_session, r.status,
                               (julianday('now') - julianday(COALESCE(r.started_at, r.created_at))) * 24 * 60 as minutes_active
                        FROM autopilot_runs r
                        WHERE r.status IN ('planning', 'executing', 'testing')
                        AND (julianday('now') - julianday(COALESCE(r.started_at, r.created_at))) * 24 * 60 > 30
                    """
                    ).fetchall()

                    for run in stale_runs:
                        print(
                            f"[Monitor] Run {
                                run['id']} appears stale ({
                                run['minutes_active']:.0f} min), checking..."
                        )

                        if run["tmux_session"]:
                            health = check_session_health(run["tmux_session"])

                            # Auto-approve sessions waiting for confirmation
                            if "waiting_input" in health.get("issues", []):
                                output = health.get("output_preview", "")
                                # Check for edit confirmation prompts - send
                                # "2" to allow all
                                if (
                                    "Yes, allow all edits" in output
                                    or " 1. Yes" in output
                                ):
                                    print(
                                        f"[Monitor] Auto-approving edit in {
                                            run['tmux_session']}"
                                    )
                                    try:
                                        subprocess.run(
                                            [
                                                "tmux",
                                                "send-keys",
                                                "-t",
                                                run["tmux_session"],
                                                "2",
                                                "Enter",
                                            ],
                                            timeout=5,
                                        )
                                    except (
                                        subprocess.SubprocessError,
                                        OSError,
                                    ):
                                        pass
                                # Check for pending prompt with text - send
                                # Enter (only once)
                                elif "> " in output and "Task #" in output:
                                    print(
                                        f"[Monitor] Submitting pending task in {
                                            run['tmux_session']}"
                                    )
                                    try:
                                        subprocess.run(
                                            [
                                                "tmux",
                                                "send-keys",
                                                "-t",
                                                run["tmux_session"],
                                                "Enter",
                                            ],
                                            timeout=5,
                                        )
                                    except (
                                        subprocess.SubprocessError,
                                        OSError,
                                    ):
                                        pass

                            # For stuck sessions, try to recover first - don't
                            # auto-block
                            elif health[
                                "status"
                            ] == "issues" and "stuck" in health.get(
                                "issues", []
                            ):
                                output = health.get("output_preview", "")

                                # Try additional recovery patterns
                                if (
                                    "Do you want to proceed?" in output
                                    or "" in output
                                ):
                                    print(
                                        f"[Monitor] Auto-approving prompt in {
                                            run['tmux_session']}"
                                    )
                                    try:
                                        subprocess.run(
                                            [
                                                "tmux",
                                                "send-keys",
                                                "-t",
                                                run["tmux_session"],
                                                "1",
                                                "Enter",
                                            ],
                                            timeout=5,
                                        )
                                    except Exception:
                                        pass
                                # Only send Enter if not already handled by
                                # task submission above
                                elif (
                                    " send" in output
                                    and "Task #" not in output
                                ):
                                    print(
                                        f"[Monitor] Sending Enter to {
                                            run['tmux_session']}"
                                    )
                                    try:
                                        subprocess.run(
                                            [
                                                "tmux",
                                                "send-keys",
                                                "-t",
                                                run["tmux_session"],
                                                "Enter",
                                            ],
                                            timeout=5,
                                        )
                                    except Exception:
                                        pass
                                elif (
                                    "> " in output
                                    and "Task #" not in output
                                    and "" not in output
                                ):
                                    print(
                                        f"[Monitor] Sending Enter to stuck prompt in {
                                            run['tmux_session']}"
                                    )
                                    try:
                                        subprocess.run(
                                            [
                                                "tmux",
                                                "send-keys",
                                                "-t",
                                                run["tmux_session"],
                                                "Enter",
                                            ],
                                            timeout=5,
                                        )
                                    except Exception:
                                        pass
                                else:
                                    # Log but DON'T auto-block - let user
                                    # decide
                                    print(
                                        f"[Monitor] Run {
                                            run['id']} session {
                                            run['tmux_session']} appears stuck - NOT auto-blocking"
                                    )
                                    print(
                                        f"[Monitor] Output preview: {output[:200] if output else 'empty'}"
                                    )

                # Periodic broadcast of tmux session status
                broadcast_tmux()

            except Exception as e:
                print(f"[Monitor] Error: {e}")

    processor_thread = threading.Thread(target=task_processor, daemon=True)
    processor_thread.start()
    print("[AutoFix] Background auto-processor started (every 15s)")

    monitor_thread = threading.Thread(target=session_monitor, daemon=True)
    monitor_thread.start()
    print("[Monitor] Session health monitor started (every 60s)")

    # Use socketio.run() for WebSocket support
    # Handle SSL context differently for eventlet vs threading mode
    run_kwargs = {
        "host": args.host,
        "port": args.port,
        "debug": args.debug,
    }

    # Eventlet doesn't support ssl_context parameter directly
    if socketio.async_mode == "eventlet" and ssl_context:
        print(
            "WARNING: SSL not fully supported with eventlet mode. Use a reverse proxy (nginx) for HTTPS."
        )
        # SSL would need to be handled by eventlet.wrap_ssl instead
    elif socketio.async_mode == "threading":
        run_kwargs["ssl_context"] = ssl_context
        run_kwargs["allow_unsafe_werkzeug"] = True  # Required for Werkzeug
    else:
        # For eventlet without SSL or other modes
        if ssl_context and socketio.async_mode != "eventlet":
            run_kwargs["ssl_context"] = ssl_context

    socketio.run(app, **run_kwargs)


if __name__ == "__main__":
    main()
